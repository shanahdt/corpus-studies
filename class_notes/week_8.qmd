---
title: "Week 8: Adding Features"
---
```{r, message=FALSE, warnings=FALSE}
library(spotifyr)
library(compmus)
library(tidyverse)
library(circlize)
library(DT)
```

```{r, echo=FALSE, warnings=FALSE}
Sys.setenv(SPOTIFY_CLIENT_ID = '05af946589794553974d293435950a5d')
Sys.setenv(SPOTIFY_CLIENT_SECRET = '2ad94ed8cd844667b98acd84ff65bd45')
access_token <- get_spotify_access_token()
```


# This Week

- Sign up for presentations (see the link on Canvas)
- Remaining Important Dates:
    - Today: Adding more features and discussing final project a bit.
    - Wednesday: Timbre and Form
    - Friday: **First draft due**
      - This is a *complete* (roughly 10-12 page) draft.
    - Next Monday and Tuesday: Individual meetings (first draft.)
    - Next Wednesday: Presentation Day 1
    - Monday (Week 10): No class; memorial day
    - Wednesday (Week 10): Presentation Day 2

## How shall we assess ourselves?




### Draft

### Paper

### Presentation


# Already Available Features

## Global features of interest

### Metadata we've been using

- artist_name
- album_release_date
- album_release_year
- album_release_date_precision
- available_markets
- track_name
- album_name

### Continuous Variables

- danceability
- energy
- loudness
- speechiness
- acousticness
- instrumentalness
- liveness
- valence
- tempo
- duration_ms
- key_confidence
- mode_confidence
- time_signature_confidence
- tempo_confidence
- start_of_fadeout
- end_of_fadeout
- duration

### Continuous Variables from Lyrics
- TF-IDF
- Sentiment analysis ()

### Categorical Variables

- mode
- explicit
- key
- key_name
- mode_name
- key_mode
- time_signature


# Additional Features We Might Explore

- Relationship to the broader key profile
- Transition probabilities
- Timbral markers

## Relationship to the Broader Key Profile

One way of exploring a piece is by looking at how it fits within a broader key profile. For example, if we have one key profile taken from a large collection, how does a specific piece relate to that collection in terms of pitch content?

Here, we can start by getting a key profile of a playlist.
```{r}
grab_playlist_info <- function(uri){
   get_playlist_audio_features("", uri) |>
   add_audio_analysis() 
}
playlist <- grab_playlist_info("37i9dQZF1DX1kCIzMYtzum")  

```

Then we can grab chroma and pitches with code from earlier in the quarter (provided by Burgoyne examples):

```{r}
get_pitch_list <- function(input){
   ##burgoyne's comp_mus code for gathering key profiles from chroma.
   input |>     
   mutate(segments = map2(segments, key, compmus_c_transpose)) |>
   select(segments) |>
   unnest(segments) |> 
   select(start, duration, pitches) |> 
   mutate(pitches = map(pitches, compmus_normalise, "euclidean")) |>
   compmus_gather_chroma() |>
   group_by(pitch_class) |>
   summarise(mean_value = mean(value))
}
```

Then we just need to grab each list, and provide a pitch correlation (here I've used a loop, which might not be the most efficient way to do it in R).

```{r}
pitch_list <- get_pitch_list(playlist)
playlist$pitch_cor <- NA
for(i in 1:nrow(playlist)){
    pitch <- get_pitch_list(playlist[i,])
    playlist$pitch_cor[i] <- cor(pitch$mean_value, pitch_list$mean_value)
}
```


### Exercise

1. Can you grab a collection, and then look at how each piece in that collection relates to the broader key profile?


## Transition Probabilities

We could also grab transition probabilities from note to note. Here we use previously used code to get chroma that go from one to another.

```{r}

chroma_names <- c("C", "C#|Db","D", "D#|Eb", "E", "F", "F#|Gb","G", "G#|Ab","A", "A#|Bb","B" )


x <- playlist |>  
    mutate(segments = map2(segments, key, compmus_c_transpose)) |>
    select(segments) |>
    unnest(segments) |>
    select(start, duration, pitches) |>
    unnest(cols = pitches)
x$chroma <- rep(chroma_names, nrow(x)/12)
x <- x |>
  filter(pitches == 1) |>
  mutate(chroma2 = lead(chroma))
x |> select(chroma, chroma2) |> table() |> heatmap(Rowv = NA,
        Colv = NA)
```

We might also want to run it as proportions, rather than raw counts:

```{r}
pairs <-  x |> select(chroma, chroma2) |> table()
prop.table(pairs) |> heatmap(Rowv = NA,
        Colv = NA)
```

We can convert this data to rows and columns like this, and can then move toward adding it to the dataset.
```{r}

grab_pitch_pairs <- function(input){
    x <- input |>  
    mutate(segments = map2(segments, key, compmus_c_transpose)) |>
    select(segments) |>
    unnest(segments) |>
    select(start, duration, pitches) |>
    unnest(cols = pitches)

    x$chroma <- rep(chroma_names, nrow(x)/12)
    x <- x |>
      filter(pitches == 1) |>
      mutate(chroma2 = lead(chroma))
    pair_proportion <- prop.table(pairs)
    pair_proportion <- as.matrix(pair_proportion)

    # melt the data.frame
    df <- reshape2::melt(pair_proportion, na.rm = TRUE)
    df$combined <- paste0(df$chroma,"-",df$chroma2)
    df$combined <- as.factor(df$combined)
    df <- as_tibble(df)
    y <- df |> select(value, combined)
    print(y)
}
```

This is how we'd get the transitions from each pitch:
```{r}

n <- grab_pitch_pairs(playlist) 

```

And we can pivot it to a table format with `pivot_wide`.
```{r}
n |> pivot_wider(names_from = combined, values_from = value)

```


We can put all of this together like so (using the `playlist` variable from before.)


```{r}
chroma_names <- c("C", "C#|Db","D", "D#|Eb", "E", "F", "F#|Gb","G", "G#|Ab","A", "A#|Bb","B" )


x <- playlist |>  
  mutate(segments = map2(segments, key, compmus_c_transpose)) |>
  select(segments, track.name) |>
  unnest(segments) |>
  select(track.name, start, duration, pitches) |>
  unnest(cols = pitches)


x$chroma <- rep(chroma_names, nrow(x)/12)

x <- x |>
  filter(pitches == 1) |>
  mutate(chroma2 = lead(chroma))  |>
  select(track.name, chroma, chroma2)


new_df <- x |>
  group_by(track.name) |>
  select(-track.name) |>
  table() |>
  prop.table() |>
  data.frame() |>
  tibble() |>
  mutate(bigram = paste(chroma, "to ", chroma2)) |>
  select(track.name, Freq, bigram) |>
  pivot_wider(names_from = bigram, values_from = Freq)

df <- cbind(playlist, new_df)
```

We can display this beast of a table like so. 
```{r}
df |> datatable(filter = "top")
```
We can also use the `map` tool for adding means and standard deviations of other nested information from the audio analysis.

### Adding a "Bar Confidence" Metric with Map

```{r}
playlist_w_bars <- playlist |> 
  mutate(
    distance_btwn_bars = map_dbl(playlist$bars, ~mean(.x$confidence)),
    bar_flex = map_dbl(playlist$bars, ~sd(.x$confidence)))

playlist_w_bars$distance_btwn_bars

```




```{r}
x <- playlist |>  
    mutate(segments = map2(segments, key, compmus_c_transpose)) |>
    select(segments) |>
    unnest(segments) |>
    select(start, duration, pitches) |>
    unnest(cols = pitches)
x$chroma <- rep(chroma_names, nrow(x)/12)
x <- x |>
  filter(pitches == 1) |>
  mutate(chroma2 = lead(chroma)) 
x
```


## Getting timbre

Timbre is measured in Spotify with _cepstra_. This measurement was derived in speech analysis (and is a re-arrangement of the word spectrum-singular: cepstrum). An excellent overview can be found [here](https://towardsdatascience.com/build-a-mfcc-based-music-recommendation-engine-on-cloud-3bf3cfbd594b). 

The Spotify API [writes that](https://developer.spotify.com/documentation/web-api/reference/get-audio-analysis) it is a "vector that includes 12 unbounded values roughly centered around 0. Those values are high level abstractions of the spectral surface, ordered by degree of importance."

The first dimension is an average loudness, the second is about "brightness", the third is about "flatness", and the fourth through the twelfth roughly correspond to the strength of the attack. 

![Spotify's Timbre Functions](https://developer.spotify.com/assets/audio/Timbre_basis_functions.png)

Timbre for "This is America"
```{r}

this_is_america <-
  get_tidy_audio_analysis("0b9oOr2ZgvyQu88wzixux9") |>  
  compmus_align(bars, segments) |> 
  select(bars) |>                                     
  unnest(bars) |>                                     
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "euclidean"              
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "mean", norm = "euclidean"            
      )
  )

```

Here, we can use the `compmus_gather_timbre` function from `compmus`. Here we see the distribution of _cepstra_ in "This is America".

```{r}
this_is_america |>
  compmus_gather_timbre() |> 
    ggplot(aes(y=value, x=basis)) + 
    geom_violin(position="dodge", alpha=0.5) +
    theme_bw()
```

Similar to a chromagram, we can plot the [https://en.wikipedia.org/wiki/Mel-frequency_cepstrum](cepstrograms) to demonstrate changing timbre throughout the piece.

```{r}
this_is_america |>
  compmus_gather_timbre() |>
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = basis,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  scale_fill_viridis_c() +                              
  theme_classic()
```

## Comparing Solo Instrument Pieces

Let's compare a solo trumpet [(BWV 1067, orchestral suite no.2)](https://open.spotify.com/track/6Tv19wcEeyvNBmhRGY59bY?si=8c50b269920c403f) and a flute 
```{r}
bwv1067_trumpet <-
  get_tidy_audio_analysis("6Tv19wcEeyvNBmhRGY59bY") |>  
  compmus_align(bars, segments) |> 
  select(bars) |>                                     
  unnest(bars) |>                                     
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "euclidean"              
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "mean", norm = "euclidean"            
      )
  )

bwv1067_trumpet |>
  compmus_gather_timbre() |>
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = basis,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  scale_fill_viridis_c() +                              
  theme_classic()

```

and flute ([this recording](https://open.spotify.com/track/2Ej8j8vN0hlRulT2DJKu52?si=603c04ffc1ef4623)).
```{r}
bwv1067_flute <-
  get_tidy_audio_analysis("2Ej8j8vN0hlRulT2DJKu52") |>  
  compmus_align(bars, segments) |> 
  select(bars) |>                                     
  unnest(bars) |>                                     
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "euclidean"              
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "mean", norm = "euclidean"            
      )
  )

bwv1067_flute |>
  compmus_gather_timbre() |>
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = basis,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  scale_fill_viridis_c() +                              
  theme_classic()

```


## Exercise:
How might we incorporate timbre in our own research questions?

I have a theory that tempo and brightness are related in our playlist. Let's see if they're related.
```{r}

timbre <- playlist |>  
  compmus_align(bars, segments) |> 
  select(track.name, bars) |>                                     
  unnest(bars) |>                                     
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "euclidean"              
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "mean", norm = "euclidean"            
      )
  )

timbre_coeffs <- 
  timbre |>
  compmus_gather_timbre() |> 
  select(track.name, basis, value) |> 
  group_by(basis, track.name) |> 
  mutate(mean_timbre = mean(value)) |> 
  select(track.name, mean_timbre) |> 
  unique() |> 
  pivot_wider(names_from = basis, values_from = mean_timbre)

new_playlist <- merge(timbre_coeffs, playlist)

new_playlist |> datatable(filter="top")
```


I now have a dataframe that includes timbre. So let's look at how brightness (here operationalized as `c02`), might correspond with tempo.

```{r}
cor(new_playlist$c02, new_playlist$tempo)
```

It's not a terribly strong correlation, but perhaps we should plot it anyway.

```{r}
plot(c02 ~ tempo, data=new_playlist)
abline(lm(c02 ~ tempo, data=new_playlist), col="red")
summary(lm(c02 ~ tempo, data=new_playlist))
```

So it's significant, but not terribly useful (not accounting for much variance).


## Self-Similarity Matrices and Form

We can look at musical form through the use of _self-similarity matrices_. There are some nice technical explanations of them [here](https://www.audiolabs-erlangen.de/resources/MIR/FMP/C4/C4S2_SSM-Synthetic.html) and [here](https://www.audiolabs-erlangen.de/resources/MIR/FMP/C4/C4S2_SSM.html).

Put succinctly, we want to compare each element of the sequence of musical events with one another.

Müller writes:

> The two most prominent structures in SSMs [...] 
> are referred to as blocks and paths. If the 
> feature sequence captures musical properties that stay somewhat 
> constant over the duration of an entire musical part, each of the 
> feature vectors is similar to all other feature vectors within this 
> segment. As a result, an entire block of large values appears in the 
> SSM. **In other words, homogeneity properties correspond to block-like 
> structures. If the feature sequence contains two repeating 
> subsequences (e.g., two segments corresponding to the same melody), 
> the corresponding elements of the two subsequences are similar to 
> each other.** As a result, a path (or stripe) of high similarity 
> running parallel to the main diagonal becomes visible in the SSM. In 
> other words, repetitive properties correspond to path-like 
> structures. (from [this notebook](https://www.audiolabs-erlangen.de/resources/MIR/FMP/C4/C4S2_SSM.html))


![From Müller](https://www.audiolabs-erlangen.de/resources/MIR/FMP/data/C4/FMP_C4_F07a.png)

### Brahms's "Hungarian Dance No. 5" (performed by Isaac Stern)
```{r}

brahms_stern <-
  get_tidy_audio_analysis("1PKtuxuLUbXeJNa05bfAOT")  |> 
  compmus_align(bars, segments) |>                     
  select(bars) |>                                      
  unnest(bars) |>                                      
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "rms", norm = "manhattan"              
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "rms", norm = "manhattan"              
      )
  )

brahms_stern |>
  compmus_self_similarity(timbre, "cosine") |> 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  scale_fill_viridis_c(guide = "none") +
  theme_classic() +
  labs(x = "", y = "")
```


### Brahms's "Hungarian Dance No. 5" (Abbado)

```{r}

brahms_abbado <-
  get_tidy_audio_analysis("02TadnJNMcVjr4baY39H1p")  |> 
  compmus_align(bars, segments) |>                     
  select(bars) |>                                      
  unnest(bars) |>                                      
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "rms", norm = "euclidean"              
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "rms", norm = "euclidean"              
      )
  )

brahms_abbado |>
  compmus_self_similarity(timbre, "cosine") |> 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  scale_fill_viridis_c(guide = "none") +
  theme_classic() +
  labs(x = "", y = "")
```

### Bowie's Life on Mars
Here can we see a self-similarity matrix of David Bowie's "Life on Mars". Let's listen along to it.
```{r}
life_on_mars <-
  get_tidy_audio_analysis("3ZE3wv8V3w2T2f7nOCjV0N")  |> 
  compmus_align(bars, segments) |>                     
  select(bars) |>                                      
  unnest(bars) |>                                      
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "rms", norm = "euclidean"              
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "rms", norm = "euclidean"              
      )
  )

```

```{r}
life_on_mars |>
  compmus_self_similarity(timbre, "cosine") |> 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  scale_fill_viridis_c(guide = "none") +
  theme_classic() +
  labs(x = "", y = "")
```

## Exercise:

Let's look at two performances of the same piece. How do timbres change? Are there any hypotheses that might be worth looking into?