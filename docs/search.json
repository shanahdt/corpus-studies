[
  {
    "objectID": "class_notes/week_4.html",
    "href": "class_notes/week_4.html",
    "title": "Week 4: Conceptual Debates: Key-Finding",
    "section": "",
    "text": "talk about key-finding\nplay with the different weightings\nwhat would it look like to devise your own key-finding algorithm?\n\n\n\n\n\n\nWhen we hear this ringtone, it sounds as though it’s in C, but why?\n\n\n\nNokia\n\n\n\nIt doesn’t begin with C, it begins with G.\nC isn’t the most common note–in fact, it only occurs once before the final bar, and it’s on the “and” of 2 in the third measure (a pretty weak position metrically).\nIs a key just whatever key the piece ends in? If we ended this on A, would it sound like it’s in A minor? It would be the same key signature, and we’d actually have a nice cadential ascent to the final A from the G in the third measure.\n\nSo what gives? Why do we hear this as being in C?\nPerhaps a follow-up question might simply be: what makes us hear something as being in a key?\n\n\n\n\n\nThis approach used what we might call an exclusionary approach, eliminating different key possibilities as pitch classes were introduced over the course of a musical passage.\nFor example, with the Nokia theme, the opening G would fit into seven major keys (G, C, D, F, B-flats, A-flat, E-flat;); six of those keys would include the opening two notes; and three of those six would still be possible when presented with the first three notes. By the end of the first measure, however, the only major key that would encompass all four melody notes would be C major. If more than one key was still available however, the algorithm would place more weight on the pitches present at the start of the piece. This worked quite well on pieces that were overtly tonal, but it was less effective for pieces that contained non-diatonic pitches (which is most pieces!)\n\n\n\nLonguet-Higgins and Steedman's 1971 Key-Finding Algorithm\n\n\n\n\n\nAs you might guess, the Longuet-Higgins and Steedman would miss a lot of musical instances. For example, pieces that have non-harmonic chords would struggle, as would pieces that had a lot of chromatic ornamentations. Ideally an algorithm would allow for these pitches to occur, but acknowledge that pitches in the key might be a better fit than those outside of the key, and that certain pitches in the key should be more heavily weighted than others.\nCarol Krumhansl and Mark Schmuckler (Krumhansl, 1990) would devise an algorithm that tallied up the pitch classes of an excerpt and compared the distribution of these pitch classes to ratings from an earlier probe-tone experiment. (Krumhansl and Kessler, 1982). The weightings can be seen below.\nWe might think of this as a correlational approach. We tally up all of the pitches in a corpus, and then run a correlation on this key-profile. We run this over all of the keys, and the one that best fits is then labeled as “the key”.\n\nks_major_key <-\n  c(6.35, \n   2.23, \n   3.48, \n   2.33, \n   4.38, \n   4.09, \n   2.52, \n   5.19, \n   2.39, \n   3.66, \n   2.29, \n   2.88)\n\nks_minor_key <-\n  c(6.33, \n  2.68, \n  3.52, \n  5.38, \n  2.60, \n  3.53, \n  2.54, \n  4.75, \n  3.98, \n  2.69, \n  3.34, \n  3.17)\n\nAn interesting distinction here is that of experiment-derived vs. corpus-derived weightings. Should a key-finding algorithm intend to match how we hear key in a controlled lab environment (with basic harmonic progression stimuli), or should they use real music as a starting point? If they use real music, which music?\n\n\n\nHector Bellman created a key-finding algorithm that used Helen Budge’s dissertation from the 1940s as a starting point. Budge tallied up note occurrences in composers from the classical music canon, looking at the tonal makeup of a large collection of pieces. Bellman then used these frequencies as the starting point for his own key-finding algorithm.\n\nmajor <- c(16.80, \n            0.86,\n            12.95,\n            1.41,\n            13.49,\n            11.93,\n            1.25,\n            20.28,\n            1.80,\n            8.04,\n            0.62,\n            10.57)\n\nminor <- c(18.16,\n            0.69,\n            12.99,\n            13.34,\n            1.07,\n            11.15,\n            1.38,\n            21.07,\n            7.49,\n            1.53,\n            0.92,\n            10.21)\n\n\n\n\nDavid Temperley (2001) also employed Western classical music as a starting point for his early key-finding work (not to be confused with his more dynamic Bayesian-informed later work). He used examples from a commonly used music theory textbook (Stefan Kostka and Dorothy Payne’s Tonal Harmony).\n\nmajor <- c(0.748, \n            0.060, \n            0.488,\n            0.082, \n            0.670, \n            0.460, \n            0.096, \n            0.715, \n            0.104, \n            0.366,\n            0.057, \n            0.400)\n\nminor <- c(0.712, \n            0.084, \n            0.474, \n            0.618, \n            0.049, \n            0.460, \n            0.105, \n            0.747, \n            0.404, \n            0.067, \n            0.133, \n            0.330)\n\n\n\n\nBret Aarden (2003) argued that folk music would be a better fit than those generated from classical music. He used the Essen Folksong collection (consisting of thousands of folksongs throughout Europe, although with an uneven balance toward German folksong), to come up with the weightings below.\n\nmajor <- c(17.7661, \n            0.145624, \n            14.9265, \n            0.160186, \n            19.8049, \n            11.3587, \n            0.291248, \n            22.062, \n            0.145624, \n            8.15494, \n            0.232998, \n            4.95122)\n            \nminor <- c(18.2648, \n            0.737619, \n            14.0499, \n            16.8599, \n            0.702494, \n            14.4362, \n            0.702494, \n            18.6161, \n            4.56621, \n            1.93186, \n            7.37619, \n            1.75623)\n\n\n\n\nCraig Sapp argued that we probably didn’t even need to get frequencies from corpora or experiments. If we just assume that the tonic and the dominant (scale degrees 1 and 5) are the most important, and the other pitches in the key are less important, but more important than those not in the key, then we have a pretty simple weighting system (that works quite well!).\n\nmajor <- c(2, 0, 1, 0, 1, 1, 0, 2, 0, 1, 0, 1)\n\nminor <- c(2, 0, 1, 1, 0, 1, 0, 2, 1, 0, 0.5, 0.5)\n\n\n\n\nJosh Albrecht and I tried our hands at this problem, and picked a set of classical works from the Humdrum corpus, looking at only the first and last eight measures of each. The numbers are below.\n\nmajor <- c(0.238, \n            0.006, \n            0.111, \n            0.006, \n            0.137, \n            0.094, \n            0.016, \n            0.214, \n            0.009, \n            0.080, \n            0.008, \n            0.081) \n\nminor <- c(0.220, \n            0.006, \n            0.104, \n            0.123, \n            0.019, \n            0.103, \n            0.012, \n            0.214, \n            0.062, \n            0.022, \n            0.061, \n            0.052)\n\nWe also tried a Euclidean distance approach, rather than a correlational approach.\nWe tried to explain it as follows:\n\nIn a two-dimensional space, if there were 70% of pitch X and 30% pitch Y, the Cartesian location of the point representing this pitch-class distribution would be at X 1⁄4 0.7 and Y 1⁄4 0.3. In this case, we are examining the distribution of 12 pitch classes, resulting in a 12-dimensional Cartesian space. The pitch-class distribution of each piece is represented by a point in that 12-dimensional space. The distance is then measured between this point and the 24 points representing the 12 major and 12 minor key pitch-class distributions, and the key separated by the shortest distance is taken to be the key of the work.\n\nBelow is a table comparing how well this did to the others.\n\n\n\nComparing the Albrecht and Shanahan to others\n\n\n\n\n\nThey each perform a bit differently on different types of tasks.\n\n\n\nComparing key-finding algorithms in major, minor, and overall (from Albrecht and Shanahan, 2013)\n\n\n\n\ncorrplot 0.92 loaded\n\n\n\ncircshift <- function(v, n) {\n  if (n == 0) v else c(tail(v, n), head(v, -n))\n}\n# \n# # ### uses the Krumhansl Schmuckler Profiles\nmajor_key <- c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)\nminor_key <- c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)\n\n##sapp's simple weightings\n# major_key <- c(2, 0, 1, 0, 1, 1, 0, 2, 0, 1, 0, 1)\n# \n# minor_key <- c(2, 0, 1, 1, 0, 1, 0, 2, 1, 0, 0.5, 0.5)\n\nkey_templates <-\n  tribble(\n    ~name, ~template,\n    \"Gb:maj\", circshift(major_key, 6),\n    \"Bb:min\", circshift(minor_key, 10),\n    \"Db:maj\", circshift(major_key, 1),\n    \"F:min\", circshift(minor_key, 5),\n    \"Ab:maj\", circshift(major_key, 8),\n    \"C:min\", circshift(minor_key, 0),\n    \"Eb:maj\", circshift(major_key, 3),\n    \"G:min\", circshift(minor_key, 7),\n    \"Bb:maj\", circshift(major_key, 10),\n    \"D:min\", circshift(minor_key, 2),\n    \"F:maj\", circshift(major_key, 5),\n    \"A:min\", circshift(minor_key, 9),\n    \"C:maj\", circshift(major_key, 0),\n    \"E:min\", circshift(minor_key, 4),\n    \"G:maj\", circshift(major_key, 7),\n    \"B:min\", circshift(minor_key, 11),\n    \"D:maj\", circshift(major_key, 2),\n    \"F#:min\", circshift(minor_key, 6),\n    \"A:maj\", circshift(major_key, 9),\n    \"C#:min\", circshift(minor_key, 1),\n    \"E:maj\", circshift(major_key, 4),\n    \"G#:min\", circshift(minor_key, 8),\n    \"B:maj\", circshift(major_key, 11),\n    \"D#:min\", circshift(minor_key, 3)\n  )\n\nLet’s look at Lucy Dacus’s “Night Shift”.\nThis grabs the track and does all the magic:\n\nnight_shift <-\n  get_tidy_audio_analysis(\"1yYlpGuBiRRf33e1gY61bN\") %>%\n  compmus_align(sections, segments) %>%\n  select(sections) %>%\n  unnest(sections) %>%\n  mutate(\n    pitches =\n      map(segments,\n          compmus_summarise, pitches,\n          method = \"mean\", norm = \"manhattan\"\n      )\n  )\n\nAnd this is just a plotting function:\n\nnight_shift %>% \n  compmus_match_pitch_template(\n    key_templates,         # Change to chord_templates if descired\n    method = \"euclidean\",  # Try different distance metrics\n    norm = \"manhattan\"     # Try different norms\n  ) %>%\n  ggplot(\n    aes(x = start + duration / 2, width = duration, y = name, fill = d)\n  ) +\n  geom_tile() +\n  scale_fill_viridis_c(guide = \"none\") +\n  theme_minimal() +\n  labs(x = \"Time (s)\", y = \"\")\n\n\n\n\n\nnight_shift <-\n  get_tidy_audio_analysis(\"1yYlpGuBiRRf33e1gY61bN\") %>%\n  compmus_align(sections, segments) %>%\n  select(sections) %>%\n  unnest(sections) %>%\n  mutate(\n    pitches =\n      map(segments,\n          compmus_summarise, pitches,\n          method = \"mean\", norm = \"manhattan\"\n      )\n  )\n\nLet’s do some exercises:\n\nVisualize a song with all of these weightings.\n\nHow do the algorithms differ?\n\nCan you write a function that would call each weighting as an argument? What would that look like?"
  },
  {
    "objectID": "class_notes/week_4.html#whats-the-key-profile-for-indie-pop",
    "href": "class_notes/week_4.html#whats-the-key-profile-for-indie-pop",
    "title": "Week 4: Conceptual Debates: Key-Finding",
    "section": "What’s the Key-Profile for “Indie-Pop”",
    "text": "What’s the Key-Profile for “Indie-Pop”\nThe basic code for getting a key-profile from a playlist is below. The process is as follows:\n\nGet the audio features from a playlist, and add the audio analysis onto the datafame.\nWe then create a “segments” column by using a map function from the tidyverse. Map functions basically apply a function over each element in a list. Here, we are saying “apply the compmus_c_transpose function to the key and segments lists from the add_audio_analysis function.”\n\nWhat does the compmus_c_transpose function do? It takes all of the chroma vectors and transposes them to the key of C, so that we can construct a single set of weightings from pieces in different keys.\n\nWe then only grab this transposed segments column and turn it into a more readable list with the unnest function.\n\nWe then grab the start, duration, and pitches info.\n\nWe then create a “pitches” column, and normalize these raw pitch counts. There are a few ways to do this, and there are different options for this.\nWe then used the compmus_gather_chroma function to take all of those chroma vectors and turn them into a list.\nWe then use the group_by and summarise functions from tidyverse, and get the mean count of each pitch class in the distribution.\n\n\n### grabs the key-profile of the indie-pop playlist.\nindie_pop_key_profile <- get_playlist_audio_features(\"\", \"37i9dQZF1DWWEcRhUVtL8n\") |>\n  \n  add_audio_analysis() |>\n  ## transpose all the chroma vectors to C.\n  mutate(segments = map2(segments, key, compmus_c_transpose)) |>\n  ## grab the segments data and unnest it, then only grabbing the start, duration, and pitches info.\n  select(segments) |>\n  unnest(segments) |> \n  select(start, duration, pitches) |> \n  mutate(pitches = map(pitches, compmus_normalise, \"euclidean\")) |>\n  compmus_gather_chroma() |>\n  group_by(pitch_class) |>\n  summarise(mean_value = mean(value)) \n\nindie_pop_key_profile\n\n# A tibble: 12 × 2\n   pitch_class mean_value\n   <fct>            <dbl>\n 1 C                0.312\n 2 C#|Db            0.170\n 3 D                0.241\n 4 D#|Eb            0.189\n 5 E                0.226\n 6 F                0.225\n 7 F#|Gb            0.175\n 8 G                0.279\n 9 G#|Ab            0.173\n10 A                0.192\n11 A#|Bb            0.172\n12 B                0.215\n\n\nIdeally, we’d be able to turn this into a more reusable function. Below we’ve just turned made the playlist URI an argument:\n\nget_key_profile_broad <- function(uri){\n   get_playlist_audio_features(\"\", uri) |>\n   add_audio_analysis() |>\n   ## transpose all the chroma vectors to C. (have I mentioned how great Burgoyne's library is??)\n   mutate(segments = map2(segments, key, compmus_c_transpose)) |>\n   ## grab the segments data and unnest it, then only grabbing the start, duration, and pitches info.\n   select(segments) |>\n   unnest(segments) |> \n   select(start, duration, pitches) |> \n   mutate(pitches = map(pitches, compmus_normalise, \"euclidean\")) |>\n   compmus_gather_chroma() |>\n   group_by(pitch_class) |>\n   summarise(mean_value = mean(value)) \n}\n\nAnd now we can just run the function like so:\n\nindie_pop <- get_key_profile_broad(\"37i9dQZF1DWWEcRhUVtL8n\")\nindie_pop\n\n# A tibble: 12 × 2\n   pitch_class mean_value\n   <fct>            <dbl>\n 1 C                0.312\n 2 C#|Db            0.170\n 3 D                0.241\n 4 D#|Eb            0.189\n 5 E                0.226\n 6 F                0.225\n 7 F#|Gb            0.175\n 8 G                0.279\n 9 G#|Ab            0.173\n10 A                0.192\n11 A#|Bb            0.172\n12 B                0.215\n\n\nand we can plot it in a pretty straightforward way:\n\nbarplot(indie_pop$mean_value)\n\n\n\n\nSo we can look at other genres pretty easily. Here is me looking at Spotify’s “EDM 2023” playlist:"
  },
  {
    "objectID": "class_notes/week_4.html#whats-the-key-profile-for-edm",
    "href": "class_notes/week_4.html#whats-the-key-profile-for-edm",
    "title": "Week 4: Conceptual Debates: Key-Finding",
    "section": "What’s the Key Profile for EDM?",
    "text": "What’s the Key Profile for EDM?\n\nedm <- get_key_profile_broad(\"37i9dQZF1DX1kCIzMYtzum\")\nedm\n\n# A tibble: 12 × 2\n   pitch_class mean_value\n   <fct>            <dbl>\n 1 C                0.311\n 2 C#|Db            0.182\n 3 D                0.217\n 4 D#|Eb            0.218\n 5 E                0.217\n 6 F                0.231\n 7 F#|Gb            0.194\n 8 G                0.264\n 9 G#|Ab            0.204\n10 A                0.197\n11 A#|Bb            0.202\n12 B                0.231\n\n\nand once again we can plot it:\n\nbarplot(edm$mean_value)\n\n\n\n\n\nSome points of interest\n\nFor both of these distributions, we see a strong showing for scale degrees 1 and 5 (they aren’t really labeled in these quickie plots, but it would be the first and seventh column, respectively).\nWith the “Indie Pop” plot, we see a strong showing of scale degrees 1 and 5, and are followed by the diatonic pitches, but with the “EDM” list, scale degrees 2, flat 3, and 3 occur with pretty much the same frequency. It might be worth splitting the major and minor pieces up a bit?"
  },
  {
    "objectID": "class_notes/week_4.html#getting-separate-major-and-minor-key-profiles",
    "href": "class_notes/week_4.html#getting-separate-major-and-minor-key-profiles",
    "title": "Week 4: Conceptual Debates: Key-Finding",
    "section": "Getting separate major and minor key-profiles",
    "text": "Getting separate major and minor key-profiles\nWe could break this into a few parts for our own comfort. Let’s start by just creating a function that grabs the data. As that’s the one that’s quite time intensive, and calls to the API, let’s try to run it only once.\n\ngrab_playlist_info <- function(uri){\n   get_playlist_audio_features(\"\", uri) |>\n   add_audio_analysis() \n}\n\nOnce we have that in place, we can create a variable, and then subset it from there. Here, I’m saving the full list, and then creating a major and a minor variable.\n\nplaylist <- grab_playlist_info(\"37i9dQZF1DX1kCIzMYtzum\")  \nminor <- playlist |> filter(mode == 0)\nmajor <- playlist |> filter(mode == 1)\n\n\nget_pitch_list <- function(input){\n   input |>     \n   ## transpose all the chroma vectors to C. (have I mentioned how great Burgoyne's library is??)\n   mutate(segments = map2(segments, key, compmus_c_transpose)) |>\n   ## grab the segments data and unnest it, then only grabbing the start, duration, and pitches info.\n   select(segments) |>\n   unnest(segments) |> \n   select(start, duration, pitches) |> \n   mutate(pitches = map(pitches, compmus_normalise, \"euclidean\")) |>\n   compmus_gather_chroma() |>\n   group_by(pitch_class) |>\n   summarise(mean_value = mean(value))\n}\n\nAnd now we can get separate pitch lists for major and minor:\n\nminor_key <- get_pitch_list(minor)\nmajor_key <- get_pitch_list(major)\n\nand then of course we can use these to inform our own key mapping.\nWe can start by putting this all into a super quick and inefficient function like this (hoping to improve it as we go along):\n\nkey_plotter <- function(uri, major, minor){\n   major_key <- major\n   minor_key <- minor\n   key_templates <-\n   tribble(\n      ~name, ~template,\n      \"Gb:maj\", circshift(major_key, 6),\n      \"Bb:min\", circshift(minor_key, 10),\n      \"Db:maj\", circshift(major_key, 1),\n      \"F:min\", circshift(minor_key, 5),\n      \"Ab:maj\", circshift(major_key, 8),\n      \"C:min\", circshift(minor_key, 0),\n      \"Eb:maj\", circshift(major_key, 3),\n      \"G:min\", circshift(minor_key, 7),\n      \"Bb:maj\", circshift(major_key, 10),\n      \"D:min\", circshift(minor_key, 2),\n      \"F:maj\", circshift(major_key, 5),\n      \"A:min\", circshift(minor_key, 9),\n      \"C:maj\", circshift(major_key, 0),\n      \"E:min\", circshift(minor_key, 4),\n      \"G:maj\", circshift(major_key, 7),\n      \"B:min\", circshift(minor_key, 11),\n      \"D:maj\", circshift(major_key, 2),\n      \"F#:min\", circshift(minor_key, 6),\n      \"A:maj\", circshift(major_key, 9),\n      \"C#:min\", circshift(minor_key, 1),\n      \"E:maj\", circshift(major_key, 4),\n      \"G#:min\", circshift(minor_key, 8),\n      \"B:maj\", circshift(major_key, 11),\n      \"D#:min\", circshift(minor_key, 3)\n  )\n\ntune <-\n  get_tidy_audio_analysis(uri) %>%\n  compmus_align(sections, segments) %>%\n  select(sections) %>%\n  unnest(sections) %>%\n  mutate(\n    pitches =\n      map(segments,\n          compmus_summarise, pitches,\n          method = \"mean\", norm = \"manhattan\"\n      )\n  ) \n\ntune |> compmus_match_pitch_template(\n    key_templates,         # Change to chord_templates if descired\n    method = \"euclidean\",  # Try different distance metrics\n    norm = \"manhattan\"     # Try different norms\n  ) %>%\n  ggplot(\n    aes(x = start + duration / 2, width = duration, y = name, fill = d)\n  ) +\n  geom_tile() +\n  scale_fill_viridis_c(guide = \"none\") +\n  theme_minimal() +\n  labs(x = \"Time (s)\", y = \"\")\n\n}"
  },
  {
    "objectID": "class_notes/week_4.html#one-piece-and-many-key-profiles",
    "href": "class_notes/week_4.html#one-piece-and-many-key-profiles",
    "title": "Week 4: Conceptual Debates: Key-Finding",
    "section": "One Piece and Many Key Profiles",
    "text": "One Piece and Many Key Profiles\nLooking at Lucy Dacus’s “Night Shift” with EDM Key Profiles:\n\nedm_major_key <- c(0.2949827,0.1842662, 0.2249348, 0.1796559, 0.2532545, 0.2391564, 0.2028676, 0.2607747, 0.1765553, 0.2105823, 0.1806760, 0.2562869)\n# \nedm_minor_key <- c(0.3247214, 0.1767437, 0.2066454, 0.2482824, 0.1811887, 0.2263670, 0.1830838, 0.2662832, 0.2340293, 0.1888321, 0.2203257, 0.2047107)\nkey_plotter(\"1yYlpGuBiRRf33e1gY61bN\", edm_major_key, edm_minor_key)\n\n\n\n\nAnd here is the piece with the more traditional Krumhansl-Schmuckler key profiles:\n\nkey_plotter(\"1yYlpGuBiRRf33e1gY61bN\", ks_major_key, ks_minor_key)\n\n\n\n\nWe can load our “indie pop” but now in major and minor:\n\nplaylist <- grab_playlist_info(\"37i9dQZF1DWWEcRhUVtL8n\")  \nindie_minor <- playlist |> filter(mode == 0)\nindie_major <- playlist |> filter(mode == 1)\nindie_minor <- get_pitch_list(indie_minor)\nindie_major <- get_pitch_list(indie_major)\n\n\n\n\nAnd then we put these weightings into the plotter:\n\nkey_plotter(\"1yYlpGuBiRRf33e1gY61bN\", ks_major_key, ks_minor_key)"
  },
  {
    "objectID": "class_notes/week_4.html#exercise",
    "href": "class_notes/week_4.html#exercise",
    "title": "Week 4: Conceptual Debates: Key-Finding",
    "section": "Exercise:",
    "text": "Exercise:\n\nPick one piece and construct a genre-specific key-profile that might be used to explain its tonal make-up.\n\nExplain this musically."
  },
  {
    "objectID": "class_notes/week_5.html#looking-at-tempo-over-time",
    "href": "class_notes/week_5.html#looking-at-tempo-over-time",
    "title": "Week 5: Testing Hypotheses with Spotify",
    "section": "Looking at tempo over time",
    "text": "Looking at tempo over time\nWe can start by eyeballing the data. Here is how we’d do it with base R (no ggplot/tidyverse):\n\nplot(tempo ~ album_release_year, data=jayz)\nabline(lm(tempo ~ album_release_year, data=jayz), col=\"red\")\n\n\n\n\nIf we’d like to use ggplot it can give us some confidence bars (the default here is a 95% confidence interval):\n\nggplot(jayz, aes(album_release_year, tempo) ) + geom_point() +\n  stat_smooth(method = lm) +\n  theme_bw()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nSo it looks promising. We can run a linear regression with a simple lm command. Here we can get a summary of the model pretty easily, as well.\n\nsummary(lm(tempo ~ album_release_year, data=jayz))\n\n\nCall:\nlm(formula = tempo ~ album_release_year, data = jayz)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-44.84 -21.17 -11.79  12.22  92.18 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(>|t|)    \n(Intercept)        -2158.9919   396.7564  -5.442 6.99e-08 ***\nalbum_release_year     1.1306     0.1979   5.714 1.55e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 30.94 on 815 degrees of freedom\nMultiple R-squared:  0.03852,   Adjusted R-squared:  0.03734 \nF-statistic: 32.65 on 1 and 815 DF,  p-value: 1.547e-08\n\n\nSo, as we can see from the results here, it’s significant (p < .001), but it really doesn’t account for much of the variance (an adjusted R-squared of .037).\n\nPost-Hoc Analyses\nPerhaps we can look at how other variables might be predictive of the year of the recording.\nLet’s look at how tempo, danceability, valence, speechiness, and energy might improve the model.\n\nsummary(lm(album_release_year ~ tempo + danceability + valence + speechiness + energy, data=jayz))\n\n\nCall:\nlm(formula = album_release_year ~ tempo + danceability + valence + \n    speechiness + energy, data = jayz)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.2530  -3.4825  -0.2837   3.5690  18.2393 \n\nCoefficients:\n               Estimate Std. Error  t value Pr(>|t|)    \n(Intercept)   2.016e+03  1.826e+00 1104.163  < 2e-16 ***\ntempo         2.861e-02  5.603e-03    5.105 4.12e-07 ***\ndanceability -5.986e+00  1.519e+00   -3.942 8.77e-05 ***\nvalence      -6.849e+00  9.630e-01   -7.112 2.51e-12 ***\nspeechiness  -7.808e+00  1.160e+00   -6.730 3.20e-11 ***\nenergy       -4.484e+00  1.348e+00   -3.326 0.000921 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.885 on 811 degrees of freedom\nMultiple R-squared:  0.2084,    Adjusted R-squared:  0.2035 \nF-statistic: 42.69 on 5 and 811 DF,  p-value: < 2.2e-16\n\n\nSo we have a more predictive model, with an adjusted R-squared of about .20.\nThere are some remaining questions, however. Firstly, is there covariance at play?\nWe can use the variance inflation factor (VIF) to look at this. Typically, each variable should be less than 5.\n\nlibrary(car)\n\nLoading required package: carData\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\njayz_model <- lm(album_release_year ~ tempo + danceability + valence + speechiness + energy, data=jayz)\nvif(jayz_model)\n\n       tempo danceability      valence  speechiness       energy \n    1.067352     1.350387     1.228451     1.093354     1.253967 \n\n\nA correlation plot can help us to visualize this a bit more.\n\nlibrary(corrplot)\n\ncorrplot 0.92 loaded\n\njz <- jayz %>% \n    select(c(\"acousticness\", \"liveness\", \"danceability\", \"loudness\", \"speechiness\", \"valence\"))\n  x <- as.matrix(cor(jz))\n  round(x, 2)\n\n             acousticness liveness danceability loudness speechiness valence\nacousticness         1.00     0.14         0.07    -0.12        0.33    0.07\nliveness             0.14     1.00        -0.20     0.01        0.03   -0.11\ndanceability         0.07    -0.20         1.00    -0.17       -0.01    0.28\nloudness            -0.12     0.01        -0.17     1.00       -0.20    0.03\nspeechiness          0.33     0.03        -0.01    -0.20        1.00    0.18\nvalence              0.07    -0.11         0.28     0.03        0.18    1.00\n\n  corrplot(x, method=\"pie\")"
  },
  {
    "objectID": "class_notes/week_5.html#sidenote-isare-the-data-normal",
    "href": "class_notes/week_5.html#sidenote-isare-the-data-normal",
    "title": "Week 5: Testing Hypotheses with Spotify",
    "section": "Sidenote: Is/Are the data normal?",
    "text": "Sidenote: Is/Are the data normal?\nWe can test to see if the tempo data is normally distributed:\n\nqqnorm(jayz$tempo)\n\n\n\nhist(jayz$tempo)\n\n\n\nshapiro.test(jayz$tempo)\n\n\n    Shapiro-Wilk normality test\n\ndata:  jayz$tempo\nW = 0.78869, p-value < 2.2e-16\n\nks.test(jayz$tempo, \"pnorm\")\n\nWarning in ks.test.default(jayz$tempo, \"pnorm\"): ties should not be present for\nthe Kolmogorov-Smirnov test\n\n\n\n    Asymptotic one-sample Kolmogorov-Smirnov test\n\ndata:  jayz$tempo\nD = 1, p-value < 2.2e-16\nalternative hypothesis: two-sided\n\n\nAt the moment, it doesn’t seem to be…"
  },
  {
    "objectID": "class_notes/week_5.html#stepwise-entry-regression",
    "href": "class_notes/week_5.html#stepwise-entry-regression",
    "title": "Week 5: Testing Hypotheses with Spotify",
    "section": "Stepwise Entry Regression",
    "text": "Stepwise Entry Regression\n\nsummary(step(lm(album_release_year ~ danceability + \n          tempo + acousticness + \n          speechiness + valence, data=jayz), \n     direction=\"backward\"))\n\nStart:  AIC=2573.56\nalbum_release_year ~ danceability + tempo + acousticness + speechiness + \n    valence\n\n               Df Sum of Sq   RSS    AIC\n<none>                      18788 2573.6\n- danceability  1    243.73 19032 2582.1\n- tempo         1    714.38 19502 2602.1\n- acousticness  1    826.79 19614 2606.7\n- speechiness   1   1422.54 20210 2631.2\n- valence       1   1687.89 20476 2641.8\n\n\n\nCall:\nlm(formula = album_release_year ~ danceability + tempo + acousticness + \n    speechiness + valence, data = jayz)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-12.9398  -3.6858  -0.2579   3.4408  16.3345 \n\nCoefficients:\n               Estimate Std. Error  t value Pr(>|t|)    \n(Intercept)   2.011e+03  1.257e+00 1599.612  < 2e-16 ***\ndanceability -4.496e+00  1.386e+00   -3.244  0.00123 ** \ntempo         3.075e-02  5.538e-03    5.553 3.80e-08 ***\nacousticness  6.752e+00  1.130e+00    5.974 3.46e-09 ***\nspeechiness  -9.233e+00  1.178e+00   -7.836 1.46e-14 ***\nvalence      -7.750e+00  9.080e-01   -8.536  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.813 on 811 degrees of freedom\nMultiple R-squared:  0.2314,    Adjusted R-squared:  0.2267 \nF-statistic: 48.83 on 5 and 811 DF,  p-value: < 2.2e-16\n\nsummary(step(lm(album_release_year ~ danceability + \n          tempo + acousticness + \n          speechiness + valence, data=jayz), \n     direction=\"forward\"))\n\nStart:  AIC=2573.56\nalbum_release_year ~ danceability + tempo + acousticness + speechiness + \n    valence\n\n\n\nCall:\nlm(formula = album_release_year ~ danceability + tempo + acousticness + \n    speechiness + valence, data = jayz)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-12.9398  -3.6858  -0.2579   3.4408  16.3345 \n\nCoefficients:\n               Estimate Std. Error  t value Pr(>|t|)    \n(Intercept)   2.011e+03  1.257e+00 1599.612  < 2e-16 ***\ndanceability -4.496e+00  1.386e+00   -3.244  0.00123 ** \ntempo         3.075e-02  5.538e-03    5.553 3.80e-08 ***\nacousticness  6.752e+00  1.130e+00    5.974 3.46e-09 ***\nspeechiness  -9.233e+00  1.178e+00   -7.836 1.46e-14 ***\nvalence      -7.750e+00  9.080e-01   -8.536  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.813 on 811 degrees of freedom\nMultiple R-squared:  0.2314,    Adjusted R-squared:  0.2267 \nF-statistic: 48.83 on 5 and 811 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "class_notes/week_5.html#comparing-fits",
    "href": "class_notes/week_5.html#comparing-fits",
    "title": "Week 5: Testing Hypotheses with Spotify",
    "section": "Comparing Fits:",
    "text": "Comparing Fits:\nWe could construct a few models But how can we tell which of these is more predictable? For this, we can look at Akaike’s ‘An Information Criterion’(or AIC). The lower number is better, and any difference of more than 2 is considered a statistically significant difference fit.\n\ndance_model <- lm(danceability ~ album_release_year, data=jayz)\nacoustic_model <- lm(acousticness ~ album_release_year, data=jayz)\nspeech_model <- lm(speechiness ~ album_release_year, data=jayz)\nvalence_model <- lm(valence ~ album_release_year, data=jayz)\ntempo_model <- lm(tempo ~ album_release_year, data=jayz)\ncombined_model <- lm(album_release_year ~ tempo + acousticness + speechiness + valence, data=jayz)\n\n\nAIC(dance_model, \n    acoustic_model, \n    tempo_model,\n    speech_model, \n    valence_model, \n    combined_model)\n\n               df        AIC\ndance_model     3 -1036.7120\nacoustic_model  3  -686.8298\ntempo_model     3  7930.2701\nspeech_model    3  -782.5959\nvalence_model   3  -438.7779\ncombined_model  6  4902.6326\n\n\nThe combined model doesn’t seem to do terribly well here, which seems to muddy the question up a bit."
  },
  {
    "objectID": "class_notes/week_5.html#is-a-linear-model-the-best-approach",
    "href": "class_notes/week_5.html#is-a-linear-model-the-best-approach",
    "title": "Week 5: Testing Hypotheses with Spotify",
    "section": "Is a linear model the best approach?",
    "text": "Is a linear model the best approach?\nWe can adjust how we are viewing this data (linear or polynomial fit) a bit with the code below. Here we can plot our data as a line:\n\nggplot(jayz, aes(album_release_year, tempo) ) + geom_point() +\n  stat_smooth(method = lm, formula = y ~ poly(x, 1, raw = TRUE)) +\n  theme_bw()\n\n\n\n\nAnd here we have it as a second order polynomial:\n\nggplot(jayz, aes(album_release_year, tempo) ) + geom_point() +\n  stat_smooth(method = lm, formula = y ~ poly(x, 2, raw = TRUE)) +\n  theme_bw()\n\n\n\n\nAnd we can compare fits here:\n\nlinear <- lm(album_release_year ~ tempo, data = jayz)\npoly_2 <- lm(album_release_year ~ tempo + I(album_release_year^2), data = jayz)\n\nAIC(linear, \n    poly_2)\n\n       df       AIC\nlinear  3  5069.029\npoly_2  4 -5640.652"
  },
  {
    "objectID": "class_notes/week_5.html#predicting-a-categorical-variable",
    "href": "class_notes/week_5.html#predicting-a-categorical-variable",
    "title": "Week 5: Testing Hypotheses with Spotify",
    "section": "Predicting a categorical variable",
    "text": "Predicting a categorical variable\nWhat does it look like to predict a categorical variable in R? We might explore this question by hypothesizing that musical features might be predictive of mode (labeled in Spotify as major or minor: 1 or 0).\nHere is what a binomial logistic regression would look like:\n\njayz.log <- glm(mode ~ tempo + danceability + valence +\n                     speechiness + acousticness, family = binomial, data = jayz)\n\nAnd it looks like “speechiness” is the most predictive of mode here.\n\nsummary(jayz.log)\n\n\nCall:\nglm(formula = mode ~ tempo + danceability + valence + speechiness + \n    acousticness, family = binomial, data = jayz)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.8836  -1.2367   0.9073   1.0739   1.3806  \n\nCoefficients:\n               Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  -1.468e-01  5.364e-01  -0.274   0.7843    \ntempo         3.210e-06  2.365e-03   0.001   0.9989    \ndanceability  1.062e-01  5.893e-01   0.180   0.8570    \nvalence      -5.733e-01  3.911e-01  -1.466   0.1427    \nspeechiness   2.561e+00  5.483e-01   4.671    3e-06 ***\nacousticness -9.680e-01  4.849e-01  -1.996   0.0459 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1120.6  on 816  degrees of freedom\nResidual deviance: 1095.8  on 811  degrees of freedom\nAIC: 1107.8\n\nNumber of Fisher Scoring iterations: 4\n\n\nWe can plot the log odds ratios as well:\n\nCI <- exp(confint(jayz.log))[-1,]\n\nWaiting for profiling to be done...\n\nsjPlot::plot_model(jayz.log,\n                   axis.lim = c(min(CI), max(CI)),\n                   auto.label = F,\n                   show.values = T) +\n                   theme_bw()"
  },
  {
    "objectID": "class_notes/other_files.qmd/sampling_exercises.html",
    "href": "class_notes/other_files.qmd/sampling_exercises.html",
    "title": "Sampling Exercise",
    "section": "",
    "text": "For each case, identify the kind of sampling employed.\n\nA researcher walks into a music library with a question: Are sharp keys more common than flat keys? Wandering through the stacks, she blindly grabs volumes off the shelves and allows each volume to open spontaneously to some page. She takes note of the key signature.\nA professional music marketer is interested in carrying out a detailed survey of musical tastes in Britain. The marketer decides to use the ACORN geodemographic profile. British households will be sampled in proportion to the second-level ACORN categories: wealthy executives (8.6 percent of the population), affluent greys (7.7%), flourishing families (8.8%), prosperous professionals (2.2%), educated urbanites (4.6%), aspiring singles (3.9%), starting out (2.5%), secured families (15.5%), settled surburbia (6.0%), prudent pensioners (2.6%), asian communities (1.6%), post-industrial families (4.8%), blue collar roots (8.0%), struggling families (14.1%), burdened singles (4.5%), high rise hardship (1.6%), and inner city adversity (2.1%).\nA researcher is interested in assembling a random sample of “classical” keyboard music. She has determined that she needs roughly 20 pieces for her study. In order to maximize data independence, she wants each piece to be written by a different composer. Using Wikipedia, she finds an alphabetical list of “classical composers.” For each letter of the alphabet, she selects the first composer who she knows has written for piano: Isaac Albéniz, Carl Philipp Emanuel Bach, Alfredo Casella, Claude Debussy, Edward Elgar, Manuel de Falla, etc.\nIn piloting an experiment, a graduate student recruits her graduate student colleagues as experimental participants.\nA team of researchers is interested in emotional expression in Hindustani film music. Indian participants are asked to characterize the emotional tenor of various film scenes. Using the descriptions, the researchers then classify the scenes into 14 categories — such as romantic, humorous, physical conflict, emotional tension, etc. The researchers then select four scenes for each of the 14 categories and analyse the associated background music. Their goal is to identify musical features in Hindustani culture that signal romance, humor, etc.\nA medievalist thinks that the Dorian mode was more likely to have been heard as comparatively “happy” whereas the Phrygian mode was more likely to have been heard as comparative “sad” for medieval listeners. In order to test this notion, the scholar examines all of the Glorias (nominally “happy” text) and Kyries (nominally “sad” text) in the Liber Usualis. The prediction is that Dorian will predominate for Glorias while Phrygian will be more likely to occur for Kyries.\nA researcher is interested in changing harmonic patterns in the masses of Palestrina. The researcher makes us of the Humdrum database of the scores for the complete 103 masses assembled by musicologist John Miller.\nPaul von Hippel and David Huron (2000) carried out a study to test the idea that melodies tend to change direction following a leap, and that this pattern is ubiquitous in musical melodies around the world. In order to test this idea, they made use of two musical samples. The first sample selected music spanning five centuries. The second sample selected music spanning five continents: Africa, Asia, Europe, North and South America.\nUnsure of the contents of a box, an archivist reaches in and grabs a couple of documents, which he then examines.\nA researcher wants to know whether there is anything Italian, French or German about augmented sixth chords. Using large computer databases, the researcher uses Humdrum to isolate 900 sonorities in which the lowered sixth and raised fourth appear concurrently (including enharmonic spellings): 300 each written by Italian, French and German composers. Each of the sonorities is then classified as either Italian, French, German or Other.\n\n\n\nPaul von Hippel & David Huron (2000). Why do skips precede reversals? The effect of tessitura on melodic structure. Music Perception, Vol. 18, No. 1, pp. 59-85."
  },
  {
    "objectID": "class_notes/week_2.html#chords",
    "href": "class_notes/week_2.html#chords",
    "title": "Week 2: Pitch",
    "section": "Chords",
    "text": "Chords\nBurgoyne’s chordogram functions allow us to look at the likely chordal spaces for specific piecses. The code below does a few things:\n\nFirst we define what a major, minor, and seventh chord looks like in terms of pitch space.\nWe then use the key-profiles from the Krumhansl-Kessler article on the probe tone experiments and store them into major_key and minor_key variables.\nThe circshift function rotates these key profiles through the chord variables and provides the best fit for that moment. This is done through the key_templates variable (Notice the compmus_match_pitch_template below).\n\n\n#      C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B\nmajor_chord <-\n  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)\nminor_chord <-\n  c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)\nseventh_chord <-\n  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)\n\nmajor_key <-\n  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)\nminor_key <-\n  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)\n\ncircshift <- function(v, n) {\n  if (n == 0) v else c(tail(v, n), head(v, -n))\n}\nchord_templates <-\n  tribble(\n    ~name, ~template,\n    \"Gb:7\", circshift(seventh_chord, 6),\n    \"Gb:maj\", circshift(major_chord, 6),\n    \"Bb:min\", circshift(minor_chord, 10),\n    \"Db:maj\", circshift(major_chord, 1),\n    \"F:min\", circshift(minor_chord, 5),\n    \"Ab:7\", circshift(seventh_chord, 8),\n    \"Ab:maj\", circshift(major_chord, 8),\n    \"C:min\", circshift(minor_chord, 0),\n    \"Eb:7\", circshift(seventh_chord, 3),\n    \"Eb:maj\", circshift(major_chord, 3),\n    \"G:min\", circshift(minor_chord, 7),\n    \"Bb:7\", circshift(seventh_chord, 10),\n    \"Bb:maj\", circshift(major_chord, 10),\n    \"D:min\", circshift(minor_chord, 2),\n    \"F:7\", circshift(seventh_chord, 5),\n    \"F:maj\", circshift(major_chord, 5),\n    \"A:min\", circshift(minor_chord, 9),\n    \"C:7\", circshift(seventh_chord, 0),\n    \"C:maj\", circshift(major_chord, 0),\n    \"E:min\", circshift(minor_chord, 4),\n    \"G:7\", circshift(seventh_chord, 7),\n    \"G:maj\", circshift(major_chord, 7),\n    \"B:min\", circshift(minor_chord, 11),\n    \"D:7\", circshift(seventh_chord, 2),\n    \"D:maj\", circshift(major_chord, 2),\n    \"F#:min\", circshift(minor_chord, 6),\n    \"A:7\", circshift(seventh_chord, 9),\n    \"A:maj\", circshift(major_chord, 9),\n    \"C#:min\", circshift(minor_chord, 1),\n    \"E:7\", circshift(seventh_chord, 4),\n    \"E:maj\", circshift(major_chord, 4),\n    \"G#:min\", circshift(minor_chord, 8),\n    \"B:7\", circshift(seventh_chord, 11),\n    \"B:maj\", circshift(major_chord, 11),\n    \"D#:min\", circshift(minor_chord, 3)\n  )\n\nkey_templates <-\n  tribble(\n    ~name, ~template,\n    \"Gb:maj\", circshift(major_key, 6),\n    \"Bb:min\", circshift(minor_key, 10),\n    \"Db:maj\", circshift(major_key, 1),\n    \"F:min\", circshift(minor_key, 5),\n    \"Ab:maj\", circshift(major_key, 8),\n    \"C:min\", circshift(minor_key, 0),\n    \"Eb:maj\", circshift(major_key, 3),\n    \"G:min\", circshift(minor_key, 7),\n    \"Bb:maj\", circshift(major_key, 10),\n    \"D:min\", circshift(minor_key, 2),\n    \"F:maj\", circshift(major_key, 5),\n    \"A:min\", circshift(minor_key, 9),\n    \"C:maj\", circshift(major_key, 0),\n    \"E:min\", circshift(minor_key, 4),\n    \"G:maj\", circshift(major_key, 7),\n    \"B:min\", circshift(minor_key, 11),\n    \"D:maj\", circshift(major_key, 2),\n    \"F#:min\", circshift(minor_key, 6),\n    \"A:maj\", circshift(major_key, 9),\n    \"C#:min\", circshift(minor_key, 1),\n    \"E:maj\", circshift(major_key, 4),\n    \"G#:min\", circshift(minor_key, 8),\n    \"B:maj\", circshift(major_key, 11),\n    \"D#:min\", circshift(minor_key, 3)\n  )\n\nHere we have a piece of code that grabs a single audio file (“Those magic changes”). In class we listened to it while going through the chordogram. Can you spot the modulation? Why do we get that yellowish color at the end of the graph?\n\nthose_magic_changes <-\n  get_tidy_audio_analysis(\"1WHauHX7U6FqOWh46lK4IV\") %>%\n  compmus_align(sections, segments) %>%\n  select(sections) %>%\n  unnest(sections) %>%\n  mutate(\n    pitches =\n      map(segments,\n          compmus_summarise, pitches,\n          method = \"mean\", norm = \"manhattan\"\n      )\n  )\n\nthose_magic_changes %>% \n  compmus_match_pitch_template(\n    key_templates,         # Change to chord_templates if descired\n    method = \"euclidean\",  # Try different distance metrics\n    norm = \"manhattan\"     # Try different norms\n  ) %>%\n  ggplot(\n    aes(x = start + duration / 2, width = duration, y = name, fill = d)\n  ) +\n  geom_tile() +\n  scale_fill_viridis_c(guide = \"none\") +\n  theme_minimal() +\n  labs(x = \"Time (s)\", y = \"\")\n\n\n\n\nSome activities:\n\nGo through some songs that you know. How close is the chordogram of providing some sort of brief explanatory analysis?\nWhat does this look like on music that might be considered less adherent to notions of western tonal music?"
  },
  {
    "objectID": "class_notes/week_3.html#midterms",
    "href": "class_notes/week_3.html#midterms",
    "title": "Week 3: Time",
    "section": "Midterms",
    "text": "Midterms\n\nDue April 28th\n1-3 pages long.\nconstruct a literature review of the topic you’re interested in writing about. This could culminate in a hypothesis, and a discussion of how your work fills the current gaps in the literature.\nBe sure to include an overview of previous research related to the topic. This should include both empirical and non-empirical work. So if you’re focusing on the memory for jazz licks, for example, research on improvisational styles should be included alongside work on memory for musical ideas. It need not be all-encompassing, but it should try to cover as much ground as possible.\nAddress a gap in the current literature, or that between theory and research. End with a discussion of your study, and how it hopes to fill in these gaps.\nFeel free to meet with me if you have any questions."
  },
  {
    "objectID": "class_notes/week_3.html#populations",
    "href": "class_notes/week_3.html#populations",
    "title": "Week 3: Time",
    "section": "Populations",
    "text": "Populations\n\nA population is everything or everyone that you’re interested in.\ne.g. all the world’s people\nall the world’s people including living and deceased\nall Western-enculturated people\nall people who enjoy listening to music\nall clarinet players\n\nA “population” does not refer only to people: Other examples:\n\nall of the music written by Vivaldi\nall solo flute music (both with and without accompaniment)\nall music in the minor mode\nall of the jazz scores available in the New York Public Library\nall performances of Rachmaninov’s 2nd piano concerto"
  },
  {
    "objectID": "class_notes/week_3.html#sample",
    "href": "class_notes/week_3.html#sample",
    "title": "Week 3: Time",
    "section": "Sample",
    "text": "Sample\n\nSample: a subset of the population that you hope closely resembles the population as a whole.\nA sample is said to be representative when the property of interest is identical in both the sample and the population."
  },
  {
    "objectID": "class_notes/week_3.html#bias",
    "href": "class_notes/week_3.html#bias",
    "title": "Week 3: Time",
    "section": "Bias",
    "text": "Bias\n\nA sample is said to be biased when the property of interest differs between the sample and the population."
  },
  {
    "objectID": "class_notes/week_3.html#weird",
    "href": "class_notes/week_3.html#weird",
    "title": "Week 3: Time",
    "section": "WEIRD",
    "text": "WEIRD\n\nWestern\nEducated\nIndustrialized\nRich\nDemocratic\n\nsee Henrich’s Work on this"
  },
  {
    "objectID": "class_notes/week_3.html#defining-your-population",
    "href": "class_notes/week_3.html#defining-your-population",
    "title": "Week 3: Time",
    "section": "Defining Your Population",
    "text": "Defining Your Population\n\nYou can’t sample a population unless you have a clear idea of what constitutes the population of interest.\n\nSuppose, for example, that you are a political pollster. Your aim is to predict the likely election results for a national election in Denmark. What, precisely, is the population you are interested in?"
  },
  {
    "objectID": "class_notes/week_3.html#defining-your-population-continued",
    "href": "class_notes/week_3.html#defining-your-population-continued",
    "title": "Week 3: Time",
    "section": "Defining Your Population (continued)",
    "text": "Defining Your Population (continued)\n\nAll Danish citizens?\nAll people living in Denmark?\nAll people living in Denmark eligible to vote?\nAll people eligible to vote in Danish elections?\nAll people likely to vote in Danish elections?"
  },
  {
    "objectID": "class_notes/week_3.html#sampling-method",
    "href": "class_notes/week_3.html#sampling-method",
    "title": "Week 3: Time",
    "section": "Sampling Method",
    "text": "Sampling Method\n\nSampling method: the way you recruit or assemble your sample. When your population consists of people, sampling methods might include soliciting information by telephone (telephone sampling), street sampling, mail sampling, web sampling, classroom sampling, concert sampling, etc."
  },
  {
    "objectID": "class_notes/week_3.html#sampling-bias",
    "href": "class_notes/week_3.html#sampling-bias",
    "title": "Week 3: Time",
    "section": "Sampling Bias",
    "text": "Sampling Bias\n\nSampling bias: when the sampling method introduces differences that cause the sample not to be representative. We try to avoid or minimize sampling bias.\nWhen conducting a telephone survey, a pollster may be tempted to ask to speak to a respondent’s spouse. However, spouses are likely to share many things in common (such as political views) so the sampling method will introduce a bias."
  },
  {
    "objectID": "class_notes/week_3.html#simple-random-sampling",
    "href": "class_notes/week_3.html#simple-random-sampling",
    "title": "Week 3: Time",
    "section": "Simple Random Sampling",
    "text": "Simple Random Sampling\n\nSimple Random Sampling. Suppose we want to know about musical instrument sales in the City of Nashville. We could use the phone book to identify all of the shops within the city boundaries that sell musical instruments. Perhaps we discover that there are 131 retailers. From this list, we might randomly select 25 retailers in order to carry out our survey."
  },
  {
    "objectID": "class_notes/week_3.html#systematic-sampling",
    "href": "class_notes/week_3.html#systematic-sampling",
    "title": "Week 3: Time",
    "section": "Systematic Sampling",
    "text": "Systematic Sampling\nSuppose that we have a questionnaire we want to distribute to people who attended a concert. There might be 500 audience members, but we have only 50 surveys to distribute. One approach would be to distribute the questionnaires to the first 50 people leaving the concert hall."
  },
  {
    "objectID": "class_notes/week_3.html#matched-random-sampling",
    "href": "class_notes/week_3.html#matched-random-sampling",
    "title": "Week 3: Time",
    "section": "Matched Random Sampling",
    "text": "Matched Random Sampling\n\nA way of linking members from two or more samples. For example, a study might involve matching each professional musician with an amateur musician who plays the same instrument."
  },
  {
    "objectID": "class_notes/week_3.html#convenience-sampling",
    "href": "class_notes/week_3.html#convenience-sampling",
    "title": "Week 3: Time",
    "section": "Convenience Sampling",
    "text": "Convenience Sampling\n\nConvenience Sampling. A convenience sample simply takes advantage of whatever might be available. For example, a sample of organ music by Gabriel Fauré might simply consist of all of the scores available in a music library. Similarly, we might stand on a street corner and ask whoever passes by to answer questions on a survey."
  },
  {
    "objectID": "class_notes/week_3.html#stratified-sampling",
    "href": "class_notes/week_3.html#stratified-sampling",
    "title": "Week 3: Time",
    "section": "Stratified Sampling",
    "text": "Stratified Sampling\n\nWhen we have reason to suspect that differences in sub-populations might influence the results, it is common to sample in such a way to ensure that each of the main sub-populations is represented.\nPost and Huron (2009) were interested in common-practice era tonal classical music. So we decided to use a stratified sample consisting of music from three periods: Baroque, Classical and Romantic. Our overall sample consisted of equivalent numbers of works from each of these historical eras."
  },
  {
    "objectID": "class_notes/week_3.html#quota-sampling",
    "href": "class_notes/week_3.html#quota-sampling",
    "title": "Week 3: Time",
    "section": "Quota Sampling",
    "text": "Quota Sampling\n\nA type of stratified sampling in which sub-samples are weighted according to their prevalence in the population.\nSuppose that we find that 52% of instrumentalists are most accomplished on guitar, 33% are most accomplished on keyboards, 12% on flute, 9% on trumpet, 8% on violin, etc. In quota sampling, we would aim to sample the same proportions for each instrument."
  },
  {
    "objectID": "class_notes/week_3.html#exercise",
    "href": "class_notes/week_3.html#exercise",
    "title": "Week 3: Time",
    "section": "Exercise",
    "text": "Exercise\nGroup Exercise"
  },
  {
    "objectID": "class_notes/week_3.html#tempo-average",
    "href": "class_notes/week_3.html#tempo-average",
    "title": "Week 3: Time",
    "section": "Tempo Average",
    "text": "Tempo Average\nThe general default at the track level is an averaging of an entire piece. This can be useful at times, but it should be noted that it is a broad average for a parameter in which the variability is often quite meaningful.\nWhat if we wanted to see how the songs of Daft Punk changed in tempo changed over time?\nFirst we would make sure that Spotify had your user access token, and then get the artist audio features:\n\naccess_token <- get_spotify_access_token()\ndaft_punk <- get_artist_audio_features('daft punk')\n\nThen we would be able to simply plot the album’s mean tempo with the variance (and outliers) with ggplot, as below:\n\nggplot(daft_punk, aes(x=album_release_year, y=tempo, group = album_name)) + geom_boxplot() +\ntheme_bw()\n\n\n\n\nWe could also explore the variance of tempo within an album by looking at the standard deviation.\n\nalbum_sd <- daft_punk %>% \n    group_by(album_name, album_release_year) %>%\n    summarise(sd_tempo = sd(tempo))\n\n`summarise()` has grouped output by 'album_name'. You can override using the\n`.groups` argument.\n\n\nAnd then we can similarly plot this information:\n\nggplot(album_sd, aes(x=album_release_year, y=sd_tempo, group = album_name)) + geom_point() +\n  geom_label(\n    aes(label=album_name)) +\ntheme_bw()\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\n\n\nWarning: Removed 1 rows containing missing values (`geom_label()`)."
  },
  {
    "objectID": "class_notes/week_3.html#other-aspects-of-tempo",
    "href": "class_notes/week_3.html#other-aspects-of-tempo",
    "title": "Week 3: Time",
    "section": "Other aspects of tempo:",
    "text": "Other aspects of tempo:\nWe can also look at other elements of tempo, such as variability within sections. Here we have a question about the differences in tempo between punk in the 1980s and later punk (1990s and 2000s). I’m interested not just in the tempo, but also the variation of tempo.\nThere are a couple of points to notice in this code:\n\nNote how we are able to get data from a playlist. A playlist can be a good way for you to construct a sample.\nNote the add_audio_analysis function from the compmus library. This adds track level analysis information to the broader list of global information. It’s great.\n\nSit for a minute with this data. You’ll see the columns at the end that provided the specific audio analysis for each piece.\n\neighties_punk <-\n  get_playlist_audio_features(\n    \"kristian\",\n    \"5sxuwIQlaByb6Sx2OEwWTx\"\n  ) %>%\n  slice(1:30) %>%\n  add_audio_analysis()\n\nnineties_and_aughts_punk <-\n  get_playlist_audio_features(\n    \"CW\",\n    \"39sVxPTg7BKwrf2MfgrtcD\"\n  ) %>%\n  slice(1:30) %>%\n  add_audio_analysis()\n\nHere we bind both playlists together as a single data frame:\n\npunk <- \n  eighties_punk %>%\n  mutate(genre = \"eighties\") %>%\n  bind_rows(nineties_and_aughts_punk %>% mutate(genre = \"newer\"))\n\nThe spotify analysis gives us section markers as well, and we can use the code below to summarise the tempo, loudness, and duration for each section. Note the use of the map function, which takes the input and applies a function to that input (here the summarise_at function, and the summarise_at itself, which provides a summary of each of these columns.\nHere we are storing it in a variable called summarised_punk.\n\nsummarised_punk <- punk %>%\n  mutate(\n    sections =\n      map(\n        sections,                                    # sections or segments\n        summarise_at,\n        vars(tempo, loudness, duration),             # features of interest\n        list(section_mean = mean, section_sd = sd)   # aggregation functions\n      )\n  )\n\nNow we take this variable and plot it using ggplot.\nThe process below is as follows:\n\ntake the table above with summarized section information and unnest it (this takes the sections list of information and turns it into rows and columns).\nPipe that into ggplot, with the aesthetics function plotting the tempo on the x-axis, the standard deviation on the y-axis, the color being which genre we used (eighties or not). The color saturation is set to the loudness variable.\nWe then tell ggplot that we want this to be a scatterplot with the geom_point function, and that the size of each point should be the duration of the piece (divided by 60 as Spotify just gives it in seconds).\nWe then add a rug plot which gives the ticks on both axes to show the distribution of events.\nWe then add a black and white theme because nobody likes default graphics.\nWe then add the size of the graph and the axis labels.\n\n\n  summarised_punk %>%\n  unnest(sections) %>%\n  ggplot(\n    aes(\n      x = tempo,\n      y = tempo_section_sd,\n      colour = genre,\n      alpha = loudness\n    )\n  ) +\n  geom_point(aes(size = duration / 60)) +\n  geom_rug() +\n  theme_bw() +\n  ylim(0, 5) +\n  labs(\n    x = \"Mean Tempo (bpm)\",\n    y = \"SD Tempo\",\n    colour = \"Genre\",\n    size = \"Duration (min)\",\n    alpha = \"Volume (dBFS)\"\n  )  \n\nWarning: Removed 7 rows containing missing values (`geom_point()`).\n\n\n\n\n\nIn-class exercises:\n\nHow is tempo treated differently across the albums of the Beatles?"
  },
  {
    "objectID": "class_notes/week_3.html#plan-for-the-day",
    "href": "class_notes/week_3.html#plan-for-the-day",
    "title": "Week 3: Time",
    "section": "Plan for the day:",
    "text": "Plan for the day:\n\nTalk about your homework a bit.\nLook at some tap data, and what we might actually be able to do with it.\nTalk about how Spotify (might) calculate tempo.\n\nHow might we look at tempo across pieces?\nOne way to get this is to just tap the tempo, and then align it to the onsets. But how do we find onsets?\nThis is where a novelty function comes in. (see Müller on Fourier Tempograms). Put (extremely) succinctly, a novelty function detects changes in the energy or the spectrum of the signal. So looking for energy peaks might be a good marker for “peaks in energy”.\n\n\n\nMüller’s Onset Detection Example (p.311)\n\n\nAfter finding these onsets, it then examines a correlation between various sinusoids and picks the most likely one. There are many different ways of approaching this.\nOne issue is the presence of so-called “tempo octaves”. That is, it finds tempos at twice the beat, half the beat, etc..\nHere’s a graph of AJR’s “World’s Smallest Violin”:\n\nget_tidy_audio_analysis(\"3jHBgKdLCf46aP3HRI0WYv\") %>%\n  tempogram(window_size = 8, hop_size = 2) %>%\n   ggplot(aes(x = time, y = bpm, fill = power)) + \n    geom_raster() + \n    scale_fill_viridis_c(guide = 'none') +\n    labs(x = 'Time (s)', y = 'Tempo (BPM)') +\n    theme_classic()\n\n\n\n\nThis seems quite strange, though. It’s not really a great indicator of tempo…\nMüller points out that the Fourier-based method tends to struggle with these tempo-octaves, and a cyclic model, which look at “subharmonics” rather than harmonics, and are a bit better for mid-level tempo finding. The example below seems to work a bit better. Notice how the cyclic option has been switched to TRUE.\n\nget_tidy_audio_analysis(\"3jHBgKdLCf46aP3HRI0WYv\") %>%\n  tempogram(window_size = 8, hop_size = 2, cyclic=TRUE) %>%\n   ggplot(aes(x = time, y = bpm, fill = power)) + \n    geom_raster() + \n    scale_fill_viridis_c(guide = 'none') +\n    labs(x = 'Time (s)', y = 'Tempo (BPM)') +\n    theme_classic()"
  },
  {
    "objectID": "class_notes/week_1.html",
    "href": "class_notes/week_1.html",
    "title": "Week 1: Representing Musical Data",
    "section": "",
    "text": "In the first week, we worked through basic introductions for the class, and went through the syllabus and the course structure.\n\n\nHere, we install the necessary library. As you can see, you will need to install devtools, which will allow you to install packages that aren’t on CRAN from github.\nThen, we install the package (you can uncomment these installation lines as necessary for you).\n\n### installing everything as needed\n# library(devtools)\n# devtools::install_github(\"Computational-Cognitive-Musicology-Lab/humdrumR\", build_vignettes = TRUE)\nlibrary(humdrumR)\n\nIn the code below, you can see how we load all of the Chopin files into a preludes variable with the readHumdrum function.\nThen we subset it by spines. We are interested in various ways of calculating pitch, so we looked at pc (pitch class), as well as solfa and deg, which gave us solfege syllables and scale degrees, respectively.\nWe then plot this data in a barplot. Note the |> or “pipe” that we are using. The older tidyverse-style pipe (%>%) will also work here.\n\n### Load in Chopin preludes, grab the left hand and see all the scale degrees.\npreludes <- readHumdrum(\"~/gitcloud/corpora/humdrum_scores/Chopin/Preludes/*.krn\")\nleft_hand <- subset(preludes, Spine == 1)\n###solfa, deg, pc\ntable_data <- with(left_hand, pc(Token,simple=TRUE)) |> table() \nbarplot(table_data)\n\n\n\n\nYou can use a similar with syntax to get rhythm variables, as seen below:\n\n## rhythminterval\nrhythms <- with(preludes[2], duration(Token))\n\nWarning in is.na(x) || is.na(y): 'length(x) = 93 > 1' in coercion to\n'logical(1)'\n\nWarning in is.na(x) || is.na(y): 'length(x) = 93 > 1' in coercion to\n'logical(1)'\n\nWarning in is.na(x) || is.na(y): 'length(x) = 93 > 1' in coercion to\n'logical(1)'\n\nWarning in is.na(x) || is.na(y): 'length(x) = 93 > 1' in coercion to\n'logical(1)'\n\n\nWarning in is.na(x) || is.na(y): 'length(x) = 84 > 1' in coercion to\n'logical(1)'\n\nWarning in is.na(x) || is.na(y): 'length(x) = 84 > 1' in coercion to\n'logical(1)'\n\n\nWarning in is.na(x) || is.na(y): 'length(x) = 93 > 1' in coercion to\n'logical(1)'\n\nWarning in is.na(x) || is.na(y): 'length(x) = 93 > 1' in coercion to\n'logical(1)'\n\n\nWarning in is.na(x) || is.na(y): 'length(x) = 11 > 1' in coercion to\n'logical(1)'\n\nWarning in is.na(x) || is.na(y): 'length(x) = 11 > 1' in coercion to\n'logical(1)'\n\n\nWarning in is.na(x) || is.na(y): 'length(x) = 2 > 1' in coercion to\n'logical(1)'\n\nWarning in is.na(x) || is.na(y): 'length(x) = 2 > 1' in coercion to\n'logical(1)'\n\n\nWarning in is.na(x) || is.na(y): 'length(x) = 93 > 1' in coercion to\n'logical(1)'\n\nWarning in is.na(x) || is.na(y): 'length(x) = 93 > 1' in coercion to\n'logical(1)'\n\n\nWarning in is.na(x) || is.na(y): 'length(x) = 84 > 1' in coercion to\n'logical(1)'\n\nWarning in is.na(x) || is.na(y): 'length(x) = 84 > 1' in coercion to\n'logical(1)'\n\n\nWarning in is.na(x) || is.na(y): 'length(x) = 11 > 1' in coercion to\n'logical(1)'\n\nWarning in is.na(x) || is.na(y): 'length(x) = 11 > 1' in coercion to\n'logical(1)'\n\n\nWarning in is.na(x) || is.na(y): 'length(x) = 5 > 1' in coercion to\n'logical(1)'\n\nWarning in is.na(x) || is.na(y): 'length(x) = 5 > 1' in coercion to\n'logical(1)'\n\n#### group exercise:\n#### using a repertoire in the Humdrum scores collection, \n#### print a table of most common musical events.\n\n\n\n\nWe can start by loading our spotifyr library, and tidyverse for good measure:\n\nlibrary(spotifyr)\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0     ✔ purrr   1.0.1\n✔ tibble  3.2.1     ✔ dplyr   1.1.1\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.4     ✔ forcats 1.0.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ purrr::%@%()         masks rlang::%@%()\n✖ dplyr::changes()     masks humdrumR::changes()\n✖ dplyr::count()       masks humdrumR::count()\n✖ tidyr::expand()      masks humdrumR::expand()\n✖ dplyr::filter()      masks stats::filter()\n✖ purrr::flatten()     masks rlang::flatten()\n✖ purrr::flatten_chr() masks rlang::flatten_chr()\n✖ purrr::flatten_dbl() masks rlang::flatten_dbl()\n✖ purrr::flatten_int() masks rlang::flatten_int()\n✖ purrr::flatten_lgl() masks rlang::flatten_lgl()\n✖ purrr::flatten_raw() masks rlang::flatten_raw()\n✖ humdrumR::int()      masks rlang::int()\n✖ purrr::invoke()      masks rlang::invoke()\n✖ dplyr::lag()         masks humdrumR::lag(), stats::lag()\n✖ dplyr::lead()        masks humdrumR::lead()\n✖ purrr::splice()      masks rlang::splice()\n✖ dplyr::symdiff()     masks bit::symdiff()\n✖ purrr::transpose()   masks humdrumR::transpose()\n\n\nYou will need your own spotify client ID and client secret. You can get them by filling out the brief online form here.\n\n### setting up spotify\nSys.setenv(SPOTIFY_CLIENT_ID = YOUR SPOTIFY CLIENT ID)\nSys.setenv(SPOTIFY_CLIENT_SECRET = YOUR SPOTIFY CLIENT SECRET)\naccess_token <- get_spotify_access_token()\n\n\n\n\nFor the most part, in this class we will be looking at global features data (the “danceability” of a song), and track-level analysis features, such as chroma vectors.\nHere we see how you might grab artist features for Ryan Adams and Taylor Swift, comparing the performances of each of their 1989 albums.\n\n###getting artist level data\nryan_adams <- get_artist_audio_features('ryan adams')\ntaylor_swift <- get_artist_audio_features('taylor swift')\n\n### cleaning up the data\nadams_swift <- rbind(ryan_adams, taylor_swift)\nadams_swift_1989 <- adams_swift %>% filter(album_name == \"1989\") \nadams_swift_1989$track_name <- tolower(adams_swift_1989$track_name)\n\n## comparing energy\nggplot(adams_swift_1989, aes(x=track_name, y=energy, group=artist_name)) +\n  geom_line(aes(linetype=artist_name, color=artist_name))+\n  geom_point(aes(color=artist_name))+\n  theme(legend.position=\"top\", axis.text.x = element_text(angle = 90, hjust = 1))"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Corpus Studies and Music",
    "section": "",
    "text": "Welcome!\nWelcome to the Corpus Studies and Music class."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is the class site for Corpus Studies in Music."
  },
  {
    "objectID": "course-schedule.html",
    "href": "course-schedule.html",
    "title": "Course Schedule",
    "section": "",
    "text": "Unit\nWeek\nDate (Date)\nTopic\nImportant Dates\n\n\n\n\nRepresenting, Searching, and Counting with Symbolic Data\n1\nT (3/28; Northwestern Monday)\nIntroductions; a history of corpus-based approaches to music.\n\n\n\n\n1\nW (3/29)\nRepresenting musical data (kern, MIDI, XML, MEI)\nAssignment #1 Due\n\n\n\n2\nM (4/3)\nPitches, melodic intervals and scale degrees\nResponse #1 Due\n\n\n\n2\nW (4/5)\nRepresenting Harmony\nAssignment #2 Due\n\n\n\n3\nM (4/10)\nRepresenting Rhythm and Meter\nResponse #2 Due\n\n\n\n3\nW (4/12)\nCharacteristic Features and TF-IDF\nAssignment #3 Due\n\n\n\n4\nM (4/17)\nn-grams and Transition Probabilities\nResponse #3 Due\n\n\n\n4\nW (4/19)\nFinding patterns\nAssignment #4 Due\n\n\nConceptual Debates\n5\nM (4/24)\nMeasurements of Similarity\nMidterm Literature Review Due\n\n\n\n5\nW (4/26)\nEntropy and Variability\nAssignment #5 Due\n\n\n\n6\nM (5/1)\nKey-Finding Algorithms\nResponse #4 Due\n\n\n\n6\nW (5/3)\nKey-Finding Algorithms (continued)\nAssignment #6 Due\n\n\n\n7\nM (5/8)\nFast Fourier Transforms\nResponse #5 Due\n\n\n\n7\nW (5/10)\nClustering and Authorship\nAssignment #7 Due\n\n\nAudio Data\n8\nM (5/15)\nPerformance Data/the Mazurka Project\nFirst Draft of Final Project Due\n\n\n\n8\nW (5/17)\nTempo flexibility, and other performance data\nAssignment #8 Due\n\n\n\n9\nM (5/22)\nThe Spotify API\nResponse #6 Due; Peer Reviews Due\n\n\n\n9\nW (5/24)\nThe Spotify API (continued)\nAssignment #9 Due\n\n\n\n10\nM (5/29)\nNo Classes (Memorial Day)\nFinal Presentations\n\n\n\n10\nW (5/31)\nFinal Presentations\nFinal Papers due on Friday, 6/2"
  },
  {
    "objectID": "course-syllabus.html",
    "href": "course-syllabus.html",
    "title": "Corpus Studies (Music 348/448) Syllabus",
    "section": "",
    "text": "Instructor: Dr. Daniel Shanahan\nContact: daniel.shanahan@northwestern.edu"
  },
  {
    "objectID": "course-syllabus.html#overview",
    "href": "course-syllabus.html#overview",
    "title": "Corpus Studies (Music 348/448) Syllabus",
    "section": "Overview",
    "text": "Overview\nCorpus studies, or distant readings of multiple musical works, are often employed as a way of better understanding issues such as the relationships between pieces, authorship, trends over time, or differences and similarities between genres. In this class, we will explore the techniques, history, and philosophy of such approaches, and will construct and analyze our own corpora. For the most part, this class will deal with notated scores, and students will be encouraged to ask their own research questions of the music that they are most interested in."
  },
  {
    "objectID": "course-syllabus.html#course-info",
    "href": "course-syllabus.html#course-info",
    "title": "Corpus Studies (Music 348/448) Syllabus",
    "section": "Course info",
    "text": "Course info\n\n\n\n\n\n\n\n\n\n\nDay\nTime\nLocation\n\n\n\n\nMeeting Times:\nMon & Weds\n12:30pm - 1:50 pm\nRCMA 1-164\n\n\nOffice Hours\nTBD (and by appointment)\nTBD (and by appointment)\nRCMA 4-181"
  },
  {
    "objectID": "course-syllabus.html#learning-objectives",
    "href": "course-syllabus.html#learning-objectives",
    "title": "Corpus Studies (Music 348/448) Syllabus",
    "section": "Learning objectives",
    "text": "Learning objectives\nBy the end of the quarter, you will…\n\nhave an understanding of how music has been examined through distant readings of scores and recordings\nbe able to explore how the concepts of concordances, schemata, key-finding, clustering, and introductory machine learning approaches can be applied to music analysis\nhave a working introductory knowledge of the R programming language and the HumDrumR package."
  },
  {
    "objectID": "course-syllabus.html#course-policies",
    "href": "course-syllabus.html#course-policies",
    "title": "Corpus Studies (Music 348/448) Syllabus",
    "section": "Course Policies",
    "text": "Course Policies\n\nAcademic Integrity\nStudents in this course are required to comply with the policies found in the booklet, “Academic Integrity at Northwestern University: A Basic Guide”. All papers submitted for credit in this course must be submitted electronically unless otherwise instructed by the professor. Your written work may be tested for plagiarized content. For details regarding academic integrity at Northwestern or to download the guide, visit this page.\n\n\nAccesibility\nNorthwestern University is committed to providing the most accessible learning environment as possible for students with disabilities. Should you anticipate or experience disability-related barriers in the academic setting, please contact AccessibleNU to move forward with the university’s established accommodation process (email: accessiblenu@northwestern.edu; p: 847-467-5530). If you already have established accommodations with AccessibleNU, please let me know as soon as possible, preferably within the first two weeks of the term, so we can work together to implement your disability accommodations. Disability information, including academic accommodations, is confidential under the Family Educational Rights and Privacy Act.\n\n\nCOVID-19 Classroom Expectations\nStudents, faculty and staff must comply with University expectations regarding appropriate classroom behavior, including those outlined below and in the COVID-19 Expectations for Students. With respect to classroom procedures, this includes:\nPolicies regarding masking, social distancing and other public health measures evolve as the situation changes. Students are responsible for understanding and complying with current University, state and city requirements. In some classes, masking and/or social distancing may be required as a result of an Americans with Disabilities Act (ADA) accommodation for the instructor or a student in the class even when not generally required on campus. In such cases, the instructor will notify the class.\nIf a student fails to comply with the COVID-19 Expectations for Students or other University expectations related to COVID-19, the instructor may ask the student to leave the class. The instructor is asked to report the incident to the Office of Community Standards for additional follow-up.\n\nIf you’re feeling sick…\nMaintaining the health of the community remains our priority. If you are experiencing any symptoms of COVID do not attend class. Follow the steps outlined on the NU sites for testing, isolation and reporting a positive case. Next, contact me as soon as possible to arrange to complete coursework.\nShould public health recommendations prevent in-person class from being held on a given day, I or the university will notify students.\n\n\n\nDiversity, Equity, and Inclusion\nThis course strives to be an inclusive learning community, respecting those of differing backgrounds and beliefs. As a community, we aim to be respectful to all students in this class, regardless of race, ethnicity, socio-economic status, religion, gender identity or sexual orientation."
  },
  {
    "objectID": "course-syllabus.html#textbooks",
    "href": "course-syllabus.html#textbooks",
    "title": "Corpus Studies (Music 348/448) Syllabus",
    "section": "Textbooks",
    "text": "Textbooks\nThere is no textbook for this course, and most of the materials will be available on Canvas or the course website (or both). Many of the readings will be taken from the forthcoming Oxford Handbook of Music and Corpus Studies, edited by Daniel Shanahan, Ashley Burgoyne, and Ian Quinn.\nHaving said that, you should sign up for a free account for Posit Cloud (formerly RStudio Cloud), where many of the class notebooks will be held.I would also recommend downloading R and RStudio onto your personal machine, if possible.\nAlthough not required, I would highly recommend having a look at:\n\nR for Data Science by Garret Grolemund and Hadley Wickham\nThe Humdrum User Guide\nThe music21 documentation\nThe Oxford Handbook of Music and Corpus Studies"
  },
  {
    "objectID": "course-syllabus.html#support-for-wellness-and-mental-health",
    "href": "course-syllabus.html#support-for-wellness-and-mental-health",
    "title": "Corpus Studies (Music 348/448) Syllabus",
    "section": "Support for Wellness and Mental Health",
    "text": "Support for Wellness and Mental Health\nNorthwestern University is committed to supporting the wellness of our students. Student Affairs has multiple resources to support student wellness and mental health. If you are feeling distressed or overwhelmed, please reach out for help. Students can access confidential resources through the Counseling and Psychological Services (CAPS), Religious and Spiritual Life (RSL) and the Center for Awareness, Response and Education (CARE). Additional information on all of the resources mentioned above can be found here:\nhttps://www.northwestern.edu/counseling/\nhttps://www.northwestern.edu/religious-life/\nhttps://www.northwestern.edu/care/\n\nHomework\nThere will be regular assignments in which you will be asked to respond to do one of the following:\n\nCritically reflect upon a reading about the history, methods, and dilemmas commonly found in corpus studies.\nWrite code that addresses a musical question (e.g. what’s the most common pitch transition in this group of pieces?)\nAnalyze a given collection of musical data.\n\nTypically, we will have reading reflections due on Mondays, and code-related questions relevant to those readings due on Wednesdays.\n\n\nMidterm Project\nThe goal of this class is for you to both understand corpus studies as a method with a long history, and for you to be able to incorporate these methods in your own research. There will be a midterm project that is primarily used a stepping stone into your final project, and it will consist of presenting a literature review in which you situate your own research question within the existing literature and propose a study that examines this question. You may use existing data, but you might find it more relevant to you if you use your own dataset. Therefore, this would be a good time to have a bulk of your data encoded, so that you are aware of the time needed to construct your corpus.\n\n\nFinal Project\nThe final project will be focused on a research question of your choosing, and will be broken up into several a peer-reviewed first draft, a presentation, and a final paper."
  },
  {
    "objectID": "course-syllabus.html#grading",
    "href": "course-syllabus.html#grading",
    "title": "Corpus Studies (Music 348/448) Syllabus",
    "section": "Grading",
    "text": "Grading\nThe final course grade will be calculated as follows:\n\n\n\nCategory\nPercentage\n\n\n\n\nReading Reflection Questions\n20%\n\n\nCode-focused assignments\n30%\n\n\nMidterm Literature Review\n15%\n\n\nFinal Project (First Draft)\n10%\n\n\nPresentation\n10%\n\n\nFinal Project (Final draft)\n15%\n\n\n\nThe final letter grade will be determined based on the following thresholds:\n\n\n\nLetter Grade\nFinal Course Grade\n\n\n\n\nA\n>= 93\n\n\nA-\n90 - 92.99\n\n\nB+\n87 - 89.99\n\n\nB\n83 - 86.99\n\n\nB-\n80 - 82.99\n\n\nC+\n77 - 79.99\n\n\nC\n73 - 76.99\n\n\nC-\n70 - 72.99\n\n\nD+\n67 - 69.99\n\n\nD\n63 - 66.99\n\n\nD-\n60 - 62.99\n\n\nF\n< 60"
  },
  {
    "objectID": "course-syllabus.html#schedule",
    "href": "course-syllabus.html#schedule",
    "title": "Corpus Studies (Music 348/448) Syllabus",
    "section": "Schedule",
    "text": "Schedule\n\n\n\n\n\n\n\n\n\nUnit\nWeek\nDate (Date)\nTopic\n\n\n\n\nRepresenting, Searching, and Counting with Symbolic Data\n1\nT (3/28; Northwestern Monday)\nIntroductions; a history of corpus-based approaches to music.\n\n\n\n1\nW (3/29)\nRepresenting musical data (kern, MIDI, XML, MEI)\n\n\n\n2\nM (4/3)\nPitches, melodic intervals and scale degrees\n\n\n\n2\nW (4/5)\nRepresenting Harmony\n\n\n\n3\nM (4/10)\nRepresenting Rhythm and Meter\n\n\n\n3\nW (4/12)\nCharacteristic Features and TF-IDF\n\n\n\n4\nM (4/17)\nn-grams and Transition Probabilities\n\n\n\n4\nW (4/19)\nFinding patterns\n\n\nConceptual Debates\n5\nM (4/24)\nMeasurements of Similarity\n\n\n\n5\nW (4/26)\nEntropy and Variability\n\n\n\n6\nM (5/1)\nKey-Finding Algorithms\n\n\n\n6\nW (5/3)\nKey-Finding Algorithms (continued)\n\n\n\n7\nM (5/8)\nFast Fourier Transforms\n\n\n\n7\nW (5/10)\nClustering and Authorship\n\n\nAudio Data\n8\nM (5/15)\nPerformance Data/the Mazurka Project\n\n\n\n8\nW (5/17)\nTempo flexibility, and other performance data\n\n\n\n9\nM (5/22)\nThe Spotify API\n\n\n\n9\nW (5/24)\nThe Spotify API (continued)\n\n\n\n10\nM (5/29)\nNo Classes (Memorial Day)\n\n\n\n10\nW (5/31)\nFinal Presentations"
  }
]