[
  {
    "objectID": "class_notes/week_1.html",
    "href": "class_notes/week_1.html",
    "title": "Week 1",
    "section": "",
    "text": "Here we will have a brief weekly document that summarizes the goals, main points and upcoming due-dates and tasks for the class.\n\n\nThe goals of this week are mostly introductory. In addition to going over the plan for the quarter, we discussed what we mean by “empirical”, how Popper would define “science” and the broader goals of scientific inquiry.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSee this page from Huron’s workshop for a nice description of these concepts.\n\n\n\nSee this page on types of studies\n\n\n\n\nFor next week, it would be good to read the Introduction to Empirical Musicology, edited by Nicholas Cook and Eric Clarke (the introduction is written by them). Although it was published a while ago, it still holds up quite well.\n\n\n\n(Exercise from Huron’s Workshop)\nIdentify whether the following statements are questions, theories, conjectures, hypotheses or none.\n\nWhy do performers tend to slow down at the ends of phrases?\nThis is a question.\nBeethoven’s metronome markings are too fast because his metronome was broken.\nThis is a theory: it proposes a causal explanation. Note the use of the word “because.” (A common mistake is to think that “theories” make big or sweeping generalizations; theories can be small and focus on minor phenomena.)\nMuch of the popularity of World Music is due to commercial exploitation.\nThis is a theory: it proposes a causal explanation — although the mechanism of influence (“commercial exploitation”) may be a bit vague. Note the use of the phrase “due to”.\nHow is it that listening to music can sometimes cause shivers to run up-and-down your spine?\nThis is a question.\nThe language we use shapes the way we think.\nNote that the word “shapes” could be replaced by synonyms such as “affects” or “influences.” In effect, the claim is that language “causes” us to think differently. Consequently, it is appropriate to regard this as a theory.\nBrahms uses a lot of hemiolas in his music.\nThis is a hypothesis. It is easily tested.\nBrahms liked hemiolas.\nUnless Brahms wrote a letter or otherwise communicated that he “likes” (or dislikes) hemiolas, it would be difficult or impossible to determine what he “likes.” Consequently, it is better to call this a conjecture rather than a hypothesis.\nAfricans have a better sense of rhythm than Europeans.\nIf you think “sense of rhythm” is not possible to measure, then this is a conjecture. Otherwise it is a hypothesis.\nThe music of Carl Nielsen echoes the spirit of the Danish people.\nAs written, this statement admits several different interpretations. It is often helpful to try re-writing a statement in order to gain some clarity. For example, we might re-write this statement as follows: “The Danish spirit is echoed in the music of Carl Nielsen.” This suggests that being Danish influenced Nielsen’s music. Formulated this way, the statement would be regarded as a theory.\n“Music hath charms to soothe the savage breast.”\n(Quote from William Congreve, 1697). This is a conjecture or hypothesis.\nRaag Shree sounds sad to experienced Indian listeners.\nThis is a hypothesis. In principle, this claim could be easily tested.\nThe music of the Lakota has been influential primarily because of frequent portrayals of Plains Indians in Hollywood films.\nThis is a theory: it proposes a causal explanation for the widespread influence of Lakota music. Note the use of the word “because.”\nThe purpose of our research is to study the relationship between music and ritual.\nThis is not a question.\nIt is not a theory since no cause is proposed.\nIt could be construed as a conjecture or hypothesis if there were some doubt about the purpose of the research (say, if two collaborators were arguing about what they are doing). E.g.\nResearcher 1: The purpose of our research is … music and ritual. Researcher 2: No, no, the purpose of our research is … music and dance.\nBut this is a stretch.\nThe best answer is “none.”\nNotice that, as a research goal, it would be much better to say “The purpose of our research is to understand the relationship between music and ritual.” The “study” of something may be a reasonable goal for a student, but not for a researcher.\n\n\n\n\n\n../slides/week1.qmd"
  },
  {
    "objectID": "emp_methods_workshop/reading03.html",
    "href": "emp_methods_workshop/reading03.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Reading #3: Shanahan & Huron (2014)\n\nRead: Daniel Shanahan and David Huron (2014). Heroes and villains: The relationship between pitch tessitura and sociability of operatic characters. Empirical Musicology Review, Vol. 9, No. 2, pp. 141-153. The article can be found at: <http://musiccog.ohio-state.edu/home/data/_uploaded/pdf/heroes_villains.pdf\nWhile reading this article, keep the following comments and questions in the back of your mind:\n\nIn this first reading, you are not expected to understand any of the statistics, so don’t worry about the statistical descriptions.\nWhat is the hypothesis?\nDo the authors have more than one hypothesis?\nWhat are the main terms in the hypothesis that need to be operationalized?\nHow do they determine “major” and “minor”? Are you convinced by the method they used?\nHow do they measures the dynamic level?\nThey sampled only the first dynamic level encountered in each piece. Could you propose a better method?\nThey identified 192 pieces for possible sampling, but they only analysed 48? Why didn’t the authors examine all 192 pieces?\nWhat is the conclusion?\nIn the conclusion, do the authors use suitably circumspect language?"
  },
  {
    "objectID": "emp_methods_workshop/freelist_task.html",
    "href": "emp_methods_workshop/freelist_task.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Task\nYou will be asked to make a list of a certain sort. You will be given just sixty seconds to perform this exercise. Your aim should be to make the list as long as you can in the short time provided. Wait for verbal instructions before you start."
  },
  {
    "objectID": "emp_methods_workshop/reductionism.html",
    "href": "emp_methods_workshop/reductionism.html",
    "title": "Reductionism",
    "section": "",
    "text": "There are at least two ways of interpreting the term reductionism. One interpretation of reductionism is the “nothing but” mode of explanation. The second is the “divide and study” method of research. These two different notions of reductionism are described below.\n\n\nThe more contentious notion of reductionism may be called the “nothing but” mode of explanation. According to this view, one attempts to explain complex phenomena as merely the interaction of simpler underlying phenomena; explanation proceeds by accounting for complex wholes in terms of simpler components. In this form of reductionism, the researcher aims to make statements of the form “X is nothing but Y.”\nUsed in this sense, reductionism can be contrasted with what is sometimes called holism. A `holist’ expects to explain phenomena as being greater than the sum of its parts (a process famously dubbed synergism by architect Buckminster Fuller). Frequently, synergism leads to “emergent properties” where complex phenomena cannot be predicted even when a thorough understanding exists of the underlying constituent phenomena. A simple example of an emergent property is the mixing of oxygen and hydrogen to form water: it would be difficult to predict in advance that combining two explosive gases might lead to a non-flammable colorless liquid.\nIn contrast to holism, the “nothing but” form of reductionism seeks to explain all complex phenomena as convoluted manifestations of a smaller number of fundamental causes or interactions. In its most sweeping formulation: culture is just sociology, sociology is just psychology, psychology is just biology, biology is just chemistry, and chemistry is just physics.\nNot everyone is comfortable with this way of interpreting the world. If such a scientific reductive synthesis is true, it might represent one of the pinnacle achievements of human inquiry. If it is false, it might represent one of the preeminent intellectual blunders in human history.\nMany humanities scholars of have derided this reductionist project. Much of the objection originates in the unsavory esthetic repercussions of nothing but' reductionism. It is argued that the purpose of explanation should be to enrich a phenomenon, not to trivialize it. The word \"explanation\" itself is suspect --- deriving from the Latin *ex planum*, meaning to smooth-out or make flat. The aim of scholarship should not be to \"flatten\" phenomenon. When a scholar claims that *X is nothing but Y*, the world as a rich enchanting place is transformed into a prosaic, colorless, and seemingly senseless enterprise. Among humanities scholars, musicians and musicologists have been among the most vocal critics ofnothing but’ reductionism. Many music scholars explicitly embrace complexity and scorn simplicity. Composer John Cage cautioned strongly against such “logical minimizations.”\n\n\n\nThere is another way of interpreting “reductionism” — namely, as a potentially useful strategy for discovery rather than a belief about how the world is. This alternative interpretation might be described as the “divide and study” method of inquiry — where the researcher examines one factor with the hope that manipulating this single factor will help shed light on a complex phenomena by isolating constituent relationships. For example, a psychologist might use the assumption of a recessive gene as a technique to help analyze a personality trait. The scholar need not believe that the theory is the reality, only that the theory is a useful tool for understanding.\nClassically, the principal research tool for “divide and study” reductionism is the concept of “control.” It is commonly thought that control entails holding one or more factors constant while the “independent variable” is manipulated and the “dependent variable” is observed. However, control more commonly entails randomizing the potentially confounding variables. In taking a political poll, for example, pollsters expect that the number of variables influencing people’s opinions is very large. It is hopeless to assume that one can hold constant such a large number of factors. Consequently, researchers seek a random sample with the hope that unknown influences will tend to cancel each other out. The formal statistical argument in support of random sampling is (mathematically) quite compelling, so there is considerable merit to this method of control.\nUsing such methods of control, it becomes possible for a researcher to investigate the effect of a given factor on some complex phenomenon. By investigating one factor at a time, it is often possible to build a sophisticated model or theory of the phenomenon in question. When the number of factors is more than five or six, the divide and study strategy often becomes intractable due to the explosion of possible interactions between purported factors. Nevertheless, the approach can still help identify important relationships in real-world phenomena.\n\n\n\nThroughout the nineteenth century, the British government maintained a number experimental farms scattered throughout various parts of the country. The goal was to better understand how to increase crop yields and provide appropriate research-based advice to farmers. Which crops grow better in which regions? Which varieties grow better at different altitudes, with different amounts of sunlight, rainfall, and fertilizers. What combination of soil-type, moisture, etc. are most productive?\nThroughout the century, employees kept detailed records for each field they planted. They measured the amount of sunlight, the average temperature, and the amount of rainfall, They recorded when the crop was planted, when they applied fertilizer, when it was harvested, and the total amount harvested. After nearly a century of collecting data, they had leger after leger of information. But they didn’t know how to analyze the data. One year, they would plant variety A of wheat and harvested 2,900 bushels. The following year they planted variety B of wheat and produced a slightly lower harvest. However, the second year received more rainfall, but had less sunlight and a lower average temperature. Was variety A better than variety B? Or would variety A also have done worse if the amount of sunlight was less and the temperature lower?\nIn order to try to make sense of this data, the British government hired statistician Ronald Fisher in 1919. Fisher took a long look at the records and concluded that no conclusion was possible in many cases. Each year, too many things had varied at the same time so it was impossible to figure out what factor or combination of factors was responsible for higher or lower harvests. Fisher changed the way in which the experimental plots were used. Instead of planting a single crop in each field, the government farmers planted two crops. The first row would be variety A, the second row would be variety B … and so on, alternating across the field. In this way, both variety A and variety B would receive the same rainfall, the same amount of sunlight, the same soil, etc. Fisher understood that if you want to understand the effects of many factors, you need to manipulate only one factor at a time.\nMuch of the data that had been collected over the course of a century was useless because too many variables changed at the same time. Within a few years after Fisher was hired, the experimental farms produced highly useful data — establishing, for example, that variety A was better than variety B, except in areas that receive high levels of rainfall, etc.\nFisher demonstrated that if you want to understand a complex system, manipulate only one variable at a time and observe what happens. After observing the effect of one variable, move on to another variable (like the amount of fertilizer). As you gain more knowledge, you can increase the number of simultaneously manipulated variables. But if you want to make progress, researchers must start by manipulating just one variable at a time.\n\n\n\nThe naturalist John Muir famously noted “When we try to pick out anything by itself, we find it hitched to everything else in the universe.” The whole world is interconnected. In explaining the world, we must ultimately account for all of the interconnections. However, we can’t start by trying to explain the whole world at once. We simply don’t have enough knowledge.\nInstead, we must begin by examining a small part, using a small theory. Theory must begin with unreasonable simplifications. Everything may be interconnected, but it is impossible to begin with a rich theory.\nStarting with a simple theory doesn’t mean you should believe the world is simple. In knowledge-related enterprises, the philosopher Alfred North Whitehead offered the following apt advice: “Seek simplicity and distrust it.” As the anthropologist Clifford Geertz wrote: “Scientific advancement commonly consists in a progressive complication of what once seemed a beautifully simple set of notions but now seems an unbearably simplistic one.” (The Interpretation of Cultures, p. 33.)\nThe heart of methodological reductionism is the “divide and study” approach to problem-solving. Take a problem and divide it into manageable pieces. Simplifying a problem does not mean we believe the problem is simple. In fact, we simplify problems because we believe them to be complex. Simplifying a problem is not a statement about how the world is, it is merely a strategy for discovery. There is no guarantee that a complex whole is merely the sum of its parts — or even that a complex whole is primarily the sum of its parts. But the divide-and-study strategy gives us a place to start.\nMethodological reductionism attempts to form good theory by progressively refining simplistic theories. Do not be afraid to start with a theory that is unreasonably simplistic. In fact, we must begin with unreasonably simplistic theories if we have any hope of understanding a phenomenon in its full richness.\nSlogan: In research, reductionism is a method, not a belief.\n\n\n\nWhen simplifying a problem, there are many possible simplifications. How do you know which simplification to begin with? The experience of researchers provides clear advice on this: start with what you think is the most important factor.\nConsider, for example, the phenomenon of musical taste. Why does a person prefer the music he/she likes? We might suppose that age, sex, personality, nationality and socio-economic background all play a role in determining taste. If you think that the most important factor influencing musical taste is teenage peer influence, then begin there.\nSimilarly, consider the question: In orchestration, why do composers selected the instruments they do? Again, we might identify lots of factors that could influence a composer’s choice of instrumental combinations. These might include the ability of instruments to “blend” with each other; the distinctiveness or novelty of different instrumental colors; the suitability of some instruments to be more noticeable than others; or the availability of especially talented performers on a given instrument. However, if you think that the most important factor influencing a composer’s choice of instruments is simply the loudness of the instrument, then start with loudness.\nBefore you begin any study, you will know that many factors influence a phenomenon. Do not start by trying to deal with all factors simultaneously. This will lead you to large volumes of uninterpretable results. Instead, start with what you think is the most important factor first. First things first. Then move on to what you think might be the next most important factor.\n\n\n\nThere is another reason why we should avoid beginning with complex theories. Complex theories are commonly difficult to test. Because of the proliferation of variables in a complex theory, and because the relationships between the variables are usually not fixed, most complex theories can be used to account for any set of observations.\nBy way of illustration, imagine two nineteenth-century British agronomists talking about crop varieties A and B. The first agronomist advocates a complex theory that variety A is superior to variety B, but only when there is high sunshine, low rainfall, and low soil acidity. So why, asks the second agronomist, did variety B in field #3 in 1893 do better than variety A in field #18 in 1887? Ah, that’s because field #18 has a higher elevation and too much fertilizer was applied. The second agronomist then recalls another case: If variety A is sensitive to too much fertilizer, how do you explain the high yield in field #18 in 1883 when the same amount of fertilizer was applied? Ah, that’s because the crop was planted earlier in the season and the fertilizer was administered twice rather than all-at-once. In short, the first agronomist advocates a theory that variety A is superior to variety B, but only when there is high sunshine, low rainfall, low soil acidity, and moderate fertilizer applied in multiple applications planted early in the season at lower elevations.\nAs a theory becomes more complex, it allows more “weasel room.” Nearly any set of observations can be seemingly explained using a suitably complex theory. Despite the appearance of usefulness, such theories are actually useless. Only by narrowing the theory, adding restrictive conditions, and limiting interactions, do we allow the world to tell us that we’re wrong.\nOccasionally you’ll hear someone praise a theory because it can’t fail: the theory accounts for all possibilities. Remember, it’s not good research if you don’t invite failure. When we have an unfalsifiable theory, we are no longer inviting the world to tell us that our theory is wrong. That is, once we have an unfalsifiable theory, we have stopped doing research.\nSimple theories are dangerous because people might erroneously believe them to be true — even though they don’t tell the whole story. Complex theories are dangerous because they are difficult to test, and so it is hard to shake people’s erroneous belief in them when they are wrong. Complex theories are best built through piece-wise additions to a simpler theory. Each addition component to the theory is tested along the way. Later, we will talk about how to formulate complex theories in a way that allows the world to tell us that the theory is wrong.\nOur slogan reminds us to avoid undue complexity:\nSlogan: Don’t try to explain the whole world at once."
  },
  {
    "objectID": "emp_methods_workshop/research_assistant.html",
    "href": "emp_methods_workshop/research_assistant.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Congratulations! You have just won a five-year grant for $200,000. This will allow you to hire a full-time research assistant for five years.\nMake a detailed list describing the background, skills, and knowledge of your ideal research assistant. Consult with others about possibilities, but tailor your list so that it addesses your personal research interests and goals."
  },
  {
    "objectID": "emp_methods_workshop/program3.html",
    "href": "emp_methods_workshop/program3.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "DAY 3 PROGRAM\n\n\nIntroduction to Statistics\n\nMeasurement; Why measure? How to measure anything\nFermi Questions\nDescriptive Statistics\nMeasures of Central Tendency\nMeasures of Variability; Standard deviation\nGroup Task #15: Descriptive Statistics\nInferential Statistics\nProbability, the null hypothesis\nConfidence and significance levels, Statistical tests\nConfidence level and Confidence Interval\nCalculating confidence interval from confidence level\nStatistical Significance\nChi-square test\nTable of critical values for chi-square\nGroup Task #16: Chi-square tests\nGroup Task #17: Funeral marches in F minor: Hypothesis\nCorrelation demonstration\nSpurious correlations\nInterpreting p; “Highly” and “marginally” significant\nMultiple Tests and Related Topics: Bonferroni Correction; File Drawer Effect\nPositive results bias. The case of Ego Depletion (video - 9 minutes)\nConcept of “research registry”\nReporting Statistical Results\nStatistical tests - General remarks\nMake friends with a statistician.\n\nSome Advanced Analytic Techniques\n\nSome advanced statistics (Introduction) Multiple Regression\nCluster analysis\nFactor analysis (and PCA) Multi-dimensional scaling (MDS)\nFoote Novelty Modeling\nHomework review: Review questions and answers for Perttu (2007)\nReview (first 23) slogans\nFeedback Day 3\n\nHomework\n\nReading Guide #3: Shanahan & Huron (2014)\nReading #3: Shanahan & Huron (2014)"
  },
  {
    "objectID": "emp_methods_workshop/preamble.html",
    "href": "emp_methods_workshop/preamble.html",
    "title": "Preamble",
    "section": "",
    "text": "Over the next several days, we’re going to be conveying a lot of information about conducting empirical research.\nIf this workshop were being offered through the philosophy department, we might be stressing the workshop content from the perspective of continental phenomenology or from formal logic.\nIf this workshop were being offered through a department in the sciences, we might be stressing the content from the perspective of practical procedures and norms in scientific method.\nHowever, this workshop is being conducted through a department in the arts and humanities. As you might expect, our preferred approach to the workshop content is inspired by a more traditional humanities perspective.\nThere is, of course, a long history within the arts and humanities of criticism of science, usually in the form of critiques of positivism and scientism. Most of the criticisms I think are justified. However, most of the criticisms also miss the mark because they are criticisms of ideas that were long-ago abandoned in the sciences. For example, humanities scholars have long been critical of positivism — a scientific ideology formulated by August Compte in the early nineteenth century. Positivism is nearly two hundred years old, and its main tenets have little echo in today’s empirical practices. Even logical positivism (associated with philosophers like Alfred Ayer) is almost a hundred years old, and logical positivism had only a modest influence on scientific practice. It’s influence in music was really limited to Milton Babbitt and some of his students at Princeton University.\nThe dominant approach in nearly all modern sciences relies on what’s technically known as the Neyman-Pearson paradigm for inductive research. It bears little resemblance to positivism. And as we will see, it is an approach that sidesteps many of the traditional criticisms found in critical humanities discourse regarding science.\n\n\nAs I’ve noted, in approaching the workshop content, our aim is to employ a more humanities based perspective. In particular, we prefer viewing empirical methods from the perspective of rhetoric and ethics. First, rhetoric. Ultimately, research is about words: it’s about saying something or writing something. It’s about forming an argument or presenting an explanatory story.\nDifferent rhetorical devices or patterns have more or less persuasive power. People find some words more compelling than others, and it’s helpful to understand what makes something persuasive. What we mean by “compelling” is utterances that are likely to change our opinions, attitudes, or beliefs.\nAs humanities scholars have long recognized, many rhetorical devices have questionable value or questionable validity. For example, we know that the words spoken by a man are often perceived to have greater persuasive power than those same words spoken by a woman. A lower-pitched voice sounds more authoritative, and men benefit from having generally lower voices. We know that in many societies (including our own) statements by older speakers carry more persuasive weight than when spoken by younger speakers. Sadly, one’s spoken accent or dialect conveys class information and suggests levels of education (whether warranted or not) that influence the persuasiveness of what a person says. Then there is the question of formal authority: the status as “Herr Doktor Professor” may lend gravitas to my words that may or may not be warranted. We rightly consider these sources of rhetorical power to rely on questionable legitimacy.\nThat’s not to say that all forms of rhetorical persuasion are illegitimate. Some rhetorical patterns have greater legitimacy than others. This is where I depart from certain strains in postmodernism and poststructuralism. There is a great deal of epistemological pessimism, even nihilism, that has taken hold in the arts and humanities because of the influence of postmodernism. What I share with classic postmodern views is a large degree of skepticism. Taken to its logical extreme, skepticism can be turned inward on itself, so we can become skeptical of skepticism. We might rightly be wary of the value of any given narrative. But we can also be wary of being dismissive of all narratives, since some narratives may indeed prove to have value. Not all narratives that cause us to change our opinions, attitudes, or beliefs are illegitimate forms of manipulation or oppression.\nSo one way of thinking about what we’ll be doing this week, is we’ll be learning particular forms of rhetoric that have the potential to lead to statements that have value beyond merely the commanding of power by whoever produces that statement.\nOf course, we’re not going to simply learn a particular way of putting words together. Research involves more than words: it’s also about the experiences, thoughts, and observations that lead to particular words or statements. Our aim is to better understand how we transform experience and contemplation into opinions and perhaps convictions.\n\n\n\nStatements and stories have ethical repercussions. Apart from proper treatment of human participants and research animals, many scientists unfortunately consider the goals of science to be independent of ethics. Specifically, for many scientists, the idea is to tell the truth, whatever the ethical repercussions. We will see that this fairly common view represents a deep misunderstanding of the Neyman-Pearson paradigm. Ethical considerations are built into the very fabric of Neyman-Pearson — they just aren’t obvious to most practicing scientists. They are better recognized by medical researchers and structural engineers whose statements or narratives can lead to injury or death.\nWe will see that the technical concept of the confidence level is much more important than simply establishing whether some observations are deemed “statistically significant.” Establishing a confidence level is much more important than its cavalier pro forma treatment in most sciences suggests. We will see that applying the Neyman-Pearson paradigm offers remarkable benefits to those in the arts and humanities who are especially sensitive to moral and ethical concerns. I would go so far as to suggest that, of all the qualitative and quantiative methods I know, the Neyman-Pearson paradigm offer the most rigorous approach for addressing the moral and ethical dimensions of knowledge-generating enterprises."
  },
  {
    "objectID": "emp_methods_workshop/program2.html",
    "href": "emp_methods_workshop/program2.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "DAY 2 PROGRAM\n\n\nReview and Elaborations\n\nReview slogans (first 16)\nMultiple operationalizations\nGroup Task #7: From question to theory to conjecture to hypothesis to protocol\nFormal observation: The taxi protocol\nThe Experiment\nThe Correlational Study\nCorrelation and causation\nThe third variable problem\n\nSampling\n\nSampling\nGroup Task #8: Sampling Approaches\nWEIRD sampling\nData Independence\nBiased Sampling: The Case of Status Thymicolymphaticus\nGroup Task #9: Sampling Issues; Discussion\nGroup Task #10: What is Random?\nHomework review: Review questions and answers for Lancashire & Hirst (2009)\nSample Size: Law of Big Numbers\nSample Size: Law of Small Numbers\nEffect Size\nRegression-to-the-Mean\nReview (first 19) slogans\nGroup Task #11: Are dynamics asymmetrical?\n\nBehavioral Data\n\nDependent and independent measures\nTypes of behaviors\nImplicit vs. explicit responses\nReactivity\nDemand characteristics\nMeasurement scales\nGroup Task #12: What kind of measurement scale?\nDesigning questionnaires\nGroup Task #13: Questionnaire Design\nReview (first 20) slogans\n\nHomework\n\nReading Guide #2: Perttu (2007)\nReading #2: Perttu (2007)"
  },
  {
    "objectID": "emp_methods_workshop/qmds.html",
    "href": "emp_methods_workshop/qmds.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Multi-Dimensional Scaling\n\nIn simple terms, multi-dimensional scaling amounts to creating a map from a list of distances.\nSuppose we have a list of the distances between major European cities. The distance between London and Berlin is 933 kilometers. Between London and Rome it is 1,435 kilometers. Between Berlin and Rome it is 1,184 kilometers. And so on. A computer program can be used to construct a map, simply using the distances. For example, the program begins by arbitrarily placing one of the cities (say London) on a blank page. With London in the center, it then draws a circle with a radius of 933 kilometers; Berlin lies somewhere on this circle. Similarly, it draws another circle with a radius of 1,435 kilometers; Rome lies somewhere on this circle. It then chooses a point on the first (Berlin) circle, so that it is 1,184 kilometers from a point on the second (Rome) circle. It continues in this way until all of the cities have been plotted.\nThe first thing to notice about this map is that the orientation of the map is undefined. The map could be drawn with “North” in any direction. Moreover, it could also draw the map inside-out. We are used to viewing maps from the “above” perspective — as though the viewer is floating in the sky. However, our resulting map might have been produced as though the viewer were positioned below ground looking up.\nThe second thing to notice about the map is that it will require a slight amount of “stretching.” The distance measures are not exact, so the point representing a given city might be pushed or pulled a little bit. Technically, the amount of stretching is referred to as stress. Each multi-dimensional scaling solution (each map) will be accompanied by a numerical value representing the total stress.\nIf the stress becomes too great, the MDS solution will add an additional dimension. Suppose for example, that our map included major cities of the world, not just European cities. You cannot draw a map of the world in two-dimensions while preserving accurate distances. The MDS solution will “pop” into three dimensions, and we will soon see that the positions of the various world cities are best represented using a three-dimensional sphere.\nWhat’s important is that MDS constructs these “maps” from lists of paired distances. MDS is quite robust — so that the distances don’t have to be exact. Suppose, for example, that we asked you to estimate all of the distances between various major cities? What is the distance between London and Berlin? Perhaps you guess 1,000 kilometers (instead of 933). What is the distance between London and Rome? Perhaps you guess 1,700 kilometers. (instead of 1,435). And so on. Based solely on these distance estimates, MDS will manage to create a remarkably convincing map. Once again, it will do some “stretching” to make things fit. But the map from estimated distances will typically be very close to the map for the real distances — although the “stress” value will be higher.\nNow suppose we use this technique, not to represent physical distances, but psychological distances. By way of example, John Grey (1977) played pairs of tones to listeners (e.g., clarinet and flute, clarinet and trumpet, clarinet and violin, etc.), and asked participants to judge the similarity of the tones on a numerical scale. Using this approach, the average distance between clarinet and flute might be 3.6, whereas the distance between clarinet and xylophone might be 9.1. With enough paired comparisons, Grey could use MDS to portray the sounds in a multi-dimensional space, where similar timbres are close to each other.\nGrey’s tones were mapped in a three-dimensional space. MDS does not “label” the dimensions. That interpretation is left to the researcher. By traversing through the tones along a single dimension, one can hear what changes along that dimension. In this way, Grey was able to interpret one of the dimensions as “brightness.” Sounds at one end tended to be darker, and they got brighter as one moved along the dimension. Grey was able to interpret another dimension as the percussiveness of the sound, and so on. In short, MDS allows the researcher to glimpse the underlying factors used by listeners when judging the similarity of the tones.\n\nIn general, multi-dimensional scaling starts with a matrix of item-item similarities, and then plots the items in some space so as to preserve the similarity relationships. similarity values as input. MDS is an exploratory data analysis method that allows the research to view items of interest in some sort of spatial representation where distances are meaningful.\n\nPitch\nSuppose we played many pairs of pitches and asked people to judge “the similarity” between the pitches. We might provide a scale of 1 to 10, where 1 is very close, and 10 is very distant. A listener might judge the pitches D4 and D#4 to be “quite similar” and give them a value of “2.” Conversely, the pitches C2 and F#6 miths be deemeds very different, and be given a value of “9.”\nOf course, pitches can be “similar” in other ways. For example, listeners might judge the pitches E3 and E4 to sound “quite similar.” As a result, the similarity ratings between E3 and E4 might be quite close — say “1.”\nThis state of affairs might lead to a seemingly odd arrangement. On the one hand E4 might be very similar to D#4; E4 might also be very similar to E5; however, D#4 might be heard as rather dissimilar to E5. Dispite this apparent paradox, multi-dimensional scaling will have no difficulty dealing with the situation.\nThe figure below arises from the MDS solution based on judgments of pitch similarity. Notice that the MDS solution is three dimensional. Two dimensions (circular) relate to what musicians would call pitch-class — what psychologists refer to as pitch chroma. The third dimension relates to the octave — what psychologists refer to as pitch height. It can be seen that in this solution, the pitch E4 is near both D#4 and E5. That is, the two-dimensional solution captures both of the ways in which pitches can be similar.\n\n\n\nMedieval Modes\nHuron and Veltman (2006) made use of MDS in their study of medieval modes. They assembled a quota sample of Gregorian chants in the eight classic modes: Dorian, hypodorian, phrygian, hypophrygian, lydian, hypolydian, mixolydian, and hypomixolydian. For each mode, they tallied the distributions of the various pitches. They then measured the similarity of the distributions between all pairs of modes: dorian/hypodorian, dorian/phrygian, etc. These distances were then used in a multidimensional scaling procedure. The resulting modal “map” is shown below.\n\nThose modes that are closest together in the MDS solution are the most similar in terms of their pitch distributions. For example, the two modes in the lower left of the graph (hypomixolydian and phrygian) turn out to be statistically indistinguishable.\nIn the 11th century, a music scholar named Johannes Cotto described the modal system from both theoretical and applied perspectives. Although the theory distinguishes eight separate modes, Cotto suggested that there are only really four modes. These are illustrated in the graph by the circles. Cotto deemed hypdorian and mixolydian as truly separate modes. But the remaining modes group into two practical modes. One group consists of dorian, hypolydian, and hypophrygian. A second group consists of phrygian, lydian, and hypmixolydian. The latter modes all share the pitch C as the tenor tone and appear on the left side of the map. The former modes all share the pitch A as the tenor.\nDimensions 1 and 2 are drawn to scale making it easier to see that Dimension 1 accounts for a greater proportion of the variance. Note the large empty region separating the left and right mode clusters. Dimension 1 suggests a nascent major/minor bifurcation. Although the musical practice represented in the sampled music pre-dates the emergence of major/minor tonality, the results suggest that the possibility for confusion between the modes is already evident in late medieval music, and that the preeminent polarity of confusion is consistent with the later major/minor distinction.\n\n\nReferences\nJohn Grey (1977). Multidimensional perceptual scaling of musical timbres. Journal of the Acoustical Society of America Vol. 61, No. 5, pp. 1270-77.\nDavid Huron and Joshua Veltman (2006). A cognitive approach to Medieval mode: Evidence for an historical antecedent to the major/minor system. Empirical Musicology Review, Vol. 1, No. 1, pp. 35-55.\nCarol Krumhansl (1990). The Cognitive Foundations of Musical Pitch. Oxford: Oxford University Press."
  },
  {
    "objectID": "emp_methods_workshop/dynamics2.html",
    "href": "emp_methods_workshop/dynamics2.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "NAME: __________________________________________________\nComposer\nWork\nHairpin Crescendo length\nHairpin Diminuendo length"
  },
  {
    "objectID": "emp_methods_workshop/sampling_problems.html",
    "href": "emp_methods_workshop/sampling_problems.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "For each study, identify one or more issues that may potentially cause the sample to be biased.\n\nAn orchestra manager is eager to better understand the likes and dislikes of the orchestra’s audience. She distibutes a survey to the first 50 people who arrive at the orchestra’s gala December matinee concert.\nA researcher is interested in which local radio stations are most listened to by people when driving. The researcher makes a deal with ten local garages. Whenever a car is brought into the shop, the mechanic first turns on the car radio and records the station to which the radio is tuned. After a month of collecting data, the researcher has over two thousand observations.\nA researcher is interested in voice-leading practices. He encodes the complete fugues from the Bach Well-Tempered Clavier and writes software to test various hypotheses in the musical organization.\nA researcher wants to sample opinions from people attending a nightclub. The quietest places in the club to conduct verbal interviews are the restrooms. Two male and two female research assistants are stationed inside the restrooms where they approach everyone who is about to leave the restroom to answer a couple of quick questions.\nFelix Mendelssohn wrote thousands of letters over the course of his life. A historical musicologist conjectures that Mendelssohn became increasingly religious towards the end of his life. From a complete list of letters, the researcher randomly selects 200. Each letter is read by the researcher and coded (yes/no) for whether it contains any spiritual, biblical or religious allusion or content.\nA researcher is interested in learning how much musicians earn and their various sources of income. The researcher succeeds in getting the e-mail list for all of the members of the American Federation of Musicians who have provided the AFM with an e-mail contact. An e-mail is sent to members soliciting participation. The e-mail contains a link to the survey’s website.\nIn the early 1980s, Christa Hansen was eager to test whether non-Westerners also show evidence of statistical learning for pitch relationships. Her aim was to recruit listeners who were not familiar with Western musical culture. Along with her husband, Putra, Hansen set out on a small motorcycle toward the remote northeastern region of the island of Bali. Whenever they met someone they asked two questions: “What is the most isolated village in this area? And does it have a gamelan?” Having reached this village, they again asked the same questions, and so continued on their quest. When they were no longer able to use their motorcycle, a footpath took them to a remote village in the shadow of the Gunung Agung volcano. Hansen concluded that she had reached a truly isolated place when the villagers surmised that this fair-skinned stranger must be Javanese. Recruiting twenty-seven participants from this village, Hansen was able to collect data for her experiment.\nSandra Trehub and her colleagues regularly carry out musical studies with newborn infants. From local-area hospitals, they are able to get contact information with names and telephone numbers of mothers who have recently given birth in the various hospitals. They recruit participants by phoning the mothers, describing the basic aims of their experiments, and asking whether they would be willing to participate in an experiment with their new infant. A small sum (roughly $20) is paid to volunteers. Since the research lab is located in a suburban area, there is ample parking. There is also reasonable bus service."
  },
  {
    "objectID": "emp_methods_workshop/sampling_haphazard.html",
    "href": "emp_methods_workshop/sampling_haphazard.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "The following historical incident offers a vivid illustration of the dangers of biased sampling. In this case, failing to identify a representative sample ultimately led to the death of thousands of people who were otherwise totally heathy.\nA second lesson from this history is how women researchers tend to be ignored. If people had paid attention to the research of a single woman scientist (Edith Boyd, way back in 1927) thousands of lives would have been saved. The prejudices we develop in our ordinary lives carry over into our research.\n\n\nFor nearly two centuries, pediatricians have been aware of the seemingly random phenomenon of crib death. A perfectly healthy baby is put to bed, and the next morning is tragically found to have died. Today, the phenomenon is know as sudden infant death syndrome (SIDS). Modern parents are terrified when they hear stories of these apparently haphazard deaths. In recent decades, some progress has been made in identify the cause of SIDS. In late pregnancy, a developing fetus may sometimes suffers an oxygen deficit that damages cells in the middle and lower brain stem. These cells regulate respiration. After birth, this preexisting brain damage predisposes the infant to sudden respiratory arrest, leading to death (Paterson, et al., 2006).\nUntil the 1990s, however, pediatricians and medical researchers had no idea what was causing SIDS. Parents were frantic to understand (and prevent) this enigmatic and horrible death.\nToward the end of the nineteenth century, an Austrian pathologist named Richard Paltauf made a sustained effort to unravel the mystery. Paltauf carried out meticulous detailed autopsies of infants who had died from SIDS. He systematically compared these autopsies with autopsies of infants who had not died of SIDS. It didn’t take him long before he discovered a smoking gun. In nearly every case of SIDS, Paltauf found that the thymus glands of SIDS infants were much larger than those for the non-SIDS infants.\nThe thymus glands are located on either side of the trachea or windpipe. Paltauf theorized that the enlarged thymus glands were pressing against the windpipe, reducing the airflow, and endangering the infant’s breathing. SIDS infants were dying, he proposed, because of suffocation induced from excessively large thymuses. A few years later, Escherich (n.d.) gave the proposed syndrome the name status thymico-lymphaticus.\nPaltauf’s theory of SIDS was quickly adopted and the search was on for an effective treatment. If a physician detects an enlarged thymus, what can be done? An exciting new medical technology available in the early 20th century was the X-ray machine. At the time, it was well-known that exposure to X-rays would often cause tissues to shrink—and indeed, this was the case with the thymus. Mainstream pediatric textbooks prescribed a standard preventive treatment: in order to prevent SIDS, an infant’s thymus should be shrunk by irradiating the throat with X-rays (Lucas, 1927). This preventive treatment became widespread and continued into the 1950s.\nOf course, X-rays are also dangerous. They can cause cell mutation, leading to cancer. The thyroid gland (also located in the throat) is especially sensitive to X-rays. Over the decades, what was intended to be a preventive treatment led to tens of thousands of cases of thyroid cancer, and ultimately led to thousands of premature deaths. In the end, the treatment proved worse than the disease.\nBut what then of Paltauf’s theory of the enlarged thymuses? Recall that Paltauf compared autopsies of SIDS infants with autopsies on infants who had died from other causes. As you might imagine, not all parents agree to have their infant autopsied after death. Throughout history, anatomists have had difficulty accessing bodies to study. If your infant died under mysterious circumstances, you might well agree to permit an autopsy. But what if your infant died for perfectly obvious reasons? Many parents would be much less likely to agree to an autopsy.\nAn exception happens in the case of poor people. If a medical researcher offers to pay for the cost of a funeral (and perhaps other costs), poor parents are much more likely to permit an autopsy. This means that most of the autopsies and anatomical studies that are carried out are done on people from impoverished backgrounds. As it turns out, poor people are also likely to be less healthy, and to suffer more from various forms of stress.\nWhen Paltauf carried out his autopsies, he didn’t realize that there was a confound between the SIDS infants and the non-SIDS infants. Non-SIDS infants were more likely to come from impoverished families. Most of these non-SIDS infants had died from chronic illnesses that had caused a certain degree of “wasting.” One of the consequences of chronic stress is that the thymus glands tend to shrink. By contrast, the SIDS infants had died suddenly—rather than following a prolonged illness. When Paltauf examined the SIDS infants, he was one of the first pathologists to observe normal infant anatomy. It was not the case that the thymuses of SIDS infants were enlarged. Instead, the thymuses of non-SIDS infants were smaller than normal. In other words, it was not the case the SIDS infants had larger than normal thymuses. Rather, the stressed infants from impoverished backgrounds had abnormally small thymuses.\nThis unfortunate history could have turned out differently if researchers had paid attention to a 1927 publication by pathologist Edith Boyd. Boyd had also carried out a number of autopsies of infants who died under a number of circumstances. Boyd found that infants suffering from malnutrition had small thymuses. By comparison, she found that infants who died due to accidents had large thymuses. She reasoned that the thymuses of infants who died due to accidents were likely to be normal in size. To Boyd, it didn’t make sense to suppose all of the children who died in automobile accidents (for example) were also “suffering” from status thymicolymphaticus. She suggested that Paltauf’s theory of SIDS was wrong, and that the difference in the sizes of the thymuses was an artifact of thinking that impoverished infants had normal thymuses. Alas, the medical community paid no attention to Boyd’s work. It wasn’t until 1945 that a pediatric textbook (Nelson, 1945) repudiated Paltauf’s theory, and admitted that treatment by irradition had been a tragic mistake. Even then, X-ray treatments continued into the 1950s.\n\n\n\nEdith Boyd (1927). Growth of the thymus, Its relation to Status Thymicolymphaticus and Thymic Symptoms. American Journal of Diseases of Children, Vol. 33, pp. 867.\nT. Escherich (n.d.). Status thymico-lymphaticus, Berlin klin,” Woeschesucher, no. 29.\nW. Lucas (1927). Modern Practise of Pediatrics. New York: Macmillan.\nW. Nelson (1945). Nelson’s Texbook of Pediatrics, 4th ed. Philadelphia: Saunders.\nPaterson, D.S., Trachtenberg, F.L., Thompson, E.G., et al. (2006). Multiple serotonergic brainstem abnormalities in sudden infant death syndrome. Journal of the American Medical Association, Vol. 296, No. 17, pp. 2124-32.\nA. Paltauf (n.d.). Plotzlicher Thymus Tod, Wiener klin. Woeschesucher (Berlin), nos. 46 and 9.\nRobert Saplosky (1997). “Poverty’s Remains” Reprinted in Saplosky’s The Trouble with Testosterone New York: Touchstone, 1997."
  },
  {
    "objectID": "emp_methods_workshop/converge.html",
    "href": "emp_methods_workshop/converge.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Informally, we feel that the way to establish greater confidence regarding some claim is to increase the number of supporting examples or observations. If a music scholar observes that an Italian sixth chord occurs in measure 58 in a work by Antonio Vivaldi, it seems unreasonable to presume that this single observation is adequate evidence supporting the general claim that Italian sixths occur more frequently in music by Italian composers. We might feel more confident if the scholar showed that Italian sixth chords occur more frequently than French or German sixths in (say) 100 works by Vivaldi. We might feel more confident yet if the scholar showed that Italian sixths predominated over other augmented sixth chords in 1,000 scores by (say) thirty Italian composers. And we might feel even more confident, if the scholar also showed that a similar preponderance of Italian sixths did not occur in a large selection of works by composers of French, German, and other nationalities.\nThis hypothetical research project notwithstanding, unfortunately, merely increasing the number of pertinent observations doesn’t always justify an increased confidence that some claim is likely to be true. Once again, the problem was identified by the eighteenth-century philosopher, David Hume. Hume was deeply impressed by the science of his day. He was impressed that disciplined observation could lead to fundamental insights about the organization of the natural world. But Hume was also troubled by the philosophical problem of how knowledge might be acquired through observation. He pointed out that, unlike the deductive methods used in logic and mathematics, inductive observation-based methods are inherently uncertain. Hume recognized that no amount of observation could ever prove a particular general claim.\nSince the eighteenth century, Hume’s worst fears about observation-based knowledge have been amply manifested by science itself. Consider, for example, the case of Newtonian physics. Throughout the nineteenth century, hundreds of physicists carried out thousands of experiments and recorded hundreds of thousands of observations that were consistent with Isaac Newton’s formulation that applying a constant force will result in a constant acceleration. In the twentieth century, Albert Einstein famously proposed a theory that contradicted Newton’s simpler view. Einstein proposed that, near the speed of light, acceleration is no longer directly proportional to force. Subsequent experiments produced results that were consistent with Einstein’s theory. However, the principal philosophical lesson arising from these experiments was not that Einstein was right; the more astonishing finding was that Newton was wrong. Note that a nineteenth-century physicist, aware of the wealth of observations consistent with Newton’s theory, would not have been justified in believing that Newton’s generalization had been “proved” or was “true.”\nGiven this history, one might be tempted to entertain two (fallacious) conclusions. One conclusion might be that increasing the number of pertinent observations has no bearing on establishing truth. A more sweeping conclusion might be that observation itself is irrelevant to establishing truth. If the volume of evidence is irrelevant, then what do we say to our hypothetical Vivaldi scholar? Would we wish to concur with the scholar who presumes that the presence of an Italian sixth chord in measure 58 of a work by Vivaldi establishes the general claim that Italian sixths occur more frequently in music by Italian composers? Moreover, would we wish to claim that no amount of evidence could ever resolve whether Italian composers favour Italian sixths? Our intuition that many observations are better than few observations is warranted, but not necessarily in the way we think.\nAs we have seen in the case of Newton, increasing the number of observations can, by itself, lead to a false sense of security about a hypothesis. A more important consideration is that the examples or observations come from a variety of different sources. The essence of converging evidence is that independent observations made under a number of circumstances, using contrasting measurement methods, by several different observers, are all consistent with one particular interpretation. In this chapter, we consider more formally some of the conditions that give researchers greater confidence in a result.\n[]{#Converging Evidence in Music}"
  },
  {
    "objectID": "emp_methods_workshop/converge.html#converging-evidence-in-music",
    "href": "emp_methods_workshop/converge.html#converging-evidence-in-music",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "Converging Evidence in Music",
    "text": "Converging Evidence in Music\n[]{#Introspective Data}\n\nIntrospective Data\nThere are innumerable sources of potential evidence in music-related research. Perhaps the first source is the scholar’s own subjective or phenomenal experience. In our everyday activities, we form informal intuitions and conjectures about the musical phenomena we observe. Our intuitions might relate to listening, performing, conducting, composing, imaging music, dancing, watching a film, or other activities. Of course introspection is a fallible source of information since we are not always privy to the inner workings of our own minds. Self-deception is common. Nevertheless, a scholar may feel some greater assurance if experimental or other sources of evidence reflect her or his subjective intuitions about what may be happening.\nSince people are not all alike, a researcher might seek wider phenomenological support by soliciting introspective reports from others. The value of these reports will be enhanced if a formal survey is conducted and efforts are made to collect introspective accounts from a wide variety of individuals. In particular, it can be valuable to seek introspective reports from people who are not our friends: our friends are more likely to think as we do (which may be one of the reasons they are our friends). Since world-views are related to background, the prudent scholar will solicit accounts from people of disparate and contrasting backgrounds.\nOf course, more formal empirical evidence can be gathered through an experimental approach, an approach that allows greater rigor and often greater reliability. Since there are many ways to carry out an experiment, contrasting sources of evidence can arise from the innumerable variations in the type of stimuli used, the choice of tasks performed, the kinds of response data gathered, and the variety of people who participate.\n[]{#Types of Stimuli}\n\n\nTypes of Stimuli\nConsider the listening experiment. In the first instance, the experimenter has considerable latitude in the choice of sound stimuli. The stimuli might be highly simplified and rigorously controlled, such as the sounds commonly used in psychoacoustic experiments. Alternatively, the sounds might be highly controlled, yet more musical in character — such as sounds produced by a computer or MIDI synthesizer with known properties. A third alternative is to use commercial recordings of actual performances; sound recordings have a higher degree of “ecological validity,” yet still provide the opportunity for repeated use of identical stimuli. A fourth approach might abandon the use of identical stimuli by relying on several musical sources — possibly including live performance.\nConverging evidence arises when experiments employing each of these types of stimuli lead to the same conclusion. For example, increasing loudness appears to evoke a heightened physiological arousal whether the stimulus is a simple sine tone with increasing amplitude, a computer-performed melody, or the crescendo in a recorded performance of a Beethoven piano sonata.\n[]{#Types of Responses}\n\n\nTypes of Responses\nApart from the type of stimuli used, converging evidence can also be sought by varying the type of responses observed in a listening situation. A researcher might measure systemic (whole body) physiological responses such as heart-rate, breathing, perspiration, etc. Alternatively, physiological measures might focus on changes in the central nervous system. The researcher might use brain imaging techniques such as PET or fMRI, or use electroencephalographic methods. Alternatively, the researcher might record behavioral responses, such as verbal responses to questions. For example, a listener might be asked to predict the next note in a sequence. The listener’s response might be active (such as singing the next note) or passive (such as assigning a numerical rating identifying how well a presented note fits the current context). A listener might be asked to make judgments, such as the “appropriateness” or “pleasantness” of particular events or passages.\n[]{#Performance Tasks}\n\n\nPerformance Tasks\nCompared with listening tasks, the behaviors of performers provide an especially contrasting source of converging evidence in music research. Performance data may be gathered by formal or informal interviewing of performers, conductors, or teachers. Such individuals might be asked to introspect, to imagine performing, to provide a retrospective report following an actual performance, or to provide a running commentary while engaged in a task such as rehearsing. The researcher might interrupt a performance to pose specific questions or solicit information at particular moments. Apart from collecting verbal responses, the research might entail gathering data through monitoring instruments directly attached to the performer — or less directly through high-speed film or through 3-D kinematic data acquisition systems. Another approach is to collect data by monitoring the musical instrument rather than the performer, such as in the case of recording MIDI data. Alternatively, a music scholar might measure performance behaviors from sound recordings — although this approach makes it nearly impossible to infer the performer’s physical movements. Sound recordings are especially useful when it is difficult to recruit professional performers as research collaborators or participants. In all of the above cases, the performance data may be gathered either in a laboratory setting or in a concert or performance setting.\nThe tasks assigned to a performer may be varied, from recording a well-rehearsed performance, to recording the rehearsal process itself. The musician(s) may be asked to perform under certain constraints, such as playing at a specified tempo or playing without visual or auditory interactions with other musicians. Mechanical or electronic constraints might be introduced, for example, to eliminate the performer’s control over loudness. Other tasks include examining the performer’s reactions to experimenter-generated perturbations. For example, the experimenter might abruptly introduce an unexpected chord change to see how a jazz improviser adapts. Performers might be vocalists or instrumentalists, children or adults, male or female, novice or professional, classical or popular, Western or non-western, etc.\n[]{#Compositional Tasks}\n\n\nCompositional Tasks\nA further source of possible converging evidence can be sought in studies of compositional activities. One approach entails simple observation of a composer engaged in writing a piece. The sequence of compositional actions might be recorded and subjected to protocol analyses; a database of compositional actions might be used to test hypotheses about the nature of composing. Alternatively, the composer might be assigned specific tasks, such as writing a passage that conforms to a predefined set of restrictions.\n[]{#Notation as Data}\n\n\nNotation as Data\nAs music theorists have long understood, one of the best sources of musical information is to be found in notated scores. Musical scores provide significant opportunities for testing hypotheses about musical organization, including tests of psychophysical phenomena. Measurements can be as primitive as simple interval counts, or as complex as Schenkerian analyses. Different styles, periods, instrumentation, etc. provide potentially contrasting repertories for seeking converging evidence for some general claim. Of course not all musically pertinent information is available in musical scores, but this criticism also applies to all other forms of musical information as well. When the analytic method involves considerable interpretation (such as in Schenkerian analysis), the reliability of this information can be determined by engaging many theorists to analyse independently the same works.\n[]{#Text as Data}\n\n\nText as Data\nAn often overlooked source of empirical data about music is historical and theoretical writings about music. Of course musical writings have been the most important focus for philosophically- and historically-oriented music scholarship. But these same sources can be used for empirical studies. Although this approach has been uncommon in music scholarship, it has been used by sociologists, anthropologists, and empirical historians. Historical texts, and the ideas they contain can be the subject of statistical approaches. For example, music reviews published in newspapers might be tabulated to estimate the point where a musician’s popularity begins to decline. Similarly, text-based measures might include counting the number of times idea X is espoused compared to idea Y in musical treatises. (Such measures might provide a rough index of the relative importance or salience of an idea or concept in different historical periods, for example.)\n[]{#Modeling and Simulation}\n\n\nModeling and Simulation\nAnother source of potential converging evidence is modeling and simulation. Computer models and simulations have been created for a wide variety of musical purposes; not all models are suitable as sources of converging evidence. In some instances, computer models have been built to emulate aspects of music, from pitch perception to instrument/performer interactions. In the more rigorous studies, scholars have validated their models by carrying out experiments that test predictions arising from the models. For example, a model of melodic expectation might predict that a novel pitch sequence will tend to evoke a specific expectation. This expectation can then be tested through listening experiments. If a model has an established record for producing accurate predictions, then a model-derived prediction might also be considered evidence that converges with other research, even if the specific model prediction has not been tested.\n[]{#Cross-Disciplinary Convergence}\n\n\nCross-Disciplinary Convergence\nA particularly powerful source of converging evidence is to be found in cross-disciplinary convergence. Many phenomena (including music) can be studied from a variety of disciplinary viewpoints — such as from biological, psychological, archeological, sociological, technological, economic, historical, religious, business, entertainment, and other approaches. From time to time, scholars working in different disciplines will independently arrive at the same conclusion, often by using unrelated and even contrasting methods.\nIn the field of music, cross-disciplinary convergence has sometimes occurred between music history and art history, between musicology and sociology, and between music theory and psychology. Cross-disciplinary convergence is especially compelling when the work is carried out independently; that is, when the work of a researcher in one field proceeds without knowledge of the results in another field. Contrary to normal intuition, the research is often strengthened when scholars in different disciplines pursue their work in ignorance of the parallel work going on in another field.\nAn especially good (though non-musical) example of cross-disciplinary convergence is evident in studies of the origin of native peoples in the Americas. Geologists have used standard geological techniques to estimate periods when lowered sea-levels facilitated the movement of people across the Bering Strait between Asia and America. Working independently, linguists have established three language groups among native American peoples, and have suggested times when the ancestral peoples diverged from a single hypothetical Asian source. Finally, population geneticists, working independently have also identified three genetic stocks among native Americans, and have estimated when each of these groups became isolated from their Asian ancestors. The geological, linguistic, and genetic research converge on three periods of Asian emigration to the American continents coinciding with three periods of lowered sea-levels (see Cavalli-Sforza & Cavalli-Sforza, 1995 for a useful review). Evidence from one discipline alone leaves considerable uncertainty about the validity of the results. However, the convergence of all three disciplines greatly increases the credibility of their joint estimates.\n[]{#Cross-Cultural Convergence}\n\n\nCross-Cultural Convergence\nAnother powerful source of converging evidence for an idea is how that idea fares when compared cross-culturally. If a scholar claims that a particular phenomenon is “universal,” then we ought to see evidence of such a phenomenon in a wide sample of contrasting cultures. Conversely, if a scholar claims that a particular feature is distinctive of a given culture (or composer, or style, etc.), then that feature should not be evident in other cultures (other composers, other styles, etc.). In other words, cross-cultural comparisons provide particularly challenging tests for both the generality and specificity of musical hypotheses.\nAll of the possible sources of evidence discussed above (types of responses, performance tasks, modeling and simulation, etc.) can be reexamined by comparing and contrasting parallel results from the plethora of cultures and subcultures that exist in the world. For each culture, one may address anew the listening issues, the performance issues, the types of musical materials studied, as well as the indigenous accounts of music-related activities and experiences.\n\n\n\nSummary\nTable 1 provides a summary overview of the types of sources of converging evidence we have described above. The table is intended to be suggestive rather than exhaustive.\n\nTable 1\nApproach\nTask\nMaterials\nResponses\nParticipants\nPhenomenological\nintrospection while listening\nvarious\nverbal/written account\nresearcher/others\nintrospection while performing\nvarious\nverbal account\nresearcher/others\nintrospection while composing\nvarious\nverbal account\nresearcher/others\nformal survey\nvarious\nverbal/written\nexpert/novice\nExperimental\nlistening\nsimple controlled stimuli\nphysiological/systemic (whole body)\nexpert/novice\nphysiological/central nervous system\nexpert/novice\nbehavioral/active (e.g. singing)\nbehavioral/passive (e.g. judging)\nlistening\nsimple music-like stimuli\nexpert/novice\nlistening\nrecorded music\nexpert/novice\nlistening\nlive music\nexpert/novice\nrehearsed performance\nvarious styles, etc.\nkinematic\nexpert/novice\nin concert/laboratory\nexpert/novice\n(EMG, film, MIDI\nexpert/novice\ndata, sound recording)\npractising\nnovice or expert materials\n”\n”\nimprovising\nnovice or expert materials\n”\n”\nadaptive performing\nnovice or expert materials\n”\n”\ncomposing\nopen-ended\nprotocol records\nexpert/novice\ncomposing\nconstrained writing\nSound Analysis\nrecorded performance\nsound recordings\ntiming, dynamics, etc.\nprofessional/experts\nNotational Analysis\nstatistical counts\ndifferent styles,\nnot applicable\ncomposers/arrangers\nperiods, instrumentation, etc.\nanalytic procedures\nscores\nwritten analyses\ntheorists\nText Analysis\nstatistical counts\nconcepts, ideas\nwriters on music\nhistory of ideas\nconcepts, ideas\nwriters on music\nModeling and Simulation\nall of above\nall of above\nall of above\nnot applicable\nCross-disciplinary\nall of above\nall of above\nall of above\nall cultures: novices & experts\n[]{#Case Example: Pitch Proximity}\n\n\n\nA Case Example: Pitch Proximity\nThere are relatively few musical phenomena that have been subjected to extensive research making use of converging evidence. In no case has there been an exhaustive study that includes all of the variations identified above. Nevertheless, it can be instructive to consider a phenomenon that has received considerable research attention. A useful illustrative example can be found in the case of pitch proximity — the tendency for musical “lines” (such as melodies) to be constructed predominantly of small pitch intervals.\nThe observation that musical melodies tend to be constructed using relatively small intervals can be found throughout historical writings on music (e.g. Zarlino, 1573; Berardi, 1687; Rameau, 1722). In more modern times, formal surveys by several scholars are all consistent with the preponderance of small intervals in melodic construction, including Ortmann (1926), Merriam, Whinery and Fred (1956), and Dowling (1967). My own work ( Huron, 1992) found a preponderance of small intervals in music from a number of cultures, including American, Chinese, English, German, Hasidic, Japanese, and sub-Saharan African (Pondo, Venda, Xhosa, and Zulu).\nIn 1950, Miller and Heise observed that alternating pitches (such as trills) produce two different perceptual effects depending on the pitch distance separating the tones and their speed of alternation (Miller & Heise, 1950). When the tones are close with respect to pitch, quick alternations evoke a sort of “undulating effect,” like a single wavering line. However, when the pitch separation is larger, the perceptual effect becomes one of two “beeping” tones of static pitch. Musicians recognize this phenomenon as that of pseudo-polyphony or compound melodic line, where a single sequence of pitches nevertheless evokes a sort of “yodeling” effect.\nMiller and Heise’s observations were replicated and extended by a number of researchers who carried out similar listening experiments. This includes experiments by Petter (1957), Bozzi and Vicario (1960) and Vicario (1960), Schouten (1962), Norman (1967), Dowling (1967), van Noorden (1971a, 1971b), and Bregman and Campbell (1971). Several of these researchers worked entirely independently, without knowledge of previously existing work.\nIn 1975 van Noorden mapped the relationship between tempo and pitch separation on perceptual integration and segregation. When the tempo is slow and/or the pitches have close proximity, the resulting sequence is always perceived as a single musical line or stream. Conversely, when the pitch distances are large and/or the tempo is fast, two lines or streams are always perceived. Van Noorden also identified an intervening grey-region, where listeners may hear either one or two streams depending on the context and the listener’s disposition.\nThe importance of pitch proximity in stream organization is supported by a number of additional experiments. Schouten (1962) observed that rhythmic or temporal relationships are more accurately perceived within lines than between lines. This finding was replicated and extended by Norman (1967), Bregman and Campbell (1971), and Fitzgibbons, Pollatsek and Thomas (1974).\nIn addition, Bregman and his colleagues have demonstrated that pitch proximity is more important than the Gestalt principal of “good continuation” when listening to pitch trajectories. For example, when some pitches in a sequence are completely masked by noise, listeners are more likely to “hear” continuations that involve simple pitch interpolations rather than extrapolating a coherent trajectory or pattern ( Steiger & Bregman, 1981; Tougas and Bregman, 1985; Ciocca & Bregman, 1987; summarized by Bregman, 1990; pp.417-442). Similar effects have been observed in speech perception ( Darwin and Gardner, 1986; Pattison, Gardner & Darwin, 1986).\nFurther experimental work has demonstrated the perceptual difficulty of tracking lines that cross with respect to pitch. Using pairs of well-known melodies, Dowling (1973) carried out experiments in which successive notes of the two melodies were interleaved. The first note of melody A' was followed by the first note of melodyB,’ followed by the second note of melody A' followed by the second note of melodyB’ etc. Dowling found that the ability of listeners to identify the melodies was highly sensitive to the pitch overlap of the two melodies. Dowling found that as the melodies were transposed so that their average pitches diverged, recognition scores increased. The greatest increase in recognition scores occurred when the transpositions removed all pitch overlap between the concurrent melodies.\nDeutsch (1975) and van Noorden (1975) found that concurrent ascending and descending tone sequences are perceived to switch direction at the point where their trajectories cross. That is, listeners tend to hear two converging glissandos as “bouncing” off each other rather than passing through each other (i.e. crossing). Although the crossed trajectories represents a simpler Gestalt figure, the bounced perception is much more common.\nIn summary, at least four empirical phenomena point to the importance of pitch proximity in helping to determine the perceptual sense of a line of sound: (1) the breaking-apart of monophonic pitch sequences into pseudo-polyphonic percepts described by Miller and Heise and others, (2) the discovery of information-processing degradations in cross-stream temporal tasks — as found by Schouten, Norman, Bregman and Campbell, and Fitzgibbons, Pollatsek, and Thomas, (3) the perceptual difficulty of tracking auditory lines that cross with respect to pitch described by Dowling, Deutsch, and van Noorden, and (4) the pre-eminence of pitch proximity over pitch trajectory in the continuation of lines demonstrated by Bregman et al. The segregation or integration of pitch sequences thus appears to depend upon the proximity of successive pitches.\nApart from the positive evidence for pitch proximity, there is also negative supporting evidence. Musicians recognize that the pitch interval between successive notes also has a significant impact in creating compound melodic lines, such as in yodeling. Dowling (1967) carried out a study of a number of Baroque solo works in order to determine the degree to which pseudo-polyphony is correlated with the use of large intervals. In measurements of interval sizes in passages deemed pseudo-polyphonic by two independent listeners, Dowling found the passages to contain intervals markedly larger than the “trill threshold” measured by Miller and Heise. In a sample of pseudo-polyphonic passages by Telemann, Dowling noted that Telemann never uses intervals less than the Miller and Heise trill threshold.\nAs further evidence of the importance of small intervals in non pseudo-polyphonic lines, Dowling examined the interval preferences of listeners. Through a series of randomly-generated stimuli, Dowling found that listeners prefer melodies employing the smaller interval sizes. A study by Carlsen (1981), moreover, demonstrated that musicians show a marked perceptual expectancy for small pitch intervals. Although Carlsen found significant differences between German, Hungarian and American musicians in their melodic expectancies, in all cases neighbouring pitch continuations were strongly favoured.\nFurther evidence has shown that musical practices are consistent with the psychological research pertaining to pitch proximity. This is particularly evident in the case of part-crossing. The crossing of parts with respect to pitch always violates the pitch proximity principle. No matter how the pitches are arranged, the aggregate pitch distance is always lowest when the upper voice consists of the upper pitches, and the lower voice consists of the lower pitches. In a study of part-crossing in polyphonic music, Huron (1991) showed that J.S. Bach avoids part-crossing, and that he becomes most vigilant to avoid part-crossing when the number of concurrent parts is three or more.\nModeling and simulation of monophonic/pseudo-polyphonic perceptions was carried out by Huron (1989). In this work, a computer model was devised based on the extant perceptual research. The model was then used to predict the relative degree of pseudo-polyphony for a wide sample of contrived passages. The model was tested against data collected from five music theorists and was shown to be highly consistent with the theorists. In fact, the model behaved in a manner closer to the “average” theorist than any of the individual participating theorists.\nIn summary, six types of empirical evidence can be identified that seem to provide converging evidence consistent with the preponderance of pitch proximity on melodic organization. One type of evidence is the preponderance of small intervals in most of the world’s melodies observed by Ortmann, Merriam, Whinery, Fred, Dowling, and Huron. A second type of evidence is the reciprocal prevalence of large pitch intervals in pseudo-polyphonic passages found by Dowling. A third type of evidence is the auditory preference for small intervals in melodies found by Dowling. A fourth type of evidence is the perceptual expectation for small intervals in continuations of melodic contours found by Carlsen. A fifth type of evidence is the avoidance of part-crossing in polyphonic music measured by Huron. And a sixth type of evidence is the successful computer modeling of expert judgments of melodic organization.\nThe strengths of this research, include the following highlights:\n\nSeveral specific experiments (such as the Miller and Heise experiment) were directly replicated.\nIn addition, several specific hypotheses were corroborated by different researchers. Of the nearly 30 researchers involved, many were unaware of the work of other scholars, suggesting a high degree of independent observation.\nMusic scholars (such as Ortmann and Merriam) carried out independent lines of research from the experimental psychologists (such as van Noorden and Bregman). The music scholars were motivated by an interest in melodic organization and voice-leading, whereas the psychologists were interested in auditory organization and streaming. The comparative independence of this work provides a degree of cross-disciplinary convergence.\nThe research involved both the use of controlled experiments and descriptive musical surveys.\nThe listening experiments entailed a variety of types of stimuli, including pure tones, complex tones, simple tone sequences, musical passages, and speech.\nThe listeners were asked to perform a number of different tasks, including reporting how “expected” the next pitch is, singing the next pitch in a sequence, judging “how good” a melody sounds, identifying the “best pitch” to fill a melodic gap, and other tasks.\nSimilar results were found in both music and speech.\nThe researchers studied both listener responses and compositional artifacts (notation).\nThe musical repertories studied included vocal and instrumental music from many different cultures. The repertoires included classical, folk, and popular idioms.\nComputer modeling was carried out and the model was validated by comparing model behavior with that of professional music theorists. The operation of the model was based on the established perceptual research.\nFinally, the empirical research was directly linked with explicit claims made in classic music theory tracts. The resulting principles are consistent with traditional advice given in music theory texts spanning centuries.\n\nOf course there is room for improvement. A better case might consider the following:\n\nAlthough the experimental research was carried out in several countries (Britain, Canada, Germany, Hungary, Italy, USA), all of the listeners were Western.\nNo performance-oriented research has been undertaken. For example, an experiment might be carried out to show that melodies improvised by performers also tend to use small pitch intervals.\n\nIt bears emphasizing that no volume of evidence is sufficient to establish the truth of some claim. As we saw in the case of Newtonian mechanics, even ideas that have a long history of converging evidence can fall apart. There remains the possibility that a superior interpretation for pitch proximity may arise.\n[]{#The Importance of Converging Evidence}"
  },
  {
    "objectID": "emp_methods_workshop/converge.html#the-importance-of-converging-evidence",
    "href": "emp_methods_workshop/converge.html#the-importance-of-converging-evidence",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "The Importance of Converging Evidence",
    "text": "The Importance of Converging Evidence\n[]{#Confirmation Bias}\n\nConfirmation Bias\nIt is clear that assembling converging evidence in support of an idea can entail a great deal of work. Is all this work worthwhile? Why do we need converging evidence?\nThe simple answer to these questions is that scholars make mistakes. In medicine, it is relatively rare that a single observation or analysis is assumed to establish a point. Yet, in humanities scholarship — including music research — it is not uncommon for scholars to assume that a single observation or analysis has established something. Failing to further test an idea can have onerous repercussions. Once a concept is accepted, it is often difficult for scholars to abandon it. In a famous series of studies, Peter Wason and Philip Johnson-Laird showed that people are tenacious in holding on to a particular view, even when the contradicting evidence is obvious ( Wason, 1960; Wason & Johnson-Laird, 1972). Observations that coincide with our theories are taken as supporting proof and are given great mental weight. On the other hand, observations that contradict our theories are taken as exceptions and have little mental impact. This psychological phenomenon is known as confirmation bias.\nUnfortunately, the problem does not end with the fact that untenable ideas fail to be abandoned. Since scholars tend to build on the work of previous scholars, this can easily lead to the construction of elaborate intellectual sand castles. Why do we need converging evidence in music research? Because many important ideas in music scholarship are not well established.\n\n\n\nConclusion\nThere are two dangers in inductive research. As we’ve seen, one danger is to assume that something can be “proven” through observation. Over the past three decades, a number of critiques in the philosophy of science have highlighted the problem of induction ( Agassi, 1975; Feyerabend, 1975; Gellner, 1974; Kuhn, 1962/1970; Lakatos, 1970; Quine, 1953; and others). However, a second danger is to assume that inductive knowledge is impossible. The most well-known critiques of inductive reasoning (such as Paul Feyerabend’s Against Method) all use historical examples to make their case. That is, specific historical observations are used to support a general claim about the fallibility of induction. In short, these authors rely on inductive reasoning itself to make the case that inductive reasoning is problematic. If inductive reasoning is problematic, then it follows that we ought to mistrust their conclusions.\nMany music scholars have mistakenly regarded the problem of induction as a fundamental problem in science. However, the problem of induction — drawing general conclusions from specific observations — is not restricted to empirical approaches to music. It is similarly found in post-modernist music scholarship. Consider, by way of example, the following argument from Burr (1995). Burr begins by discussing how attitudes towards children have changed historically:\nWhether one understands the world in terms of men and women, pop music and classical music, urban life and rural life, past and future, etc., depends upon where and when in the world one lives. For example, the notion of childhood has undergone tremendous change over the centuries. What it has been thought `natural’ for children to do has changed, as well as what parents were expected to do for their children (e.g. Aries, 1962). It is only in relatively recent historical times that children have ceased to be simply small adults (in all but their legal rights). And we only have to look as far back as the writings of Dickens to remind ourselves that the idea of children as innocents in need of adult protection is a very recent one indeed. We can see changes even within the timespan of the last fifty years or so, with radical consequences for how parents are advised to bring up their children. This means that all ways of understanding are historically and culturally relative. [pp. 3-4]\nBurr’s argument is inductive. The specific example of changes in attitudes towards children is cited as supporting Burr’s general conclusion that “all ways of understanding are historically and culturally relative.” As Hume pointed out, such forms of reasoning are useful, but unreliable. Notice, moreover, that the argument that I have just made is also inductive. That is, I have claimed, from a single example, that inductive reasoning is ubiquitous. There is no getting away from it: inductive reasoning is commonplace, useful and problematic.\nAt this juncture, the more serious problem is to assume that inductive knowledge is impossible — that we cannot learn from observation. We may be damned if we learn from experience, but we are certainly damned if we don’t learn from experience. Inductive reasoning is a necessary evil, but prudent measures like converging evidence can help to keep the beast at bay.\nAs we have argued, the most reliable knowledge arises when several independent research methods point to the same interpretation. We can be most confident of our knowledge when, no matter how we look at the phenomenon, the same answer is supported.\nIn music research, converging evidence can come from a number of different sources. In laboratory environments, listening experiments may be done using highly controlled stimuli. Alternatively, experiments may be carried out in more natural settings using authentic musical materials. In other cases, physiological evidence (such as cardiograms or electroencephalograms) may be consistent with a particular hypothesis or interpretation. Evidence may also be sought in samples of musical notations or in measurements of performance nuances or sound recordings.\nIntrospective phenomenological accounts might also tend to support one hypothesis over another. Similarly, studies of groups of people in certain social situations may be informative. Historical treatises and other theoretical writings might provide evidence in support of a given account. Finally, cross-cultural evidence may be examined — through experiments, studies of musical materials, performances, improvisations, or indigenous accounts that are either similar or contrasting from culture to culture.\nUnfortunately, relatively few musical phenomena have been investigated from so many points of view. For most phenomena, we must remain skeptical of the preliminary results until further corroborating studies are carried out."
  },
  {
    "objectID": "emp_methods_workshop/converge.html#references",
    "href": "emp_methods_workshop/converge.html#references",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "References",
    "text": "References\nAgassi, Joseph. (1975). Science in Flux. Dordrecht, Holland: D. Reidel Publishing Co.\n\nBerardi, A. (1687/1970). Documenti armonici. Bologna: Giacomo Monti, 1687; reprint edition, Bologna: Forni, 1970.\n []{##Vicario}\nBozzi, P., & Vicario, G. (1960). Due fattori di unificazione fra note musicali: la vicinanza temporale e la vicinanza tonale. Rivista di psicologia, 54 (4), 253-258.\n\nBregman, A. S. (1990). Auditory scene analysis; The perceptual organization of sound. Cambridge, MA: M.I.T. Press.\n[]{#Bregman and Campbell}\nBregman, A. S., & Campbell, J. (1971). Primary auditory stream segregation and perception of order in rapid sequences of tones. Journal of Experimental Psychology, 89 (2), 244-249.\n\nBurr, Vivien (1995). An Introduction to Social Constructionism. London: Routledge.\n\nCarlsen, J. C. (1981). Some factors which influence melodic expectancy. Psychomusicology, 1, 12-29.\n\nCavalli-Sforza, L., & Cavalli-Sforza, F. (1995). The Great Human Diasporas; The History of Diversity and Evolution. Reading, MA: Addison-Wesley.\n\nCiocca, V., & Bregman, A. S. (1987). Perceived continuity of gliding and steady-state tones through interrupting noise. Perception & Psychophysics, 42, 476-484.\n\nDarwin, C. J., & Gardner, R. B. (1986). Mistuning a harmonic of a vowel: Grouping and phase effects on vowel quality. Journal of the Acoustical Society of America, 79, 838-845.\n\nDeutsch, D. (1975). Two-channel listening to musical scales. Journal of the Acoustical Society of America, 57, 1156-1160.\n\nDowling, W. J. (1967). Rhythmic fission and the perceptual organization of tone sequences. Unpublished doctoral dissertation, Harvard University, Cambridge, MA.\n\nDowling, W. J. (1973). The perception of interleaved melodies. Cognitive Psychology, 5, 322-337.\n\nFeyerabend, Paul (1975). Against Method: Outline of an Anarchistic Theory of Knowledge. London: Verso Edition.\n\nFitzgibbons, P. J., Pollatsek, A., & Thomas, I. B. (1974). Detection of temporal gaps within and between perceptual tonal groups. Perception & Psychophysics, 16 (3), 522-528.\n\nGellner, Ernest. (1974). Legitimation of Belief. Cambridge: Cambridge University Press, 1974.\n\nHuron, D. The avoidance of part-crossing in polyphonic music: Perceptual evidence and musical practice. Music Perception, Vol. 9, No. 2 (1991) pp. 135-154.\n\n\nHuron, D. Voice segregation in selected polyphonic keyboard works by Johann Sebastian Bach. Unpublished PhD dissertation, University of Nottingham, UK, 1989.\n\nKuhn, Thomas S. (1962/1970). The Structure of Scientific Revolutions. (2nd edition, 1970) Chicago: University of Chicago.\n\nLakatos, Imre & Musgrave, Alan (eds.) (1970). Criticism and the Growth of Knowledge. Cambridge: Cambridge University Press.\n\nMerriam, A. P., Whinery, S., & Fred, B. G. (1956). Songs of a Rada community in Trinidad. Anthropos, 51, 157-174.\n\nMiller, G. A., & Heise, G. A. (1950). The trill threshold. Journal of the Acoustical Society of America, 22 (5), 637-638.\n[]{#van Noorden71}\nvan Noorden, L. P. A. S. (1971). Discrimination of time intervals bounded by tones of different frequencies. IPO Annual Progress Report, 6, 12-15.\n[]{#van Noorden75}\nvan Noorden, L. P. A. S. (1975). Temporal coherence in the perception of tone sequences. Doctoral dissertation, Technisch Hogeschool Eindhoven; published Eindhoven: Druk vam Voorschoten.\n\nNorman, D. (1967). Temporal confusions and limited capacity processors. Acta Psychologica, 27, 293-297.\n\nOrtmann, O. R. (1926). On the melodic relativity of tones. Princeton, NJ: Psychological Review Company. (Vol. 35, No. 1 of Psychological Monographs.)\n\nPattison, H., Gardner, R. B., & Darwin, C. J. (1986). Effects of acoustical context on perceived vowel quality. Journal of the Acoustical Society of America, 80, Supplement 1.\n\nPetter, G. (1957). Osservazioni sperimentali sulla natura dell effetto tunnel. Rivista di psicologia, 51 (3), 1-15.\n\nQuine, Willard. (1953). From a Logical Point of View. Cambridge, MA: Harvard University Press.\n\nRameau, J.-P. Traite’ de l’harmonie. Paris: Ballard, 1722; trans. by P. Gossett as Treatise on Harmony. New York: Dover, 1971.\n\nSchouten, J. F. (1962). On the perception of sound and speech. Proceedings of the 4th International Congress on Acoustics (Copenhagen), 2, 201-203.\n\nSteiger, H., & Bregman, A. S. (1981). Capturing frequency components of glided tones: Frequency separation, orientation and alignment. Perception & Psychophysics, 30, 425-435.\n\nTougas, Y., & Bregman, A. S. (1985). Crossing of auditory streams. Journal of Experimental Psychology: Human Perception and Performance, 11 (6), 788-798.\n\nWason, P.C. (1960). On the failure to eliminate hypotheses in a conceptual task. Quarterly Journal of Experimental Psychology, 12, 129-140.\n\nWason, P.C. & Johnson-Laird, P.N. (1972). Psychology of Reasoning: Structure and Content. London: Batsford.\n\nZarlino, G. (1573/1965). Le istitutioni harmoniche III. Venice: Francesco Senese, 1573; reprint edition, New York: Broude Brothers, 1965."
  },
  {
    "objectID": "emp_methods_workshop/slogans23.html",
    "href": "emp_methods_workshop/slogans23.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Huron’s Research Slogans\n\n\nMotivated by truth, with no hope of Proof\nThere is no inductive proof. We are not in the business of proving something to be true. We would love to know the truth (if that exists), but we understand that we could never be sure of the truth, even if we had it. The best we can hope for is that what we observe is consistent with our theories.\nThe best research invites failure.\nGive the world an opportunity to tell you that you’re wrong. (This is the essence of good research.)\nWe invite failure by testing predictions.\nTest an idea by making a prediction, and then determine whether the observations are consistent with the prediction.\nWe recognize failure by drawing a line in the sand.\nIn order to make failure obvious, establish a criterion in advance that says, “If the evidence doesn’t cross this line, then I’ll admit failure.” In statistics, the line is referred to as the confidence level.\nAim not to be right, but to be not not right.\nInstead of establishing The Truth, our more modest aim is to be not obviously wrong. When our observations turn out to be consistent with our hypothesis, we don’t claim that we are right; instead the observations suggest that our hypothesis may not be wrong.\nTest hypotheses by operationalizing terms.\nTranslate all of the terms in a hypothesis into concrete things you can measure. We can’t directly measure concepts like “sadness.” We have no choice but to measure things using imperfect rulers.\nOperationalize, but don’t essentialize.\nAll concepts are inherently enigmatic and fuzzy. Terms like “melody,” “listen” or “note” can never be pinned-down. It is impossible to provide comprehensive definitions or grasp the essence of some concept. We are forced to approximate or estimate concepts through operational definitions — but don’t confuse the operational definition with the concept itself, and don’t assume concepts are “real.”\nCompare, compare, compare.\nContrast a “treatment” condition with one or more “control” conditions.\nThe rhetoric of science is the rhetoric of prophecy.\nPeople are most impressed when someone accurately foretells the future. Science is a form of rhetoric whose persuasive power resides in the testing of predictions. The rhetorical power of science comes not from scholars assembling evidence, but from scholars testing predictions.\nHindsight is 20/20.\nMost things seem obvious in retrospect (hindsight bias). When the results aren’t obvious, humans are enormously gifted at coming up with explanatory accounts. We can make up a story for just about any set of data. Post hoc theories don’t have the same plausibility as a priori theories. The true test is making up the story first (i.e., prediction)! Prefer theorizing first, then collect your data.\nReductionism is a method, not a belief.\nWe simplify problems, not because we believe problems to be simple, but because we believe problems to be complex. Restricting our gaze is a useful strategy for discovery.\nDon’t try to explain the whole world at once.\nManipulate one variable at a time. Seek simplicity, even as you distrust it.\nGeneralize, but don’t universalize.\nWhen presenting your results, frame them narrowly rather than broadly.\nAvoid chronic hypothesislessness.\nExploratory and descriptive studies are important, but you can’t invite failure without testing predictions.\nBeware of the post hoc theory.\nThe scholar who only offers theories after looking at the evidence is a scholar who is never wrong. Post hoc theorists don’t allow the world to tell them when their ideas are problematic.\nFrom Question to Theory to Conjecture to Hypothesis to Protocol.\nStart with a question, propose an explanatory theory, derive a conjecture, refine the conjecture into a hypothesis, then operationalize the terms of the hypothesis into a protocol. The protocol provides an action plan for how to carry out the research.\nNo causation without manipulation.\nCausality cannot be inferred unless you manipulate one of the variables. The Experiment is the only type of study in which it is possible to infer causality. Correlational studies don’t allow us to discount the possibility of a “3rd variable.”\nDon’t get stuck with sticky data.\nSeek data independence. Ideally, each piece of data should be gathered from a different source. (Collecting independent data is another way of minimizing the effect of unknown third variables.)\nThe law of large numbers does not apply to small numbers.\nPay attention to sample sizes. The smaller the sample size, the greater the variability.\nAlways debrief.\nListen carefully to what people say about their experiences. Look for ways in which participants are misunderstanding the instructions. Be vigilant for possible demand characteristics — where the participant forms an idea about the experiment that confounds the results.\nMake friends with a statistician.\nBefore you collect any data, talk with a statistician. Describe what you are planning to do and listen carefully to the advice. Statistical consultants are thrilled when people come and talk with them before the data are collected. Take advantage of their expertise.\nCorrect for multiple tests.\nEach statistical test increases the likelihood of making a Type I (false positive) error. Repeating an experiment also increases the probability of a Type I error.\nIf you torture the data long enough it will confess to anything.\nIt’s not true that “statistics can prove anything,” but it is possible to manipulate data in ways that deceive yourself and others. Create a data analysis plan. Do not exclude outliers, normalize data, set conditions for excluding participants, or introduce post hoc tests without some principled prior reasoning. Be honest in reporting the analyses you carry out. Don’t hide multiple tests."
  },
  {
    "objectID": "emp_methods_workshop/sampling_problems_answers.html",
    "href": "emp_methods_workshop/sampling_problems_answers.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "For each study, identify one or more issues that may potentially cause the sample to be biased.\n\nAn orchestra manager is eager to better understand the likes and dislikes of the orchestra’s audience. She distibutes a survey to the first 50 people who arrive at the orchestra’s gala December matinee concert.\nThe first people to arrive are likely to differ in several ways from people who arrive later. For example, early arrivals might be especially eager, or especially social, or may come early because they take the bus (perhaps less affluent), etc.\n\nIt would be better to distribute survey randomly (using a systematic method like every 10 people).\n\nIn addition, the survey is distributed at only one concert. As a matinee (afternoon) concert, it may attract a different clientele. Also, concerts in December (during the holiday season) may tend to attract a different audience than concerts during the rest of the year.\n\nBetter to sample more than one concert.\n\nA researcher is interested in which local radio stations are most listened to by people when driving. The researcher makes a deal with ten local garages. Whenever a car is brought into the shop, the mechanic first turns on the car radio and records the station to which the radio is tuned. After a month of collecting data, the researcher has over two thousand observations.\nIn general, this is a pretty good sampling method.\n\nThe sample may be biased toward older cars (that require repairs), and/or toward new cars under warrantee.\nWe have no idea whether a given motorist spends little or lots of time listening to the radio.\nThe sample may be biased towards white-collar workers and women; blue-collar workers and men are perhaps more likely to do their own repairs. N.B. If the researcher were to ask the participating garages to keep track of the sex of their clients, then the research can compensate for paucity of males. Standing on (several) street corners, the research could estimate the proportion of male-to-female drivers, and so weight the garage results (quota sampling) by sex.\n\nNotice that it would be bad to use car rental agencies for this sampling method.\nA researcher is interested in voice-leading practices. He encodes the complete fugues from the Bach Well-Tempered Clavier and writes software to test various hypotheses in the musical organization.\n\nBiased towards Bach.\nBiased towards Baroque practice.\nBiased towards fugal practice.\nBiased towards voice-leading in keyboard works.\n\nA researcher wants to sample opinions from people attending a nightclub. The quietest places in the club to conduct verbal interviews are the restrooms. Two male and two female research assistants are stationed inside the restrooms where they approach everyone who is about to leave the restroom to answer a couple of quick questions.\n\nThe sample may be biased towards those individuals who are drinking (since they are more likely to visit the restroom).\nThe sample may be biased against those individuals who spend only a little time in the discotheque (“club-hopping”), since they are less likely to visit the restroom.\nThe sample may be biased towards those individuals who are cooperative/conscientious.\nThe sample may be biased towards those individuals who are dating a person of the opposite sex (since that person is elsewhere).\nThere is a slight danger or reinterviewing the same individual.\nDepending on the goal, the researcher might want to sample from more than one discotheque. Otherwise, the sampling method looks good. Over several hours, there is a good chance that the researchers will interview the majority of patrons.\n\nFelix Mendelssohn wrote thousands of letters over the course of his life. A historical musicologist conjectures that Mendelssohn became increasingly religious towards the end of his life. From a complete list of letters, the researcher randomly selects 200. Each letter is read by the researcher and coded (yes/no) for whether it contains any spiritual, biblical or religious allusion or content.\n\nBetter to use a quota sample in which equal numbers of letters are selected from each decade of life.\nMight also want to control for the length of the letters, since longer letters are more likely (by chance) to contain spiritual, biblical or religious content.\n\nA researcher is interested in learning how much musicians earn and their various sources of income. The researcher succeeds in getting the e-mail list for all of the members of the American Federation of Musicians who have provided the AFM with an e-mail contact. An e-mail is sent to members soliciting participation. The e-mail contains a link to the survey’s website.\n\nBiased towards those musicians with an e-mail account.\nBiased towards those musicians who are willing to divulge their e-mail account.\nBiased towards those who are cooperative/conscientious and will complete the survey. (Biased towards those who have time on their hands.)\nBiased towards American and Canadian musicians (AFM serves both countries).\nBiased towards unionized (rather than ununionized) musicians.\nBiased towards performers (rather than composers, conductors, teachers) etc.\n\nIn the early 1980s, Christa Hansen was eager to test whether non-Westerners also show evidence of statistical learning for pitch relationships. Her aim was to recruit listeners who were not familiar with Western musical culture. Along with her husband, Putra, Hansen set out on a small motorcycle toward the remote northeastern region of the island of Bali. Whenever they met someone they asked two questions: “What is the most isolated village in this area? And does it have a gamelan?” Having reached this village, they again asked the same questions, and so continued on their quest. When they were no longer able to use their motorcycle, a footpath took them to a remote village in the shadow of the Gunung Agung volcano. Hansen concluded that she had reached a truly isolated place when the villagers surmised that this fair-skinned stranger must be Javanese. Recruiting twenty-seven participants from this village, Hansen was able to collect data for her experiment.\nA superb sampling approach. Just two problems.\n\nAll of her participants were from the same village, so the data aren’t entirely independent from each other. (A few people from several different villages would have been better — although more difficult.)\nShe collected data from only a single culture, so the conclusions must be described regarding people from the Gunung Agung region.\n\nSandra Trehub and her colleagues regularly carry out musical studies with newborn infants. From local-area hospitals, they are able to get contact information with names and telephone numbers of mothers who have recently given birth in the various hospitals. They recruit participants by phoning the mothers, describing the basic aims of their experiments, and asking whether they would be willing to participate in an experiment with their new infant. A small sum (roughly $20) is paid to volunteers. Since the research lab is located in a suburban area, there is ample parking. There is also reasonable bus service.\nIn general, a pretty good sampling method. Biases:\n\nWestern (Canadian) moms.\nBiased towards stay-at-home moms with cars.\nBiased towards cooperative/conscientious mothers."
  },
  {
    "objectID": "emp_methods_workshop/feedback5b.html",
    "href": "emp_methods_workshop/feedback5b.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "The workshop aimed to achieve nine specific goals. For each goal, indicate how well you think the workshop succeeded.\n\nTo communicate the main concepts and techniques in modern empirical research. ———– —— —— —— ———– very poor poor okay well very well ———– —— —— —— ———–\nTo provide practical experience in research skills, including posing research questions, formulating and operationalizing hypotheses, designing experiments and questionnaires, and analyzing data. ———– —— —— —— ———– very poor poor okay well very well ———– —— —— —— ———–\n\n\n\nprovide pra\nctical\nresearc\nh advic\ne.\n\n\n\n\nvery poor\npoor\nokay\nwell\nvery well\n\n\n\nTo expose participants to examples of empirical music studies through lectures and readings. ———– —— —— —— ———– very poor poor okay well very well ———– —— —— —— ———–\nTo build critical skills when reading empirical research studies — identifying both strengths and weaknesses. ———– —— —— —— ———– very poor poor okay well very well ———– —— —— —— ———–\nTo stimulate participants’ creative imaginations in posing and pursuing musical questions. ———– —— —— —— ———– very poor poor okay well very well ———– —— —— —— ———–\nTo provide sufficient background so that participants will feel confident in beginning their own program of empirical music research. ———– —— —— —— ———– very poor poor okay well very well ———– —— —— —— ———–\nTo identify resources for continuing education in empirical musicology. ———– —— —— —— ———– very poor poor okay well very well ———– —— —— —— ———–\nTo offer a philosophical framework for understanding how empirical methods can be contextualized within traditional concerns in the arts and humanities. ———– —— —— —— ———– very poor poor okay well very well ———– —— —— —— ———–\n\nWhat goal(s) would you like to see added to the workshop aims?"
  },
  {
    "objectID": "emp_methods_workshop/slogans09.html",
    "href": "emp_methods_workshop/slogans09.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Huron’s Research Slogans\n\n\nMotivated by truth, with no hope of Proof\nThere is no inductive proof. We are not in the business of proving something to be true. We would love to know the truth (if that exists), but we understand that we could never be sure of the truth, even if we had it. The best we can hope for is that what we observe is consistent with our theories.\nThe best research invites failure.\nGive the world an opportunity to tell you that you’re wrong. (This is the essence of good research.)\nWe invite failure by testing predictions.\nTest an idea by making a prediction, and then determine whether the observations are consistent with the prediction.\nWe recognize failure by drawing a line in the sand.\nIn order to make failure obvious, establish a criterion in advance that says, “If the evidence doesn’t cross this line, then I’ll admit failure.” In statistics, the line is referred to as the confidence level.\nAim not to be right, but to be not not right.\nInstead of establishing The Truth, our more modest aim is to be not obviously wrong. When our observations turn out to be consistent with our hypothesis, we don’t claim that we are right; instead the observations suggest that our hypothesis may not be wrong.\nTest hypotheses by operationalizing terms.\nTranslate all of the terms in a hypothesis into concrete things you can measure. We can’t directly measure concepts like “sadness.” We have no choice but to measure things using imperfect rulers.\nOperationalize, but don’t essentialize.\nAll concepts are inherently enigmatic and fuzzy. Terms like “melody,” “listen” or “note” can never be pinned-down. It is impossible to provide comprehensive definitions or grasp the essence of some concept. We are forced to approximate or estimate concepts through operational definitions — but don’t confuse the operational definition with the concept itself, and don’t assume concepts are “real.”\nCompare, compare, compare.\nContrast a “treatment” condition with one or more “control” conditions.\nThe rhetoric of science is the rhetoric of prophecy.\nPeople are most impressed when someone accurately foretells the future. Science is a form of rhetoric whose persuasive power resides in the testing of predictions. The rhetorical power of science comes not from scholars assembling evidence, but from scholars testing predictions."
  },
  {
    "objectID": "emp_methods_workshop/hypothesisless.html#the-importance-of-hypotheses",
    "href": "emp_methods_workshop/hypothesisless.html#the-importance-of-hypotheses",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "The Importance of Hypotheses",
    "text": "The Importance of Hypotheses\n\n\nHypothesis:\nA claim whose truth or falsity could, in theory, be tested (but not proven) by making one or more observations.\n\n\nHypothesisless:\nThe state of being without a hypothesis.\nIn exploratory studies, the researcher makes no prior claim, and therefore can never be wrong.\nA common approach in traditional arts and humanities scholarship is to make some observations, and then to present a story, theory or analysis that makes sense of these observations. Since the story or theory is not presented prior to making the observations, there is no way to test the researcher’s interpretation. When research is done this way, the researcher always looks right since the observations are always consistent with the theory.\nRecall that the best research invites failure. We invite failure by making predictions. That is, we invite failure by testing hypotheses that are put forward before we collect or examine the evidence.\nThere is an important role for descriptive and exploratory research. Often we have no idea what needs to be explained. We may need to “open the box” before we know what phenomena need to be theorized about.\nThere is considerable attraction to pursuing solely descriptive or exploratory research. Once again, if you don’t test a hypothesis, there is no chance of being wrong.\n\n\nHypothesislessness:\nA persistent and sometimes chronic disease where a researcher is perpetually without a hypothesis.\nSome scholars just don’t like being wrong. These scholars will tend to focus exclusively on descriptive, analytic and exploratory studies. The researcher who advances theories/analyses/stories only after making observations is a researcher who will appear to be omniscient.\nThe researcher who boldly proposes a theory or hypothesis before collecting data is frequently regarded as cocky and self-assured—the antithesis of the careful scholar. On the contrary, this boldness is the requirement for intellectual humility. The humble scholar is the one who is prepared to be wrong—especially when he or she is shown to be wrong in public. It is a mistake to believe that the researcher who proposes no theory or hypothesis is the humblest of scholars.\nSlogan: Avoid chronic hypothesislessness."
  },
  {
    "objectID": "emp_methods_workshop/modeling.html#modeling---the-b-flat-valve-trumpet",
    "href": "emp_methods_workshop/modeling.html#modeling---the-b-flat-valve-trumpet",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "Modeling - The B-flat Valve Trumpet",
    "text": "Modeling - The B-flat Valve Trumpet\n\nRecall that a model is miniature world that attempts to emulate something about the real world. In meteorology, models are used to forecast the weather or climate. In music, an example of a modeling study is found in the work of Huron & Berec (2009) who built a model of the trumpet-performer interaction. They then applied this model to an analysis of various trumpet works.\n\nOverview\nThe immediate goal of the model was to provide a measure of performance difficulty for any notated score when played on the B-flat valve trumpet. The input to the model consists of a notated musical passage or complete score. The output is a series of estimates of the difficulty of playing the score on the trumpet.\nThe model produces 10 difficulty estimates. For example, one of the estimates it produces is the average breathing difficulty. The model treats the lungs as a reservoir that may be depleted at various rates, and refilled when opportunities arise. Notes deplete the air supply in proportion to their durations. These rates depend on the dynamic level and register. For example, tones in the highest and lowest registers exhaust the air supply faster. In addition, the air supply is more quickly depleted at louder dynamic levels. The model assumes that the performer begins with full lungs and takes breaths whenever a rest period greater than 0.25 seconds occurs, or whenever an explicit breath mark is notated in the score. It takes about 1.3 seconds to fully inhale, so the amount of replenishment depends on the duration of the rest periods. The average breathing difficulty is calculated so that it depends on the average volume of air found in the player’s lungs over the course of the work. If the performer’s lungs are perpetually near “empty” then the difficulty score rises.\nApart from the average volume of air, the model is vigilant for places where the performer completely runs out of air. This doesn’t occur often in music, but it can occur for especially difficult works. The model keeps a tally of these “out-of-breath” instances.\nOther estimates of difficulty are calculated for average tonguing difficulty, average embouchure difficulty, average fingering difficulty, average dynamic difficulty, average range difficulty, and average interval difficulty. Finally, the model calculates an overall performance difficulty. These values are calibrated so that the average difficulty is 1.0.\nUsing the model, for example, we estimated the difficulty for a number of musical works. For example, Herbert Clarke’s virtuoso work Stars in a Velvety Sky produced an overall average difficulty of 1.8. But the highest estimated difficulty was for Malcolm Arnold’s Fantasy for Trumpet (Opus 100) with an overall average difficulty of 2.6.\n\n\nHow We Built the Model\nOur model made use of various measures we made with two trumpet players. One was a professional and the other was an amateur performer. We measured how long it took the performers to take a full breath, how fast the breath was exhausted when playing at different dynamic levels in different registers, how fast the performer could tongue at different dynamic levels and ranges, and so on.\nIn the case of fingering difficulty, we asked the performers to judge (on a 10-point scale) the degree of difficulty for different finger/valve combinations. The table below shows the average difficulty for finger/valve transitions as judged by our two trumpet players. (“0” means open position, “1” means first valve, “2-3” means valves 2 & 3 are depressed.) The table shows, for example, that valve 3, followed by 1-2 has an average dificult of 7.0 (out of a maximum difficulty of 10).\nValve combination for the consequent note.\n\n\n\n\n0\n1\n2\n3\n1-2\n1-3\n2-3\n1-2-3\n\n\n0:\n0.0\n1.0\n1.0\n1.9\n1.5\n3.0\n3.0\n3.5\n\n\n1:\n1.0\n0.0\n2.0\n3.0\n2.0\n4.5\n7.5\n6.0\n\n\n2:\n1.0\n1.5\n0.0\n5.3\n3.0\n9.5\n6.0\n9.0\n\n\n3:\n2.5\n4.0\n4.5\n0.0\n7.0\n4.0\n4.0\n5.5\n\n\n1-2:\n1.5\n1.5\n2.3\n7.5\n0.0\n6.0\n6.0\n5.0\n\n\n1-3:\n3.5\n4.0\n9.5\n1.5\n5.5\n0.0\n6.0\n4.0\n\n\n2-3:\n2.5\n6.0\n5.5\n4.0\n5.0\n5.5\n0.0\n3.8\n\n\n1-2-3:\n3.0\n4.0\n8.5\n3.5\n6.0\n5.0\n5.0\n0.0\n\n\n\n\n\nValidation Study\nAny model should be tested before you use it. How do we know that the model does a reasonable job of predicting the difficulty of trumpet works? In order to test the validity, we need an independent judge of the difficulty of different musical passages. For this, we made use of the grading system published by the Royal Conservatory of Music of Toronto. For the trumpet, the curriculum consists of six grades: II, IV, VI, VIII, IX and X.\nBecause our model was not designed to account for performance difficulties related to musical interpretation, we limited our selection of graded materials to trumpet studies. We assumed that the difficulty associated with etudes would arise primarily from technical difficulties of performance, and that interpretive challenges would be relatively less important than is the case for non-etude musical works. For each conservatory grade, we randomly selected two solo etudes from the published syllabus. We then used the model to estimate the difficulty for each study. If our model has any validity, we ought to observe a positive correlation between the grade level and the difficulty estimates. The results are shown in the table below.\nRCMT Grade\nStudy No.\nComposer\nOverall Performance Difficulty\nII\n6\nEndresen\n0.59\nII\n12\nEndresen\n0.50\nIV\n16\nClodomir\n0.46\nIV\n25\nClodomir\n1.05\nVI\n64\nClodomir\n0.99\nVI\n65\nClodomir\n1.23\nVIII\n1\nBalasanyan\n1.29\nVIII\n4\nBalasanyan\n1.03\nIX\n4\nDubois\n1.20\nIX\n5\nDubois\n0.97\nX\n2\nCharlier\n1.06\nX\n16\nCharlier\n1.43\nAlthough the pattern isn’t perfect, we found a statistically significant positive correlation between the grade level and the difficulty estimates. We also found positive correlations for each of the constitutent difficulty components, such as breathing difficulty. In general, the results are consistent with the notion that the model has some predictive value above chance level.\n\n\nApplication\nHuron and Berec used their model as a way of demonstrating the difference between difficulty and idiomatic. Musicians will commonly say that such-and-such a work is “idiomatic” to the violin. It is tempting to assume that an “idiomatic” work is a work that is easy to perform. However, a work that is easily performed on a given instrument is also likely to be easy to perform on other instruments as well.\nA difficult work may be defined as a work that places stringent demands on the performer, such as extraordinary physical endurance, highly refined or accurate motor skills, taxing motor coordination, or other awkward or strenuous tasks. By idiomatic, we mean that, of all the ways a given musical goal or effect may be achieved, the method employed by the composer/musician is one of the least difficult. That is, the effect is produced with comparative or relative ease. This can be illustrated by examining the effect of key on performance difficulty. Suppose we were to transpose a work through all twelve pitch-classes, and compare the difficulty for all transpositions. If a work had been written in the key of E major, and E major turned out to be the most difficult of all possible keys, then it would be hard to claim that the work has been arranged idiomatically. On the other hand, if we found that the key of E major exhibited the lowest possible difficulty score, then this would lend weight to the claim that the work was created with the instrument in mind.\nFigures 1 and 2 show the effect of transposition on fingering difficulty estimates for works by Malcolm Arnold, Guillaume Balay, and Herbert Clarke (all composers who were trumpet players). The graphs cover a range of plus or minus one octave. The actual key of composition is plotted in the center of the graph (0 transposition). Notice, first all, that the fingering difficulty shows a general tendency to fall as the work is transposed up in pitch. Brass players will recognize that this is a simple consequence of the way the harmonics and valves interact. As a work is transposed higher, there is less need to use some of the more difficult finger combinations.\nSuperimposed on this general downward trend are local fluctuations in difficulty depending on the particular key. With one exception, there is a notable minimum evident at zero transposition (i.e., the original key composition). The one exception occurs for the slow second movement in Arnold’s trumpet concerto — a movement that has a relatively low overall difficulty (0.96). The predominance of local dips at zero transposition is consistent with the suggestion that the composers explicitly chose a key that facilitates performing the work.\nFigure 1\n\nFigure 2\n\nFigure 3, by contrast, shows the comparable results for Paul Hindemith’s Trumpet Sonata. Hindemith was not a trumpet player. Here there is no clear effect of key, nor is there any notable dip coinciding with the key chosen by Hindemith.\nIt bears emphasizing that measures of performance ease and measures of instrumental idiomaticism cannot be regarded prima facie as indices of compositional merit. Difficult works are not necessarily better than easy works, and idiomatic works are not necessarily better than unidiomatic works.\nFigure 3\n\n\n\nReference:\nD. Huron & J. Berec (2009). Characterizing idiomatic organization in music: A theory and case study of musical affordances. Empirical Musicology Review, Vol. 4, No. 3, pp. 103-122."
  },
  {
    "objectID": "emp_methods_workshop/program1.html",
    "href": "emp_methods_workshop/program1.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "DAY 1 PROGRAM\n\n\nWelcome & Introduction\nGenerals Aims & Preview\n\nLearning objectives\n\nEmpirical Research\n\nTypes of knowledge\nSeven big ideas\nA line in the sand\nRefutation versus confirmation\nOperationalizing terms and concepts\nComparison & counterfactuals\nThe rhetoric of science (video - 8 minutes)\nReview the first 9 slogans: Quiz #1\nGroup Task #1: What’s worth knowing?\nQuestions, conjectures, hypotheses and theories\nGroup Task #2: Question, theory or hypothesis?\nGrandmother research\nThe quantitative/measurement obsession\nGroup Task #3: Obvious theories; Group task debriefing Hindsight bias; Grandmother research revisited\nTwo forms of reductionism\nEpistephobia; Types of failure\nReview (first 12) slogans\n\nTypes of Empirical Studies\n\nTypes of empirical studies\nGroup Task #4: Types of studies\nGeneralizing versus universalizing (video - 10 minutes)\nExploratory studies\nMeasurement studies\nHypothesislessness\nGroup Task #5: Operationalize the following hypotheses\nSyncopation: From question to hypothesis\nOpinions as operationalizations\nDouble use data; post hoc theories\nExploratory & Confirmatory: Contexts of discovery & legitimation (lecture)\nExplore-then-test approach; An example\nFrom question to theory to conjecture to hypothesis to protocol\nReview (first 16) slogans\nFeedback Day 1\n\nHomework\n\nHomework 1: Reading: An Arts and Humanities Approach to Empirical Methodology\nHomework 2: Reading Guide #1: Lancashire & Hirst (2009)\nHomework 3: Individual Task #6: From question to theory"
  },
  {
    "objectID": "emp_methods_workshop/measurement_answers.html",
    "href": "emp_methods_workshop/measurement_answers.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "For each case, identify the kind of measurement scale implied — either nominal, ordinal, interval, or ratio.\n\nWorks categorized as either ragtime, stride bass, big band, bebop, Dixieland, free jazz or other.\nNominal: These are named categories.\nThe age of audience members.\nRatio: Zero is meaningful. A doubling of age (from 20 to 40) is a true doubling.\nYear of birth for audience members.\nInterval: The year “0” has no special meaning; The year 1000 A.D. is not half as old as 2000 A.D.\nThe participant’s sex, coded as male or female.\nNominal: These are named categories.\nInterval size in semitones.\nRatio: Zero has meaning, namely a unison distance. Twelve semitones is twice six semitones.\nYear of composition.\nInterval: The year “0” has no special meaning; The year 1000 A.D. is not half as old as 2000 A.D.\nEither: medieval, renaissance, baroque, classical, romantic, or modern.\nOrdinal: There is a rank ordering in terms of history, but the distances between successive items is not the same.\nDynamic level, according to the scale: ppp, pp, p, mp, mf, f, ff, or fff.\nOrdinal: There is a rank order, but the distances between successive items is not necessarily the same.\nThe skin temperature of listeners in Fahrenheit.\nInterval: Recall that zero degrees has no special meaning. One hundred degrees is not twice as warm as fifty degrees.\nWatts of power produced by an amplifier.\nRatio: Zero watts means there is no power at all.\nA list of instruments.\nNominal: These are named categories.\nA list of instruments from brightest to darkest timbres.\nOrdinal: There is a rank order from brightest to darkest, but the distance between neighboring instruments on the list is not constant.\nMIDI key numbers (e.g., middle C = 60).\nInterval: notice that MIDI key “0” has no special meaning and that MIDI key “60” (C4) is not “twice as much or twice as high” as MIDI key “30” (F#2).\nCoded S, A, T or B.\nOrdinal: There is a rank order in terms of pitch height. However, the distance in tessitura is not necessarily the same. For example, the distance between the bass and the tenor might well be larger than the distance between the soprano and the alto.\nNumber of hours of practice.\nRatio: Zero has meaning (no practice at all); 2 hours of practice is twice as much as 4 hours of practice.\nIQ.\nInterval: Zero has no special meaning. Instead “100” has a special meaning, namely the average intelligence. A person who has an IQ of “200” is not twice as smart as the average person.\nNumber of instruments.\nRatio: Zero has a special meaning. Four instruments is half of eight instruments.\nReaction times in milliseconds.\nRatio: Zero has a special meaning; 50 milliseconds is half 100 milliseconds.\nThree categories: (1) Obviously major, (2) obviously minor, or (3) not obviously major or minor.\nNominal: These are named categories.\nThe amount of money a busker makes per hour.\nRatio: Zero has meaning; $6/hr is twice as much as $3/hr.\nLength of hairpin markings in centimeters.\nRatio: Zero means no distance at all. Six centimeters is twice the distance of three centimeters.\nThe notated key of a passage.\nNominal: These are named categories, like F major or B-flat minor.\nThe loudness of each instrument, measured in decibels.\nInterval: This is really a trick question that requires special knowledge. Most people don’t know that zero decibels doesn’t mean the absence of sound. It means a sound of 0.0002 dynes per square centimeter, which is considered the quietest sound that a normal person can just hear. The total absence of sound is undefined in the decibel scale, making the scale an interval scale rather than a ratio scale.\nThe number of sharps (positive) or flats (negative) in a key signature. (E.g., F# & C# = +2; Bb = -1.)\nRatio or interval: A difficult question. If one is thinking purely in terms of the notational symbols, then zero has a special meaning — i.e., no sharp or flat symbols. This would make the scale a ratio scale. However, it is likely that the researcher is thinking in terms of the circle of fifths. One might argue that there is nothing special about C major/A minor compared with other keys. D major might have twice the number of sharps as G major, but does G major have infinitely more sharps than C major? If the researcher is thinking in terms of the circle of fifths, then no sharps or flats is not special, and therefore the measurement would be considered an interval scale.\nResponses: never, rarely, sometimes, frequently, always.\nOrdinal: We can’t say that the distance between never and rarely is the same as the distance between sometimes and frequently.\nMusical preference on a 7-point scale:\n\n\n\n\n\n\n\n\n\n\n\n\nlike\n◉\n◉\n◉\n◉\n◉\ndislike\n\n\n\nInterval: respondents tend to treat the points as positions along a hypothetical line; although there is no meaningful “zero” position, one can interpret the distances between successvie points as meaningful."
  },
  {
    "objectID": "emp_methods_workshop/chi_square_problems.html",
    "href": "emp_methods_workshop/chi_square_problems.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "GROUP TASK #16: Chi-Square Tests\n\nFor each of the following statistical tests, use an\na priori</95% confidence level.\nIn a survey of orchestral works it was found that 70 percent include an oboe as part of the orchestration. A music scholar finds that composer A uses an oboe in only 10 of 22 orchestral works (just 45%). Is this a statistically significant difference?\nA music scholar finds that a sample of Italian baroque music shows 51 works in major keys and 9 works in minor keys. In a diverse sample of baroque music from many European countries (including Italy), 480 works were found to be in major keys and 210 works in minor keys. Compared with their European colleagues, did Italian baroque composers tend to favor works in major keys?\nAn ethnomusicologist finds that just 130 of 220 Xhosa listeners say they prefer bumbuju rhythms to nobuju rhythms. In one Xhosa village however, 15 of 19 people say they prefer bumbuju. Can we claim that the members of this village differ significantly from other Xhosa in their rhythmic preferences?"
  },
  {
    "objectID": "emp_methods_workshop/effect_size.html#effect-size",
    "href": "emp_methods_workshop/effect_size.html#effect-size",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "Effect Size",
    "text": "Effect Size\n\nA common question in empirical research is “How big should my sample be?”\nSometimes you see research publications in which just five or six people are tested. How can one possibly generalize from just five or six people?\nConsider the following scenario. Our experimental task is to test the toxicity of a new drug. The drug is called cyanide. We have recruited a number of volunteers to help us try out the drug. We give a 100 mg of cyanide to our first volunteer, who promptly drops dead. Hmm, “A coincidence” we might think. We reassure our second volunteer that “the person must have had a bad heart.” Our second volunteer then consumes the cyanide, and also drops over dead. Already, all of the remaining volunteers have left—claiming that they forgot about other pressing business. We are disappointed because we had hoped to test at least 100 participants. How can we draw any conclusion from just two participants?\nOf course, there is no need for any more testing. The likelihood that a given person would spontaneously drop dead is very small. Do we have any doubt that cyanide is deadly?\nThe reason why we are justified in concluding that a single participant provides an adequate sample size is what’s called the effect size. The effect size is a measure of the strength of a relationship between two variables. If we administer 100 mg of mercury to someone, they may or many not die. But if we administer 100 mg of cyanide, they will most certainly drop dead. In other words, the effect size is greater for cyanide than for mercury.\nConsider two weight-loss programs. After participating in the first program, 80% of the participants have lost weight, whereas only 40% of the participants in the second program have lost weight. Which is the better program?\nBefore concluding that the first program is better, we should look at the effect size. In the first program, the average weight-loss for those who lost weight was 1 kilo. In the second program, the average weight-loss for those who lost weight was 9 kilos. In first program, we may justifiably claim that the majority of participants lost weight. However, the second program has a much bigger effect size.\nIn matters of health and safety, people very often ignore effect sizes. People worry about terrorist bombs, fear that aircraft will crash, and are concerned about non-organic foods. But the probability that these will bring about your demise is extremely small. Wear your seatbelt, don’t smoke, maintain a moderate weight, get regular exercise, and take an antibiotic whenever you have an infection. The effect sizes for these latter behaviors are gigantic by comparison.\nIncidentally, in the case of cyanide, the effect size is not only very large, the generality is also large. We don’t need to perform separate tests of men and women, or test children and the elderly. In fact, chimpanzees, cows, snakes, penguins, catfish, and (most) bacteria will all die when exposed to cyanide.\nSo back to our original question: “How big should my sample be?” The answer depends on the effect size. When effects are big, you will get statistically significant results with only a small sample. For truly huge effect sizes, just one or two cases may be sufficient. However, if the effect size is small, you may need thousands of samples.\n\nPower Calculation\nThere is something called a power calculation in statistics. A power calculation will allow you to estimate the number of observations (or participants) you will need in an experiment in order to test your hypothesis. Unfortunately, as part of the power calculation, you will need to estimate the effect size. Is this drug likely to cure every patient who takes it? 1 in 10 patients? 1 in 100 patients? After you estimate the effect size, the power calculation will then tell you how many observations (or people) you will need to recruit for your experiment. For a low effect size, the power calculation may tell you that you will need 10,000 participants in order to determine whether the drug is having some a beneficial effect. In cases like this, a power calculation may help us realize that a proposed study is impractical.\nInstitutional Review Boards (IRBs) will sometimes require a power calculation before a researcher is given permission to carry out a medical study with human subjects. The power calculation may show that 200 participants will be needed in order to establish statistical signficiance. If the researcher is only able to recruit 50 participants with the specific medical condition, then the research will most likely fail. Even if the drug is helpful, 50 participants may be too few to reach statistical significance. Consequently, the IRB may argue that there is no moral value in subjecting these patients to the planned research since there is little likelihood of learning anything.\nIn any field of research, one wants to discover the most important things first. It doesn’t make much sense to concentrate most of our research efforts on disease X if the vast majority of people are dying from disease Y. Similarly in music. In pursuing our research, we should prefer to address the biggest phenomenon first. We should aim for phenomena that have big effect sizes, and then, over time, turn our attention to the smaller effect sizes.\nIn much music research, researchers choose to study just a handful of people—perhaps only 10 or 20 people. For many questions, 10 or 20 participants will be too small. However, if the effect size is big, then we should have little difficulty seeing the effect with such small numbers. Some researchers intentionally keep to small number of participants because if forces the researcher to focus on phenomena that have large effect sizes. That is, using small numbers of observations forces the researcher to focus on the big stuff, rather than the subtle stuff. Using small samples means that we are likely to get statistically significant results only for phenomena that have large effect sizes.\nIn any field of research, we aim to learn the “big” things first. As the field matures, we aim to address more subtle considerations. So over time, most research starts by examining phenomena that have large effect sizes, and progresses to phenomena with progressively smaller effect sizes. Consequently, as a field of inquiry advances, empirical studies often require increasing numbers of observations."
  },
  {
    "objectID": "emp_methods_workshop/slogans20.html",
    "href": "emp_methods_workshop/slogans20.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Huron’s Research Slogans\n\n\nMotivated by truth, with no hope of Proof\nThere is no inductive proof. We are not in the business of proving something to be true. We would love to know the truth (if that exists), but we understand that we could never be sure of the truth, even if we had it. The best we can hope for is that what we observe is consistent with our theories.\nThe best research invites failure.\nGive the world an opportunity to tell you that you’re wrong. (This is the essence of good research.)\nWe invite failure by testing predictions.\nTest an idea by making a prediction, and then determine whether the observations are consistent with the prediction.\nWe recognize failure by drawing a line in the sand.\nIn order to make failure obvious, establish a criterion in advance that says, “If the evidence doesn’t cross this line, then I’ll admit failure.” In statistics, the line is referred to as the confidence level.\nAim not to be right, but to be not not right.\nInstead of establishing The Truth, our more modest aim is to be not obviously wrong. When our observations turn out to be consistent with our hypothesis, we don’t claim that we are right; instead the observations suggest that our hypothesis may not be wrong.\nTest hypotheses by operationalizing terms.\nTranslate all of the terms in a hypothesis into concrete things you can measure. We can’t directly measure concepts like “sadness.” We have no choice but to measure things using imperfect rulers.\nOperationalize, but don’t essentialize.\nAll concepts are inherently enigmatic and fuzzy. Terms like “melody,” “listen” or “note” can never be pinned-down. It is impossible to provide comprehensive definitions or grasp the essence of some concept. We are forced to approximate or estimate concepts through operational definitions — but don’t confuse the operational definition with the concept itself, and don’t assume concepts are “real.”\nCompare, compare, compare.\nContrast a “treatment” condition with one or more “control” conditions.\nThe rhetoric of science is the rhetoric of prophecy.\nPeople are most impressed when someone accurately foretells the future. Science is a form of rhetoric whose persuasive power resides in the testing of predictions. The rhetorical power of science comes not from scholars assembling evidence, but from scholars testing predictions.\nHindsight is 20/20.\nMost things seem obvious in retrospect (hindsight bias). When the results aren’t obvious, humans are enormously gifted at coming up with explanatory accounts. We can make up a story for just about any set of data. Post hoc theories don’t have the same plausibility as a priori theories. The true test is making up the story first (i.e., prediction)! Prefer theorizing first, then collect your data.\nReductionism is a method, not a belief.\nWe simplify problems, not because we believe problems to be simple, but because we believe problems to be complex. Restricting our gaze is a useful strategy for discovery.\nDon’t try to explain the whole world at once.\nManipulate one variable at a time. Seek simplicity, even as you distrust it.\nGeneralize, but don’t universalize.\nWhen presenting your results, frame them narrowly rather than broadly.\nAvoid chronic hypothesislessness.\nExploratory and descriptive studies are important, but you can’t invite failure without testing predictions.\nBeware of the post hoc theory.\nThe scholar who only offers theories after looking at the evidence is a scholar who is never wrong. Post hoc theorists don’t allow the world to tell them when their ideas are problematic.\nFrom Question to Theory to Conjecture to Hypothesis to Protocol.\nStart with a question, propose an explanatory theory, derive a conjecture, refine the conjecture into a hypothesis, then operationalize the terms of the hypothesis into a protocol. The protocol provides an action plan for how to carry out the research.\nNo causation without manipulation.\nCausality cannot be inferred unless you manipulate one of the variables. The Experiment is the only type of study in which it is possible to infer causality. Correlational studies don’t allow us to discount the possibility of a “3rd variable.”\nDon’t get stuck with sticky data.\nSeek data independence. Ideally, each piece of data should be gathered from a different source. (Collecting independent data is another way of minimizing the effect of unknown third variables.)\nThe law of large numbers does not apply to small numbers.\nPay attention to sample sizes. The smaller the sample size, the greater the variability.\nAlways debrief.\nListen carefully to what people say about their experiences. Look for ways in which participants are misunderstanding the instructions. Be vigilant for possible demand characteristics — where the participant forms an idea about the experiment that confounds the results."
  },
  {
    "objectID": "emp_methods_workshop/title_workshop.html",
    "href": "emp_methods_workshop/title_workshop.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "**\n**"
  },
  {
    "objectID": "emp_methods_workshop/title_workshop.html#a-workshop-for-music-scholars",
    "href": "emp_methods_workshop/title_workshop.html#a-workshop-for-music-scholars",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "A Workshop for Music Scholars",
    "text": "A Workshop for Music Scholars\nWorkshop Instructors: David Huron & Daniel Shanahan\nOhio State University Columbus, Ohio May 14-17, 2019"
  },
  {
    "objectID": "emp_methods_workshop/factor_analysis.html",
    "href": "emp_methods_workshop/factor_analysis.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Factor Analysis\n\nFactor analysis is a statistical method that can be used to reduce or distill many variables into a smaller number of variables (called factors). Many variables are highly correlated, and this suggests that there is an underlying cause responsible for the similar behavior of correlated variables. This unobserved variable can be regarded as “latent” in the data. The purpose of factor analysis is to expose or reveal possible latent variables.\nAn example of the use of factor analysis is found in a study by Thomas Schäfer and Peter Sedlmeier (2009). They asked 507 Germans to rate how well they liked 25 musical styles. Of course people have different preferences: Some people like classical music, some like jazz, and some like rock. But it is not simply the case that people like one style and are indifferent to other styles. For example, a person might most prefer jazz, but also enjoy reggae. It turns out that there are broad patterns evident in people’s likes and dislikes. For example, a person who likes goth is also likely to enjoy punk and metal. People who like gospel also tend to enjoy soul. People who like folk music are more likely than other people to enjoy country music. Factor analysis uses the correlations between different variables to reduce the number of independently varying qualities.\nIn their factor analysis of stylistic preferences, Schäfer and Sedlmeier found an underlying commonality was between techno, trance, dance and house. Whatever it is that causes a person to like any one of these also tends to dispose that person to like the others as well. In other words, factor analysis suggests a common cause that leads to a preference for all four styles.\nThe table below identifies the so-called factor “loadings” for 25 musical styles on the six dimensions arising from the factor analysis carried out by Schäfer and Sedlmeier.\n\n\n\nStyle\nSophisticated\nElectronic\nRock\nRap\nPop\nBeat, folk, & country\n\n\n\n\nJazz\n.81\n\n\n\n\n\n\n\nBlues\n.77\n\n\n\n\n\n\n\nSwing\n.70\n\n\n\n\n\n\n\nClassical\n.69\n\n\n\n\n\n\n\nTechno\n\n.88\n\n\n\n\n\n\nTrance\n\n.87\n\n\n\n\n\n\nHouse\n\n.77\n\n\n\n\n\n\nDance\n\n.71\n\n\n\n\n\n\nPunk\n\n\n.77\n\n\n\n\n\nMetal\n\n\n.70\n\n\n\n\n\nRock\n\n\n.67\n\n\n\n\n\nAlternative\n\n\n.66\n\n\n\n\n\nGothic\n\n\n.56\n\n\n\n\n\nSka\n\n\n.54\n\n\n\n\n\nHip hop\n\n\n\n.73\n\n\n\n\nRap\n\n\n\n.71\n\n\n\n\nReggae\n\n\n\n.66\n\n\n\n\nPop\n\n\n\n\n.82\n\n\n\nSoul\n\n\n\n\n.64\n\n\n\nRhythm & Blues\n\n\n\n\n.63\n\n\n\nGospel\n\n\n\n\n.55\n\n\n\nBeat music\n\n\n\n\n\n.69\n\n\nFolk\n\n\n\n\n\n.67\n\n\nCountry\n\n\n\n\n\n.67\n\n\nRock & roll\n\n\n\n\n\n(.46)\n\n\n\nNotice that the factor analysis points to an underlying commonality between jazz, blues, swing and classical. It is important to understand that these styles are not a “cluster.” They are simply the variables that exhibit the greatest “loading” on the proposed factor. Other likes & dislikes may contribute to this factor, but these four contribute the most. It’s also important to note that not everyone will share the same constellation of tastes. For example, not everyone who enjoys jazz will enjoy classical, and vice versa. Instead, there is an underlying “something” that seems to encourage people to like jazz, blues, swing and classical, more than other styles. Notice that jazz (.81) and blues (.77) load on this factor more than swing (.70) and classical (.66).\nInstead of refering to a factor as “Factor 1” or “Factor 2,” it is often helpful to refer to a factor by giving it a name. For example, Schäfer and Sedlmeier, decided to refer to the first factor using the name “sophisticated.” Similarly, they decided to refer to the second factor (with high loadings on techno/trance/dance/house) using the term “electronic.” It is important to understand that these names are interpretations offered by the researcher, and that there is no official status to these labels. Other researchers may disagree with a factor name, and in some cases the name may be deceptive or reflect researcher bias. For example, one could argue that the jazz/blues/swing/classical factor might be better labelled “born before 1960,” while the punk/metal/rock/alternative/gothic/ska factor might be better labelled “born between 1965 and 1980.” Factor analysis does not tell you the source or reason for the underlying factor; it simply indicates that such a factor probably exists.\nIt is important to understand that factor analysis doesn’t simply discard some of the original variables. In Schäfer and Sedlmeier’s analysis of styles, the factor analysis is not merely a way to reduce the number of styles. The styles may be truly distinct, and one style can’t simply be regarded as a “variant” of some other style. Instead, the underlying factors may relate to entirely different phenomena. For example, the factors accounting for stylistic preferences might relate to age, race, geography, or parental educational attainment. Factor analysis doesn’t mean that soul and rhythm & blues are similar. It means that whatever causes a person to like soul is similar to whatever causes a person to like rhythm & blues. Similarly, a liking for jazz shares some underlying cause with a liking for classical music—even though the styles are clearly different.\nFactor analysis is similar to another statistical method known as Principal Components Analysis (PCA). The two methods use different numerical methods, but share many commonalities. An important difference between PCA and factor analysis is that factor analysis can also be used to test an a priori theory. Suppose, for example, that you have a theory that there are four factors contributing to the quality of a musical performance. Factor analysis can be used to test your theory, determining the probability that the observational data are consistent with the four proposed factors. So in addition to providing a method for exploratory data analysis, factor analysis can also be used as an inferential test—a way of testing a prior hypothesis.\n\nReferences\nThomas Schäfer and Peter Sedlmeier (2009). From the functions of music to music preference. Psychology of Music, Vol 37, No. 3, pp. 279-300.\nThomas Schäfer, Peter Sedlmeier, Christine Städtler and David Huron (2013). The psychological functions of music listening. Frontiers in Psychology, Vol 4, No. 511, pp. 1-33. DOI: 10.3389/fpsyg.2013.00511"
  },
  {
    "objectID": "emp_methods_workshop/reading05.html",
    "href": "emp_methods_workshop/reading05.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Reading #5: Huron (2007)\n\nRead: David Huron (1999). The New Empiricism: Systematic Musicology in a Postmodern Age. Lecture 3: Methodology. The Ernest Bloch Lectures, University of California, Berkeley.\n\nWhat are the changes that Huron thinks are driving musicology toward more empirical approaches?\nWhat is the difference between prospective and retrospective data? Can historical data ever be prospective data?\nWhat are the two kinds of skepticism?\nWhat does Huron view as the positive attributes of postmodernism?\nWhat is meant by a high risk theory?"
  },
  {
    "objectID": "emp_methods_workshop/chi_table.html",
    "href": "emp_methods_workshop/chi_table.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Critical Values of Chi-square\n\nUpper critical values of chi-square distribution with x degrees of freedom\nProbability of exceeding the critical value:\n\ndf\n90%\n95%\n97.5%\n99%\n99.9%\n⇐ Confidence Level\n⇓\n0.10\n0.05\n0.025\n0.01\n0.001\n⇐ Significance Level\n1\n2.706\n3.841\n5.024\n6.635\n10.828\n2\n4.605\n5.991\n7.378\n9.210\n13.816\n3\n6.251\n7.815\n9.348\n11.345\n16.266\n4\n7.779\n9.488\n11.143\n13.277\n18.467\n5\n9.236\n11.070\n12.833\n15.086\n20.515\n6\n10.645\n12.592\n14.449\n16.812\n22.458\n7\n12.017\n14.067\n16.013\n18.475\n24.322\n8\n13.362\n15.507\n17.535\n20.090\n26.125\n9\n14.684\n16.919\n19.023\n21.666\n27.877\n10\n15.987\n18.307\n20.483\n23.209\n29.588\n11\n17.275\n19.675\n21.920\n24.725\n31.264\n12\n18.549\n21.026\n23.337\n26.217\n32.910\n13\n19.812\n22.362\n24.736\n27.688\n34.528\n14\n21.064\n23.685\n26.119\n29.141\n36.123\n15\n22.307\n24.996\n27.488\n30.578\n37.697\n16\n23.542\n26.296\n28.845\n32.000\n39.252\n17\n24.769\n27.587\n30.191\n33.409\n40.790\n18\n25.989\n28.869\n31.526\n34.805\n42.312\n19\n27.204\n30.144\n32.852\n36.191\n43.820\n20\n28.412\n31.410\n34.170\n37.566\n45.315\n21\n29.615\n32.671\n35.479\n38.932\n46.797\n22\n30.813\n33.924\n36.781\n40.289\n48.268\n23\n32.007\n35.172\n38.076\n41.638\n49.728\n24\n33.196\n36.415\n39.364\n42.980\n51.179\n25\n34.382\n37.652\n40.646\n44.314\n52.620\n26\n35.563\n38.885\n41.923\n45.642\n54.052\n27\n36.741\n40.113\n43.195\n46.963\n55.476\n28\n37.916\n41.337\n44.461\n48.278\n56.892\n29\n39.087\n42.557\n45.722\n49.588\n58.301\n30\n40.256\n43.773\n46.979\n50.892\n59.703\n31\n41.422\n44.985\n48.232\n52.191\n61.098\n32\n42.585\n46.194\n49.480\n53.486\n62.487\n33\n43.745\n47.400\n50.725\n54.776\n63.870\n34\n44.903\n48.602\n51.966\n56.061\n65.247\n35\n46.059\n49.802\n53.203\n57.342\n66.619\n36\n47.212\n50.998\n54.437\n58.619\n67.985\n37\n48.363\n52.192\n55.668\n59.893\n69.347\n38\n49.513\n53.384\n56.896\n61.162\n70.703\n39\n50.660\n54.572\n58.120\n62.428\n72.055\n40\n51.805\n55.758\n59.342\n63.691\n73.402\n41\n52.949\n56.942\n60.561\n64.950\n74.745\n42\n54.090\n58.124\n61.777\n66.206\n76.084\n43\n55.230\n59.304\n62.990\n67.459\n77.419\n44\n56.369\n60.481\n64.201\n68.710\n78.750\n45\n57.505\n61.656\n65.410\n69.957\n80.077\n46\n58.641\n62.830\n66.617\n71.201\n81.400\n47\n59.774\n64.001\n67.821\n72.443\n82.720\n48\n60.907\n65.171\n69.023\n73.683\n84.037\n49\n62.038\n66.339\n70.222\n74.919\n85.351\n50\n63.167\n67.505\n71.420\n76.154\n86.661\n51\n64.295\n68.669\n72.616\n77.386\n87.968\n52\n65.422\n69.832\n73.810\n78.616\n89.272\n53\n66.548\n70.993\n75.002\n79.843\n90.573\n54\n67.673\n72.153\n76.192\n81.069\n91.872\n55\n68.796\n73.311\n77.380\n82.292\n93.168\n56\n69.919\n74.468\n78.567\n83.513\n94.461\n57\n71.040\n75.624\n79.752\n84.733\n95.751\n58\n72.160\n76.778\n80.936\n85.950\n97.039\n59\n73.279\n77.931\n82.117\n87.166\n98.324\n60\n74.397\n79.082\n83.298\n88.379\n99.607\n61\n75.514\n80.232\n84.476\n89.591\n100.888\n62\n76.630\n81.381\n85.654\n90.802\n102.166\n63\n77.745\n82.529\n86.830\n92.010\n103.442\n64\n78.860\n83.675\n88.004\n93.217\n104.716\n65\n79.973\n84.821\n89.177\n94.422\n105.988\n66\n81.085\n85.965\n90.349\n95.626\n107.258\n67\n82.197\n87.108\n91.519\n96.828\n108.526\n68\n83.308\n88.250\n92.689\n98.028\n109.791\n69\n84.418\n89.391\n93.856\n99.228\n111.055\n70\n85.527\n90.531\n95.023\n100.425\n112.317\n71\n86.635\n91.670\n96.189\n101.621\n113.577\n72\n87.743\n92.808\n97.353\n102.816\n114.835\n73\n88.850\n93.945\n98.516\n104.010\n116.092\n74\n89.956\n95.081\n99.678\n105.202\n117.346\n75\n91.061\n96.217\n100.839\n106.393\n118.599\n76\n92.166\n97.351\n101.999\n107.583\n119.850\n77\n93.270\n98.484\n103.158\n108.771\n121.100\n78\n94.374\n99.617\n104.316\n109.958\n122.348\n79\n95.476\n100.749\n105.473\n111.144\n123.594\n80\n96.578\n101.879\n106.629\n112.329\n124.839\n81\n97.680\n103.010\n107.783\n113.512\n126.083\n82\n98.780\n104.139\n108.937\n114.695\n127.324\n83\n99.880\n105.267\n110.090\n115.876\n128.565\n84\n100.980\n106.395\n111.242\n117.057\n129.804\n85\n102.079\n107.522\n112.393\n118.236\n131.041\n86\n103.177\n108.648\n113.544\n119.414\n132.277\n87\n104.275\n109.773\n114.693\n120.591\n133.512\n88\n105.372\n110.898\n115.841\n121.767\n134.746\n89\n106.469\n112.022\n116.989\n122.942\n135.978\n90\n107.565\n113.145\n118.136\n124.116\n137.208\n91\n108.661\n114.268\n119.282\n125.289\n138.438\n92\n109.756\n115.390\n120.427\n126.462\n139.666\n93\n110.850\n116.511\n121.571\n127.633\n140.893\n94\n111.944\n117.632\n122.715\n128.803\n142.119\n95\n113.038\n118.752\n123.858\n129.973\n143.344\n96\n114.131\n119.871\n125.000\n131.141\n144.567\n97\n115.223\n120.990\n126.141\n132.309\n145.789\n98\n116.315\n122.108\n127.282\n133.476\n147.010\n99\n117.407\n123.225\n128.422\n134.642\n148.230\n100\n118.498\n124.342\n129.561\n135.807\n149.449\n100\n118.498\n124.342\n129.561\n135.807\n149.449\nLower critical values of chi-square distribution with X degrees freedom\nProbability of exceeding the critical value:\n\n\n\n\n0.90\n0.95\n0.975\n0.99\n0.999\n\n\n1. .\n016 .\n004 .\n001 .\n000 .\n000\n\n\n2. .\n211 .\n103 .\n051 .\n020 .\n002\n\n\n3. .\n584 .\n352 .\n216 .\n115 .\n024\n\n\n4. 1\n.064 .\n711 .\n484 .\n297 .\n091\n\n\n5. 1\n.610 1\n.145 .\n831 .\n554 .\n210\n\n\n6. 2\n.204 1\n.635 1\n.237 .\n872 .\n381\n\n\n7. 2\n.833 2\n.167 1\n.690 1\n.239 .\n598\n\n\n8. 3\n.490 2\n.733 2\n.180 1\n.646 .\n857\n\n\n9. 4\n.168 3\n.325 2\n.700 2\n.088 1\n.152\n\n\n10. 4\n.865 3\n.940 3\n.247 2\n.558 1\n.479\n\n\n11. 5\n.578 4\n.575 3\n.816 3\n.053 1\n.834\n\n\n12. 6\n.304 5\n.226 4\n.404 3\n.571 2\n.214\n\n\n13. 7\n.042 5\n.892 5\n.009 4\n.107 2\n.617\n\n\n14. 7\n.790 6\n.571 5\n.629 4\n.660 3\n.041\n\n\n15. 8\n.547 7\n.261 6\n.262 5\n.229 3\n.483\n\n\n16. 9\n.312 7\n.962 6\n.908 5\n.812 3\n.942\n\n\n17. 1\n0.085 8\n.672 7\n.564 6\n.408 4\n.416\n\n\n18. 1\n0.865 9\n.390 8\n.231 7\n.015 4\n.905\n\n\n19. 1\n1.651 1\n0.117 8\n.907 7\n.633 5\n.407\n\n\n20. 1\n2.443 1\n0.851 9\n.591 8\n.260 5\n.921\n\n\n21. 1\n3.240 1\n1.591 1\n0.283 8\n.897 6\n.447\n\n\n22. 1\n4.041 1\n2.338 1\n0.982 9\n.542 6\n.983\n\n\n23. 1\n4.848 1\n3.091 1\n1.689 1\n0.196 7\n.529\n\n\n24. 1\n5.659 1\n3.848 1\n2.401 1\n0.856 8\n.085\n\n\n25. 1\n6.473 1\n4.611 1\n3.120 1\n1.524 8\n.649\n\n\n26. 1\n7.292 1\n5.379 1\n3.844 1\n2.198 9\n.222\n\n\n27. 1\n8.114 1\n6.151 1\n4.573 1\n2.879 9\n.803\n\n\n28. 1\n8.939 1\n6.928 1\n5.308 1\n3.565 1\n0.391\n\n\n29. 1\n9.768 1\n7.708 1\n6.047 1\n4.256 1\n0.986\n\n\n30. 2\n0.599 1\n8.493 1\n6.791 1\n4.953 1\n1.588\n\n\n31. 2\n1.434 1\n9.281 1\n7.539 1\n5.655 1\n2.196\n\n\n32. 2\n2.271 2\n0.072 1\n8.291 1\n6.362 1\n2.811\n\n\n33. 2\n3.110 2\n0.867 1\n9.047 1\n7.074 1\n3.431\n\n\n34. 2\n3.952 2\n1.664 1\n9.806 1\n7.789 1\n4.057\n\n\n35. 2\n4.797 2\n2.465 2\n0.569 1\n8.509 1\n4.688\n\n\n36. 2\n5.643 2\n3.269 2\n1.336 1\n9.233 1\n5.324\n\n\n37. 2\n6.492 2\n4.075 2\n2.106 1\n9.960 1\n5.965\n\n\n38. 2\n7.343 2\n4.884 2\n2.878 2\n0.691 1\n6.611\n\n\n39. 2\n8.196 2\n5.695 2\n3.654 2\n1.426 1\n7.262\n\n\n40. 2\n9.051 2\n6.509 2\n4.433 2\n2.164 1\n7.916\n\n\n41. 2\n9.907 2\n7.326 2\n5.215 2\n2.906 1\n8.575\n\n\n42. 3\n0.765 2\n8.144 2\n5.999 2\n3.650 1\n9.239\n\n\n43. 3\n1.625 2\n8.965 2\n6.785 2\n4.398 1\n9.906\n\n\n44. 3\n2.487 2\n9.787 2\n7.575 2\n5.148 2\n0.576\n\n\n45. 3\n3.350 3\n0.612 2\n8.366 2\n5.901 2\n1.251\n\n\n46. 3\n4.215 3\n1.439 2\n9.160 2\n6.657 2\n1.929\n\n\n47. 3\n5.081 3\n2.268 2\n9.956 2\n7.416 2\n2.610\n\n\n48. 3\n5.949 3\n3.098 3\n0.755 2\n8.177 2\n3.295\n\n\n49. 3\n6.818 3\n3.930 3\n1.555 2\n8.941 2\n3.983\n\n\n50. 3\n7.689 3\n4.764 3\n2.357 2\n9.707 2\n4.674\n\n\n51. 3\n8.560 3\n5.600 3\n3.162 3\n0.475 2\n5.368\n\n\n52. 3\n9.433 3\n6.437 3\n3.968 3\n1.246 2\n6.065\n\n\n53. 4\n0.308 3\n7.276 3\n4.776 3\n2.018 2\n6.765\n\n\n54. 4\n1.183 3\n8.116 3\n5.586 3\n2.793 2\n7.468\n\n\n55. 4\n2.060 3\n8.958 3\n6.398 3\n3.570 2\n8.173\n\n\n56. 4\n2.937 3\n9.801 3\n7.212 3\n4.350 2\n8.881\n\n\n57. 4\n3.816 4\n0.646 3\n8.027 3\n5.131 2\n9.592\n\n\n58. 4\n4.696 4\n1.492 3\n8.844 3\n5.913 3\n0.305\n\n\n59. 4\n5.577 4\n2.339 3\n9.662 3\n6.698 3\n1.020\n\n\n60. 4\n6.459 4\n3.188 4\n0.482 3\n7.485 3\n1.738\n\n\n61. 4\n7.342 4\n4.038 4\n1.303 3\n8.273 3\n2.459\n\n\n62. 4\n8.226 4\n4.889 4\n2.126 3\n9.063 3\n3.181\n\n\n63. 4\n9.111 4\n5.741 4\n2.950 3\n9.855 3\n3.906\n\n\n64. 4\n9.996 4\n6.595 4\n3.776 4\n0.649 3\n4.633\n\n\n65. 5\n0.883 4\n7.450 4\n4.603 4\n1.444 3\n5.362\n\n\n66. 5\n1.770 4\n8.305 4\n5.431 4\n2.240 3\n6.093\n\n\n67. 5\n2.659 4\n9.162 4\n6.261 4\n3.038 3\n6.826\n\n\n68. 5\n3.548 5\n0.020 4\n7.092 4\n3.838 3\n7.561\n\n\n69. 5\n4.438 5\n0.879 4\n7.924 4\n4.639 3\n8.298\n\n\n70. 5\n5.329 5\n1.739 4\n8.758 4\n5.442 3\n9.036\n\n\n71. 5\n6.221 5\n2.600 4\n9.592 4\n6.246 3\n9.777\n\n\n72. 5\n7.113 5\n3.462 5\n0.428 4\n7.051 4\n0.519\n\n\n73. 5\n8.006 5\n4.325 5\n1.265 4\n7.858 4\n1.264\n\n\n74. 5\n8.900 5\n5.189 5\n2.103 4\n8.666 4\n2.010\n\n\n75. 5\n9.795 5\n6.054 5\n2.942 4\n9.475 4\n2.757\n\n\n76. 6\n0.690 5\n6.920 5\n3.782 5\n0.286 4\n3.507\n\n\n77. 6\n1.586 5\n7.786 5\n4.623 5\n1.097 4\n4.258\n\n\n78. 6\n2.483 5\n8.654 5\n5.466 5\n1.910 4\n5.010\n\n\n79. 6\n3.380 5\n9.522 5\n6.309 5\n2.725 4\n5.764\n\n\n80. 6\n4.278 6\n0.391 5\n7.153 5\n3.540 4\n6.520\n\n\n81. 6\n5.176 6\n1.261 5\n7.998 5\n4.357 4\n7.277\n\n\n82. 6\n6.076 6\n2.132 5\n8.845 5\n5.174 4\n8.036\n\n\n83. 6\n6.976 6\n3.004 5\n9.692 5\n5.993 4\n8.796\n\n\n84. 6\n7.876 6\n3.876 6\n0.540 5\n6.813 4\n9.557\n\n\n85. 6\n8.777 6\n4.749 6\n1.389 5\n7.634 5\n0.320\n\n\n86. 6\n9.679 6\n5.623 6\n2.239 5\n8.456 5\n1.085\n\n\n87. 7\n0.581 6\n6.498 6\n3.089 5\n9.279 5\n1.850\n\n\n88. 7\n1.484 6\n7.373 6\n3.941 6\n0.103 5\n2.617\n\n\n89. 7\n2.387 6\n8.249 6\n4.793 6\n0.928 5\n3.386\n\n\n90. 7\n3.291 6\n9.126 6\n5.647 6\n1.754 5\n4.155\n\n\n91. 7\n4.196 7\n0.003 6\n6.501 6\n2.581 5\n4.926\n\n\n92. 7\n5.100 7\n0.882 6\n7.356 6\n3.409 5\n5.698\n\n\n93. 7\n6.006 7\n1.760 6\n8.211 6\n4.238 5\n6.472\n\n\n94. 7\n6.912 7\n2.640 6\n9.068 6\n5.068 5\n7.246\n\n\n95. 7\n7.818 7\n3.520 6\n9.925 6\n5.898 5\n8.022\n\n\n96. 7\n8.725 7\n4.401 7\n0.783 6\n6.730 5\n8.799\n\n\n97. 7\n9.633 7\n5.282 7\n1.642 6\n7.562 5\n9.577\n\n\n98. 8\n0.541 7\n6.164 7\n2.501 6\n8.396 6\n0.356\n\n\n99. 8\n1.449 7\n7.046 7\n3.361 6\n9.230 6\n1.137\n\n\n100. 8\n2.358 7\n7.929 7\n4.222 7\n0.065 6\n1.918"
  },
  {
    "objectID": "emp_methods_workshop/quiz1.html",
    "href": "emp_methods_workshop/quiz1.html",
    "title": "quiz 1",
    "section": "",
    "text": "Motivated by …\nThe best research …\nWe invite failure by …\nWe recognize failure by …\nAim not to be right, …\nTest hypotheses by …\nOperationalize, but …\nCompare …\nThe rhetoric of science is …\n\nOn the reverse of this page, recall the above nine slogans without using the initial prompts."
  },
  {
    "objectID": "emp_methods_workshop/program4.html",
    "href": "emp_methods_workshop/program4.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "DAY 4 PROGRAM\n\n\nReview\nData Considerations\n\nTest-retest reliability & Intersubjective reliability\nOutliers\n\nExperimental Design\n\n21 Experimental Paradigms\n\nUseful Research Tools\nQualitative Methods\n\nParticipant-Observation\nTypes of interviews\nGrounded Theory method\nAnecdotal evidence: Uses and abuses\nHypotheses - Good and Bad\nGroup Task #18: Freelisting and pile sorting\n\nThe Experience of Research Participants\n\nFloor and Ceiling Effects; Use of Distractor Tasks\nPrimacy and recency\nDebriefing participants\n\nReading and Writing Research Articles\n\nWriting an empirical paper\nCritical reading of empirical articles\nGroup Task #19: Writing a draft paper together\n\nPotpourri\n\nWhy experiments fail\nReserved data sets\nReplication & Converging evidence\n\nResearch Practices\n\nResearch ethics\nInteracting with participants; Institutional Review Boards (IRB)\nEconomics of research\nHow to revise a manuscript in response to reviewers’ comments\nHow to criticize the work of others\nAdvice when reviewing submissions\n\nMaking Your Own Way\n\nFacilities, equipment and people\nStarting a subject pool\nGroup Task #22: Job description for research assistant\nThe value of collaboration\nGroup Task #23: Assess your Strengths and Weaknesses\n\nFinal advice\n\nSome research advice (12-minute video)\nFurthering Your Education\nFinal Feedback"
  },
  {
    "objectID": "emp_methods_workshop/funeral_answers.html",
    "href": "emp_methods_workshop/funeral_answers.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Answer\nVolume 17 of the New Grove Dictionary of Music and Musicians identifies seven classic funeral marches. As it turns out 5 of those 7 marches are in the key of F minor. Test the hypothesis that funeral marches tend to be composed in the key of F minor.\nIn order to calculate chi-square, we need to determine the probability of any given musical work being written in F minor.\nThe following table shows a distribution of keys from a convenience sample of 3,121 works from the common practice Western art music tradition.\n\n\n\nTonic\nMajor\nMinor\n\n\nC\n358\n6\n\n\nC#\n2\n2\n\n\nD\n194\n96\n\n\nD#\n0\n2\n\n\nEb\n125\n0\n\n\nE\n24\n35\n\n\nF\n757\n10\n\n\nF#\n2\n4\n\n\nG\n978\n119\n\n\nG#\n0\n2\n\n\nAb\n11\n0\n\n\nA\n126\n61\n\n\nBb\n198\n2\n\n\nB\n2\n5\n\n\nTotals:\n2,777\n344\n\n\n\nNotice that works in F minor account for just 0.32 percent of all works in the sample (10/3121). However, the sample includes both major- and minor-key works. As a proportion of minor-key works, works in F minor account for 2.9 percent of all works (10/344). As it stands, our hypothesis is perhaps poorly defined. One might reasonably argue that funeral marches would naturally all be written in the minor mode. So it seems unfair to include major key works in our calculation of what would be the “expected” distribution. A more conservative statistical test would limit our comparison to the distribution of works in minor keys. If we ignore the major mode, then the probability of a given minor-mode work being in F minor is 2.9 percent (i.e., 10/344). Therefore, for a sample of 7 works, we would normally expect 7 × 10/344 works in F minor. That is, we would expect 0.203489 of a piece.\nAnother question arises regarding the number of categories. Excluding the major keys, there are 14 categories in the above table, representing 14 possible minor-mode keys. Actually, since there were no instances of minor-mode works in A-flat minor, we might exclude that, reducing the number of minor-mode keys to 13. But are there 13 categories in our test? The hypothesis focuses specifically on F minor. So a better way to think of the data is between two categories: either in F minor, or in some other minor key. Accordingly, our chi-square test will be limited to two categories.\nIs the tendency to write funeral marches in F minor statistically significant at the 0.01 level?\nOnce again, for a sample of 7 minor-key works, we would expect 0.2035 (2.9% of 7) works to be in the key of F minor.\n \nOur chi-square value is 113.05. With one degree of freedom, at the 99% confidence level, the critical value of chi-square is 6.635. Since our calculated chi-square value exceeds the critical value, we can discard the null hypothesis, and accept that the results are consistent with the hypothesis. The results are statistical significant at p<0.01. The data are therefore consistent with the hypothesis that funeral marches tend to be written in the key of F minor.\nActually, our chi-square value is so big, that it would still have been statistically significant even if we had chosen the 99.99% confidence level."
  },
  {
    "objectID": "emp_methods_workshop/slogans19.html",
    "href": "emp_methods_workshop/slogans19.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Huron’s Research Slogans\n\n\nMotivated by truth, with no hope of Proof\nThere is no inductive proof. We are not in the business of proving something to be true. We would love to know the truth (if that exists), but we understand that we could never be sure of the truth, even if we had it. The best we can hope for is that what we observe is consistent with our theories.\nThe best research invites failure.\nGive the world an opportunity to tell you that you’re wrong. (This is the essence of good research.)\nWe invite failure by testing predictions.\nTest an idea by making a prediction, and then determine whether the observations are consistent with the prediction.\nWe recognize failure by drawing a line in the sand.\nIn order to make failure obvious, establish a criterion in advance that says, “If the evidence doesn’t cross this line, then I’ll admit failure.” In statistics, the line is referred to as the confidence level.\nAim not to be right, but to be not not right.\nInstead of establishing The Truth, our more modest aim is to be not obviously wrong. When our observations turn out to be consistent with our hypothesis, we don’t claim that we are right; instead the observations suggest that our hypothesis may not be wrong.\nTest hypotheses by operationalizing terms.\nTranslate all of the terms in a hypothesis into concrete things you can measure. We can’t directly measure concepts like “sadness.” We have no choice but to measure things using imperfect rulers.\nOperationalize, but don’t essentialize.\nAll concepts are inherently enigmatic and fuzzy. Terms like “melody,” “listen” or “note” can never be pinned-down. It is impossible to provide comprehensive definitions or grasp the essence of some concept. We are forced to approximate or estimate concepts through operational definitions — but don’t confuse the operational definition with the concept itself, and don’t assume concepts are “real.”\nCompare, compare, compare.\nContrast a “treatment” condition with one or more “control” conditions.\nThe rhetoric of science is the rhetoric of prophecy.\nPeople are most impressed when someone accurately foretells the future. Science is a form of rhetoric whose persuasive power resides in the testing of predictions. The rhetorical power of science comes not from scholars assembling evidence, but from scholars testing predictions.\nHindsight is 20/20.\nMost things seem obvious in retrospect (hindsight bias). When the results aren’t obvious, humans are enormously gifted at coming up with explanatory accounts. We can make up a story for just about any set of data. Post hoc theories don’t have the same plausibility as a priori theories. The true test is making up the story first (i.e., prediction)! Prefer theorizing first, then collect your data.\nReductionism is a method, not a belief.\nWe simplify problems, not because we believe problems to be simple, but because we believe problems to be complex. Restricting our gaze is a useful strategy for discovery.\nDon’t try to explain the whole world at once.\nManipulate one variable at a time. Seek simplicity, even as you distrust it.\nGeneralize, but don’t universalize.\nWhen presenting your results, frame them narrowly rather than broadly.\nAvoid chronic hypothesislessness.\nExploratory and descriptive studies are important, but you can’t invite failure without testing predictions.\nBeware of the post hoc theory.\nThe scholar who only offers theories after looking at the evidence is a scholar who is never wrong. Post hoc theorists don’t allow the world to tell them when their ideas are problematic.\nFrom Question to Theory to Conjecture to Hypothesis to Protocol.\nStart with a question, propose an explanatory theory, derive a conjecture, refine the conjecture into a hypothesis, then operationalize the terms of the hypothesis into a protocol. The protocol provides an action plan for how to carry out the research.\nNo causation without manipulation.\nCausality cannot be inferred unless you manipulate one of the variables. The Experiment is the only type of study in which it is possible to infer causality. Correlational studies don’t allow us to discount the possibility of a “3rd variable.”\nDon’t get stuck with sticky data.\nSeek data independence. Ideally, each piece of data should be gathered from a different source. (Collecting independent data is another way of minimizing the effect of unknown third variables.)\nThe law of large numbers does not apply to small numbers.\nPay attention to sample sizes. The smaller the sample size, the greater the variability."
  },
  {
    "objectID": "emp_methods_workshop/tools.html#some-useful-research-tools",
    "href": "emp_methods_workshop/tools.html#some-useful-research-tools",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "Some Useful Research Tools",
    "text": "Some Useful Research Tools\n\n[1][H/S]\n\nR The most comprehensive statistical package. Open source and free.\nORCID - research name registry\nContributor Roles Taxonomy (CRediT). A website whose aim is to help resolve how to order authors in multi-authored research papers. The website identifies 14 tasks and helps researchers better recognize who did what, and\nTests, instruments and batteries Psychological instruments: Big Five Personality Inventory, Interpersonal Reactivity Index (measure empathy), etc.\nHearing screen inventory. A simple and fast 12-item questionnaire that can be used to quantiatively assess the hearing of a participant without having to conduct an audiometric exam. Questionnaire includes questions like: Can you hear the telephone ring when you are in the room next door? (Never Seldom Occasionally Frequently Always) The questionnaire has been validated by comparing survey responses to actual hearing tests. Hearing screening inventory Coding the hearing screening inventory (Corren & Hakstian, 1992).\nOllen Musical Sophistication Index (OSMI). A simple and quick validated ten-item questionnaire that provides a numerical asssessment of “musical sophistication.” Optimized for adolescents and adults. Does not include any listening or performance tasks. http://marcs-survey.uws.edu.au/OMSI/ OMSI\nGoldsmiths Musical Sophistication Index (Gold-MSI). Goldsmiths Musical Sophistication Index (or Gold-MSI), a novel instrument that measures musical sophistication in a comprehensive way by explicitly considering a wide range of facets of musical expertise as they occur in a Western society. The instrument is designed to measure the broad range of individual differences in the general population, while placing less importance on the much smaller pathological groups (e.g. ‘amusics’) and highly specialist populations (professional musicians).\nSubject Pool Survey\nGordon’s Musical Aptitude Profile.\nMontreal Battery of Evaluation of Amusica (MBEA). A series of tests designed to measure music-related deficits (often arising due to brain damage of various sorts). The Montreal Battery consists of six tests examining contour, interval, scale, rhythm, meter and memory tests. Sound stimuli are typically 5 to 10 Compared with the Gordon;s Musical Aptitude Profile, the Montreal Battery aims to identify disabilities rather than identifying musical talent.\nSocial Networks for Scholars. www.researchgate.net academia.edu Most scholars prefer ResearchGate (RG) over academia.edu. RG derives income from advertising. Academia.edu constantly pesters users to pay to upgrade to Academia Premium.\nHumdrum Toolkit. One of two general software systems (the other being Music21) for conducting corpus studies from digital scores. Humdrum Toolkit\nMusic21. One of two general software systems (the other being the Humdrum Toolkit) for conducting corpus studies from digital scores.\nResearchmatch.com. An online tool for recruiting research participants. Participants pre-register providing detailed demographic information. As a researcher one can recruit participants who meet a variety of criteria, such as as participants who live within a certain radius of some place, with certain specified skills, and meet certain demographic characteristics such as age, sex, race, native language, education level, etc. Pre-registered participants on researchmatch.com typically expect to be paid for their participation.\nGoogle scholar. (scholar.google.com) A useful search engine for scholarly publications.\nGoogle scholar button. An efficient way of identify full references from abbreviated citations (e.g., Smith, Jones & Brown, 2015), and for locating PDF files of the corresponding works.\nZotero. Freeware for organizing references, articles, citations, etc.\nEndnote.\nWord Cloud https://wordart.com\nStackoverflow. Useful site for answering questions about programming. Helpful for trouble-shooting software that isn’t working.\nDatathief. A useful tool for changing graphical figures (such as bar graphs) back into numbers.\nLinguistic Inquiry and Word Count (LIWC). The LIWC project … lists of positive and negative words\nGoogle n-gram.\nMechanical Turk (MTurk). An Internet crowdsourcing marketplace where workers from around the world (called Turkers) are paid to perform tasks (such as completing a survey, answering a question, or participating in an online experiment). Turkers are recruited from 49 approved countries. Depending on the task, payments are quite modest. Consequently, Mechanical Turk is useful for recruiting participants from different cultures at a reasonable cost to researchers.\nExcel. A Microsoft spreadsheet application.\nGoogle Forms\nQualtrics survey software, Qualtrics is a popular software app for producing online questionnaires or online experiments. Qualtrics is able to display images as well as play sound stimuli. It is used for collecting data from web users. Qualtrics requires a subscription. Most universities pay for a blanket subscription that allows use by students and faculty.\nMax-MSP.\nSibelius, Finale, Verovio/Humdrum.\nRISM, RILM,\nThemefinder.org. A search tool for identifying musical themes and works. Allows the user to specify a pitch sequence, interval sequence, contour sequence, scale degree sequence, or rhythm, and returns notated themes that match the search criteria. Limited to three databases: 10,000 Western art music works, Renaissance music, and Germanic folksongs.\n\n\nReferences:\nCorren, S. & Hakstian, A.R. (1992). “The development and cross-validation of a self-report inventory to assess pure-tone threshold hearing sensitivity. Journal of Speech & Hearing Research, Vol. 35, No. 4, pp. 921-928.\nHenry, M. J., & McAuley, J. D. (2013). Failure to apply signal detection theory to the Montreal Battery of Evaluation of Amusia may misdiagnose amusia. Music Perception, 30(5), 480-496.\nMüllensiefen, D., Gingras, B., Musil, J., & Stewart, L. (2014). The musicality of non-musicians: an index for assessing musical sophistication in the general population. PloS One, 9(2), e89642.\nMüllensiefen, D., Gingras, B., Musil, J., & Stewart, L. (2014). Measuring the facets of musicality: The Goldsmiths Musical Sophistication Index (Gold-MSI). Personality and Individual Differences, 60, S35.\nMüllensiefen, D., Gingras, B., Stewart, L., & Musil, J. J. (2013). Goldsmiths Musical Sophistication Index (Gold-MSI) v1. 0: Technical Report and Documentation Revision 0.3. London: Goldsmiths, University of London.\nParaskevopoulos, E., Tsapkini, K., & Peretz, I. (2010). Cultural aspects of music perception: Validation of a Greek version of the Montreal Battery of Evaluation of Amusias.\nJournal of the International Neuropsychological Society,” 16(4), 695-704.\nPeretz, I., Champod, A. S., & Hyde, K. (2003). Varieties of musical disorders: the Montreal Battery of Evaluation of Amusia. Annals of the New York Academy of Sciences, 999(1), 58-75.\nPfeifer, J., & Hamann, S. (2015). Revising the diagnosis of congenital amusia with the Montreal Battery of Evaluation of Amusia. Frontiers in Human Neuroscience, 9, 161."
  },
  {
    "objectID": "emp_methods_workshop/dynamics.html",
    "href": "emp_methods_workshop/dynamics.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Task\nResearch in the field of auditory attention suggests that listeners are more sensitive to crescendos than to diminuendos. In contrast to diminuendos, crescendos are easier to perceive and recognize. Gradual or long diminuendos are especially likely to escape the listener’s attention. Diminuendos are more noticeable if they are rapid rather than gradual. This asymmetry suggests that, if a composer wants the listener to attend to diminuendos as much as crescendos, then the diminuendos should typically be faster (shorter).\n\n\nHypothesis\nIn this task, we will test the hypothesis that\nH1. Musical diminuendos tend to be shorter than crescendos..\n\n\nOperationalization\nWe will operationalize the terms of the hypothesis as follows:\n\n\n\n“diminuendos”\nany hairpin diminuendo marking\n\n\n“crescendos”\nany hairpin crescendo marking\n\n\n“shorter”\nthe horizontal length of the hairpin marking measured in centimeters; if the hairpin spans two or more systems, the summed length of the markings from start to finish\n\n\n\n\n\nSampling\nFor this task we will use a systematic matched-pairs sampling method. Start in a random location in the scores (“M”) section of the Music & Dance Library. For every volume, flip to a random page and identify the first hairpin crescendo you encounter. Measure and record it’s length. Continue forward through the score until you encounter the next occurrence of a hairpin diminuendo. Measure and record it’s length. For the next volume, flip to a random page and identify the first hairpin crescendo you encounter. Measure and record it’s length. To find it’s matched pair, continue backward through the score until you encounter the previous occurrence of a hairpin diminuendo. Measure and record it’s length. Alternate in this way, moving forward and backward from random hairpin crescendos to locate a matching hairpin diminuendo. Complete the accompanying data sheet. Note that each crescendo should be paired with a neighboring diminuendo.\n\n\nRationale\nThis task demonstrates that it is often possible to test theories empirically with a minimum of technical equipment or facilities."
  },
  {
    "objectID": "emp_methods_workshop/reading04.html",
    "href": "emp_methods_workshop/reading04.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Reading #4: Huron (2001)\n\nRead: David Huron (2001). What is a musical feature? Forte’s Analysis of Brahms’s Opus 51, No. 1, Revisited. Music Theory Online, Vol. 7, No. 4. The article can be found at: <http://www.mtosmt.org/issues/mto.01.7.4/mto.01.7.4.huron.html\nN.B. You can hear a recording of the 1st movement of Brahm’s string quartet no. 1 on Youtube at: <http://www.youtube.com/watch?v=AlnhPbsnDFc&feature=related\nThe article is divided into two parts. The first part asks “What is a musical feature?” The second part presents some analytic observations of a string quartet movement by Brahms. While reading this article, keep the following comments and questions in the back of your mind:\n\nWhat motivated this study?\nWhat is “distinctiveness?”\nWhat music is used as the “control” group for the analysis? Is this music an appropriate control?\nWhat rationale is given for the choice of feature measurements?\nWhat is the main conclusion of the analysis section of the article?\nWhat is the main conclusion for the entire paper?\nIf you were Allen Forte, how would you respond to this article?"
  },
  {
    "objectID": "emp_methods_workshop/quiz2.html",
    "href": "emp_methods_workshop/quiz2.html",
    "title": "Understanding the Research Slogans",
    "section": "",
    "text": "Motivated by truth, with no hope of Proof A. Translate all of the terms in a hypothesis into concrete things you can measure. We can’t directly measure concepts like “sadness.” We have no choice but to measure things using imperfect rulers.\nThe best research invites failure. B. All concepts are inherently enigmatic and fuzzy. Terms like “melody,” “listen” or “note” can never be pinned-down. It is impossible to provide comprehensive definitions or grasp the essence of some concept. We are forced to approximate or estimate concepts through operational definitions — but don’t confuse the operational definition with the concept itself, and don’t assume concepts are “real.”\nWe invite failure by testing predictions. C. Exploratory and descriptive studies are important, but you can’t invite failure without testing predictions.\nWe recognize failure by drawing a line in the sand. D. The scholar who only offers theories after looking at the evidence is a scholar who is never wrong. Post hoc theorists don’t allow the world to tell them when their ideas are problematic.\nAim not to be right, but to be not not right. E. People are most impressed when someone accurately foretells the future. Science is a form of rhetoric whose persuasive power resides in the testing of predictions. The rhetorical power of science comes not from scholars assembling evidence, but from scholars testing predictions.\nTest hypotheses by operationalizing terms. F. Test an idea by making a prediction, and then determine whether the observations are consistent with the prediction.\nOperationalize, but don’t essentialize. G. Give the world an opportunity to tell you that you’re wrong. (This is the essence of good research.)\nCompare, compare, compare. H. Most things seem obvious in retrospect. When the results aren’t obvious, humans are enormously gifted at coming up with explanatory accounts. We can make up a story for just about any set of data. Post hoc theories don’t have the same plausibility as a priori theories. The true test is making up the story first (i.e., prediction)! Prefer theorizing first, then collect your data.\nThe rhetoric of science is the rhetoric of prophecy. I. Contrast a “treatment” condition with one or more “control” conditions.\nHindsight is 20/20. J. There is no inductive proof. We are not in the business of proving something to be true. We would love to know the truth (if that exists), but we understand that we could never be sure of the truth, even if we had it. The best we can hope for is that what we observe is consistent with our theories.\nReductionism is a method, not a belief. K. We simplify problems, not because we believe problems to be simple, but because we believe problems to be complex. Restricting our gaze is a useful strategy for discovery.\nDon’t try to explain the whole world at once. L. When presenting your results, frame them narrowly rather than broadly.\nGeneralize, but don’t universalize. M. Manipulate one variable at a time. Seek simplicity, even as you distrust it.\nAvoid chronic hypothesislessness. N. Start with a question, propose an explanatory theory, derive a conjecture, refine the conjecture into a hypothesis, then operationalize the terms of the hypothesis into a protocol. The protocol provides an action plan for how to carry out the research.\nBeware of the post hoc theory. O. In order to make failure obvious, establish a criterion in advance that says, “If the evidence doesn’t cross this line, then I’ll admit failure.” In statistics, the line is referred to as the confidence level.\nFrom Question to Theory to Conjecture to Hypothesis to Protocol. P. Instead of establishing The Truth, our more modest aim is to be not obviously wrong. When our observations turn out to be consistent with our hypothesis, we don’t claim that we are right; instead the observations suggest that our hypothesis may not be wrong."
  },
  {
    "objectID": "emp_methods_workshop/small_numbers.html#the-law-of-small-numbers",
    "href": "emp_methods_workshop/small_numbers.html#the-law-of-small-numbers",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "The Law of Small Numbers",
    "text": "The Law of Small Numbers\n\nThere are 3,141 counties in the United States. Some of these counties have higher rates of cancer than others. The lowest cancer rates are evident in sparsely populated rural areas, especially in the West, Midwest, the South, and Alaska. These counties also tend to be politically Republican. Why do you suppose these counties have lower cancer rates?\nOne possibility is that these counties have less air and water pollution, and that people in rural areas are more likely to engage in regular physical activity. It is also possible that people in rural areas eat a healthier diet with fewer food additives. Another possibility is that strong religious convictions lead to cleaner living with less drug or alcohol abuse. Perhaps the closer friendships and community values found in rural areas tend to lower stress. What do you think? Are these plausible explanations for the lower cancer rates?\nWe might find it helpful to contrast these counties with those U.S. counties that have the highest rates of cancer. The highest cancer rates are evident in sparsely populated rural areas, especially in the West, Midwest, the South, and Alaska. These counties also tend to be politically Republican.\nWhat’s wrong here?\nSuppose that the true normal incidence of cancer is 0.1% of the population per year. If we have a small sample of people, our estimates will be more variable. Small samples are likely to exhibit rates that are potentially somewhat higher or somewhat lower, simply because the sample size is smaller. Urban counties have more people; in effect, they are larger sample sizes, and so the cancer rates are more likely to approach the true population mean of 0.1% per year. It follows that highly populated counties will have less extreme (high or lower than normal) rates of cancer.\n\nSmall Samples are More Variable:\nDaniel Kahneman (2012) summarizes the problem in the following way: “you must exert some mental effort to see that the following two statements mean exactly the same thing:\n\nLarge samples are more precise than small samples.\nSmall samples yield extreme results more often than large samples do.\n\nThe first statement has a clear ring of truth, but until the second version makes intuitive sense, you have not truly understood the first.” (Kahneman, 2012; p.111)\nPeople have difficulty recognizing that smaller samples are less likely to be representative. Small samples will exhibit greater variability, for exactly the same reason that large samples exhibit less variability. Unfortunately, there is a tendency for people to place too much faith in small samples.\nIs a coin fair? Suppose we flipped the coin 50 times. We find that the coin turns up heads 26 times out of 50 (i.e. 52%). But randomly selected smaller samples of the same sequence exhibit considerably greater variability:\n\nThis leads to our next slogan:\nSlogan: The law of large numbers does not apply to small numbers.\nExpect small samples to be poorer estimates of the population values. In particular, be careful when comparing large samples with small samples.\n\n\nMelodic Peaks:\nDo music scholars fall prey to the “law of small numbers?”\nConsider the book HighPoints: A Study of Melodic Peaks by the music theorist, Zohar Eitan. Eitan’s book deals with melodic peaks or climax pitches. He analyzed the melodic high-points in a sample of 100 musical works. On the basis of his analyses, he was able to offer a number of general observations about peak melodic pitches. Most of Eitan’s generalizations are methodologically solid and musically interesting. For example, peak melodic pitches are more likely to occur toward the end of a work, are more likely to correspond with points of harmonic tension, and tend to coincide with the culmination of a crescendo. However, one of Eitan’s observations is problematic.\nEitan noticed that melodic peaks tend to appear uniquely (only once) in a segment. Especially, when compared with other pitches in the melody, the highest pitch tends to appear more rarely.\nIt turns out that most of the pitches in a given melody tend to cluster in the center of the range for that melody. As you move farther away from the average pitch, those pitches tend to occur less frequently. Suppose we asked a group of undergraduate juniors to state their ages. We might find that most students are 20 and 21 years old, somewhat fewer are 19 and 22 years old, and fewer yet are 18 and 23 years of age. If the oldest person in the class is 24, what’s the likelihood that someone else in the class is the same age? We would “discover” that the oldest person in the class is likely to be the only person of that age. Similarly, the youngest person in the class is also more likely to hold a unique age in comparison to the rest of the group.\nThe same thing happens with pitches in melodies. Both the lowest and highest pitches are less likely to be repeated within a phrase than pitches closer to the center of the melodic range. In order to make the claim that high-pitches tend to be unique, one must compensate for the relatively rarity of extreme values — which Eitan did not do. [1]\n\n\nReferences:\nZohar Eitan (1997). Highpoints: A Study of Melodic Peaks. Philadelphia: University of Pennsylvania Press.\nDavid Huron (1999). Zohar Eitan: Highpoints: A study of melodic peaks [book review]. Music Perception, Vol. 16, No. 2, pp. 257-264.\nDaniel Kahneman (2012). Thinking, Fast and Slow. New York: Farrar, Straus and Giroux.\n[1] The pertinent statistical analysis was carried out by Huron (1999)."
  },
  {
    "objectID": "emp_methods_workshop/big_numbers.html#the-size-of-samples",
    "href": "emp_methods_workshop/big_numbers.html#the-size-of-samples",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "The Size of Samples",
    "text": "The Size of Samples\n\nThe purpose of sampling is to allow us to infer some property of a population without having to examine every member of the population. This raises the question of how big the sample should be.\nSuppose we were interested in identifying the average height of people in developed countries. Roughly a billion people live in developed countries, so measuring everyone’s height would be impractical. How many people would we need to include in our sample in order to produce a good estimate of the true average for this large population?\nIn general, people get taller with age until around 20 years of age. With old age, people begin to shrink. With improved nutrition, in most developed countries people have been getting taller over time. Measurements made between 2003 and 2006 indicate that the average height of females (twenty years of age and older) is 1.622 meters (around 5 ft, 4 inches). The average height of males is is 1.763 meters (around 5 ft 9.5 inches).\nThere is no guarantee that every random sample will be representative of the population. Even if we sample a thousand people, we might, simply by chance, have sampled people who are shorter than average, and so our estimate will be biased. This is highly improbably, but nevertheless possible.\nRoughly 95% of all people will be with plus or minus 10 cm of the average height. For women (with a mean of 1.622 meters) this means that 95% of all women will be between 1.522 and 1.722 cm in height. Statisticians are able to determine the likely error for any given sample size. The graph below shows the effect of sample size for estimates of average height. For a sample consisting of just one person, 95% of the time, that person will be within about 10 cm of the true average. If your sample consists of two randomly selected individuals, 95% of the time, the average of these two individuals will be within about 6.6 centimeters of the true average. With a sample of five individuals, 95% of the time, the average of this sample will be within about 2.5 centimeters of the population average. With a sample of twenty people, 95% of the time, the sample average will be within about half a centimeter of the population mean. With a sample of 100 people, 95% of the time, the sample average will be less than 25 millimeters of the population mean.\n\nNote that these values assume that the sampling is truly random. It assumes that any given person in the world is as likely to be sampled as any other person.\n\nThe Law of Large Numbers\nThe law of large numbers simply states that as the sample size increases, the sample is more likely to provide a more accurate estimate of the true population value.\nA fair coin should be equally likely to appear heads or tails. That is, it should appear heads 50% of the time. If you flip a fair coin just once, it will either come up heads 100% of the time or 0% of the time. If you flip a fair coin twice, it will either come up heads 100% of the time, 0% of the time, or 50% of the time. In fact, it will come up heads 50% twice as often as either 100% or 0%. The greater the number of times you flip a coin, the greater the likelihood that the number of times it appears heads will approach the fair value of 50%. That is, the more times you sample the coin’s behavior, the greater the likelihood that the sample value will approach the true population value.\nAt this point, we can introduce some useful terminology.\nThe average (or mean) for the population is represented by the lower-case Greek letter mu (μ).\nThe average or mean for the sample is represented by the value x with a horizontal bar placed on top (x̄). When spoken, this symbol is referred to as “x-bar.” Sometimes the population mean is represented by the uppercase letter “M.”\nThe number of items in a sample is typically represented by the italicized uppercase letter N. The italicized lowercase letter n is sometimes used to identify the number of cases in some sample. For example, twenty-five people were recruited for an experiment (N=25); each participant responded to 10 musical works resulting in 250 data points (n=250).\nIn statistical calculations, we are always interested in the number of ways that values are free to vary. This concept is referred to as the degrees of freedom (abbreviated df). Typically, the degrees of freedom are related to the number of observations made. So as the sample size increases, normally the degrees of freedom also increase."
  },
  {
    "objectID": "emp_methods_workshop/intro.html",
    "href": "emp_methods_workshop/intro.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Welcome and Personal Introductions\n\n\nOrganization\n\nWelcome to Ohio State University. For those of you who are from out-of-town, welcome to Columbus. And for those of you who have traveled further afield, welcome to Ohio and to the United States. We will do introductions in a minute, but first let’s deal with some organizational issues.\nMy names is David Huron [and my name is Dan Shanahan]. We’re the instructors of this workshop.\nIntroduce ____________ (workshop assistant). If you haven’t already completed payment, __________ will be happy to help you.\nRegarding wireless: EDUROAM - from other institutions; otherwise PUBLIC free wifi. (Many local eateries and coffee shops have free WIFI.)\nNo need to take detailed notes. Everything covered will be distributed as printed notes. However, it is helpful to take notes on the main ideas since research suggests that moderate note-taking assists in learning and retention.\nA handful of presentations (about 2 hours in total) I have pre-recorded on video (to save my voice). The videos are also available to you.\nIn addition, all of the powerpoint presentations—will also be made available. (We’ll pass around a data stick at the end of each day, so you can copy them onto your computers.)\nMany group activities planned.\n15 minute breaks at 10:00 and around 2:30. (End of day may vary.)\nDuring breaks, remember that we’re located in a library. It’s okay to converse here in the seminar room and downstairs in the cafe, but please be considerate and avoid conversing immediately outside the seminar room in the main library area.\nLunch is 70 minutes — from noon to 1:10.\nCoffee shop on ground floor.\nWe will have daily feedback questionnaires—which will help us with mid-workshop corrections. Will be anonymous, coded by our workshop assistant.\nThere will be (modest) homework each night.\nFriday afternoon we hope to have time for each of you to discuss your own research plans—and to receive feedback. [Your homework on Thursday night will be to prepare your mini presentation; don’t prepare before Thursday however.]\nLearning by slogan.\n\n\n\nPersonal Introductions\n\nPersonal introductions by participants.\n(Introduce self 2nd-to-last): I am a music lover;\na 13-year-old meets Mrs. Lanuette\nmy experience reading feminist literature (Betty Friedan, Germaine Greer, Susan Brownmiller and Gloria Steinem)—men dominate conversations; my first empirical study: measuring the amount of speaking time\nbegan my career as a composer: led me to psych of music & ethnomusicology\nno formal training in psych, methodology or stats\nmy background in music; intellectual history; philosophy; lapsed Adorno; PhD musicology in Britain\nPlease call me David.\n\nDan Shanahan’s introduction.\n\n\nBackground\n\nOutside of the arts, the principal purpose of scholarship is the acquisition of new knowledge (or the application of that knowledge for practical purposes, e.g. technology, medicine).\nIn the arts, scholarship is more than that. Art has no pre-defined purpose. Art can entertain us, amuse us, enlighten us, even insult us. The notion that art has no pre-defined purpose means that art can be used for all sorts of purposes—including no purpose at all.\nNot all arts scholarship is oriented toward the acquisition of new knowledge. For example, the purpose of much scholarly writing might be to enhance our enjoyment of music. E.g. When I travel, I find it useful to read about a place before I visit. It often makes the experience more meaningful and interesting. Similarly, when viewing a painting or listening to a piece of music, some knowledge of the history of that object often enhances my experience of it.\nMuch music scholarship—including music history—serves this function. In the case of my own field (music theory), a lot of analysis, is what Davy Temperley would call “suggestive.” The analysis is not necessarily intended to tell you truthfully how the music is put together, but to suggest a way of listening that might enhance the listening experience.\nMy piano teacher’s story about Bach’s Art of Fugue: the apex of contrapuntal art, Bach’s awareness of his impending death, and signing his life’s work. My disappointment in learning that this story wasn’t true.\nIn some ways, the origins of modern musicology arose from disappointment when music lovers discovered that stories about their favorite musicians were fabricated or exaggerated. The “truth” can serve to increase our experience of beauty.\nSusan McClary’s description of Beethoven’s fifth?? as a musical portrayal of rape.\n\n\n\nLearning Objectives\n\nDistribute handout: review learning objectives."
  },
  {
    "objectID": "emp_methods_workshop/pothole_answers.html",
    "href": "emp_methods_workshop/pothole_answers.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Circle and identify each of the methodological transgressions in the following description.\nN.B. Some events may be regarded as transgressing more than one methodological principle.\n\n\nFor a number of decades now, considerable controversy has surrounded a well-known theory of melody developed by Dr. Hoch of the Music Department at Poxford College. The theory relates people’s heights to music.\n[]{#double-use data}\nDr. Hoch formed the theory while doing research in the Congo. Hoch discovered that Pygmys sing melodies containing predominantly small intervals, whereas Bantu’s sing melodies containing much larger intervals. In Hoch’s first book, Pitch Reduction in Congolese Pygmy Melody, Hoch noted “This is evidence for my new theory that interval size is proportional to the heights of the people who make the music.” [1]\n[]{#discovery fallacy}\nHoch’s arch rival, Dr. Klein, has noted that Hoch’s theory is suspect because the theory originates in the Egyptian Book of the Dead, where an obscure verse is often translated: “Big people make big music.” [2]\n[]{#ad hominen argument}\nProfessor Ruse, one of Hoch’s defenders, has countered that Dr. Klein’s own views are suspect since Klein has admitted to having taken illegal drugs in the 1960s and holds a degree from Golly College (which is not even rated by the Pricetown Review). [3]\n[]{#research hoarding} []{#data neglect}\nKlein notes that, if only Hoch and Ruse could see Klein’s as-yet-unpublished data from Belize, they would see the error of their ways. [4] Dr. Wise points out that considerable pertinent data is available at the Smithsonian, if only Hoch and Klein would care to examine it. [5]\n[]{#naturalist fallacy}\nAs might be expected, Hoch’s theory has attracted considerable popular attention. An editorial writer for the Lower Arlington Snob has started a crusade:\n“Given that the national anthem contains so many large leaps, it is obvious that the [short] children in our kindergartens should not be singing it.” [6]\n[]{#third variable problem}\nOn the contrary, the OSU basketball Buckeyes coach has realized that the correlation between large melodic intervals and size is the perfect way to increase players’ heights. This year, Swiss yodeling has become a standard part of team practice. [7]\n[]{#cohort bias}\nIn order to resolve the question of whether yodeling causes increased height, Dr. Dee Velop decided to compare the melodies sung by people at different ages. Dr. Velop randomly sampled 900 people — 150 each at the ages of 2, 5, 8, 10, 16, and 25. Dr. Velop found that it is indeed the case that the older (taller) people sing melodies with wider intervals. However, many of the older participants in the study noted that yodeling was especially popular 10 years ago. [8]\n[]{#control failure}\nIn Hoch’s second book, Swedish Swing, Hoch reported on a replication study carried out in Sweden (where people are typically tall). Hoch found that the average interval size for Swedish melodies was 240 cents — which, he says, “only goes to prove my point.” [9]\n[]{#multiple tests} []{#post-hoc hypothesis}\nThe ever-suspicious Dr. Klein decided to study the Inuit of Greenland (who are somewhat short). Klein didn’t find that Inuit melodies had smaller intervals compared with Bantus. Nor did Dr. Klein find evidence in support of another 31 popular hypotheses about melodic organization. But Klein did find a statistically significant (p<0.05) result that Inuit melodies have slighly more notes per phrase than for Bantu melodies. [10] After spending time carefully examining the data, Klein also thought there was some evidence suggesting that Inuit singers take slightly longer to inhale while singing. A formal test of this hypothesis using Klein’s existing data showed that indeed, on average, Inuit singers took 15 milliseconds longer to inhale than Bantu singers and this was significant at p<0.05. [11] [12]\nMotivated by the controversy, Prof. Long carried out a 30-year experiment in rural Bolivia. Dr. Long recruited 1,255 children in 28 orphanages. Each orphanage was randomly assigned to either a “small interval” or “large interval” group. The children in the “small interval” orphanages were taught folk melodies with predominantly small intervals while children in the “large interval” orphanages were taught folk melodies with predominantly large intervals. Children had no access to radio or television. Keeping detailed records of the children’s heights, Dr. Long found that there was no systematic difference in the rates of growth and no difference in the absolute heights between the two groups.\n[]{#ad-hoc hypothesis}\nDr. Hoch was naturally displeased by the study, and has suggested that the reason why Dr. Long failed to find the effect was because of possible differences in diets between the various orphanages. Hoch pointed out that some of the orphanages in Long’s study are known to be located in mountainous areas where diets are less nutritious. [13]\n[]{#instrument decay}\nA more serious problem with Dr. Long’s study was that mid-way through the study, Bolivia changed to the metric system, so the numerical records are an indecipherable mixture of inches and centimeters. [14]\n[]{#anti-operationalizing problem}\nDr. Bliss has questioned the use of either inches or centimeters for measuring height:\n“We all know what height is, and it is only crudely related to such reductionistic measures as”yards” or “meters”.”\nGiven the origin of the theory in the Egyptian Book of the Dead, Bliss thinks cubits would be a more appropriate (though not infallible) measure of height. But Bliss thinks even this is impossible since no one can be sure how long a cubit is. [15]\n[]{#mortality problem}\nOne group that was pleased with Long’s study was the orphanage operators — since the researchers brought funds to support the children’s education. This income was a welcome supplement to selling the shorter orphans to the North Korean submarine program and the taller orphans to OSU basketball Buckeyes. [16]\n[]{#positive results bias} []{#bottom-drawer effect}\nThe Journal of Musicological Musings decided not to publish Dr. Long’s study because the results didn’t support any widely accepted theory of melody. [17] As it turns out, a number of other scholars have independently produced negative results. But each of these scholars has been reluctant to even submit the results of their research. [18]\n[]{#head-in-the-sand syndrome} []{#confirmation bias}\nDr. Hoch thinks there is enough research already, and there is no need for further testing of this well-established and obvious theory. [19] Hoch notes that Stravinsky once said of Ravel “Il est trés petit, non?” — and does anyone doubt that Stravinsky uses wider intervals than Ravel? [20]\n[]{#ipse dixit} []{#universalist phobia}\nDr. Hoch has also noted that no less a musicologist than Sir Percy Smart has stated that the matter is now resolved. [21] However, Prof. Bliss remains skeptical. Says Bliss, “It is pure folley to expect such a simple theory to apply to all of the world’s cultures.” [22]\n[]{#presumptive representation}\nBy contrast, Dr. Pomo has argued that this whole business is part of an upper-class conspiracy to denigrate short people. Yodeling has become an entrenched canon whereas chanting is now widely desparaged as “music for little people.” Paid programing on cable TV frequently shows vertically-challenged people engaged in IQ-demeaning activities while Gregorian chant plays in the background. Short people find that everyone expects them to like “The Monks of Saint Bede.” [23]\n[]{#relativist fallacy}\nDr. Pomo says that no music is any better or worse than any other music. “We all know that no theory can be conclusively shown to be better than any other theory. It simply doesn’t matter whether the height/melodic-interval theory is true or false.” [24]\n\n\n\n[1] double-use data [2] discovery fallacy [3] ad hominen argument [4] research hoarding [5] data neglect [6] naturalist fallacy [7] correlation/causation - 3rd variable problem [8] cohort bias or cohort effect [9] control failure [10] multiple tests problem [11] post hoc hypothesis [12] magnitude blindness [13] ad hoc hypothesis [14] instrument decay [15] anti-operationalizing problem [16] mortality problem [17] positive results bias [18] bottom-drawer effect [19] head-in-the-sand syndrome [20] confirmation bias [21] ipse dixit [22] universalist phobia [23] presumptive representation [24] relativist fallacy"
  },
  {
    "objectID": "emp_methods_workshop/workshop_advice.html",
    "href": "emp_methods_workshop/workshop_advice.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Apart from the breaks, it may be useful to have a 5th-inning stretch from time-to-time.\nDAY 3 (Statistics) is now in pretty good shape — although it is essential to review the material thoroughly before teaching. With regard to statistics, there are four tasks awaiting. First, I need to add a powerpoint/handout on measuring variance. Second, I need to add z-scores/standard scores. This can be illustrated by my work on the interval “preference” in J.S. Bach. Third, I really ought to have a section on measuring effect size. Measuring Cohen’s d is very straightfoward. There is a very nice website with an excellent dynamic demonstration at: http://rpsychologist.com/d3/cohend/ Moreover, a segment on measuring variance would set the stage for measuring Cohen’s d. Fourth, I need to complete my handout on “Some advanced statistics (Introduction).”\nFINAL DAY: Running subjects can be excessively slow if there are a lot of participants. Next time, consider running subjects in the Ethno Lab and 504 so things move more quickly. On FINAL DAY, it is appropriate to have lunch with the participants. On FINAL DAY, one of the students brought cookies (home-baked snacks). I should do that on DAY 5 — was well appreciated on the final day. On FINAL DAY, I stayed until 6PM to converse with participants. People really appreciated that."
  },
  {
    "objectID": "emp_methods_workshop/protocol.html",
    "href": "emp_methods_workshop/protocol.html",
    "title": "The Protocol",
    "section": "",
    "text": "What distinguishes formal observation from informal observation is that formal observation is guided by a protocol. A protocol is a written procedure for doing something, such as gathering information.\n\n\nConsider the following question. I’m curious about sad music. It seems odd that people would willing listen to music that apparently makes them sad. Among my friends, most people say they enjoy listening to sad music, but not everyone agrees. So here is my question: What proportion of the general population enjoys listening to sad music?\nUnfortunately, I can’t afford to pay a pollster to carry out a worldwide survey, and I don’t have the resources to do it myself. I could carry out a survey among students in the School of Music, but they are all musicians. I could simply ask my friends or students, but many of my friends are professional musicians and the majority of my friends are academics—hardly representative of the general population. I spend most of my time in Ohio, California, and Canada. It would be nice to sample from a wider range of people.\nQuestion: When (I asked myself) do I most frequently encounter people who are not like me—people from other regions of the world who are not academics? Answer: Taxi drivers! Almost all taxi drivers I meet are foreigners. Moreover, I am paying the driver. Why not engage the driver as, in effect, a paid research informant?\n\n\n\nSo here is my observational protocol. It only works for taxi rides of about 10 minutes or longer—and only if the traffic and noise conditions permit a safe conversation.\n\nEvery time I enter a cab, I start up a conversation with the driver. Where are you from? How long have you lived in this country? When was the last time you visited your home country? Do you have family here? Do you have any children? How old are they?\n(The aim is to establish a rapport with the driver and to determine his/her cultural background. Also, I want to know if the driver is married—see below.)\nWhat do you miss from home? Food? Family? Climate? What about music from your home country? Who is a musician you like?\n(The aim is to shift the conversation toward music.)\nI like music: can you recommend someone from your culture I could listen to?\n(The aim here is provide me with an opportunity to pull out a pen and paper. I write down the names of recommended musicians and also record the driver’s cultural background. I keep my pen ready.)\nI often ask about dancing, wedding traditions, whether the local community sponsors concerts of visiting musicians, etc.\n(The aim is to keep the discussion centered on music and music-related matters.)\nHaving laid the groundwork, now comes the critical research question. I use the same exact wording each time: “Some people like listening to sad music, and some people don’t like listening to sad music. Which are you?” I ask.\n(With pen in hand, I unobtrusively record the exact words spoken by the driver.)\nIf the driver is married, I then follow up with the same question about his/her spouse.\n(The aim is to expand the sample beyond (typically) male drivers to include females.)\n\nOver past eight years, I’ve collected the responses from thirty taxi drivers, including drivers from Denmark, Ecuador, England, Estonia, Ethiopia, Germany, Haiti, Israel, Kashmir, Morocco, Netherlands, Mali, Pakistan, Peru, Punjab, Somalia, Tunisia, and Turkey. This is hardly a random sample. The majority are Moslems from north Africa or the Indian subcontinent. They are all men between 25 and 50 years of age. They tend to be ambitious (most emigrated from their home countries) and they all enjoy driving cars. Nevertheless, this is a convenience sample that gets me out of the English-speaking academic world in which I am normally stuck.\nThe results so far? Like sad music (13), don’t like sad music (15), sometimes like sad music (2). (I just added the spouse question, and so don’t yet have pertinent results for spouses.) The responses have ranged from one driver who said “I never listen to sad music; it makes we want to commit suicide,” to the New York cabbie from Mali who said “Sad music touches my soul.”\nIncidentally, on a trip to Peru, the driver didn’t speak English, but fortunately, we had a translator in the vehicle. When I asked the question, the translator responded “Oh, all Peruvians love sad music.” She didn’t think it was necessary to ask the driver. Nevertheless, I politely asked her to ask the question of the driver. His answer was that he didn’t like sad music.\nYour research opportunity awaits."
  },
  {
    "objectID": "emp_methods_workshop/contents.html",
    "href": "emp_methods_workshop/contents.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Methods in Empirical Music Research A Workshop for Music Scholars\nN.B. This document is effectively the Table of Contents for all of the text documentation (web pages) for the workshop. The instructor uses this page to access presentation materials. At the end of each day, participants receive an edited copy of this contents document up to the end of the particular workshop day. At the end of the workshop, participants receive an edited copy of the entire document and copies of all the linked web pages. Depending on timing it will be necessary to omit some of planned workshop material. Content materials are marked as follows:\n[1] - means essential material, don’t skip [2] - means important material, aim to include [3] - means unessential material, can omit depending on time [H] - to be presented by Huron [S] - to be presented by Shanahan [H/S] - to be presented by Huron & Shanahan Lines beginning with a plus sign [+] are comments for the instructors. These lines will be deleted in the document sent to participants.\n\nDAY 1\n\n\nTitle page\n\n\nWelcome & Introduction\n\n+Introduction [1][H/S]\n\nGenerals Aims & Preview\n\nDay 1 Program + Participants receive a printed copy of the program at the beginning of each day. [1][S]\nLearning objectives [1][S]\n\nEmpirical Research\n\nPreamble: An Arts and Humanities Approach to Empirical Method [3][H] +(Philosophical [continental phenomenological or formal logic approach to content; +Science [procedural orientation; norms of scientific method] +Humanities approach [We approach it from a rhetorical and ethical perspective] +Please read the “Preamble” as part of your homework today.)\nTypes of knowledge: Inductive, Deductive, Intuitive (powerpoint presentation #1) [1][H]\nSeven big ideas\n\nNo Proof: Motivated by truth, with no hope of proof. [1][S]\nInviting Failure: The best research invites failure. [1][H]\nTesting Predictions: We invite failure by testing predictions. [1][S]\nRecognizing Failure: We recognize failure by drawing a line in the sand. [1][H]\nRefutation is Easier than Confirmation: Aim not to be right, but to be not not right. [1][S]\nOperationalizing: Test hypotheses by operationalizing terms. [1][H] Essentializing: Operationalize, but don’t essentialize. [1][H]\nControl: Compare, compare, compare. [1][S] Counterfactuals: (powerpoint presentation #2) [1][H] Randomized Control Studies - The Case of Microfinance +For participant reading only.\n\nThe rhetoric of science (video - 8 minutes) [1][H]\n\nProphetic Rhetoric: The rhetoric of science is the rhetoric of prophecy. “Science is a narrative activity, conducted by a community of scholars who hold each other to a methodological commitment to making and testing predictions.”\n\nReview the first 9 slogans: Quiz #1 [1][S]\nGroup Task #1: What’s worth knowing? An audience with God [1][H]\nQuestions, conjectures, hypotheses and theories [1][S]\nGroup Task #2: Question, theory or hypothesis? [1][H] (Answers) [1][H]\nGrandmother research [1][S]\nThe quantitative/measurement obsession [1][H]\nGroup Task #3a: Obvious theories - Part 1 [1][S]\nGroup Task #3b: Obvious theories - Part 2 [1][H]\n(Group Task #3 Debriefing) Obvious theories: Hindsight bias\n\nHindsight is 20/20. [1][H]\n\nGrandparent research revisited [1][S]\nTwo forms of reductionism\n\nIn research, reductionism is a method, not a belief.\nDon’t try to explain the whole world at once. [1][H]\n\nEpistephobia [2][S]\nTypes of failure (powerpoint presentation #3) [1][H]\nReview (first 12) slogans [1][S]"
  },
  {
    "objectID": "emp_methods_workshop/behaviors_brief.html",
    "href": "emp_methods_workshop/behaviors_brief.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Types of Behaviors\n\nRecall that empirical knowledge is knowledge gained through observation. When we carry out research involving people, what we observe are various kinds of behaviors.\nFor convenience, it’s helpful to conceptually distinguish six types of behaviors: gross behaviors, social behaviors, topical behaviors, metabolic behaviors, self-report behaviors, and artifactual behaviors. As we’ll see, some behaviors straddle more than one of these six categories, so these aren’t hard-and-fast distinctions. However, they’re still helpful ways of organizing observations.\n\n1. Gross Behaviors\nGross behaviors are those behaviors that involve externally observable actions. Examples of gross behaviors include bobbing your head, swaying, blinking, humming, singing, talking, shouting, and laughing. Two of the most common gross behaviors evident in Western culture including clapping hands and tapping feet. Gross behaviors can also involve aspects of posture, such as slumping, or leaning, lowering your head, or sitting upright. Gross behaviors also include facial expressions, such as frowning, raising or lowering your eyebrows, sneering, pouting, and so on.\nAn interesting example of the use of gross behaviors in a music study is the work of Olaf Post. The famous Concertgebouw concert hall in Amsterdam maintains a library of video-footage of all the concerts that take place. One of the unique features of the Concertgebouw is that there are several rows of seats behind the stage. So the video recordings capture — not just what’s going on on-stage — but also the behavior of (at least) some of the audience members.\nNow when people attend a symphony concert, they tend to just sit and occasionally applaud. But in fact, people (from time-to-time) cross their legs, or scratch their noses — adjust their glasses, turn their heads, fold their arms — in short, people in the audience figet.\nUsing the video recordings, Olaf Post counted the number of figetting events over the course of different performances. He found that the amount of figetting parallels the musical structure in a variety of ways. What’s especially remarkable is that Post found that audiences were significantly less figety when listening to music by Gustav Mahler, compared with a composer like Anton Bruckner.\nThe important point is that — even the systematic observation of figetting carries information that can be potentially useful in music-related research.\n\n\n2. Social Behaviors\nWhen behaviors involve groups of people, the social element becomes foremost. Many social behaviors are really types of gross behaviors, but when the behaviors relate to how people interact it’s more common to speak of social behaviors.\nThe actions of crowds at a concert may include dancing, group clapping, swaying, arm-waving, kneeling, shouting, singing or chanting in unison, and so on. Group movements may be synchronous or asynchronous. Interpersonal gross behaviors may include conversing, holding hands, winking, and other interpersonal actions. \nSocial behaviors include body language such as making eye contact, nodding in agreement, orienting toward or away from someone, and smiling or frowning. Since facial expressions are normally intended to be communicative, one might want to regard all facial expressions as social behaviors rather than gross behaviors.\nSocial behaviors also include networking information. On Facebook, for example, social networks are evident in the patterns of “likes.” Social networks are also evident in e-mail contact lists, in stored phone numbers, subscription lists, and so on.\nIn recent years, important research on the social aspects of music have been carried out using large amounts of data collected through popular web services such as Youtube, Hulu, i-Store, and Amazon. We know, for example, that how people evaluate music is strongly influenced by how other people evaluate music within a perceived social group. For example Matthew Salganik and his colleagues split 14 thousand people on the world-wide web into eight randomly assigned social groups. They showed that the popularity of new (never heard-before) pop songs depended primarily on the mutual awareness of what other people in the social group were listening to. All eight social groups started off with the identical roster of new songs, but they diverged in their musical preferences simply through the happenstance of social influence.\n\n\n3. Topical Behaviors\nIn science, the word topical means “pertaining to the surface.” A “topical medicine” is a medicine that is applied to the skin. Hence, topical behaviors are behaviors that relate in some way to the skin. There are four classic topical measures: skin temperature, skin conductance, pilomotor activity, and electromyographic responses.\nSkin temperature changes slowly and can be measured with a topical thermometer. Some parts of the body tend to be warmer than others, and the distribution of heat can change over time. Skin temperature can also be imaged using an infrared camera. Skin temperature is determined principally by the amount of blood flowing near the surface of the skin. Some changes of blood flow can be seen directly — such as when a person blushes or when they go pale. But most changes are more subtle — although they can still be measured. When a performer is suffering from stage-fright, their hands and feet will tend to become cold. This occurs because of the constriction of blood vessels in the body’s extremeties. This phenomenon is referred to as peripheral vaso-constriction.\nA second topical measure relates to how much we sweat. The skin is covered with sweat glands, and the amount of sweating can change rapidly. Sweat is a fluid with a high salt content, which means that it’s a relatively good conductor of electricity. Two electrodes are placed on the skin, and a small (unnoticeable) voltage is applied continuously. When sweating increases, the resistance of the skin drops and so the electrical current increases.\nThese changes can be quite rapid (on the order of half a second or so) and are easily (and cheaply) recorded. Measures of skin conductance are variously referred to as galvanic skin response (which is abbreviated GSR) or as skin conductance response (which is abbreviated SCR). Skin conductance is widely used in polygraphs — that is, in lie detectors. Sweating increases rapidly when the sympathetic nervous system is active. This includes experiences of fear, the startle response, the defense reflex, the orienting response, feelings of anger, and sexual arousal.\n A third topical response involves body hair. hair follicles can flex or relax. When the follicles flex the skin takes on a distinctive goose-bump texture. The technical name for this is the pilomotor response. This response is musically important in the phenomenon of frisson where a listener experiences “chills” or “thrills.” With the help of a video camera and a close-up lense, the pilo-motor response can be observed visually. We simply point the camera at the back of the neck or the arm of a participant. Looking at the camera output, an observer can then visually characterize the degree to which the skin takes on a gooseflesh appearance.\n A fourth topical measure arises from the muscles underneath the skin. Muscles produce electrical potentials that can be measured. These measures are referred to as electromyography (abbreviated EMG). EMG potentials can be measured either at the surface of the skin, or more directly by inserting a small needle or electrode into the muscle. Muscles generate large electrical potentials when they’re flexed, but they also produce spontaneous electrical activity when at rest. The resulting muscle activity can be highly informative depending on the muscle being monitored. For example, facial muscles echo various emotional states. By way of example, EMG activity in the zygomatic muscles (involved in smiling) provides a good indication of positive feelings — even if there’s no visible evidence of smiling. An example of the use of EMG in music research is in the work of Ulf Dimberg who has demonstrated that EMG activity provides a useful measure of the perceived pleasantness or unpleasantness of a sound.\nA particularly illuminating set of muscles are those related to the eyes. We can identify three main aspects to eye behavior. The simplest is blinking — which is one of the most reliable indicators of the startle response. When you hear a loud unexpected sound, it’s very likely that you’ll blink. A second aspect is the size of your pupils. The dark region at the center of the eye responds very quickly and continuously to changes of stimulation, including sounds. A third aspect is the movement of the eyes themselves — known as saccadic movements.\nJohn Sloboda has carried out studies examining how the eyes move when musicians read musical notation. It turns out that the movements of the eyes differ depending on whether the music is predominantly homophonic or predominantly polyphonic. There are also observable differences between how the eyes move for highly trained musicians versus less experienced musicians reading musical scores.\nNotice that eye behaviors form a special category that might be regarded as either gross behaviors or as topical behaviors.\n\n\n4. Metabolic Behaviors\nMetabolic behaviors relate to body conditions under the skin’s surface. Classic examples of metabolic behaviors include heart-rate, blood pressure, (core) body temperature, and respiration.\nOf course many metabolic observations can be measured from the body’s surface. For example, we can measure a person’s pulse simply by pressing a couple of fingers against their wrist. Or we can measure blood pressure using a cuff wrapped around a person’s arm — and so on. There’s certainly room to debate whether these should be categorized as metabolic or topical measures. Nevertheless, traditionally, heart rate and blood pressure are considered metabolic measures.\nPulse can be measured with wrist-watch like devices that can collect data continuously over many hours or even days. Two classic heart responses are of particular interest: the tachycardic and bradycardic responses. A tachycardic response occurs when the heart-rate increases briefly and then returns to normal. This is associated with fear or alarm. A bradycardic response occurs when the heart-rate decreases briefly, rebounds above normal briefly, and then returns to normal. This is associated with interest or attentiveness.\nAnother measure of interest is the heart-rate variability (or HRV). This has been found to be especially informative in a variety of different ways.\n Another metabolic behavior is breathing or respiration. Respiration can be measured using a string-like or band-like device that’s placed snugly around a participant’s chest. It can also be measured using an electronic device taped to a person’s chest. Breathing influences the amount of oxygen in the body. Rather than measuring respiration, there exists a simple device (an oximeter) that clips onto the finger — and can be used measure more directly the oxygen level in the blood.\n One of the most useful physiological measures is the electrical activity generated by the brain. Eletroencephalography (or EEG) measures tiny changes of voltage that reflect the electrical activity of large groups of neurons. Although these voltages are very small, they can still be measured by placing electrodes on a person’s scalp. EEG measures are rather crude — representing the average activity of assemblies of millions of neurons. When enough neurons fire at roughly the same time, they create a big-enough electrical potential to be measured at the surface of the head.\nIn music-related research, it’s most common to look at the electrical activity that’s evoked in response to a particular sound. These are so-called event-related potentials (abbreviated ERPs). ERPs have characteristic signatures that can indicate, for example, whether a person noticed or paid attention to a given sound. These signatures have proved particularly useful in answering questions about what infants are able to hear or distinguish. For example, research by Laurel Trainor and her colleagues has determined when infants are able to recognize atonal violations of tonal melodies. Infants can’t talk about what they hear, but their EEG responses can give important clues about how they’re experiencing the sounds.\nAn important class of metabolic measures is to be found in hormone levels. Hormones are chemical messengers, commonly transported by the blood that influence cell behavior in many ways. Examples of hormones include epinephrine and norepinephrine (also known as adrenaline and noradrenaine), dopamine, serotonin, oxytocin, prolactin, cortisol, histamine, estrogen, testosterone, insulin, as as others. Some of these hormones (like testosterone) can be measured in saliva. So we can have a participant simply spit into a small cup, or use a mouth swab. Another common technique has the participant chew on a small absorptive plastic device.\nOther hormones can be measured only by taking a blood sample. Depending on the sensitivity of the assay method, the researcher may need only a small dab of blood (that can be collected from a pin-prick), or a larger sample of blood that would require a professional nurse to do a blood-draw.\nSome hormones are present only in the brain. Unfortunately, these hormones can be measured only by examining the cerebro-spinal fluid — which is gathered through a spinal tap. This procedure is much too invasive to be used for casual research purposes, like studying music.\nAn example of observing changing hormone levels in response to music is the work of Hajime Fukui who found that testosterone levels are lower when people listener to their preferred music.\n In the past decade, it’s become increasingly common to use brain scanners in music research. At least four types of scanners can be distinguished. Functional Magnetic Resonance Imaging (or fMRI) can be used to measure how much oxygen is being used in different parts of the brain. Areas of the brain that are the most active use more oxygen. The localized blood volume in that area changes rapidly, and fMRI methods can be used to pin-point these places when a person is engaged in different tasks. MRI machines are noisy — producing a loud intermittent banging sound that limits their use in auditory tasks. The machines are bulky and expensive.\nPositron Emission Tomography (or PET) involves the injection of mildly radio-active substances into the blood-stream. Depending on the substance that is injected, PET can provide very useful indications of where in the brain a particular neurochemical is released or congregates. Using this technique, for example, Varlery Salipoor and her colleagues were able to show that listening to chill-inducing music released dopamine in regions of the brain associated with the experience of pleasure. PET is quite invasive since participants are injected with radio-active substances. The preparation of these materials by an experienced chemist makes PET especially expensive.\n A third technique is MagnetoEncephalography (or MEG). Like EEG, this method traces the electrical activity of the brain. However, it monitors magnetic information rather than voltage — giving much better resolution and is able to provide better information about activity going-on deeper in the brain. Compared with fMRI and PET, MEG has an especially good temporal resolution, resolving electrical events in the brain with a precision around 10 milliseconds. Since MEG scanners measure magnetic fields, the scanning must take place in a magnetically shielded room. In addition, liquid helium is used to cool the machine, which raises the cost significantly.\nA fourth method is functional Near-Infrared Spectroscopy (or fNIRS). Infrared light is able to penetrate through the scalp and skull, and for some distance into the cortex. The reflected light is useful for detecting changes in blood hemoglobin. Like fMRI, fNIRS can be used to infer oxygen uptake in regions of the brain associated with neural activity. In addition, NIRS is much more portable than fMRI, PET, or MEG machines. Some manufacturers provide wireless instruments that allow researchers to study freely moving people. However, due to the absorption of light, NIRS can only be used to scan the outer cortical tissues of the brain. It is unable to measure subcortical structures. NIRS is especially effective for imaging the brains of infants. The technique is non-invasive, and the thin skulls of infants allows the light to penetrate more deeply into the neural tissue.\n\n\n5. Self-Report\nWithout question, the most common behaviors used in research is the self-report. Self-report is the label given to any behavior in which we simply ask participants something. An obvious example of a self-report is when we ask someone to indicate their age on a questionnaire. In these cases, we rely on special knowledge possessed by the participant.\nSelf-report observations can be collected in many ways, including both formal and informal interviews. For example, we might ask a child what songs she knows, how long she’s been studying an instrument, or what she thinks of her music teacher.\nQuestionnaires and surveys all involve self-report. These can be done with pencil and paper, through brief interviews, or using electronic media such as having people answer questions via the web.\n Many self-report behaviors involve asking someone to offer an opinion. IQ tests and personality tests — all rely on self-report. Often responses are structured, so that the participant simply selects one of a set of predefined answers. For example, a statement may be presented, and the participant is asked to indicate the degree to which they agree with it: strongly-agree, agree, agree somewhat, undecided, disagree somewhat, disagree, or strongly disagree. In music, it’s common to ask which of two sounds is more dissonant, or more memorable, or more whatever.\nThese are referred to as forced-choice responses. If only two choices are provided, the mode of responses is commonly known as a two alternative forced choice — which is frequently abbreviated 2AFC. Responses might involve typing on a computer, pointing-and-clicking, or pressing a button of some sort.\nSelf-reports include elicited statements and introspective reports — such as asking a performer to describe (for example) how he or she begins practising a new musical work.\nApart from interviews, conversations, surveys or questionnaires, responding by pressing a button is also typically regarded as a form of self-report.\nSelf-report is commonly used in research, even when there are more objective methods available. For example, rather than asking a person their age, we might follow-up by using government records to obtain a copy of their birth certificate. But in the majority of studies, the researcher has little reason to doubt the accuracy of the information provided by the participant.\nNevertheless, there are common situations where participants do not accurately self-report. For example, in self-report, people often claim to be taller than they are, and it’s very common for people to mis-report their weight. When accurate data is needed, the researcher may choose to perform objective measurements.\nThe biggest advantage of self-reports is that they are often the easiest kind of data to collect. However, they are also easily confounded by the views, the ideas or the beliefs of the participant. These beliefs may or may not be accurate. For example, a musician might intellectually conclude that there’s nothing inherently sad about the minor chord. Our musician might even claim that he or she doesn’t hear the minor chord as having any sad connotations. In many cases, we should simply believe what a person says. But sometimes people deceive themselves — and so what is said doesn’t necessarily accurately reflect what they’re experiencing.\n\n\nImplicit Measures\nIn these sorts of cases it’s useful to use a method in which the person’s beliefs are sidelined. One of the best ways to do this, is by employing so-called implicit measures. A simple example is provided by the affective priming method. Frank Ragozzine carried out a simple experiment where he flashed words on a screen and asked participants to respond as quickly as possible to whether the word was a Happy word or a Sad word.\nFirst of all, this task is easy. The words are very clearly happy or sad. For example, sad words might include glum, down, sorrow, blue and blah. Happy words might include smile, jolly, pleased, jumping and sunny. The participant simply had to press one of two buttons as quickly as possible — either the happy-word button or the sad-word button.\nNow consider what happens when we play either a major or minor chord immediately before the word appears on the screen. What Ragozzine found was that playing a major chord improved the reaction speed for happy words but reduced the speed for sad words. At the same time, he found that playing a minor chord reduced the speed of reaction for happy words and improved the reaction speed for sad words. In other words, the chords facilitated performance when they conformed to the major-happy or minor-sad conventions — but they interferred with performance when the didn’t match the major-happy or minor-sad conventions. What’s nice about this method is that the task is simply too fast for people to give any thought. We can test people from different cultures and different backgrounds and see how they respond — without having to rely exclusively on what they say or claim.\nIn general, researchers prefer these implicit observations over explicit measures. Implicit methods reduce the impact of what a person believes — and instead focuses on how they behave. The most revealing kinds of behaviors are those that are spontaneous and unconscious. When a person taps their foot or smiles, it’s likely that they’re enjoying the music — whatever they might say. Spontaneous and implicit measures have been used, for example, to determine how prejudiced a person is. Few people would ever say that they are prejudiced against people of African descent, or women or foreigners. However, various implicit association tasks can be quite revealing about a person’s unspoken dispositions. Our actions can betray attitudes that our conscious selves might find quite uncomfortable.\nAs researchers, then, we’re always on the look-out for spontaneous, easily observable behaviors that are not strongly regulated by conscious thought.\nI recall one day, casually listening to a recording by the musical humorist Peter Schickele — better known as P.D.Q. Bach. It was a live recording — and of course there was laughter. Now music-induced laughter isn’t a common behavior. But I remember being so struck by the behavior itself: laughter is an easily observable behavior that’s quite spontaneous and not strongly regulated by conscious thought.\nThat observation led to a major study of musically-evoked laughter. With Joy Ollen, for example, we carefully studied 640 instances of audience laughter in response to musical jokes.\nAlthough it might seem tangential to more commonplace musical experiences, we learned a lot from that research.\nSo once again, although self-report is perhaps the most important source of behavioral observations for studying music, there are times when the researcher needs to be suspicious of the capacity of participants to accurately introspect and honestly report what they’re experiencing.\nA useful check on observations from self-report, is to look for converging evidence using implicit methods where the conscious thoughts and beliefs of the participant are sidelined.\n\n\n6. Artifactual Behaviors\nA final category of behaviors might be called artifactual behaviors. People create things: in cultures all over the world we find all kinds of different musical instruments; musicians also make recordings — in lots of different formats; people make music videos, they take photographs, they write in diaries and blogs, they write program notes, they even write lyrics down on slips of paper.\nIn the case of music, some of the most important artifacts include sound recordings and musical scores. Scores and recordings are the familiar starting points for many forms of traditional music scholarship — especially the work of music theorists and scholars doing music analysis.\nWhen approached systematically, empirical observations rely on the usual quantitative measures. A scholar might count the number of instances of a certain event — like the number of Neapolitan chords in some corpus. Even simple counts — like the number of notes or the number of measures in a work can sometimes prove useful.\n Historical musicologists make use of a variety of techniques for making sense of manuscripts. For example, paying close attention to the hand-writing in a manuscript might help to resolve whether two manuscripts were written by the same person, or by two different people.\nIt’s often helpful to measure or count features as a way of helping to resolve a question or conjecture. What is the average angle of the stems compared with the horizontal staff. Is there a slight leaning to the left or to the right? How long are the stems? What kind of cursive style is used for the textual underlay — and so on. Tallying-up a series of physical measures from a manuscript can help provide converging evidence for a particular historical interpretation.\nConventionally, analysis might involve interpreting the harmonies in a score and perhaps relating certain harmonic progressions to different styles, genres or periods.\nIn recent decades, many musical scores have become available online, and so many measurements can be automated with the assistance of computer software.\nA simple example here might be the so-called melodic arch. Musicians long ago observed an apparent tendency for melodic phrases to rise upward and then fall downward forming a sort of arch. Think of “My Bonnie Lies Over the Ocean” or “Somewhere Over the Rainbow.” In each case, the phrases tend to ascend at the beginning and descend towrd the cadence. But no sooner do you think of this — than all kinds of exceptions come to mind. So, for example, the song “Joy to the World” and the American national anthem both begin high — dropping downward — and then rise upward toward the end. So is there any merit to the notion of a melodic arch? \n The three accompanying graphs are from Huron (1996). The graphs show what happens when the pitches for a large number of melodic phrases are averaged together. The three graphs illustrate phrases of 7-, 8- and 9-notes in length. In each graph, the first plotted point represents the average pitch height (in semitones above middle C) for all the first notes in the phrases. The second point represents the average pitch height for the seconds notes in the phrases, and so on.\n Although we can identify plenty of individual exceptions, on average, it is indeed the case that for many genres of Western music, the melodic arch is more than a figment of our imaginations. In general, there is a tendency for melodies to ascend and then descend over the course of individual phrases.\nAs we’ve seen, there are innumerable artifacts arising from human behavior that can be measured or tabulated. These include scores, sound recordings, and even from musical instruments. All of these can provide useful observations — either for exploratory research or for hypothesis-testing.\n\n\nReferences:\nUlf Dimberg (1987). Facial reactions and autonomic activity to auditory stimuli with high and low intensity. Psychophysiology, Vol. 24, p. 586.\nUlf Dimberg (1989). Perceived unpleasantness and facial reactions to auditory stimuli. Uppsala, Sweden: Uppsala Psychological Reports, No. 414.\nHajime Fukui (2001). Music and testosterone: A new hypothesis for the origin and function of music. Annals of the New York Academy of Sciences, Volume 930, pp. 448-451.\nDavid Huron (1996). The melodic arch in Western folksongs. Computing in Musicology, Vol. 10, pp. 3-23.\nOlaf Post (2011). “The way these people can just listen!”: Inquiries about the Mahler tradition in the Concertgebouw. PhD Dissertation, Columbia University Department of Music.\nFrank Ragozzine (2011). Cross-modal affective priming with musical stimuli: Effect of major and minor triads on word-valence categorization Journal of ITC Sangeet Research Academy, Vol. 25, pp. 8-24.\nMatthew Salganik, Peter Dodds & Duncan Watts (2006). Experimental study of inequality and unpredictability in an artifical cultural market. Science, Vol. 311 (February 10, 2006), pp. 854-856.\nValorie Salimpoor, Mitchel Benovoy, Kevin Larcher, Alain Dagher & Robert Zatorre (2011). Anatomically distinct dopamine release during anticipation and experience of peak emotion to music. Nature Neuroscience, Vol. 14, pp. 257-262.\nJohn Sloboda (1985). The Musical Mind. Oxford: Oxford University Press.\nWeaver (1943). [On eye movements while reading music.]"
  },
  {
    "objectID": "emp_methods_workshop/interviews_open.html",
    "href": "emp_methods_workshop/interviews_open.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "The Exploratory Open Interview\n\nThe open interview is one of the very best exploratory research methods. Open interviews are commonly used by social workers, detectives, clinical psychologists, psychiatrists, sociologists, historians, investigative journalists, as well as anthropologists and ethnomusicologists. The open interview is also known by other names, including qualitative interview, unstructured interview, free interview, narrative interview, depth interview and nondirective interview.\nDifferent disciplines have different names for the person being interviewed. A detective interviews witnesses and suspects. Social workers interview clients. Clinical psychologists interview patients. Sociologists interview subjects, participants or respondents. Anthropologists and ethnomusicologists interview informants, experts or research collaborators. In our discussion, we’ll refer to people being interviewed as either interviewees or respondents.\nOpen interviews are valuable for at least three reasons. First, open interviews invite observations that are unanticipated by the researcher. Interviewees provide access to first-person “facts” that are otherwise inaccessible. Interviews can draw attention to unique experiences that are beyond the personal experience of the researcher. The interview can draw attention to novel phenomena, and different forms of human experience.\nSecond, interviews can introduce the researcher to different ways of thinking about a question or phenomenon. Interviewees are apt to offer different perspectives or unique interpretations. The ideas offerred by interviewees inevitably expand the horizons of the researcher. Interviews often provide inspiring new conjectures and theories that stimulate the researcher’s imagination. As Robert Weiss has noted, “those who do qualitative interview studies invariably wind up knowing a lot about the topic of their study” (Weiss, 1995).\nThird, interviews can provide vivid first-person anecdotes. Charities have long recognized the value of personalizing tragic situations. One might make a charity appeal by distributing statistics, such as evidence that 2.2 million people in a particular region of the world are starving. However, charity appeals are much more effective if one tells the story of a single individual: Maria, an 8-year-old child, who, with her mother, is starving in a modest hut. Although the starvation of millions of people is clearly a more serious problem than the starvation of one or two people, the concrete personalized story is far more compelling to human minds.\nA similar phenomenon occurs in research. People often find research results dry and uninteresting. Researchers can make the results of research much more compelling to readers by illustrating various findings through personal anecdotes — the sorts of anecdotes that arise from personal interviews rather than through formal experiments or through impersonal questionnaires. Interviews give the researcher access to potentially evocative narratives — stories that can illustrate a phenomena in concrete, human terms. Lifeless theoretical accounts can be brought alive through the judicious relaying of narrative examples.\nBy way of summary, open interviews can lead to (i) new facts, (ii) new theories, and (iii) vivid illustrations. Open interviews give us access to the experiences of others. Interviews offer new insights.\n“Quantitative studies pay a price for their standardized precision. Because they ask the same questions in the same order of every respondent, they do not obtain full reports. Instead, the information they obtain from any one person is fragmentary, made up of bits and pieces of attitudes and observations and appraisals.” [l. 142]\nInterviews also suffer from a number of problems. The information collected is anecdotal, often impressionistic, and conveyed in a social situation in which the respondent is strongly motivated to be viewed with sympathy or admiration. The “presentation of self” typically looms larger in inter-personal situations (such as interviews) compared with the more impersonal or anonymous questionnaire or experiment. We will address the advantages and disadvantages of interviews in more detail at the end of our discussion.\n\nRecruiting Respondents\nBecause interviews are labor-intensive, sample sizes are commonly small. It is common, therefore, for researchers to abandon the goal of recruiting a representative sample. Instead, it is common to recruit people who have special knowledge or extensive experience related to some phenomenon. Highly experienced or knowledgeable respondents are commonly referred to as informants, witnesses, experts or research collaborators. Alternatively, the researcher might aim to recruit a heterogeneous group of participants — people who are apt to represent a range of contrasting experiences or perspectives. As an exploratory research method, the main aim of open interviewing is to collect new ideas. Consequently, a representative sample may be less valuable than a heterogeneous sample.\nBy way of example, a study examining “the concert experience” might focus on interviewing audience members attending classical and popular concerts. However, depending on the aims of the study, a researcher might wish to expand the list of interviewees to including concert promoters, artist managers, publicists, house managers, ticket sellers, scalpers, conductors, musicians, and ushers. Once again, the aim of the sample is not to be representative of average or typical experience, but to be representative of the range or variability of experience.\nIn ethnographic work, it is common for the researcher to seek out an expert, sometimes called a “key informant.” This is someone with an unusually high level of expertise or advanced knowledge related to the researcher’s subject of interest. Commonly, this is a master musician or master teacher in some musical tradition. Once again, it is hoped that what is lost in representativeness is made up for by the wealth of nuanced detail and the opportunities to glimpse unanticipated experiences.\nIn some studies, representativeness may be an important goal. Especially if the interview duration is short, it may be possible to assemble a representative sample. From time-to-time there are some well-funded projects that may hire a number of people to carry out the interviewing. A big project might have 30 interviewers. If each interviewer conducts 35 interviews, then the project will be able to draw on information from over a thousand participants. In these projects, it may be possible to assemble a good representative sample.\nEven for non-representative samples, a researcher needs to know when they have interviewed a “sufficient” number of people. A general rule of thumb is to stop interviewing new people when the interviews keep covering the same ground with little new information. Robert Weiss offers an unusual criterion for knowing when to stop an interview study:\n“A colleague of mine once offered as a general rule that if your report on a group you have studied has truly gotten to the inner dynamics of the group, you can never again visit that group with safety. His idea was that every group has secrets kept from outsiders that the group would be loath to have revealed and still other secrets, even more fundamental, kept from itself, whose surfacing would enrage it.” (Weiss, 1995, l.3701)\nIn recruiting interviewees, it is important to consider a control group. For example, Ehrensaft (1990) carried out a study of married couples who share parenting. However, Ehrensaft only interviewed couples who share parenting duties without interviewing a comparison group of couples who do not sharing parenting. This makes it nearly impossible to claim that any observation is characteristic of couples who share parenting. Unfortunately, it is common for interview-based researchers to overlook the need for a control group.[1] (See also Weiss, 1995).\n\nSnowball Sampling\nIn the case of recruiting experts, one must often rely on the experts themselves to help you contact and recruit other experts for interviews. This is referred to as snowball sampling. As you might expect, snowball sampling is likely to introduce bias, since there is a strong likelihood of interviewing individuals who are similar. In order to reduce this bias, the researcher should aim to follow lines of referral that lead to people who are unfamiliar with earlier interviewees. For example, suppose that Alice recommends Bill, and you interview them both. Then Bill recommends Carol and Daniel. If Alice knows Daniel but not Carol, then it is preferrable to interview Carol over Daniel. By following this strategy, it is less likely that the researcher will interview only individuals within a single social network. In ethnographic fieldwork, it is common for the researcher to switch to a different village or locale in order to avoid undue emphasis on a single social group. Snowball sampling is a sort of convenience sampling — one that helps in recruiting people with special knowledge or expertise.\nNotice that snowball sampling is biased against those who have smaller social networks. Experts who are less sociable are less likely to be known by other experts, and so are less likely to be interviewed.\n\n\nDuration\nAll research requires a time commitment from those who participate in the study. The duration of the commitment can be quite variable. A questionnaire might take only 5 minutes to complete. An experiment might be finished in 30 minutes. Interviews, by contrast, are frequently the most demanding in terms of time commitment. In most interview research, 90 minutes is a common time period for an interview, and many studies interview participants several times. As a result, interviews often place considerable time demands on participants.\nThe time committment for an interview can lead to sampling problems. Some people are more cooperative in interviews than others. People who are retired, unemployed, or eager for company are most welcoming to interviewers. People who are busy simply cannot afford the time. Consequently, interview methods tend to be biased toward people who have leisure time. This sampling bias can sometimes be alleviated by choosing places where even busy people have time to spare. Busy people may find themselves with free time when in waiting rooms, in airports, or at hotels in the evening. Some interviewers choose to recruit interviewees at leisure or vacation destinations, like beaches, pools, union halls, church socials, sporting events, yacht clubs, etc.\nPotential respondents are often suspicious of anyone requesting an interview. It helps to identify an institutional affiliation, such as a university, or a source of government funding, such as the NSF. Respondents are less likely to participate if the research has a corporate sponsor, and even less likely to participate if there is no apparent institutional affiliation. It helps to provide a business card, with contact information. This will reassure possible interviewees. It helps to have an institutional position (professor), or to cite a principal investigator when recruiting. Recruiting can be done via fliers, email, web announcements, telephone solicitation, personal approach, through local newspaper, media advertising or news stories.\nFor some topics, the researcher might employ a form of quota sampling, in which a fixed number of interviews are conducted with people representing several roles: e.g. interview 3 composers, 3 conductors, 3 performers, and 3 listeners.\n\n\n\nThe Interview Guide\nAlthough open interviews may have few predefined questions, they are normally shaped by an Interview Guide. The interview guide is a written document that identifies the goals of the interviews, lists the topics of interest, and provides some suggested questions. The guide may or may not be consulted during the interview itself, although it is common toward the end of the interview for the interviewer to refer to the guide to ensure that all of the pertinent topics have been covered.\nAs with other research approaches, it is important to test the interview guide by conducting some pilot interviews. It may be impossible to employ an appropriate expert for the pilot interview. Nevertheless, even a non-expert interviewee in a pilot interview may help you recognize ways to refine and improve the interview guide.\nWhen the research topic relates to specific activities, like composing or practising, the guide may follow a diachronic sequence. For example, a guide related to the experience of attending a concert might include the following:\n\nBecoming aware of an up-coming concert.\nThe decision to attend.\nInviting friends, securing tickets.\nTravel to the concert.\nPre-concert activities (e.g., dinner).\nIn the hall before the start.\nThe concert experience.\nIntermission activities.\nPost-concert activities.\nThe next day: discussion with friends. Monday-morning quarter-backing.\nConcert memories, reminiscence, assessment.\n\nThe guide is intended to help the interviewer organize questions, or to ensure that the respondent has thoroughly addressed the various components in a concert experience.\n\n\nConducting the Interview\nEnter the interview prepared. Confirm the interview appointment in advance and show up a little early. Make sure you are fully familiar with the Interview Guide, that you have two copies of the consent form, and that your recording gear is operating propertly.\nThe first order of business is to establish a positive and professional rapport with the respondent. This begins with an explanation of the aims of the research, a conversation about the formal consent form, and a discussion about the use of the recording. By clearly stating what is expected, respondents are more likely to focus on the research topic and less likely to digress into unrelated topics\nExplicitly solicit the interviewee’s partnership. Tell them that you need their help. If there is a significant difference in social status, it is sometimes helpful to adopt the personna of the student or apprentice in a “teacher-student” or “master-apprentice” relationship. By demonstrating your respect and admiration, you may benefit by more detailed narratives and explanations. By contrast, at other times, an interview may produce the best results by being confrontational — pushing an interviewee to go into greater depth, or showing that you are not willing to accept self-serving glosses.\nAsk only one question at a time. Give the respondent time to collect his/her thoughts before answering. Show patience. Don’t ask questions simply because you are curious. All of the questions should relate to the research aims. Questions motivated by personal interest immediately mark the interview as unprofessional. Without a professional manner, respondents will begin to doubt that the contents of an interview will remain confidential, and so will resist providing details they might find embarrassing.\nIn questionnaire research, it is common to ask simple questions near the beginning of the questionnaire; Such questions might include questions of age, musical training, etc. However, in open interviews, it is best to leave these questions to the end of the interview. Beginning an interview with a series of simple factual questions sets the wrong tone. The interviewee is likely to respond to all of the subsequent questions in a perfunctory, factual manner rather than using a conversational or narrative approach.\nDiscourage respondents from talking in generalities. You want to hear about their specific experiences. Also, discourage respondents from talking about the experiences of others — like friends or partners. This is second-hand information, and the main attraction of interviews is collecting detailed first-hand accounts. Discourage respondents from offering or discussing theories — unless this is part of the aim of the research. Keep the discussion concrete. For example, instead of asking about their concert-going experiences in general, have them recall their most recent concert-going experience. Phrase your questions to help the respondent focus on specific events: “Can you tell me about your most recent …?” “Can you walk me through that event …?”\nIf the respondent recounts a story that is especially pertinent to the aims of study, don’t hesitate to press for more detail. One of the main advantages of open interviews is the opportunity to obtain such detailed accounts.\nProvide “back-channel” feedback with frequent “un-huh,” “yeah,” etc. This demonstrates interest and tells the interviewee that you are not bored. Maintain a respectful and serious demeanor.\nIn general, let the respondent talk. People are usually more interested in talking about things that are important to them than things that are unimportant. So letting the respondent talk about matters they deem important is apt to be revealing. Moreover, people rarely have the chance to talk at length about their experiences; interviewees will appreciate the opportunity to tell the whole story.\nAvoid prompting the respondent with your own words. For example, suppose you ask a question: “Was your argument with the stage manager a clash of personalities?” Don’t be surprised if the respondent then characterizes the argument as resulting from “a clash of personalities.” The words are your’s, not the respondent’s, and so it would be inappropriate to quote the respondent as saying “a clash of personalities.” The final narrative should be in their words, not in your words.\nNever judge someone’s behavior. Your purpose is to gather information, even if you find the relayed behavior obnoxious or immoral. If you are recording the interview, sometimes the recording gear may become an impediment: the interviewee may be uncomfortable talking about certain topics or people. At times you may need to turn off the recorder. If so, do your best to retain exactly what is said, and then make notes immediately following the interview. Your personal notes don’t have the same legal status as a recording or a transcript of a recording. So this may allow you to gather information about activities that might be considered illegal or socially unacceptible.\nYou are not the respondent’s friend or therapist. Don’t offer advice or criticize or judge a person’s actions. Avoid interpreting the respondent’s narrative. If an interviewee becomes emotional, don’t abandon your role as a professional. Be compassionate and sympathetic but maintain a professional image. Offer quiet sympathy, but don’t offer advice, don’t pass judgment, and don’t respond with your own confession. Encourage a sense of collaborative partnership in creating a productive and informative interview.\nThe cardinal sin of interviewing is to talk about yourself. Resist the temptation to recount your own experiences. You may have your own story that reinforces a story just told by the respondent. In ordinary conversation, it would be appropriate to take turns, and tell you story. However, this is not the purpose of a research interview. Don’t talk about yourself. Don’t tell your own stories. Leave the story-telling to your respondent. The principal role of the interviewer is to listen sympathetically, and to encourage the respondent to speak. Aim to be unobtrusive.\nAs the interview winds-down, ask the respondent for leads or recommendations about other possible respondents you might recruit for the project. That is, solicit information for snowball sampling. Ask whether the respondent knows other people you have interviewed. This will help you determine the heterogeneity of your sample of interviewees. End the interview by expressing your gratitude. Tell the respondent that you might have some follow-up questions in the future, and ask whether it would be okay to contact them again later.\nFor informants with considerable knowledge, more than one interview may be appropriate. When a respondent is especially informative, don’t be afraid to ask for more additional meetings. Try to avoid cramming everything into a single interview, unless you know that further interviews are impossible. With more interviews, the respondent is apt to become more comfortable with you. Experienced interviewers know that with more interviews, more is revealled. For example, a person is more likely to admit to illegal activities, or to portray themselves in a less-than-ideal light.\n\n\nMarkers\nRobert Weiss, an expert on open interviews, talks about “markers.” A marker is “a passing reference made by a respondent to an important event or feeling state.” (Weiss, 1995, l.1469] For example, the respondent might mention the break-up of their band, or make passing reference to an argument with a conductor. In normal conversation, markers are invitations for the conversation partner to pursue the matter further. Markers usually indicate the the respondent is eager to talk about the matter (otherwise it wouldn’t be mentioned).\nIn ordinary conversation, it is always appropriate to follow up on markers. However, in a research interview, a marker topic may take the interview in a direction that is irrelevant to the aims of the researcher. Here, the interviewer needs to exercise some judgment. If the marker is germane to the research goals, then ask the respondent to expand: “You mentioned the break-up of your band; could you say more about that?” If the subject of the marker is not relevant, then the interviewer should simply let the marker pass. Or, if the interviewer is eager to establish a closer relationship with the respondent, pick-up on the marker, even if the subject is a distraction from the research goals.\n\n\nInterview Advice\nWe might summarize the above advice as follows:\n\nPrepare and use an Interview Guide. Update the guide as you continue doing interviews.\nExplain the aims of the research to the respondent.\nOnly ask one question at a time.\nDon’t ask questions out of idle curiosity, or questions that are irrelevant to the research goals. Such questions will make you appear less professional.\nDiscourage respondents from talking in generalities. Tell the respondent you are interested in specific experiences.\nProvide “back-channel” feedback with frequent “un-huh,” “yeah,” etc. This demonstrates interest and encourages the respondent to speak.\nBe vigilant for markers that signal topics that are pertinent to the research goals. Don’t pick-up on markers that are irrelevant to the research goals.\nAvoid “seeding” the respondent with certain words or phrases. Ensure that the narrative is in their words rather than your words.\nNever judge someone’s behavior. Your purpose is to gather information, even if you find the relayed behavior obnoxious or immoral.\nDon’t offer advice. You are not the respondent’s therapist or friend. Simply listen, and offer a sympathetic ear.\nDon’t talk about yourself. A major attraction for respondents is the personal attention paid by the interviewer. People feel validated when they receive someone’s uninterrupted attention.\nAim to be unobtrusive.\nAsk the respondent for leads or recommendations about other possible respondents you might recruit for the project. That is, solicit information for snowball sampling.\nEnd the interview by expressing your gratitude, and asking the respondent whether it would be possible to follow-up with further questions in the future.\n\n\n\nAnalyzing Interviews\nThe aim of analysis depends on the goals of the interviews. For investigative journalists, detectives and historians, the goal will be to uncover the sequence of events, and to draw attention to the ways in which different informants interpret the events differently. For the clinical psychologist or psychiatrist, the aim of interview analysis might be to formulate a diagnosis and to assess the conditions that might lead to a helpful intervention. For sociologists and political scientists, the aim of analysis might be to chronicle the effects of particular policies. For anthropologists and ethnomusicologists, the aim of analysis might be to glimpse the cultural factors that shape thought and behavior.\nThe analysis of interviews may be highly structured (such as following the DSM procedures for diagnosing mental disorders), or may be highly informal. Just listening to (or reading transcripts of) the interviews may be highly informative without any analysis. For example, a series of vignettes of people describing their experiences of a hurricane may stand on their own, without the need to interpret or analyze them. An interviewer might choose to tell a story simply based on the overall impressions arising from the interviews.\nThere are few formal methods for analyzing interviews. However, most formal analyses include the following elements:\n\ntranscribe the recordings\nproofing the transcripts\nbreaking up the transcripts into topic-units or vignettes\nsorting the comments into topic categories\ncoding the comments within each topic\ncollating and summarizing the coded material in each topic\n\nWe expand on these analysis elements below.\nAfter transcribing and proofing the transcripts, it is common to mark-up the transcript by labelling topics of conversation. For example, a researcher might label a transcript segment “dressing before going to a concert.” Most topic labels will emerge in a straightforward manner from the transcript. However, in light of the goals of the research, certain topics will be regarded as important even before you begin your analysis. Many of these topics will already be evident from the interview guide. For example, a series of interviews related to the concert experience might include becoming aware of an up-coming concert, the decision to attend, inviting friends, securing tickets, travel to the concert, pre-concert activities, etc. Label interview segments according to these topics.\nAs you label the topics of conversation, new topics will emerge from the interviews themselves. For example, concert memorabilia (T-shirts, buttons, posters, etc.) may emerge as a pertinent topic — even though it was not anticipated in the interview guide. In general, be vigiliant to allow the emergence of new topic from the interviews themselves.\nHaving labelled the topics, the researcher typically extracts similar topic materials from various interview transcripts and copies them into topical folders. For example, all of the discussions regarding intermission activities may be collated together.\nOnce the materials are collated, they may be coded according to some criterion or criteria. These criteria may be a priori ideas, or they may emerge from the transcripts themselves. For example, the researcher may become aware that it is common for people to change their concert plans for various reasons. The researcher might then elect to code the material according to the reason for the change. For example, you might code comments as “Decided to pursue an alternative activity because the concert was deemed too expensive.” Or “Decided to purse an alternative activity because transportation to the concert venue would be difficult.” Or “Decided to pursue an alternative activity because of the failure to find a companion.” etc.\nIn some cases, the coding may be uncontroversial. However, in other cases, the researcher may be “reading between lines” — infering something that is not explicit. In order to enhance coding validity, it may be appropriate to recruit an assistant or collaborator who independently codes the transcripts according to the same criteria. The researcher might then only rely on codings that agree between the two coders.\nAnalysis of interviews is not guarranteed to lead to any coherent conclusions. Sometimes, the researcher can summarize the interviews by simply relaying various anecdotes, without offering any interpretation. At the other extreme, an obvious over-arching pattern may be evident in the interviews. In this case, the researcher may be able to summarize the interviews by proposing or describing a broad theory. More likely, only some aspects of the interviews will admit a generalized interpretation. Coded data might contribute to the plausibility of certain interpretations. For example, a researcher might conclude (on the basis of coded transcripts) that the most common reason for deciding against attending a concert is the failure to interest a companion.\nIn dealing with descriptions of events, both detectives and historians will hope to be able to use the interviews to assemble a coherent descriptive narrative. If all of the interviews converge on a coherent story, then the task of weaving together the various perspectives offered by different informants should be straightforward. However, if there are discrepancies between informants, then both the historian and the detective will need to draw attention to the different interpretations of what is nominally the same event.\n\n\nAdvantages and Disadvantages of Interviews\nIn general, empirical researchers have tended to underestimate the value of exploratory interviews. Open interviews provide quick access to rich ideas and interpretations of a range of human experiences. Interviews immerse the researcher in a dense world, full of relationships, contexts, nuanced distinctions, competing interpretations, and (sometimes) fascinating anecdotes. It would take many decades for hypothesis-driven research to achieve the sort of complex picture that emerges from open interviewing. Interviews are apt to generate a lot of ideas with relatively little effort.\nInterviews are especially valuable when investigating something new. When an anthropologist visits a novel culture, it is important to be open to whatever it is that is distinctive, unique or surprising about that culture. Of course anthropologists have their own agendas. An anthropologist might be interested in child-rearing practices or religious observances. However, a good anthropologist will be alert to other possibilities. For example, the child-rearing or religious practices might be very similar to those found in other cultures, while the medicinal practices or decorative arts might be extraordinary. A researcher doesn’t know what is important until they are exposed to lots of possibilities. There are times when the research agenda should not be set by the researcher in advance. This is the whole purpose of exploratory research. Open interviews provide one of the very best methods for exposing the researcher to unexpected phenomena.\nAt the same time, qualitative researchers have tended to overestimate the value of open interviews. First, interviewers rarely question the accuracy of the information provided by an interviewee or informant. Although qualitative researchers acknowledge the problems of validity, they rarely (if ever) verify specific claims by examining independent sources. Interviews are only as good as the accuracy of introspection and memory. Validity is not simply an issue of respondents telling the whole truth. Research on eyewitness testimony is sobering. For example, research by Elizabeth Loftus and her colleagues has established the extraordinary fallibility (and suggestibility) of human memory (Loftus, 1979, Loftus, et al. 2008). Moreover, validity involves more than just avoiding conscious lying. Dozens of psychological experiments have shown that people often have very limited self-knowledge. We often have no insight into our own motivations. We all engage in unconscious self-justification, and freely confabulate spurious explanations for things we don’t understand (e.g., Wilson, 2002; Ariely, 2009; Kahneman, 2012).\nResearch in social psychology suggests that parallel problems plague the interviewer. For example, research on hiring practices has shown that unstructured interviews with job candidates actually leads to worse hiring decisions than if no interview occurs (Ross & Nisbett, 2011). As interviewers, we grossly over-estimate our ability to judge character. Past experience (as documented in the resumé), the ability to write (as documented in an application letter), and confidential letters of recommendation are much more predictive of the future success of a candidate than the impressions gained from an open interview (Ross & Nisbett, 2011).\nTraditionally, marketing researchers have made the greatest use of interviews in their work. Because of the large amounts of money involved, consumer researchers have been most motivated to “get it right.” Yet in recent years, marketing researchers have become highly suspicious of consumer interviews and focus groups precisely because of the inability of people to accurately introspect (Graves, 2010). People are mostly unaware of their own motivations, and will confidently confabulate answers to any question a researcher may pose.\nIn his otherwise fine book on interviewing methodology, Robert Weiss offers the following regarding the crucial issue of validity:\n“Sometimes, we can check on the validity of a respondent’s account by interviewing other respondents. Occasionally, there are records we can look to for corroboration. But for the most part we must rely on the quality of our interviewing for the validity of our material. Ultimately, our best guarantee of the validity of interview material is careful, concrete level, interviewing within the context of a good interviewing partnership.” (Weiss, 1995)\nIn short, qualitative researchers don’t have good ways to address the problem of validity. There is a notable tendency among qualitative researchers to avoid discussions about validity.\nBecause of the close face-to-face interaction, a second problem is that interviews are highly susceptible to investigator bias as well as demand characteristics. Interviews are the virtual antithesis of the double-blind procedure. As a result, confirmation bias is especially problematic. The open interview method has proved especially attractive to those researchers who are motivated by a particular social cause. The more strongly held the political beliefs, the greater the likelihood that the researcher will rely solely on interview methods. Again, qualitative researchers are generally aware of these difficulties, however, little work has been carried out to either measure the magnitude of this bias, or to develop techniques to minimize these confounds.\nAs Robert Weiss has noted, interviews are more likely to be used by practitioners (social workers, consumer marketers, clinical personnel, political activitists) than by academics, because practitioners are less picky about the quality of their evidence. (See Weiss, 1995; l.3420).\nAs noted earlier, people find personal stories more compelling than raw statistics. News stories and documentaries are often more effective when they begin or end with a concrete personal example. Interviews can provide vivid case examples whose influence far exceeds the reporting of representative statistics. A danger is that examples can typically be found that seem to illustrate opposite cases. One can find stories of human heroics as well as human cowardice. One can find stories where seat-belts save lives and stories where wearing a seat-belt causes a person’s death. Good research is needed to resolve which account is the more common or representative. Readers and viewers tend to interpret case studies as typical, and regard them as holding important general lessons. Empirical work using representative samples can benefit by recounting an anecdote that illustrates or exemplifies what is found. However, relaying anecdotes that are contrary to the common or typical experience can amount to propaganda. Ideally, exceptional cases will lead to careful thought about how to promote more good results, or minimize bad results. However, policy based on exceptional cases can sometimes make matters worse.\nA third and more onerous problem is that qualitative researchers tend to view open interviews as confirmatory research rather than exploratory research. Ideas that arise from interviews are rarely tested. Recall the problem of double-use data where the same data should not be used to both inspire a hypothesis, and as independent evidence in support of that hypothesis. However, it has been common for qualitative researchers to use narrative anecdotes in precisely this way. Said another way, the researcher who relies exclusively on open interviewing is a researcher who never invites failure. Interviews are important starting points for research. But, for the conscientious researcher, interviews cannot be the end point. Open interviews belong as one part in an explore-then-test research program.\n\n\nReferences:\nDan Ariely (2009). Predictably Irrational: The Hidden Forces That Shape Our Decisions. Revised and expanded edition. New York: Harper Collins.\nBob Blauner (1987). Problems of editing ‘first person’ sociology. Qualitative Sociology, Vol. 10, No. 1, pp. 46-64.\nRobert Emerson, Rachel Fretz and Linda Shaw (1995). Writing Ethnographic Fieldnotes. Chicago: University of Chicago Press.\nDiane Ehrensaft (1990). Parenting Together: Men and Women Sharing the Care of Their Children. University of Illinois Press.\nPhilip Graves (2010). Consumerology: The Market Research Myth, the Truth about Consumers and the Psychology of Shopping. London: Nicholas Brealey.\nDaniel Kahneman (2012). Thinking, Fast and Slow. New York: Farrar, Straus and Giroux.\nMirra Komarovsky (1987). Blue-Collar Marriage. 2nd edition. New Haven: Yale University Press.\nElizabeth Loftus (1979, 1996). Eyewitness Testimony. Cambridge, MA: Harvard University Press. Reissued with new Preface in 1996.\nElizabeth Loftus, J.M. Doyle, & J. Dysert (2008). Eyewitness Testimony: Civil and Criminal. 4th edition. Charlottesville, Va: Lexis Law Publishing.\nLee Ross and Robert Nisbett (2011). The Person and the Situation: Perspectives of Social Psychology. 2nd edition. London: Pinter & Martin Limited.\nRobert S. Weiss (1995). Learning From Strangers: The Arts and Method of Qualitative Interview Studies. New York: The Free Press.\nTimothy D. Wilson (2002). Strangers to Ourselves: Discovering the Adaptive Unconscious. Cambridge, MA: Harvard University Press.\n\n\nFootnote\n[1] Another example is found in Mirra Komarovsky’s study of blue-collar families (Komarovksy, 1987). Komarovsky interviewed 58 blue-collar couples without interviewing any non-blue-collar couples. Once again, this makes it difficult to offer generalizations about what makes a blue-collar couple distinctive."
  },
  {
    "objectID": "emp_methods_workshop/debriefing.html",
    "href": "emp_methods_workshop/debriefing.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Debriefing is an essential part of most experiments. The goal of debriefing is to gather information about the experiment from the participant’s point-of-view. “Did you have a strategy you were using for this experiment?” “Did you simply follow your intuitions, or did you follow a particular way of doing the task?” Let the subject talk about his/her experience. What did they think about the experiment? Were they able to concentrate on the task? Did they find the task easy or hard?\nIn one experiment, we were looking to see if phrases played in low transpositions are perceived as being “heavier” than mid-register or high transpositions. When debriefing one subject, he mentioned that pitch-height was an obvious factor influencing whether the phrase sounds heavy or light. Consequently, he said that he explicitly ignored the overall pitch height and made his judgments of “heaviness” on other factors! In this case, the debriefing provided critical information that allowed us to avoid using this data in our analysis.\nIn many experiments, it may be necessary to discard some collected data depending on certain criteria. For example, subjects who do not achieve a certain minimum performance level, or subjects who have certain skills. After a subject completes an experiment, do not then tell the subject that you are likely to throw away their data. Participants are understandably discouraged when they learn that their data is unusable.\nFinally, ask the subject if there was anything about their experience that was stressful, annoying, painful or uncomfortable.\nInteresting, in past debriefings of subjects we found that the most stressful on-site experience for our research participants is signing the consent form. Most people are naturally skeptical of signing a document. The fact that a signature is required tends to make them wary and wonder what is in store. By collecting this data, we were able to successful get an exemption from our IRB, so that only verbal consent is necessary, rather than written consent. This has made the experimental experience much less intimidating for our participants."
  },
  {
    "objectID": "emp_methods_workshop/empirical2.html",
    "href": "emp_methods_workshop/empirical2.html",
    "title": "Types of Empirical Studies",
    "section": "",
    "text": "Many of the things music scholars do is empirical—in the sense that the activities involve observation. Examples might include deciphering manuscripts, studying a score, or observing a gamelan rehearsal. What makes “empirical research” different from the tasks that humanities scholars typical engage in?\nA distinction can be made between formal and informal observation. An informal observation might simply be noticing that there are more women than men on the dance floor of a club. Another informal observation might involve recognizing that a printed score switches from using the abbreviated “sfz” marking to the verbose “sforzando” in the final pages. Formal observation is distinguished by some sort of prior procedure. The researcher plans to observe specific things. For example, a researcher might view videos of two performers with the aim of determining which performer moves more while playing. Or a researcher might search through old newspapers from November 1884 in order to determine if any reviews of a concert were published. What makes something Empirical Research is that the observations are planned in advance.\nAt least seven types of empirical studies can be distinguished: (1) reconnaissance study, (2) descriptive study, (3) measurement study, (4) correlational study, (5) experimental study, (6) meta-study, and (7) modeling study.\n\n\n\nA reconnaissance study is a preliminary or exploratory investigation intended to gain familiarity with a new field or phenomenon. Charles Darwin spent five years sailing around the world on the Beagle: the purpose was exploration and reconnaissance. The goal was to see what plants and animals existed in different parts of the world.\nAn ethnomusicologist might travel to a remote village simply to become exposed to the culture and music of that region. A historical musicologist might browse through the uncatalogued contents of an archive box, simply to see what is there. A music sociologist might interview some pre-teens about their musical tastes, simply to become aware of their concerns and enthusiasms.\nReconnaissance studies are not hypothesis-driven and do not include any quantitative measurement. Reconnaissance studies are common when a researcher begins work on a novel topic or discovers a new phenomenon. The principal purpose of the reconnaissance study is to alert the researcher to new possibilities. The research endeavors to cast a wide net, trying to observe anything that might be potentially informative. Reconnaissance studies may involve some sort of collecting, such as recording live music, purchasing local CDs and tapes, clipping magazine or newspaper articles, video recording children’s play activities, etc.\n\n\n\nDescriptive studies attempt to document and interpret some phenomenon. Like the reconnaissance study, descriptive studies are not hypothesis-driven and do not involve any quantitative measurement. The principal purpose of the descriptive study is to understand some phenomenon through detailed description and interpretation. When the phenomenon pertains to people, descriptive studies are usually refered to as ethnographic studies. When the phenomenon is a musical work, descriptive studies commonly take the form of analyses. When the phenomenon pertains to understanding the past, descriptive studies are commonly called historical studies. There are innumerable descriptive methods, including open interviews and participant-observation methods—which will be covered in detail later.\n\n\nIn ethnography, the difference between Reconnaissance and Descriptive studies is echoed in the terms thin description and thick description (Denzin, 1989). Thin description emphasizes what might be regarded as factual reporting, whereas thick description emphasizes how the phenomenon is interpreted or understood—usually within a culture context (Geertz, 1973). In ethnography, both Reconnaissance and Descriptive studies entail some sort of fieldwork in which one establishes rapport with the members of a community of interest, selects local informants or research collaborators, and begins recording or documenting various observations, commonly in a field diary. However, “thick description” expands the enterprise, mostly by emphasizing the context of various phenomena, interpreting the behaviors in terms of intentions and meanings, and tracing the historical changes of development of the activity.\nNote that descriptive ethnographic studies carried out in the field are often referred to as field studies.\n\n\n\n\nLike the reconnaissance and descriptive studies, the measurement study is not hypothesis-driven. However, the researcher engages in some quantitative activity. That is, the researcher counts or measures something.\nWhen a paleoanthropologist discovers the skull of a long dead human ancestor, the first order of business is to describe the skull by reporting a series of detailed measurements. Publishing a detailed description is useful, even if the anthropologist has no opinion or interpretation to offer, and no theory or hypothesis to test.\nMeasurement studies often present so-called descriptive statistics. A descriptive statistic is a measurement without any accompanying interpretive claim. Examples of descriptive statistics: the average American listens to four hours of music each day; the average European folksong is 52 notes in length. Measurement studies and descriptive statistics may invite a “so what?” response. Their value usually lies in later studies that make use of the published measurements.\nThe principal purpose of the measurement study is to assemble quantitative descriptors of some phenomenon, with a minimum of interpretation. A famous historical example of a measurement study is the classic work of the Danish astronomer, Tycho Brahe (1546-1601). Brahe built the most sophisticated telescope of his era and devised ways to measure the positions of celestial bodies with excellent precision. His measurements of the positions of the stars and planets were far more accurate than earlier measures. His work formed the basis for the later theories of Johannes Kepler, who determined that the planets moved in elliptical orbits. Brahe himself did not discover the elliptical orbits of the planets, but his careful measurements provided an essential precursor to that discovery.\n\n\n\nCorrelational studies aim to identify linkages or relationships between things. We say two things are correlated when there is some sort of connection or association between them. For example, music in the minor mode tends to be slower in tempo than music in the major mode. Although there are exceptions to this, in general, there is a correlation between mode and tempo.\nCorrelational studies always involve some form of measurement or counting. In fact, correlational studies involve collecting at least two different sets of measurements. The aim is to determine whether there is any relationship between the two sets of measurements. For example, a survey of middle-school students might find that more female than male students play flute, while more male than female students play trombone. That is, we might find a correlation between gender and instrumentation. Correlational studies cannot be used to identify causation. The study itself gives us no idea of why a particular association might exit.\nA common type of correlational study is the survey (although many surveys are descriptive or measurementive rather than correlational). For example, a survey might reveal that people with high incomes are more likely to prefer jazz than country music, or that social conservatives are less likely to enjoy sad music. Once again, correlational studies say nothing about causation: they simply suggest that certain relationships exist.\n\n\nCorrelational studies may or may not be hypothesis-driven. In many cases, the researcher has an explicit interest in testing whether a proposed relationship exists. In other cases, the researcher has no prior hypothesis to test and may be looking to see if anything correlates with a concept of interest. When the study is motivated by an a priori hypothesis, the principal purpose of the correlational study is to test an idea by inviting failure. When no prior hypothesis is being tested, the study is referred to as an exploratory correlational study.\n\n\n\n\nA study is “experimental” when the researcher manipulates some aspect of the world. For example, an experimenter may expose listeners to musical excerpts that vary from sad to happy and observe the effect of the different moods on, say, memory. The property that is manipulated by the researcher (in this case mood) is referred to as the independent variable. The property that is observed by the researcher (in this case memory recall) is referred to as the dependent variable (or dependent measure).\nExperimental studies are nearly always hypothesis-driven. That is, the researcher makes a prediction about the effect of manipulating the independent variable on the dependent variable. When hypothesis-driven, the experiment is referred to as a true experiment. All experiments involve some sort of measurement. Experiments may involve more than one independent variables and more than one dependent variables. The principal purpose of the experimental study is to test an idea by inviting failure.\nOf all the different kinds of studies, the experimental study is the most highly regarded by empirical researchers. There is a reason for this: the experimental study is the only type of study that allows the researcher to say something about causation.\n\n\nAn Exploratory Experiment involves manipulation and measurement, but the manipulation is not motivated by some prior theory, hypothesis or conjecture. For example, a researcher might play traditional Japanese and Andean pop music to naive Western listeners while making a series of measurements, such as heart-rate, respiration, body temperature, and observable behavior, etc. The researcher may have no idea what to expect. That is, no prediction was made. Nevertheless, having collected the data, the researcher might then carry out statistical tests to see whether a significant increase in body temperature resulted. Notice that the study involves manipulation of the world (playing different kinds of the music), but the study is not motivated by some prior theory or idea.\n\n\n\n\nA meta-study is a “study of studies.” It is typically done when a large number of studies have been carried out related to some problem. For example, many studies have been carried out related to whether television violence promotes violent behavior in viewers. Some of the studies seem to show a link, whereas other studies seem to show no link. In a meta-analysis, the researchers identify all of the pertinent studies. They then evaluate the quality of each study, including the quality of the samples used, the number of participants, the quality of the stimuli, the extensiveness of the controls, and other factors. Poor studies are simply discarded if they fail to achieve the minimum quality criteria established by the researchers. Then the researchers combine together all of the good studies, and do a statistical analysis on the aggregate data. The principal purpose of the meta-study is to determine whether all of the studies pertaining to some topic ultimately tell a coherent story.\n\n\n\nTheories can often be implemented as models. An example of a physical model is a large model of San Francisco Bay built by the U.S. Army Corps of Engineers (see photo). The actual bay is 100 km long. The model is 1 km in length so the scale is 1 meter = 100 meters. Models are useful for testing hypotheses that are impossible (or unethical) to test in reality. For example, how long will it take an oil spill in Oakland to reach the mouth of the Sacramento River?\nPhysical models are rather rare. More commonly, models are rendered as computer programs. There are a number of commercial software products designed explicitly to help researchers build models. Models might be used, for example, to predict the spread in popularity of South Korean pop music introduced into North Korea. Or a model might be used to predict a listener’s musical preferences based on past listening habits.\nAn advantage of models is that they can be used to investigate “what-if” scenarios. The researcher can change some of the initial states, and then see what happens when the model is set in motion. Running a model with a set of initial conditions is referred to as a simulation.\nThe principal purpose of a modeling study is to build a model that has some predictive value related to some phenomenon. In general, few models have been created related to musical phenomenon. Models are more common in disciplines that have a mature research base to build on.\n\nPart of a 1 km-long physical model of San Francisco Bay. (The Golden Gate Bridge can be seen near the center of the photo.)\n\n\n\nWhatever type of study one uses, it is often useful to begin with a sort of “practice” study—known as a pilot study. A pilot study is carried out simply as a way of testing the research procedure. Pilot studies usually involve relatively small numbers of participants or small sample sizes. Pilot studies can prove very useful by exposing various unanticipated problems that help the researcher fine-tune an ensuing main study. The principal purpose of a pilot study is to determine whether a full-fledged study is feasible and to uncover possible problems with the research design.\n\n\n\nIt is not uncommon for a published journal article to report several different studies, often presenting different types of studies within a single report. The research might begin by reporting a descriptive study. The results from the descripive study may inspire the authors to formulate a theory, from which a hypothesis is generated. The article might then go on to report a correlational study or an experiment whose purpose is to explicitly test the hypothesis. Frequently, two or more experiments are reported, with each succesive experiment aimed at testing a different refinement of the initial hypothesis. The resulting report is said to make use of “mixed methods.”\n\n\n\nA special kind of experiment is the so-called natural experiment. A natural experiment relies on a manipulation of the real world that occurs without the intervention of the researcher. In the field of climatology, the best-known example of a natural experiment relates to the influence of commercial aircraft on heating of the earth’s surface. Everyday, thousands of aircraft fly—creating vapor trails that often produce “linear clouds.” What effect do these vapor trails have on the earth’s temperature? On the one hand, the clouds partially block the sun and reflect light back into space—suggesting that their presence cools the earth. On the other hand, the clouds tend to insulate the earth—reflecting heat radiating from the earth back toward the surface. So what effect do vapor trails have on the earth’s temperature?\nThe effect (if any) of vapor trails is difficult to observe against the constantly fluctuating general weather patterns. Vapor trails are relatively small, often temporary, and winds in the upper atmosphere blow them so they don’t hover over the same point on the earth. It is almost impossible to measure the effect of a single vapor trail. Ideally, it would be helpful to carry out an experiment. Imagine if an experimenter could manipulate the world—banning all air traffic on one day, and then having the skies filled with aircraft the next day? This would allow the calculation of the average surface temperature over a very large geographical area, and so allow the researcher to test whether the vapor trails tend to cool or warm the earth. This is the sort of experiment meterologists dream about, but obviously it is impractical.\nIn the aftermath of the terrorist attacks of September 11, 2001, the US Federal Aviation Authority (FAA) shut down all air traffic across the United States for three days. Although this event was a tragedy of the first order, it offered an unexpected opportunity for climate researchers and meterologists to measure the effect of vapor trails on the surface temperature of the earth. Scientist David Travis averaged the daily highs and lows for some 4,000 locations across the U.S. and compared temperatures during the FAA ban, with data when the planes were flying. This natural experiment allowed researchers to determine that vapor trails raise nighttime temperatures and lower daytime temperatures. That is, at night, the presence of vapor trails reduces heat-loss, but during the day they block sunlight. Moreover, the natural experiment also established that the main overall effect is to lower daytime temperatures. Contrary to the views of some climatologists, vapor trails appear to have a net cooling effect on the earth. The effect is about 1 degree C.\nIn this case, the sort of manipulation researchers could only dream of, arose due to other (non-research) circumstances. From time-to-time, it is possible to carry out such “natural experiments.”\n\n\n\nIn general, it is helpful to classify studies according to four criteria:\n\nDoes the researcher simply describe and observe? Or does the researcher offer an interpretation or explanation? (That is, does the study offer an explanatory theory?)\nDoes the researcher make a prediction? (That is, does the method invite failure?)\nDoes the researcher manipulate the world? (That is, does the method allow the researcher to infer causality?) And\nDoes the researcher count or measure something? (That is, is the research qualitative or quantitative?)\n\nReconnaissance and Descriptive studies involve no manipulation, no hypothesis test, and no measurement. They differ in whether the researcher interprets the observed phenomenon. Measurement studies involve no manipulation and no hypothesis test—they simply report some sort quantitative measures. An Experimental study involves manipulation, testing of a hypothesis, and (necessarily) some measurement. An Exploratory Experiment involves manipulation and measurement, but no hypothesis testing. A Correlational study involves no manipulation, but it does involve measurement. Usually, correlational studies also involve the testing of a hypothesis. If a correlational study involves no hypothesis, then it is an Exploratory Correlational study. (What researchers commonly call an “Exploratory study” is either an Exploratory Experiment or an Exploratory Correlational study.)\n\n\n\n\nExplanatory Theory\nHypothesis Tested\nInfer Causality\nQuantitative\n\n\n\n\nReconnaisance\n\n\n\n\n\n\nDescriptive\npost hoc\n\n\n\n\n\nMeasurement\n\n\n\n✓\n\n\nCorrelational\na priori\n✓\n\n✓\n\n\nExploratory Correlational\npost hoc\n\n\n✓\n\n\nExploratory Experimental\npost hoc\n\n✓\n✓\n\n\n(True) Experimental\na priori\n✓\n✓\n✓\n\n\nMeta Study\na priori\n✓\n(✓)\n✓\n\n\n\n\n\n\nThe single most important distinction between types of empirical studies is whether a theory precedes data collection (a priori) or whether a theory follows after data collection (post hoc). Only a priori methods invite failure. That is, only a priori studies partake of the rhetoric of prophecy.\nMethodologists commonly group together all pre-theory methods under the term exploratory studies. This includes reconnaissance studies, descriptive studies, studies, and exploratory experiments. Since exploratory studies don’t test prior hypotheses, they are rhetorically (and methodologically) relatively weak. Exploratory studies cannot show that a researcher’s theory or hypothesis is wrong. So the scholar who engages in only exploratory research will never suffer the indignity of “being wrong.”\nBy contrast, there are three types of a priori studies: the (non-exploratory) correlational study, the (non-exploratory) experiment, and the meta-study. In these types of studies, the researcher states the hypothesis in advance, and assembles data in order to perform a test. Since the researcher invites failure, these types of studies are rhetorically (and methodologically) stronger.\nIn contrast to exploratory methods, these stronger methods are commonly dubbed confirmatory. However, this terminology is unfortunate. In empirical research, there is no such thing as “proof,” and similarly, there is no such things as “confirming” a hypothesis. Instead, researchers test hypotheses; if the hypothesis survives the test, then we simply say that “the results are consistent with the hypothesis.” Although the terms “exploratory” and “confirmatory” are commonly heard, it is better to characterize methods as either “exploratory” or “testing.”\n\n\n\nMost correlational studies involve testing some hypothesis. However, depending on the status of the hypothesis, the correlational study may be either exploratory or testing (confirmatory). If the hypothesis is formulated in advance, then the collected data may provide a proper test of the hypothesis. In other cases, the data might be assembled and relationships sought without any initial idea of what to expect. A researcher may begin testing several possible relationships with the data. This exploratory approach is informally referred to as a “fishing expedition.” The researcher is “fishing&148; around for possible connections.\n\n\n\nA grave methodological error can occur when a researcher engages in an exploratory study, but presents the work as though it were a true experiment or hypothesis-testing correlational study. Notice that this implies that the researcher had the idea before seeing the data and so gives the audience or readers the impression that the statistical tests invited failure. However, since the test was carried out only after the data were seen, there is no chance for the researcher to be wrong. Presenting post hoc hypotheses as though they are a priori hypotheses is morally reprehensible in the same way that making-up data is unethical.\n\n\n\nNorman Denzin (1989). The Research Act: A Theoretical Introduction to Sociological Methods. Englewood Cliffs, NJ: Prentice-Hall.\nClifford Geertz (1973). The Interpretation of Cultures. New York: Basic."
  },
  {
    "objectID": "emp_methods_workshop/ceiling_discussion.html#tsay-2013-sight-versus-sound-discussion",
    "href": "emp_methods_workshop/ceiling_discussion.html#tsay-2013-sight-versus-sound-discussion",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "Tsay (2013): Sight versus Sound — Discussion",
    "text": "Tsay (2013): Sight versus Sound — Discussion\n\nIn general, the Tsay (2013) article reports a very nice piece of research. In total, Tsay tested 1,164 participants. Tsay recruited both novice and highly expert participants, and employed several different experiments that addressed the question in different ways, and so produced converging evidence. Moreover, Tsay continued with the experiments in order to attempt to identify those visual features that are most influential when judging the quality of musical performances.\nAs often happens in research, we can question the generality. Tsay claims that “the findings demonstrate that people actually depend primarily on visual information when making judgments about music performance.” This is arguably a gross over-generalization.\nSuppose that we had several sound recordings representing different levels of musicianship. Do you think you could tell the difference (just from sound alone) between a person who has only played the piano for two weeks, versus a person who has had a year of piano lessons, versus a person who has had five years of piano lessons, versus a professional concert pianist?\nThe quality of performances at international music competitions is simply extraordinarily high. That means that the range of skill (or variance) is very small. In short, there is a ceiling effect. At this point, it makes sense that other factors would tend to dominate.\nWhat I think is interesting is that the visual appearance of the performer (sex, race, age, attractiveness) proved not be to very predictive of the final outcome. Instead, is was the visual dynamic movement or gestures that proved to best predict the competition results.\nDespite the high quality of this research, Tsay made unfortunately failed to use the circumspect language we aim to use in empirical research. Tsay should have avoided using the word “demonstrate:” — “the findings demonstrate that people actually depend primarily on visual information when making judgments about music performance.” His conclusion would have been more acceptable if he had used more careful language: “the findings are consistent with the notion that people depend primarily on visual information when making judgments about music performance.” Nevertheless, even this conclusion is over-stated. Without a wider variance in his stimuli, there is insufficient support for the broader claim that vision is more important than audition when judging the quality of musical performances."
  },
  {
    "objectID": "emp_methods_workshop/instruments.html",
    "href": "emp_methods_workshop/instruments.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Instruments\n\nIn research, it is common to make use of standardized tests or measures called “instruments.” An instrument is often in the form of a questionnaire, but there are many instruments that involve tests of various sorts, including physical tests. Common instruments include the Wechsler Adult Intelligence Scale (WAIS), the Scholastic Aptitude Test (SAT), and the Minnesota Multiphasic Personality Inventory (MMPI).\nPeople aiming to become firefighters are typically required to pass a series of tests, including a Firefighter Physical Fitness Test (FPFT). For example, this test includes such tasks as carrying a 75 pound (34 kilo) weight up five floors and using a 9-pound (4 kilo) sledgehammer to drive a 160-pound (73 kilo) steel beam on a Keiser Force Machine.\nThere are literally hundreds of instruments which endeavor to measure a wide variety of concepts. These include such tests as the Kinsey scale of sexual orientation (KSRS), the Biddle Agility Test, the Authentic Happiness Inventory Questionnaire, the World Health Organization Quality of Life Questionnaire (WHOQOL), and many many more.\n\nNorm-referenced and Criterion-referenced\nInstruments are either norm-referenced or criterion-referenced. A norm-referenced instrument compares the candidate with the general population. An IQ test is an example of a norm-referenced instrument since an IQ score of 100 is (by definition) the average intelligence. A criterion-referenced instrument compares the candidate with some standard level of achievement. The FPFT instrument is criterion-referenced — few people would be able to complete the physical tasks demanded for prospective firefighters.\nPsychologists have created a large number of instruments intended to measure a wide variety of attributes. For example, the Beck Depression Inventory is intended to characterize how depressed a person feels.\n\n\nValidity and Reliability\nA useful instrument must be both reliable and valid. Reliability relates to the consistency of the score. A test is said to be reliable when a person scores the same value over time in similar circumstances. Sometimes the reliability has to do with the consistency of the raters. For example, the Royal Conservatory of Music (in Canada) has a graded system for musical performance, ranging from grade 1 to 10. Performance examinations are held across the country twice each year. The RCM trains examiners in an effort to make the grading criterion as consistent as possible, so that the final score received doesn’t depend on who the examiner is. If the adjudication method varies from rater to rater, then the instrument is said to have a low reliability.\nValidity relates to whether the instrument measures what it purports to measure. One might claim that an instrument measures “musicality” when it really measures “performance skill.” Or one might claim that an instrument measures “feeling sad” when it really measures “the degree to which someone is willing to say they feel sad.” Etc. A purported test of “intelligence” might be better characterized as a measure of “language facility” or “logical reasoning.” In the case of music, the Seashore Tests of Musical Ability are better regarded as tests of “auditory perceptual acuity.”\nMany tests are used for diagnostic purposes, such as the Comprehensive Aphasia Test (CAT) or the Luria-Nebraska Neuropsychological Battery. The first test you ever underwent was probably the Apgar test. All newborns are given a Apgar score between 0 and 10 according to color, pulse, reflex responsiveness, muscle tone, and breathing. Normal scores are in the range of 7 and above. A score of 4 to 6 is considered low, and lower scores indicate a critical condition. The aim of the Apgar test is to provide a quick index of the newborn’s health, and a way of determining whether immediate medical care is needed. The test is quick and easy to do, and is highly predictive of the general medical health of the infant. That is, the test has a reasonably good validity.\n\n\nMusical “Instruments”\nIn music cognition research, three tests have been important. One is the Montreal Battery of Evaluation of Amusia (MBEA) developed by Isabelle Peretz and her colleagues. This instrument includes a number of listening and production tasks that can be used to assess “tone-deafness.”\nA second test is the Ollen Musical Sophistication Index (OMSI). This instrument consists of 10 questions that are used to estimate the degree of musical sophistication. A third test is the Goldsmiths Musical Sophistication Index (Gold-MSI). This is a more elaborate index for assessing musicality, is not limited to a musician population. It involves various perceptual tasks, and doesn’t rely solely on self-report. However, the Gold-MSI index is much longer to administer than the OSMI index.\n\n\nOllen Musical Sophistication Index\nIn music-related experiments, it is common for researchers to distinguish “musicians” from “non-musicians.” Of course, there is no clear dividing line between musicians and non-musicians. However, it is often useful to operationally define these in order to pursue the research. Two of the most commonly used criteria are number of years of private music lessons and music major versus non-music major. These are serviceable criteria, but Joy Ollen was eager to produce a more refined instrument that could be quicky administered in the context of typical music-related research.\nOllen first carried out a literature review — identify the kinds of criteria used by researchers for distinguishing the level of musical sophistication. She identified 743 published music studies and found 38 different indicators used to determine degree of musicality.\nOllen then carried out a survey of 27 experienced musicians and asked them what sorts of questions they might ask to determine how musically sophisticated an individual might be. These musicians identified many of the same 38 criteria found in the published studies, but they also added four new criteria.\nOllen distilled the 42 criteria to the most promising 29 criteria. She then developed a 36-item questionnaire which asked such as questions as the number of years of private musical instruction, whether the individual had taken college-level music courses, the number of hours per week in musical practice, and so on.\nShe then administered this questionnaire to 633 adults who participated in various groups involved in music-related activities. This ranged from members of church choirs, to members of professional orchestras. The leader of each group (a musical expert) was then asked to rate the musical sophistication of every member of the group using a 10-point scale.\nThe data were then analyzed to determine which of the 36 questions were most predictive of the experts’ assessments of musical sophistication. Using a logistic regression analysis, nine indicators were identified that accounted for nearly 80% of the variability. From this, Ollen created a 10-item questionnaire that can be completed in under 2 minutes.\nFinally, Ollen administered her instrument to a large independent sample of people. Once again, group leaders rated the musicality of the group members. Although the accounted variance dropped to about 50%, the results show that the OMSI instrument provides reasonably valid estimates of musical sophistication with a minimum of effort.\n\n\nReferences\nA.T. Beck, R.A. Steer, and G.K. Brown (1996). Manual for the Beck Depression Inventory, 2nd ed. San Antonio: The Psychological Corporation.\nBert Goldman (1974, 1997, 2003, 2008). Directory of Unpublished Experimental Mental Measures. Washington, DC American Psychological Association.\nCharles S. Newmark (Ed.) (1996). Major Psychological Assessment Instruments. 2nd Edition. Hertford, UK: Allyn & Bacon.\nR.L. McGhee, D. Ehrler and J. Buckhalt (2008). Manual for the Five Factor Personality Inventory — Children. Austin: Pro Ed, Inc.\nMüllensiefen, D., Gingras, B., Musil, J., & Stewart, L. (2014). The musicality of non-musicians: an index for assessing musical sophistication in the general population. PLoS One, Vol. 26;9(2):e89642.\nH.A. Murray (1943). Thematic Apperception Test Manual. Cambridge, Massachusetts: Harvard University Press.\nJoy Ollen (2006). A criterion-related validity test of selected indicators of musical sophistication using expert ratings. PhD Dissertation, School of Music, Ohio State University.\nIsabelle Peretz, Sophie Champod, and Krista Hyde (2003). Varieties of Musical Disorders: The Montreal Battery of Evaluation of Amusia. Annals of the New York Academy of Sciences, Vol. 999, pp. 58-75."
  },
  {
    "objectID": "emp_methods_workshop/stats_descr.html",
    "href": "emp_methods_workshop/stats_descr.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Descriptive Statistics\n\nThe field of statistics can be broadly divided into two area: descriptive statistics and inferential statistics. As might be expected, descriptive statistics is intended to describe some properties of some set of data. For example, we might calculate that the average pitch in Western music is D4 (the D immediately above middle C). The goal of descriptive statistics is to characterize something.\n\nVariables\nVariable: An observable property of an object or phenomenon that can take on different values. Examples of variables include length and weight. Musical examples include pitch height, duration, year of composition, and ensemble size.\nContinuous variable: A variable that can take on an infinite number of values. Frequency is an example of a continuous variable.\nDiscrete variable: A variable that takes on a limited number of possible values (such as gender), or is “binned” (like height in feet and inches). For example, when pitch is measured in terms of scale tones it is a discrete variable. When pitch glides are measured in cents, the variable is continuous.\nIndependent variable: A variable that is manipulated by the experimenter.\nDependent variable: A variable not under the experimenter’s control that is measured.\n\n\nStatistics\nDescriptive statistics: Measures that describe a set of data.\nStatistic: Some value derived from a sample, such as the sample average or mean.\nInferential statistics: The process by which population parameters are inferred from sample statistics.\nParameter: A true population value, such as the population mean (μ).\nVariable-statistic-parameter: We infer a population parameter on the basis of a statistic which we calculate from measured variables.\n\n\nDistributions\nBar graph: A graph that uses a nominal scale on the X (horizontal) axis. (That is, a frequency-of-occurrence graph that distinguishes discrete categories, such as gender, or instrument played.) In a bar graph, bars are separated by spaces in order to emphasize that neighboring bars are not contiguous cateogires.\n\nHistogram: A frequency-of-occurrence graph that uses an ordinal, interval, or ratio scale on the X (horizontal) axis. (In other words, a graph that does not use a nominal scale.) In a histogram, bars are not separated by spaces in order to emphasize that neighboring bars represent contiguous categories.\n\nCumulative Distribution: A distribution (graph) that plots cumulative frequencies for successive bins on the X axis.\n\nBack-to-back Histograms: Histogram-pairs that are plotted back-to-back, such as the gender/age histograms commonly used to plot population demographics.\n\nSkewness: The tendency of a distribution to lean to the left or right.\nPositive skew: Where the tail of the distribution extends to the right (higher values).\nNegative skew: Where the tail of the distribution extends to the left (lower values).\n\nKurtosis: The degree to which a distribution is either flat or peaked. A relatively flat distribution is called platykurtic. A highly peaked distribution is said to be leptokurtic. A distribution that is neither flat nor especially peaked is deemed mesokurtic.\n\nKurtosis can be described for both continuous distributions and discrete distributions. The discrete distributions below illustrate platykurtic (left) and leptokurtic (right) kurtosis.\n\nSymmetry: The property of a distribution that exists when the distribution has the same shape on either side of the center.\nModality: The number of major peaks in a distribution. Where a distribution has only a single peak it is said to be unimodal. Where a distribution has two peaks it is said to be bimodal.\nUnimodel: A distribution containing a single peak.\nBimodal: A distribution containing two peaks. Again, bimodal distributions may be continuous (upper illustration) or discrete (lower illustration).\n \nDistribution Descriptors: Distributions can be generally described according to three properties: skewness, kurtosis, and modality (unimodal, bimodal, etc.).\n\n\nCentral Tendency\nIt is often useful to be able to summarize a distribution. Suppose someone is thinking of moving to Columbus, and asks How much does it cost to rent an apartment in Columbus? We could respond by giving them a list of all apartments in the city and their associated rents. That is, we could present them with a complete distribution. However, this would be too much information. The questioner is hoping we will give them a more general answer. An example of a summary generalization is the average (what statisticians called the mean). The average rent is likely to convey the gist of what the person wants to know.\nThe average or mean is just one example of a measure of ‘middleness’ or ‘centrality’ in a distribution. In statistics, these are referred to as measures of central tendency. There are three classic measures of central tendency: mode, median, and mean. The mode is simply the most frequentlly occurring score or value in a distribution. For bimodal distributions, we report the mode for both peaks.\nThe median is the middle-most score—that is, half the values are above the median, and half the values are below the median. The mean is the average score, which is the sum of all values, divided by the number of values.\nBy way of example, for the values: 1, 2, 2, 5, 10, 20, 30 — the mode is 2 (2 is the most common value), the median is 5 (3 values above and 3 below), and the mean (or average) is 10.\nIn statistics, it is useful to distinguish the mean (average) of the sample, from the mean of the population. The sample mean is represented mathematically by the letter x with a bar drawn above (x̄ - spoken “X-bar”). By contrast, the population mean is represented by the Greek letter mu (μ).\nEach measure of central tendency (mean, median and mode) has unique advantages and disadvantages. The principal advantage of the mode is that it conveys a “typical” or “common” value. Another advantage of the mode is that it can be calculated for nominal data. For example, a distribution might convey the number of performers playing various instruments. We cannot calculate an “average instrument,” nor can we determine the “median instrument.” However, we can identify the “modal instrument”—it is simply the most commonly played instrument. A problem with the mode is that it may be quite far away from the majority of values in some data set. Another disadvantage of the mode is that for continuous data, no two values may be equivalent. In these cases, the mode is undefined.\nThe mean is an admirable measure of central tendency that has a number of advantages—the most important of which we’ll discuss shortly. However, the mean also has some problems. First, the mean is especially influenced by extreme scores. The average of the values 1, 1, 1, 1, 1, 100 is 17.5—which is quite far away from the majority of the values in the set. This effect is commonly observed when calculating average house prices. Most house prices are moderate. However, occasionally someone sells a multimillion-dollar mansion. So the average cost of a house in a given city is apt to fluctuate wildly from month-to-month, depending on whether any mansions were sold. (This is the reason why real estate professionals report the median house price, not the average house price.) A second problem with means is that the value may not actually exist in the data. Both the median and mode report a value that is present in the data set. For example, we may find that the average symphony orchestra performance involves 53.7 musicians. Of course, no actually performance finds .7 of a musician on stage. Finally, the mean can be calculated only for interval and ratio scales. We cannot calculate an average for ordinal scales.\nIf we want to calculate central tendency for ordinal data, then it is appropriate to calculate the median. This is an important advantage for using the median. In addition, the sum of deviations for all data is least for the median; no other point has less cumulative deviation to all data points.\nThe principal advantage of the mean is rather technical. Apart from central tendency, another consideration in summarizing a distribution is the variability or dispersion of the data. It turns out that the mean has a special mathematical relationship to the best measure of data variability. Statisticians have demonstrated that squared deviation is a much better measure of ‘goodness’ than the unsquared deviation (see below). It turns out that the sum of squares for deviation for all data is least for the mean; no other point has less cumulative squared deviation to all data points. This means that the mean has a favorable relationship with the standard deviation (see below). The mean and standard deviation make an excellent mathematical pair.\n\nFortunately, for many distributions, the mode, median, and mean values are very similar. In fact, for data that is normally distributed, the mode, median and mean values are equivalent. Much (though not all) data in the world is normally distributed.\n\n\nVariability\nApart from characterizing central tendency, it is often help to characterize the variability or dispersion present in a set of data. At least six measures of variability have been distinguished: (1) range, (2) interquartile range, (3) average deviation (not used), (4) mean absolute deviation (m.a.d.) (rarely used), (5) variance, and (6) standard deviation.\n\nRange: The distance between the highest and lowest data values. For example, for a musical work we might characterize the range between the highest and lowest pitches, or between the shortest and longest notes. There are three common problems with using the range to characterize the variability or dispersion of some data: First, the range is susceptible to outliers—extreme data points that often arise for spurious reasons (such as a glitch in some measuring equipment). Secondly, the range depends on sample size: the larger the sample size, the greater the tendency for the range to increase—even though the distribution changes very little. Thirdly, since the range typically involves just two data points, the range ignores most of the data.\nInterquartile range: The range after discarding the upper and lower quartiles (25%). Problems with interquartile range: (1) discards too much of the data—for many purposes we want to know which of two distributions has the greater variability.\nAverage Deviation: The mean of the (signed) deviations from the mean: Σ(x - x̄)/N. This is a useless statistic; better to use the mean absolute deviation (m.a.d.).\nMean Absolute Deviation (m.a.d.): The mean of the deviations from the mean: Σ(| x - x̄ |)/N. This statistic is rarely used.\nVariance: The average of the sum of the squared deviations. We normally distinguish the sample variance (s2) from the population variance (σ2).\ns2 = σ (x - x̄)2 / N - 1\nSample Variance: The variance of the sample (s2). When calculating the variance of the sample, we must divide by N-1 rather than N.\nPopulation Variance: The variance of the population (σ2). When calculating the variance of the population, we must divide by N rather than N-1.\nStandard Deviation: The square root of the variance. Specifically, the square root of the average of the sum of the squared deviations. The standard deviation is sometimes preferred over the variance because it is in the original units rather than units-squared. We normally distinguish the sample standard deviation (s) from the population standard deviation (σ).\ns2 = &squareroot; σ (x - x̄)2 / N - 1\n \nSample Standard Deviation: The standard deviation of the sample (s). When calculating the standard deviation of the sample, we must divide by N-1 rather than N.\nPopulation Standard Deviation: The standard deviation of the population (small sigma). When calculating the standard deviation of the population, we must divide by N rather than N-1.\nCalculating Variance without knowing the mean: A useful formula for calculating the variance without knowing the mean is: s-squared equals ((Σ Sigma X-squared) minus ((Sigma X)squared divided by N) all divided by N - 1 [or N for the population variance]\nVariance and Outliers: The variance and standard deviation are very sensitive to extreme scores. It is important to keep in mind that outliers will have a significant effect on the variance.\nCoefficient of Variation: The coefficient of variation (CV) is defined as the standard deviation divided by the mean. The coefficient of variation provides a useful way of comparing the standard deviations for distributions that have different means. The coefficient of variation is restricted to interval and ratio data; it cannot be used for nominal and ordinal data.\nEstimators: Statistics are valued primarily according to their ability to estimate the corresponding population parameters. For example, statisticians usually prefer the mean over other measures of central tendency primarily because it is a better estimator of central tendency in the population (in this case μ). That is, sample means cluster more closely around μ than do sample median cluster around the population median.\n\n\nBoxplots\nBoxplots: (Also known as box-and-whisker plots.) An exploratory data analysis (EDA) method developed by Tukey. A box is plotted whose edges coincide with the hinges. The median is plotted as a line dividing the box into two. From the edges of the box, two lines (whiskers) are drawn to the upper-most and lower-most values. If the extreme data points are considered outliers, they are plotted as individual points. In this case, the whiskers embrace only all of the adjacent data. Sometimes the average or mean is plotted using a plus sign (+).\n\nAn example of the use of boxplots is given below. The graph plots aircraft departure delays over a period of eight days, between March 1, 1990 and March 8, 1990. Each boxplot pertains to a different day. Typical delays are less than ten minutes, with the median delays around five minutes. Notice the plotting of five outliers (as individual points). For example, one aircraft was over 30 minutes late departing. Notice the use of the plus sign to plot the means. In this example, the width of the boxplots has been used to represent the total number of observations. For example, the narrow boxplot on March 7 indicates that there were fewer flights/observations made that day.\n\nOutlier: An extreme data point. In Tukey boxplots, outliers are defined as any data points lying outside the inner fences. Inner fences, in turn, are defined as points falling 1.5 times the H-spread above and below the hinges. The H-spread is the range between the medians for the lower and upper halves of the data set (i.e. the “hinges”).\nA tip: It is often helpful to plot data in the order in which the data was collected. Normally, we would not expect the data to show any systematic trend with respect to time, and so such a plot can help to expose problems in data collection.\nQuartiles: Those values that divide the data (distribution) into four quarters.\nDeciles: Those values that divide the data (distribution) into ten tenths.\nQuantiles: A generic name for equal divisions of a distribution—such as quartiles or deciles.\nLinear Data Transformations: When a constant value “a” is added to all values in a data set, the following effects arise: The new mean is the old mean plus “a”; the variance and standard deviation remain the same.\nWhen all values in a data set are multipled by a constant value “b”, the following effects arise: The new mean is the old mean times “b”; the new variance is the old variance times the square of the constant “b”; the new standard deviation is the old standard deviation times the constant “b”.\nDeviation Scores: A common linear transformation is to subtract the mean from each data value so that the mean is centered at zero. The resulting data values are called deviation scores. The standard deviation and variance are not affect by this transformation.\nCentering: The process of transforming data values so that they become deviation scores. This is achieved by subtracting the mean from each data value.\nStandard Scores: The result when deviation scores are divided by the standard deviation. That is, where data values are transformed so that their mean is zero (see centering) and their standard deviation is 1.\nStandardization: The process of transforming data values so that they become standard scores. This is achieved by subtracting the mean from each data value, and dividing by the standard deviation.\n\n\nNormal Distribution\nNormal distribution: A special unimodal symmetrical distribution in which the mean = median = mode. The equation for a normal distribution is as follows:\nf(X) = ( 1/(σ × &squareroot;(2 × π)) ) × e^( -(X - μ)2 / 2 × σ2 )^ where π = 3.14159 and e = 2.7183\n  \nNormal Distribution Notation: Normal distributions can be characterized according to their means (μ) and variances (σ2, sigma-squared). The following notation is used: N(μ,σ2). For example, N(-2.5,18.3) represents a distribution whose mean is -2.5 and whose variance is 18.3.\nStandard Normal Distribution: A standard normal distribution has a mean of zero and a variance of 1; i.e. N(0,1) in normal distribution notation. In order to transform a data set so that it has a standard normal distribution, we need to transform each value to the corresponding standard score. We do this by calculating z.\nz = (X - μ) / σ\nThat is, each standard score is equal to the its original value, less the mean, all divided by the standard deviation.\nZ Score: A Z score represents the number of standard deviations that X is above or below the mean. The Z score assumes that the original distribution is normal."
  },
  {
    "objectID": "emp_methods_workshop/creative_evaluation.html",
    "href": "emp_methods_workshop/creative_evaluation.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Task\nList as many uses as you can for a large plastic soda bottle.\n\n\nMethod of Evaluation\n\n\n\nCriterion\n3 points\n2 points\n1 points\n0 points\n\n\nFluency\n20 or more relevant responses\n10 to 19 relevant responses\n1 to 9 relevant responses\nNo relevant responses\n\n\nFlexibility\n14 or more different categories\n6-13 different categories\n2-5 different categories\nAll responses in the same category\n\n\nOriginality\nAt least one response that is unique or common to no more than 10% of the population.\nOne or more responses that are novel; common to no more than 19% of the population.\nOne or more responses that are slightly novel; common to 20%-49% of the population.\nResponses common to 50% of the population; no novel responses.\n\n\n\n\n\nRationale\nGood research requires creative thinking. This task is intended to develop your powers of associative thinking."
  },
  {
    "objectID": "emp_methods_workshop/creative_answers.html",
    "href": "emp_methods_workshop/creative_answers.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Task\nIn the next five minutes, list as many uses as you can for a large plastic soda bottle.\nYou will be graded on the number, variety, and originality of your suggestions.\n\nsomething to store or carry water in\na life jacket (sealed and empty, a float for bouyancy)\na funnel (cut off the bottom and use the top)\na toy for an infant or toddler\na cup (cut off and use the bottom)\na drum (strike it with sticks or hands)\na whistle (blow across the top)\nhot water bottle (put hot water in it, and use to warm a sleeping bag)\nwith frozen water, use as a food cooler\nanchor - filled with stones (and then water) with rope attached\nexercise weight (filled with water)\nin windy weather, a wind-block for starting a fire (cut a suitably large hole in the side)\n\n\n\nRationale\nGood research requires creative thinking. This task is intended to develop your powers of associative thinking."
  },
  {
    "objectID": "emp_methods_workshop/question2theory.html",
    "href": "emp_methods_workshop/question2theory.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Introduction\nThere are two ways to formulate theories. One approach is to examine some evidence or research results and then try to explain the evidence by creating a plausible story (using the word because). This is what we did in our earlier exercise regarding findings from The American Soldier study. This approach is known as post hoc theorizing.\nA second approach to theory formation begins with a question. We ask a question (which may arise from intuition or past informal experience), and then invent a plausible answer. This approach is known as a priori theorizing.\n\n\nTask\nFormulate a musically-pertinent question. It may be one of the big “God” questions you discussed earlier. Better yet, consider choosing a much more modest question that you think is interesting. Consider two or three theories pertinent to the question. The theories don’t need to explain every aspect of the question, (or even the main part of the question). It is better if your theories address one aspect or component of the question (i.e., Don’t try to explain the whole world at once.).\nHaving considered different possibilities, decide on a single theory that you think would warrant empirical exploration. (You don’t necessarily have to think it is a good or even adequate theory.)\nWrite a two- or three-sentence description of your theory. Be sure to use the word because in your theory.\nIf we have time within the workshop, share your question and theory with your participant group. If you can, would you describe your theory as post hoc or a priori?\n\n\nRationale\nThe purpose of this exercise is to develop skills in theory formation."
  },
  {
    "objectID": "emp_methods_workshop/syncopation.html#syncopation",
    "href": "emp_methods_workshop/syncopation.html#syncopation",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "Syncopation",
    "text": "Syncopation\n\nRecall that a measurement study involves no testing of a hypothesis. It simply involves making and reporting some measurements. An example of a measurement study is Huron & Ommen (2006) which looked at the historical changes in syncopation in American popular music. The study took advantage of newly released digital re-issues of early wax cylinder recordings, as well as 78-RPM and 45-RPM recordings of popular music from the early twentieth century. We limited our study to the first 50 years of commercial recording—namely the period from 1890 to the beginning of World War II (1939). The recorded sample of music included 437 musical works. For each work, we randomly selected a single moment (a particular second on the time counter) and transcribed the bar or measure in which the second appeared. From the transcribed notation of the rhythm we determined whether any syncopation was present, and if so, we categorized the type of syncopation.\n\nHow We Measured Syncopation\nIn order to measure syncopation, we had to make an operational definition. In metrically organized music, each moment can be regarded as occupying a particular point in the metric hierarchy. The highest point (labelled “1”) might be defined as coinciding with the downbeat. The second highest point (“2”) coincides with the third beat in 4/4 meter. Other points in the metric hierarchy can be similarly labelled as shown in Figure 1.\nFigure 1\n Numerical representation of the metrical hierarchy for 4/4 meter. The value “1” designates the highest value in the hierarchy.\nUsing these “metric hierarchy numbers” we can see an interesting pattern emerge for syncopated moments. For example, in Figure 2a (below), a syncopation occurs because the first note is tied to the second note. With each syncopated event, a note onset occurs at a relatively low metrical level (i.e., a higher number), and is sustained through a moment in the metrical hierarchy that has a higher metrical level (i.e., a lower number) and that does not manifest a corresponding event onset. In other words, there is a kind of metrical “hole” in the rhythm, which we dubbed a lacuna. The lacuna became our operational definition of a syncopated moment.\nFigure 2\n Examples of syncopation. As in Figure 1, numerical values represent positions in the metrical hierarchy.\n\n\nWhat We Found\nFor each of our sampled musical moments, we counted the number of lacunae, and categorized the type of lacunae pattern. The results of our study are summarized in Figures 3 and 4.\nFigure 3\n The average number of lacunae (our operationalization of syncopations) per sampled passage. The graph is consistent with the (post hoc) notion that the density of syncopation increased over five decades.\nFigure 4\n The percentage of irregular syncopations by decade.\nThere are two conclusions from this study. The first conclusion is entirely expected—grandmother research: it appears that over the 50-year period, the amount of syncopation per unit of time nearly doubled, from just over one syncopation to nearly two syncopations per unit of time. The second conclusion was unexpected. The variety of patterns of syncopations appears to peak in the 1910s. In other words, musicians in the 1910s seemed to have tried all sorts of rhythmic syncopations, and then reduced the variety in the 1920s and 1930s. Until about 1920, there appears to have been an increase in both the quantity and variety of syncopations; after 1920, the variety seems to have dropped, but the density of syncopations continued to increase.\nNow recall that empirical studies never prove anything. This is also the case here. It could be that our sample of 437 recordings does not adequately refect the music of the time. It could be that our random sampling of musical moments led to unrepresentative results. Or our rhythmic transcription of these passages might have been inaccurate. So we can’t conclude that “the density of syncopation increased over the 50-year period.” Instead, we should say, “the results are consistent with the idea that the density of syncopation increased over the 50-year period.” Similarly, we should say that “our study implies that the variety of syncopation patterns peaked in the 1910s.” Notice the use of the words “appears” and “seems” in the previous paragraph.\n\n\nMethodological Observations\nNotice that the study was not motivated by any theory. We did not use some theory to generate a conjecture, and then refine the conjecture into a testable hypothesis. The study was, however, motivated by a question: How did the use of syncopation change in American popular music over the first 50 years of sound recording? More specifically, how did the density of syncopation change? And what happened to the various syncopation patterns?\nIf you read the actual published article, you’ll see that we took care to avoid defining “syncopation.” Instead, we operationally defined a “lacuna.” Many people might be tempted to say that a lacuna is a pretty good way of defining a syncopation, however, we try to avoid essentializing the concept syncopation. Because we gave a precise definition of “lacuna” we could unambiguously count them: “this rhythm has a lacuna right there,” “this passage contains no lacunae at all.” Notice that Figure 3 (above) plots the number of lacunae (not the number of syncopations—whatever that might be).\nIn Figure 4, we plotted the “percentage of irregular syncopations.” Once again, in the published article, we operationally defined something we called “irregular syncopations.” Once again, we don’t believe that rhythms divide into regular syncopations and irregular syncopations (as though these are “natural kinds” of some sort). This was simply a provisional way of distinguishing commonplace syncopated patterns from less common ones, and then seeing how often the less common patterns occurred in different decades.\nNotice that our study tells us nothing about causality. It has no story to tell about why something might have happened. Also notice that the study did not invite failure; we did not test an a priori hypothesis or theory. Having presented our observations, the study might inspire other researchers to form theories regarding why this happened.\n\n\nReference:\nDavid Huron & Ann Ommen (2006). An empirical study of syncopation in American popular music, 1890-1939. Music Theory Spectrum, Vol. 28, No. 2, pp. 211-231."
  },
  {
    "objectID": "emp_methods_workshop/paper.html#writing-an-empirical-paper",
    "href": "emp_methods_workshop/paper.html#writing-an-empirical-paper",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "Writing an Empirical Paper",
    "text": "Writing an Empirical Paper\n\nFor correlational and experimental studies, the research report typically contains the following components:\n\nTitle\nAbstract\nIntroduction\nHypothesis\nMethod\nSample / Participants\nStimuli\nResults\nConclusion\nDiscussion\nReferences\n\nNow in more detail.\n\nTitle. At least two issues are important in composing a good title. First, what is the size of the community of readers you are targeting? Some titles are obviously intended for a small group of readers: Coactivator and corepressor regulation of the agonist/antagonist activity of the mixed antiestrogen, 4-hydroxytamoxifen. Especially in interdisciplinary fields, it is important to have titles that make sense to a large number of readers. Always try to reach out beyond the narrow audience of professionals in your specialty.\nThe second consideration is whether the title identifies a topic or states a conclusion. A title such as The effect of scale degree on melodic accent identifies a topic. Reading the title does not tell you what the researchers discovered. By contrast, consider the title of a paper by Jane and William Siegel published in 1977: Categorical perception of tonal intervals: Musicians can’t tell sharp from flat.[1] One of the main conclusions of this study is that for small mistuning, musicians can tell that something is out-of-tune, but they can’t reliably tell you whether the mistuning is sharp or flat. In general, “conclusion” titles are better than “topic” titles. It is very helpful for readers if the title describes the main finding. Of course, not every study will produce a clear conclusion. Use a “topic” title when the main finding is too complicated to distill into a few words.\nGood topical titles often pose a question — the question that motivated the research. Examples include: What is melodic accent?[2], Is music an evolutionary adaptation?[3], What is a musical feature?[4] And Why is sad music pleasurable?[5] A good empirical paper will trace the research path: from question, to theory, to conjecture, to hypothesis, to protocol. The title may be a good place to present the motivating question.\nAbstract. Provide a single paragraph that describes the essential elements of your study. Abstracts are typically 100-250 words in length and report in simple terms the hypothesis, a cursory description of the method, and a clear statement of the conclusions. Especially in the arts, scholars are sometimes tempted to organize the abstract like a film trailer — something intended to tantilize or charm a reader into reading the full paper. For most types of scholarship, this is the wrong approach. A bad film trailer is one that gives away the ending of the film. By contrast, a bad research abstract is one that doesn’t give away the ending. Good abstracts always report the main findings. If the results are negative, then say so. If an abstract doesn’t report a conclusion or make a point then it suggests that the research may not have much useful to say.\nIntroduction. Introduce the problem you are interested in. Introductions should retrace the main intellectual path in research: Question-Theory-Conjecture-Hypothesis-Protocol. Once again, if you can, begin with a question: Why do people tap their foot in time to music? Continue by reviewing the main theories people have proposed in the past — that is, provide a literature review. Use the introduction to set the scene for your study. Identify an unresolved issues. Your study might provide an additional test of an already existing theory, or test a new theory that you describe.\nEnd the introduction by giving a one- or two-sentence preview of your experiment or correlational study. The final paragraph might begin:\nIn brief, a study was carried out to determine …\nOR:\nIn light of this question/debate/issue, three studies are described here. In the first study …\nResearch papers are easier to read when the reader has some sense of where you are going. So end the introduction:\nTo anticipate our results, we will see that …\nBe sure to use the tentative language of empirical research. Say “we will see that the results are consistent with the notion that …” Of course we never say “the results prove that …” or “the results establish that …” Don’t say “we will demonstrate that …” or “our results show that …” Simply say “the results are consistent with …”\nHypothesis. The introduction should lead you right up to the statement of hypothesis.\nIn light of the Smith-Jones theory of music-induced foot-tapping, we might propose the following hypothesis:\nIndent, and state your hypothesis.\nH1. Listeners are more likely to tap their feet as the beat approaches 88 beats per minute.\nIf there is more than one hypothesis, label them H1, H2, etc. Then continue by acknowledging that there is no way to directly test the hypothesis:\nAs it stands, the main terms of your hypothesis must be operationalized in order to allow us to procede with a test.\nIdentify the main conceptual terms in the hypothesis. For each term describe different ways of operationalizing it; identify the advantages and disadvantages for each of the various operationalizations. For example, some operationalizations may simply be too laborious, and so are impractical. At the end of the hypothesis section, readers should be convinced that you have provided a reasonable operationalization of your hypothesis.\nNotice that intelligent people can disagree about what makes a good operationalization. Especially rigorous research will identify two or more ways of operationalization a hypothesis. One operationalization might be considered rather narrow (looking at a single composer, or single style, or looking at notated scores). Another operationalization might be considered rather broad (sampling with very lose criteria, having judges listen to recordings, etc.). The most impressive studies seek converging evidence by presenting two or more tests of the hypothesis, where each test operationalizes the terms differently.\nMethod. The “Method” section usually has several subsections. The section may begin with one or two sentences describing the method in general:\nIn brief, the study involved floor-level video capture of foot activity for seated patrons in a discotheque. The tempo of the music was manipulated and the videos coded in order to relate foot-tapping activity to musical tempo.\nIn some cases, there is no need for such a general summary since the overal approach is clear from the discussion of the operationalized hypothesis. In these cases, the “Method” title may be followed immediately by a subtitle, such as “Sample.”\nSample / Participants. Describe the method by which you sampled materials or recruited participants. Indicate whether you are using a convenience sample, using quota sampling, stratified sampling, systematic sampling, etc. For example, what procedure did you use when selecting the scores for analysis? How did you recruit participants? Identify any “exclusionary criteria” — that is, conditions for excluding particular people, performances, recordings, scores, etc.\nFor the purposes of this study, we excluded musicians who reported possessing absolute pitch.\nOR\nIn analyzing the videos, we excluded dancers who were holding objects like drinks or purses, as well as people who were deemed to be merely walking across the dance floor.\nFor research participants, provide some basic demographic information (average age, number of participants of each sex, musical training, etc.).\nTwenty-eight musicians participated in the study, 16 females and 12 males, with an average age of 21.6 years (range 18-27). Most (23) were instrumentalists; five were voice majors.\nStimuli. Describe any stimuli used. If the stimuli were created by the researcher, what criterion were used in their construction? How long were the sounds? What timbres were used? Etc. Where possible, describe the stimuli in sufficient detail that another researcher might be able to duplicate them. Consider posting the stimuli on the web so that readers can hear them for themselves.\nProcedure. Describe what happened. What were the precise instructions given to participants? How long did the experiment last? How many stimuli did they hear? How were the data collected? Etc. Report the instructions verbatim. For example: Participants received the following verbal instructions:\n*INSTRUCTIONS: “In this experiment you will view 20 photographs of people in everyday situations. Imagine that you are present in the scene. For each photograph, identify whether you think there is music playing in the background. If you think there is music, click on the”music” button. If you think there is no music present, click on the “no music” button. When you are finished, click on the NEXT button to view the next photo.\nDo you have any questions?“*\nResults. Conceptually, four things need to occur in a Results section. First, you should establish that the data are not purely noise. Second, you should providing a broad description of the data. Third, you should provide a formal statistical test of the hypothesis or hypotheses. Finally, you should test any supplementary or post hoc hypotheses.\nIn working with data, you want to know that the data are not simply random noise. This is especially pertinent when you ask people to judge things. Test for both intra-subjective reliability and inter-subjective reliability. INTRA-subjective reliability means that a participant judges a stimulus roughly the same way each time. Poor intra-subjective reliability commonly occurs when a participant is not paying attention to the task, or the task is too difficult, or the participant has no task-pertinent skill. In order to test for intra-subjective reliability you need to include some duplicate stimuli. For example, if you have 100 stimuli, you should probably repeat 20 or so. This will allow you to determine whether the participant responded in a similar way when the same stimuli appeared. Measure the correlation between repeated trials in order to determine whether there is intra-subjective reliability. This procedure is referred to as test-retest reliability.\nIn order to determine intra-subjective reliability, the correlation between responses for duplicate stimuli was calculated individually for each participant. The mean correlation for all 30 participants was +0.38.\nYou may want to consider excluding data from unreliable participants. Before you test your hypothesis, establish a possible exclusionary criterion.\nPrior to hypothesis testing it was resolved that data would be discarded for any participant whose test-retest correlation was less than +0.1. According to this criterion, data for two participants was eliminated.\nOR\nAlthough the mean test-retest correlation was +0.72, the correlation for two participants were considered outliers at +0.31 and +0.22. Before testing the hypothesis, it was decided to exclude the data from these two less consistent participants. Having excluded this data, the mean test-retest correlation was +0.82.\nAre all of the participants doing similar things? Measure the correlation between the same trials across multiple participants. If the correlations are significantly positive, then it suggests that all of the participants are behaving in similar ways. If there is no significant positive correlation between subjects, then it suggests that (i) participants may be engaged in different tasks; for example, the participants may have interpreted the instructions in different ways; or (ii) the participants have no pertinent skill related to the task; for example, if asked to judge how “snerky” a musical passage is, participants may be at a loss; or (iii) there is some other problem. If there is no significant positive correlation between subjects, then there is no good reason to average all the data together. If there is a significant positive correlation between subjects, then there is reason to suppose that they were engaged in similar behaviors and so it is reasonable to aggregate the data.\nThe American Psychological Association (2010, p.34) recommends that all empirical papers report effect sizes. In addition, in order to facilitate possible future meta-analyses, research reports should provide complete descriptive statistics (number of subjects, means, standard deviations), and report statistical values to three significant digits, including non-significant results.\nConclusion. The conclusion is essentially an expanded abstract. Like the abstract, the conclusion will reiterate the question or hypothesis, recap the method, and state the results. Where the abstract may state only the main finding, the conclusion restates all of the findings. The conclusion should also assemble and restate all of the caveats and assumptions. Typically, the conclusion is roughly three times the length of the abstract.\nA good conclusion should be written as a “stand-alone” section. Having read the title and abstract, an interested reader should be able to skip directly to the conclusion, and find a good summary of the research. In stating the conclusion, always use that circumspect language of empirical research. No “the results prove …, establish …, demonstrate…, or show …”. Instead — “the results are consistent with the theory that …” or “the results are consistent with the hypothesis that …”\nDiscussion. Only a minority of research articles include a Discussion section. Unlike the Conclusion, the Discussion section provides a more open forum to consider the repercussions of your work. The Discussion section allows you to speculate about what you think may be going on. You may also propose post hoc interpretations of your data. For example, post-experiment interviews might have alerted you to other possible phenomena:\nIn post-experiment interviews, several participants indicated that they had been singing-along with the stimuli. A post-hoc test showed that these participants performed better than the other participants. Moreover, when all of the trained vocalists were included, these participants collectively performed well above the chance level. It may be that “singing” — either overtly or clandestinely makes it easier for people to perform the task.\nYou may also make suggestions for future research.\nReferences. Only include references to works you cite in the article. Be sure that you have read everything you cite. Don’t ignore the early and historical literature.\n\n\nHelping Readers\nIn general, an empirical research paper should be written to provide readers with an efficient way of accessing information. Everyone is busy and no one has time to read everything. Don’t expect scholars to read your entire paper. Some readers will be casually interested in your work. Others will be looking for results related to a particular interest they have. Yet other readers will be very interested in the subject and will read your report carefully — looking to ensure that the method is solid, that the research was done carefully, etc.\nDifferent levels of interest are echoed in which parts of a research paper people read. Suppose that 100 people encounter your paper and read the title. Of that 100, perhaps 50 will flip through the article looking at the figures or illustrations. Perhaps 35 will read the abstract. Of those 35, 10 may also skip ahead and read the conclusions. Perhaps 10 people will begin reading the entire paper, but only 3 will finish it. A good research paper will cater to all of these people. Ideally, a good title will convey the essence of your work. This is not always possible since results are often messy. Nevertheless, readers appreciate titles that are informative. This is the reason why the titles for research papers are often so long. Similarly, the Abstract should be crafted so that readers have some idea of what question/hypothesis you addressed, the approach your study took, and what your main results were. The Conclusion should act like an expanded Abstract.\nTip: Newspaper, magazine, and book editors have long known about the importance of good photos and illustrations. Readers are attracted to the pictures, and that is just as true of research articles. Spend time creating interesting and instructive figures. However, the most under-valued element of a good research article is not the figures but the figure captions. Wherever possible, write a figure caption that tells the whole story. Imagine a reader who has read the title and perhaps skimmed the abstract. They have read nothing else in your paper, but now they are looking at a figure you have provided. Try to make your figure caption summarize the main result so that a casual reader will understand what you have achieved. People appreciate a research article whose results are nicely captured by a single good figure with an informative caption.\n\n\nReferences:\nAmerican Psychological Association (2010). Publication manual of the American Psychological Association, Sixth edition. Washington, DC: American Psychological Association.\nDavid Huron (2001). Is music an evolutionary adaptation? Annals of the New York Academy of Sciences, Vol. 930, pp. 43-61.\nDavid Huron (2001). What is a musical feature? Forte’s analysis of Brahms’s Opus 51, No. 1, Revisited. Music Theory Online, Vol. 7, No. 4.\nDavid Huron (2011). Why is sad music pleasurable? A possible role for prolactin. Musicae Scientiae, Vol. 15, No. 2, pp. 146-158.\nDavid Huron and Matthew Royal, (1996). What is melodic accent? Converging evidence from musical practice. Music Perception, Vol. 13, No. 4, pp. 489-516.\nJane Siegel and William Siegel (1977). Categorical perception of tonal intervals: Musicians can’t tell sharp from flat. Perception & Psychophysics, Vol 21, No. 5, pp. 399-407."
  },
  {
    "objectID": "emp_methods_workshop/truth.html",
    "href": "emp_methods_workshop/truth.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Types of Failure\n\nIn making any knowledge claim (including the claim that something is unknowable) there are two kinds of errors we can make. Methodologists call these Type I and Type II errors. It doesn’t matter whether the source of a knowledge claim is intuition, deductive, or empirical.\n\n\nSkepticism\nAt least since the time of the ancient Greeks, the essence of scholarship has been skepticism. There are two kinds of skepticism: (1) False positive skepticism, and (2) False negative skepticism. A false positive skeptic is afraid of making knowledge claims that aren’t correct. A false negative skeptic is afraid of discarding ideas that might have value. A false positive skeptic is ready to discard an idea given a small amount of contradicting evidence. A false negative skeptic requires overwhelming contradicting evidence before giving up on an idea.\nThe two forms of skepticism are evident in everyday statements such as the following:\nFalse Positive Skeptic: “You don’t know that for sure.” “I really doubt that that’s useful.” “There’s no way you could ever know that.” “There’s not enough evidence for me to believe that.”\nFalse Negative Skeptic: “It might well be true.” “It could yet prove to be useful.” “We might know more than we think.” “There’s not enough evidence for me to stop believing …”\nIn short, the two forms of skepticism might be summarized by the following contrasting assertions:\nFalse-Positive Skeptic: “There is insufficient evidence to support that.” False-Negative Skeptic: “There is insufficient evidence to reject that.”\nThere is nothing inherently superior about one form of skepticism compared with the other. All of us rely on both types of skepticism in our lives.\nPeople who engage in research are optimists, since they must think that through their own actions it may be possible to acquire new knowledge. People who are pessimistic about knowledge claims typically don’t engage in research, since they are skeptical about the possibility of generating new knowledge.\nSince researchers are typically hopeful and optimistic, the principal danger in empirical research is our own eagerness to discover something. For this reason, active researchers are more likely to make Type I errors (i.e., claiming something to be true that is not true/useful/knowable) than Type II errors. Consequently, throughout the history of empirical research, the principal concern has been to minimize or avoid Type I errors.\nAs we will see later, when research involves quantitative data, it is often possible to estimate the probability of making a Type I or Type II error. Although modern science is several hundred years old, the ability to measure the probability of error is less than a century old, and is one of the most important advances in modern empirical methodology.\n\n\nRecap\nWhat’s a Type I Error? Wrongly claiming something to be true, useful or knowable. What’s a Type II Error? Wrongly claiming something to be false, useless or unknowable. In quantitative empirical research, it is often possible to estimate the probability of making either error.\n“But as for certain truth, no man has known it Nor will he know it; neither of the gods, Nor yet of all the things of which I speak. And if by chance he were to utter The final truth, he would himself not know it: For all is but a woven web of conjectures.”\n\nXenophanes"
  },
  {
    "objectID": "emp_methods_workshop/interviews.html",
    "href": "emp_methods_workshop/interviews.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Types of Interviews\n\nOne of the most powerful ways in which we can demonstrate respect for someone is simply by listening to what they have to say. Listening can occur in both informal and formal contexts. Informal non-directed conversation can be remarkably insightful. However, researchers often have specific interests (and limited time), so the researcher might wish to steer the conversation toward particular topics.\nThe interview is a type of formal conversation that takes place between an interviewer and one or more interviewees. Three types of interviews are commonly distinguished according to whether the questions are all established in advance, and according to whether respondents answer by selecting one of several predefined responses, or by answering in a free-form narrative. The three types of interviews are identified in the table below.\n\n\n\n\nFixed Questions\nOpen Questions\n\n\nClosed Responses\nPredefined questions with multiple choice answers.\n\n\n\nOpen Responses\nPredefined questions inviting open answers.\nMixture of predefined and spontaneous questions inviting open answers.\n\n\n\nFixed-question interviews are really a sort of “oral questionnaire.” As with questionnaires, questions may be “closed” (with fixed answer categories that are chosen by the respondent), or “open” (where the respondent is expected to answer in a narrative fashion). These can be referred to as fixed-question-closed-response and fixed-question-open-response interviews respectively. Interviews that rely on an open question format necessarily involve an open response format. It is commonly to refer to these interviews as open interviews.\nInterviews may be conducted in person or via telephone. Interviews may take place in the interviewee’s home, place of work, city street, a neutral space, or a special locale — such as in a retail environment, at a concert hall, green room, in a recording studio, etc.\n\nFixed-Question-Closed-Response Interviews\nAs mentioned, the fixed-question-closed-response interview is essentially an orally-administered questionnaire. This approach is essential when members of the target population are illiterate. For this reason, it is common in many countries to use the fixed-question-closed-response interview when conducting a national census. In addition, face-to-face contact can reduce large-scale fraud — which may occur when using written questionnaires. For example, enterprising people will sometimes fill-out multiple copies of a questionnaire in order to try to bias the research results in a particular direction.\nThe fixed-question-closed-response interview is often convenient to administer in busy settings such as shopping malls and airports where people may be reluctant to sit-down and complete a written questionnaire.\nA final advantage of the fixed-question-closed-response interview is that the interviewer can ensure compliance. First, the interviewer can ensure that each question is answered (no missing data!). More importantly, there is no paper distributed to a respondent, who might then forget to return the questionnaire. This is especially helpful for studies involving quota sampling. For example, the research might require certain numbers of several types of individuals (e.g., students, etc.). By first identifying whether a prospective interviewee conforms to a particular target group, the interviewer can ensure that the quota goals are achieved.\nThese advantages notwithstanding, orally-administered questionnaires also introduce problems. When the response categories are fixed, most people prefer the anonymity of a written questionnaire rather than talking with someone. They are more likely to respond honestly with fewer demand characteristics when they are not interacting with another person.\n\n\nFixed-Question-Open-Response Interviews\nThe principal advantage of the fixed-question-open-response interview is that the respondent is free to talk at length in response to a particular question. Typically, the responses are recorded by the interviewer (with the prior approval of the respondent). All respondents are asked the same questions, but are free to answer in their own way.\nInviting open responses can also be done through a written questionnaire, however, people are much less likely to write at length about their experiences or views — and much more likely to talk at length. In short, the open-response interview invites more detailed responses. Especially when the interview has been arranged in advance and is conducted in a comfortable place, the open response format becomes a sort of guided conversation that encourages the interviewee to say whatever they feel is important.\nCompared with the open interview (see below), one of the disadvantages of fixed-question interviews is the tendency to create a situation in which the respondent is not free to “tell their story.” The danger is that each question is answered in a perfunctory fashion, and the respondent simply waits for the next question, rather than launching into a detailed description of their experiences, impressions, or views.\n\n\nThe Open Interview\nThe open interview is one of the very best exploratory research methods. Open interviews are widely used in consumer research, in criminal investigations (such as eyewitness testimony), in clinical psychology and psychiatry, in social work, by historians, investigative journals, as well as sociologists, anthropologists, and ethnomusicologists.\nOpen interviews can lead to new facts, new theories, and vivid illustrations. Open interviews give us access to the experiences of others. Open interviews often lead to new insights."
  },
  {
    "objectID": "emp_methods_workshop/criticism_advice.html",
    "href": "emp_methods_workshop/criticism_advice.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Criticizing Others Amicably\n\n“Never ascribe to malice that which can be adequately explained by ignorance.”\n-Oscal Wilde (also ascribed to James Thurber)\nAs in any walk of life, engaging in scholarship will bring you into contact with many people, and these contacts will inevitably lead to the formation of both casual acquaintanceships and close friendships. In scholarship, it is equally inevitable that you will come to disagree with some of your fellow scholars. When writing a book review or other scholarly paper, it is not uncommon to find yourself disagreeing with someone whose friendship you value. Or you may find yourself in disagreement with someone you simply don’t want to antagonize. You may struggle with the dilemma: how do I publish critical comments while retaining an amicable relationship?\nFirst, it is helpful to remind ourselves of the ultimate goal. As scholars, we are all part of one big community endeavoring to advance human knowledge.\nGeorg von Békésy won the Nobel Prize for his research regarding the physiological basis for human hearing. In the Preface to von Békésy’s book, Experiments on Hearing, he notes that the worst consequence of winning the Nobel Prize was that no one dared to criticize his subsequent work. Von Békésy was acutely aware of the importance of critics, and he lamented the fact that, after the Nobel Prize, he had lost his best critics. As a result, he felt that the quality of his later research suffered.\nOf course in presenting our criticisms, it is important to avoid certain pitfalls. Here are eight pieces of advice for writing critical responses:\n\nIn the media we frequently encounter stories about people who have perpetrated acts that are downright evil. Malicious people really do exist in the world. But they are extremely rare. I’ve met people who are lazy, forgetful, socially inept, misguided, and uninformed. But I’ve never met anyone who is evil. Especially in the world of scholarship, every one of the hundreds of scholars I’ve encountered over the decades turned out to be conscientious and eager to do the right thing. Scholars make mistakes, but I’ve never had any reason to doubt someone’s integrity. Avoid thinking of someone you disagree with as an “enemy,” a “bad person,” or someone who is “sinful.” Never ascribe to malice that which can be adequately explained by ignorance.\nFailing to be critical does not serve the aim of scholarship.\nAvoid hurtful language. The aim is to be critical of ideas, while simultaneously being supportive of the scholar.\nThe purpose of criticism is not to “show off” your knowledge. It is unfortunate when a reviewer focuses on lesser or tangential issues as a way of demonstrating their superior knowledge.\nChoose your battles. It may be that there are dozens of mistakes and problems in a book or article. Avoid a laundry list of criticisms. Instead, prioritize: what are the most onerous issues? Focus your criticism on the ideas that have consequences.\nIf you’re not sure of something, don’t be afraid to check with the author before rushing to publish. You may have misread or misinterpreted a passage.\nAlways say something nice. Identify the ideas that are truly useful and novel. Be laudatory about the good things. In a book review, provide at least one sentence that gives pleasure to the author, and can be quoted in promotional materials for the book. By way of example, I am probably one of the most persistent critics of various ideas promoted by Dr. Carol Krumhansl. But I have also published the most laudatory words in print about her work. When Prof. Krumhansl received a lifetime achievement award from the Society for Music Perception and Cognition, the enscription was a quotation from my review of her book.\nFinally, after your critical remarks are published — and when you next meet the author at a conference — strike up a conversation. Mention the published criticism, and say that you hope the criticisms are taken in good faith. If an author feels hurt, he or she is likely to avoid you. Don’t let that happen. Take the initiative and be friendly. Don’t give animosity an opportunity to fester.\n\nNotice that creating enemies and alienating people is also antithetical to our shared scholarly goals. We need each other in order to help us grow intellectually.\nOf course, we also need to be gracious with those who are critical of our own work. Resist the temptation to think of your critics as competitors. The worst fate for a scholar is not that people are critical of your work: the worse fate is that no one bothers to read or engage with your work. As we mature as scholars, we become increasingly thankful for a good critic. A good critic helps us develop more quickly. And no kind word will be relished so much as the occasional positive word from our harshest critic."
  },
  {
    "objectID": "emp_methods_workshop/probability.html",
    "href": "emp_methods_workshop/probability.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Probability\n\nProbabilities are represented by numbers between 0 (impossible) and 1 (completely certain). Probabilities are abbreviated using the lower-case letter p The value p=0.5 means a 50/50 chance; the value p=0.25 means a 1-in-4 chance; the value p=0.999999… means almost certainly; the value p=0.000000…1 means very rarely."
  },
  {
    "objectID": "emp_methods_workshop/measure_anything.html",
    "href": "emp_methods_workshop/measure_anything.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Measurement\n\n\nAgainst Measurement\nThe very concept of measuring things seems antithetical to the humanistic spirit. Why would anyone attempt to measure something like happiness or musicality? Many artworks are best regarded as priceless, and the value of a human life is infinite. Counting and measuring seem inherently dehumanizing.\nConsider the case of the so-called Intelligence Quotient or IQ. IQ measurements have been used for all sorts of nafarious purposes, including criteria for categorizing people as “idiots” or “morons.” In The Mismeasure of Man, Stephen J. Gould argued vociferously against attempts to characterize people’s “intelligence.” Intelligience measures are linguistically and culturally biased. What is regarded as “intelligent” behavior to a Wall Street banker may be quite different for a Kalahari Bushman. Surely, comparing the intelligence of different human beings is egregious and barbaric.\nLet’s consider a concrete example of the use of IQ measures. In the past, a common ingredient in paint was lead. However, several decades ago, lead was banned from paint because the lead was found to be detrimental to health, especially the health of children. Animal studies implicated lead as bad for brain development. But how could researchers be sure that the lead in household paints could be having a detrimental effect? After all, paint just sits on walls; it is rare that a child will eat a paint chip. The quantities of lead that are released to the air are miniscule. Surely, lead paint is not a significant health concern. It was discovered that the IQs of children who lived in houses with lead paint was lower than the IQs for children from matched socio-economic backgrounds who lived in houses without lead paint.\nIQ may be only a “crude” estimate of a person’s mental functioning, but measuring IQ proved to be invaluable in improving the quality of health for millions of people. Similarly, differences in IQ also proved to be essential in discovering the terrible effects of methyl mercury on children’s mental development (Hubbard, 2010, p.40). How would researchers have ever discovered these toxic effects without measuring IQ?\nFar from being dehumanizing or barbaric, in these cases, the measurement of IQ resulted in benevolent and humane consequences. Like anything else, measurements can be used for both moral and immoral ends.\n\n\nIn Praise of Measurement\nAs we discussed earlier, there are two main advantages to measurement. First, if we want to invite the world to tell us that we’re wrong, we need measurement in order to make it clear when we’re wrong (“We recognize failure by drawing a line in the sand.”) Secondly, measurement provides opportunities that alert us to phenomena whose existence we might otherwise never see.\nNevertheless, we must address two questions regarding measurement: First, aren’t there some things that simply can’t be measured? And second, aren’t there many things (like happiness) that can’t be measured with any reasonable precision?\n\n\nDefinition of Measurement\nHubbard (2010, p.23) provides a good definition of measurement: “A quantitatively expressed reduction of uncertainty based on one or more observations.” In order to be useful, a measurement doesn’t need to eliminate all uncertainty. A useful measurement simply needs to be better that what you might guess.\nThere are many things that people regard as intangible or immeasurable. Things like “quality,” “creativity,” or “conscientiousness” seem to exclude the possibility of measurement. First, dispense with the idea that measurement is about precision. The question is whether we can estimate through observation. Our measurement will never full grasp the concept: our aim is not to “essentialize.” We can never truly measure “love” in the same we that we can never truly measure “height.” In each case we make estimates using operationalizations that are approximations of the true concept.\nIf we care about something, then it must have consequences in the world. If something has consequences, then it must be possible to observe or recognize the consequences. If we can observe or recognize consequences, then it must be possible to see when the consequences are more or less. That is, we must be able to detect amount. If we can observe differences in amount, then we can estimate it.\nConsider, for example, the following problems, posed by the administration for the Cleveland Orchestra: How do we know whether the quality of orchestral performances is getting better or getting worse? How do we know whether one conductor is better than another potential conductor? There are lots of possible ways of addressing these questions. We might ask the members of the orchestra for their opinions. We might count the number of good and bad newspaper reviews. We might pole the audience members for their opinions.\nWhat the Cleveland Orchestra did was something very simple. They kept count of the number of standing ovations (Hubbard, 2010, p.34). This approach has several advantages. First, it is much easier to tabulate than running audience surveys or coding newspaper reviews. In addition, since the orchestra depends financially on ticket sales, the response of the audience is more important than the judgments of music critics, or even the musicians themselves. Of course, there is more to musical quality than whether a performance evokes a standing ovation. “Musical quality” is ephemeral—we will never be able to directly measure this. Instead, the aim is to make an estimate by observing real-world consequences that are likely related to the concept of interest.\nDavid Moore, the former president of the American Statistical Association, offers the following advice: “If you don’t know what to measure, measure anyway. You’ll learn what to measure.”* (as quoted in Hubbard, 2010, p.31). All measurements begin as rather crude estimates. As you continue to measure something, you will discover various problems, and slowly refine your measure, both in terms of precision, and in terms of what you should be measuring. Like research itself, measurement methods become more refined with experience.\n\n\nFermi Questions\nEnrico Fermi was one of the star theoretical physicists of the twentieth century. Fermi got in the habit of estimating answers before carrying out a detailed calculation. This habit often prevented onerous mistakes by ensuring that the calculations were reasonable. In a famous example, Fermi estimated the size of the atomic explosion by dropping pieces of paper from his hand during the blast. He estimated the size as 10 kilotons. The detailed calculation was around 20 kilotons.\nFermi would ask his students to estimate various things as exercises. For example, a student might be asked to estimate the number of windows in New York City. The most famous of these so-called “Fermi questions” was: Estimate the number of piano tuners in Chicago.\nThe population of the greater Chicago area is 10 million. If the average household contains 2.5 people per household, then there are roughly 4 million households in Chicago. If 1 in 100 households have an acoustic piano, then there are 40,000 pianos in Chicago. If each piano is tuned once every two years, then 20,000 pianos are tuned per year. Suppose that a piano tuner tunes 4 pianos per day (including transportation). Also suppose that each tuner works perhaps 250 days per year. Consequently, each piano tuner tunes roughly 1,000 pianos per year. If there are 20,000 piano tunings each year, then Chicago can employ only 20 full-time professional piano tuners. There currently are 16 piano tuners listed in the directory for Chicago (http://www.chacha.com/question/how-many-piano-tuners-are-there-in-chicago). At the time that Fermi posed this question (in the 1940s), there were many more households that had pianos. So in the 1940s, Fermi’s estimate was 100 tuners. At the time, there were roughly 80 tuners employed.\nFermi estimates are not true measurements since they involve no observation. But these sorts of “back-of-the-envelope” estimates often prove helpful when engaged in research.\n\n\nMeasuring the Unmeasurable\nWhen we think of “unmeasurable” things, we tend to think of concepts like “honesty,” “stylishness,” or “reputation.” However, even straightforward concepts can raise practical difficulties when it comes to measurement. Biologists, for example, are commonly interested in such questions as How many fish are there in this lake? Apart from draining all of the water, it is hard to imagine how one might measure this. Nevertheless, biologists have developed useful estimation methods. In the case of counting fish, biologists commonly use a catch and release method. For example, the biologist might begin by catching 100 fish. These fish are tagged and then released back into the lake. The fish are given time to mix throughout the lake. The biologist then returns and catches another 100 fish. Suppose that 2 fish in the second catch were tagged. This suggests that the original catch of 100 fish represents about 2 percent of the fish in the lake—implying that the lake contains about 5,000 (catchable) fish.\nSimilarly, empirical musicologists have devised a number of techniques for measuring various things, such as How often does a person have a tune stuck in his/her head?\n\n\nSubjective Judgment\nIn measuring things, one of the simplest approaches is to poll people’s subjective impressions. For example, in order to assess the amount of pain a person is experiencing, medical personnel will simply ask the patient to judge, on a scale of 1 to 10 (where 10 is the most intense pain they have ever experienced), how they would characterize their current pain level. Similarly, we can ask people simply to judge how “beautiful” a musical passage is, or judge the “skillfulness” of a given performance.\nThere are good reasons to be wary of subjective judgments. We’d often prefer more objective measures. If we want to know how much a particular car is worth, we can look up the manufacturer’s suggested retail price. However, many of the measures we consider objective, are, in fact, subjective measures. How much is gold worth? It’s worth whatever people will pay for it. A listing on a stock exchange is simply a record of the current price people are paying. Is a paining by Rembrandt truly priceless? If you put it on the market, you may well find that it is only worth $20 million (infinitely smaller than priceless). How much do you value good medical care? Let’s find out what proportion of your income you are paying for health insurance. How valuable is friendship? Let’s find out how much of your free time you spend with your friends.\n\n\nConclusion\nWhen most people hear the word “measurement,” they tend to think of something “precise.” However, the best way to think of measurement is as a way of reducing uncertainty. Fundamentally, all measurements are types of estimates. Even the most imprecise measurements can prove useful.\nFinally, despite our intutions to the contrary, there isn’t anything that can’t be estimated—and therefore there isn’t anything that can’t be measured. In his book, How to Measure Anything, Douglas Hubbard presents a strong case that even the most intangible of concepts can be measured.\n\n\nReferences\nStephen J. Gould (1981). The Mismeasure of Man. New York: W.W. Norton.\nDouglas W. Hubbard (2010). How to Measure Anything: Finding the Value of “Intangibles” in Business. 2nd edition. Hoboken, NJ: John Wiley & Sons.\nClaude Shannon (1948). A mathematic theory of communication. The Bell System Technical Journal, Vol. 27 (July/October, 1948): 379-423, 623-656.\nStanley Smith Stevens, (1946). On the theory of scales and measurement. Science, Vol. 103, pp. 677-680."
  },
  {
    "objectID": "emp_methods_workshop/chi_table_short.html",
    "href": "emp_methods_workshop/chi_table_short.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Critical Values of Chi-square\n\nUpper critical values of chi-square distribution with x degrees of freedom\nProbability of exceeding the critical value:\n\ndf\n90%\n95%\n97.5%\n99%\n99.9%\n⇐ Confidence Level\n⇓\n0.10\n0.05\n0.025\n0.01\n0.001\n⇐ Significance Level\n1\n2.706\n3.841\n5.024\n6.635\n10.828\n2\n4.605\n5.991\n7.378\n9.210\n13.816\n3\n6.251\n7.815\n9.348\n11.345\n16.266\n4\n7.779\n9.488\n11.143\n13.277\n18.467\n5\n9.236\n11.070\n12.833\n15.086\n20.515\n6\n10.645\n12.592\n14.449\n16.812\n22.458\n7\n12.017\n14.067\n16.013\n18.475\n24.322\n8\n13.362\n15.507\n17.535\n20.090\n26.125\n9\n14.684\n16.919\n19.023\n21.666\n27.877\n10\n15.987\n18.307\n20.483\n23.209\n29.588"
  },
  {
    "objectID": "emp_methods_workshop/omsi.html#ollen-musical-sophistication-index",
    "href": "emp_methods_workshop/omsi.html#ollen-musical-sophistication-index",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "Ollen Musical Sophistication Index",
    "text": "Ollen Musical Sophistication Index\n\nAvailable online at: http://marcs-survey.uws.edu.au/OMSI/\nHow old are you today?\n_____ age in years\nAt what age did you begin sustained musical activity? “Sustained musical activity” might include regular music lessons or daily musical practice that lasted for at least three consecutive years. If you have never been musically active for a sustained time period, answer with zero.\n_____ age at start of sustained musical activity\nHow many years of private music lessons have you received? If you have received lessons on more than one instrument, including voice, give the number of years for the one instrument/voice you’ve studied longest. If you have never received private lessons, answer with zero.\n_____ years of private lessons\nFor how many years have you engaged in regular, daily practice of a musical instrument or singing? “Daily” can be defined as 5 to 7 days per week. A “year” can be defined as 10 to 12 months. If you have never practiced regularly, or have practiced regularly for fewer than 10 months, answer with zero.\n_____ years of regular practice\nWhich category comes nearest to the amount of time you currently spend practicing an instrument (or voice)? Count individual practice time only; not group rehearsals.\n[ ] I rarely or never practice singing or playing an instrument [ ] About 1 hour per month [ ] About 1 hour per week [ ] About 15 minutes per day [ ] About 1 hour per day [ ] More than 2 hours per day\nHave you ever enrolled in any music courses offered at college (or university)?\n[ ] No (Skip to #8) [ ] Yes\n(If Yes) How much college-level coursework in music have you completed? If more than one category applies, select your most recently completed level.\n[ ] None [ ] 1 or 2 NON-major courses (e.g., music appreciation, playing or singing in an ensemble) [ ] 3 or more courses for NON-majors [ ] An introductory or preparatory music program for Bachelor’s level work [ ] 1 year of full-time coursework in a Bachelor of Music degree program (or equivalent) [ ] 2 years of full-time coursework in a Bachelor of Music degree program (or equivalent) [ ] 3 or more years of full-time coursework in a Bachelor of Music degree program (or equivalent) [ ] Completion of a Bachelor of Music degree program (or equivalent) [ ] One or more graduate-level music courses or degrees\nWhich option best describes your experience at composing music?\n[ ] Have never composed any music [ ] Have composed bits and pieces, but have never completed a piece of music [ ] Have composed one or more complete pieces, but none have been performed [ ] Have composed pieces as assignments or projects for one or more music classes; one or more of my pieces have been performed and/or recorded within the context of my educational environment [ ] Have composed pieces that have been performed for a local audience [ ] Have composed pieces that have been performed for a regional or national audience (e.g., nationally known performer or ensemble, major concert venue, broadly distributed recording)\nTo the best of your memory, how many live concerts (of any style, with free or paid admission) have you attended as an audience member in the past 12 months? Please do not include regular religious services in your count, but you may include special musical productions or events.\n[ ] None [ ] 1 - 4 [ ] 5 - 8 [ ] 9 - 12 [ ] 13 or more\nWhich title best describes you?\n[ ] Nonmusician [ ] Music-loving nonmusician [ ] Amateur musician [ ] Serious amateur musician [ ] Semiprofessional musician [ ] Professional musician"
  },
  {
    "objectID": "emp_methods_workshop/behaviors.html",
    "href": "emp_methods_workshop/behaviors.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Types of Behaviors\n\nRecall that empirical knowledge is knowledge gained through observation. When we carry out research involving people, what we observe are various kinds of behaviors.\nFor convenience, it’s helpful to conceptually distinguish six types of behaviors: gross behaviors, social behaviors, topical behaviors, metabolic behaviors, self-report behaviors, and artifactual behaviors. As we’ll see, some behaviors straddle more than one of these six categories, so these aren’t hard-and-fast distinctions. However, they’re still helpful ways of organizing observations.\n\n1. Gross Behaviors\nGross behaviors are those behaviors that involve externally observable actions. Examples of gross behaviors include bobbing your head, swaying, blinking, humming, singing, talking, shouting, and laughing. Two of the most common gross behaviors evident in Western culture including clapping hands and tapping feet. Gross behaviors can also involve aspects of posture, such as slumping, or leaning, lowering your head, or sitting upright. Gross behaviors also include facial expressions, such as frowning, raising or lowering your eyebrows, sneering, pouting, and so on.\n  In the case of performers, there are many other forms of gross behaviors to observe. When playing a musical instrument, performers move theirs arms, wiggle their fingers, blow air through their mouths, and so on. Collecting information about these behaviors is often facilitated through the use of special instrumentation, such as motion capture devices, or by recording MIDI data.\n Performance-related gestures don’t need to be real: listeners often engage in imitative gestures. A classic example is the movements involved in “air guitar.” Another example is imaginary conducting. There are even formal competitions for would-be air guitarists and would-be conductors. In general, gross behaviors can be captured through photographs, through video and audio recording, or simply by live, on-site observation.\nGross behaviors can also include what might be thought of as static stateS rather than dynamic actions. For example, a research project might rely on observations of a person’s height, or weight, body shape, or hair style. A study concerning reggae, for example, might focus on whether listeners tend to have dreadlocks.\n Pertinent information for a research project might include observations of music-related ornaments such as jewelry, or certain kinds of clothing. There are even people who sport music-related tattoos. A research project related to musical style, for example, might aim to observe the make of automobiles people drive to a concert. All of these might be regarded as gross behaviors — or at least gross measures.\nAn interesting example of the use of gross behaviors in a music study is the work of Olaf Post. The famous Concertgebouw concert hall in Amsterdam maintains a library of video-footage of all the concerts that take place. One of the unique features of the Concertgebouw is that there are several rows of seats behind the stage. So the video recordings capture — not just what’s going on on-stage — but also the behavior of (at least) some of the audience members.\nNow when people attend a symphony concert, they tend to just sit and occasionally applaud. But in fact, people (from time-to-time) cross their legs, or scratch their noses — adjust their glasses, turn their heads, fold their arms — in short, people in the audience figet.\nUsing the video recordings, Olaf Post counted the number of figetting events over the course of different performances. He found that the amount of figetting parallels the musical structure in a variety of ways. What’s especially remarkable is that Post found that audiences were significantly less figety when listening to music by Gustav Mahler, compared with a composer like Anton Bruckner.\nThe important point is that — even the systematic observation of figetting carries information that can be potentially useful in music-related research.\n\n\n2. Social Behaviors\nWhen behaviors involve groups of people, the social element becomes foremost. Many social behaviors are really types of gross behaviors, but when the behaviors relate to how people interact it’s more common to speak of social behaviors.\nThe actions of crowds at a concert may include dancing, group clapping, swaying, arm-waving, kneeling, shouting, singing or chanting in unison, and so on. Group movements may be synchronous or asynchronous. Interpersonal gross behaviors may include conversing, holding hands, winking, and other interpersonal actions. \nSocial behaviors include body language such as making eye contact, nodding in agreement, orienting toward or away from someone, and smiling or frowning. Since facial expressions are normally intended to be communicative, one might want to regard all facial expressions as social behaviors rather than gross behaviors.\nSocial behaviors also include networking information. On Facebook, for example, social networks are evident in the patterns of “likes.” Social networks are also evident in e-mail contact lists, in stored phone numbers, subscription lists, and so on.\nIn recent years, important research on the social aspects of music have been carried out using large amounts of data collected through popular web services such as Youtube, Hulu, i-Store, and Amazon. We know, for example, that how people evaluate music is strongly influenced by how other people evaluate music within a perceived social group. For example Matthew Salganik and his colleagues split 14 thousand people on the world-wide web into eight randomly assigned social groups. They showed that the popularity of new (never heard-before) pop songs depended primarily on the mutual awareness of what other people in the social group were listening to. All eight social groups started off with the identical roster of new songs, but they diverged in their musical preferences simply through the happenstance of social influence.\n\n\n3. Topical Behaviors\nIn science, the word topical means “pertaining to the surface.” A “topical medicine” is a medicine that is applied to the skin. Hence, topical behaviors are behaviors that relate in some way to the skin. There are four classic topical measures: skin temperature, skin conductance, pilomotor activity, and electromyographic responses.\nSkin temperature changes slowly and can be measured with a topical thermometer. Some parts of the body tend to be warmer than others, and the distribution of heat can change over time. Skin temperature can also be imaged using an infrared camera. Skin temperature is determined principally by the amount of blood flowing near the surface of the skin. Some changes of blood flow can be seen directly — such as when a person blushes or when they go pale. But most changes are more subtle — although they can still be measured. When a performer is suffering from stage-fright, their hands and feet will tend to become cold. This occurs because of the constriction of blood vessels in the body’s extremeties. This phenomenon is referred to as peripheral vaso-constriction.\nA second topical measure relates to how much we sweat. The skin is covered with sweat glands, and the amount of sweating can change rapidly. Sweat is a fluid with a high salt content, which means that it’s a relatively good conductor of electricity. Two electrodes are placed on the skin, and a small (unnoticeable) voltage is applied continuously. When sweating increases, the resistance of the skin drops and so the electrical current increases.\nThese changes can be quite rapid (on the order of half a second or so) and are easily (and cheaply) recorded. Measures of skin conductance are variously referred to as galvanic skin response (which is abbreviated GSR) or as skin conductance response (which is abbreviated SCR). Skin conductance is widely used in polygraphs — that is, in lie detectors. Sweating increases rapidly when the sympathetic nervous system is active. This includes experiences of fear, the startle response, the defense reflex, the orienting response, feelings of anger, and sexual arousal.\n A third topical response involves body hair. hair follicles can flex or relax. When the follicles flex the skin takes on a distinctive goose-bump texture. The technical name for this is the pilomotor response. This response is musically important in the phenomenon of frisson where a listener experiences “chills” or “thrills.” With the help of a video camera and a close-up lense, the pilo-motor response can be observed visually. We simply point the camera at the back of the neck or the arm of a participant. Looking at the camera output, an observer can then visually characterize the degree to which the skin takes on a gooseflesh appearance.\n A fourth topical measure arises from the muscles underneath the skin. Muscles produce electrical potentials that can be measured. These measures are referred to as electromyography (abbreviated EMG). EMG potentials can be measured either at the surface of the skin, or more directly by inserting a small needle or electrode into the muscle. Muscles generate large electrical potentials when they’re flexed, but they also produce spontaneous electrical activity when at rest. The resulting muscle activity can be highly informative depending on the muscle being monitored. For example, facial muscles echo various emotional states. By way of example, EMG activity in the zygomatic muscles (involved in smiling) provides a good indication of positive feelings — even if there’s no visible evidence of smiling. An example of the use of EMG in music research is in the work of Ulf Dimberg who has demonstrated that EMG activity provides a useful measure of the perceived pleasantness or unpleasantness of a sound.\nA particularly illuminating set of muscles are those related to the eyes. We can identify three main aspects to eye behavior. The simplest is blinking — which is one of the most reliable indicators of the startle response. When you hear a loud unexpected sound, it’s very likely that you’ll blink. A second aspect is the size of your pupils. The dark region at the center of the eye responds very quickly and continuously to changes of stimulation, including sounds. A third aspect is the movement of the eyes themselves — known as saccadic movements.\nJohn Sloboda has carried out studies examining how the eyes move when musicians read musical notation. It turns out that the movements of the eyes differ depending on whether the music is predominantly homophonic or predominantly polyphonic. There are also observable differences between how the eyes move for highly trained musicians versus less experienced musicians reading musical scores.\nNotice that eye behaviors form a special category that might be regarded as either gross behaviors or as topical behaviors.\n\n\n4. Metabolic Behaviors\nMetabolic behaviors relate to body conditions under the skin’s surface. Classic examples of metabolic behaviors include heart-rate, blood pressure, (core) body temperature, and respiration.\nOf course many metabolic observations can be measured from the body’s surface. For example, we can measure a person’s pulse simply by pressing a couple of fingers against their wrist. Or we can measure blood pressure using a cuff wrapped around a person’s arm — and so on. There’s certainly room to debate whether these should be categorized as metabolic or topical measures. Nevertheless, traditionally, heart rate and blood pressure are considered metabolic measures.\nPulse can be measured with wrist-watch like devices that can collect data continuously over many hours or even days. Two classic heart responses are of particular interest: the tachycardic and bradycardic responses. A tachycardic response occurs when the heart-rate increases briefly and then returns to normal. This is associated with fear or alarm. A bradycardic response occurs when the heart-rate decreases briefly, rebounds above normal briefly, and then returns to normal. This is associated with interest or attentiveness.\nAnother measure of interest is the heart-rate variability (or HRV). This has been found to be especially informative in a variety of different ways.\n Another metabolic behavior is breathing or respiration. Respiration can be measured using a string-like or band-like device that’s placed snugly around a participant’s chest. It can also be measured using an electronic device taped to a person’s chest. Breathing influences the amount of oxygen in the body. Rather than measuring respiration, there exists a simple device (an oximeter) that clips onto the finger — and can be used measure more directly the oxygen level in the blood.\n One of the most useful physiological measures is the electrical activity generated by the brain. Eletroencephalography (or EEG) measures tiny changes of voltage that reflect the electrical activity of large groups of neurons. Although these voltages are very small, they can still be measured by placing electrodes on a person’s scalp. EEG measures are rather crude — representing the average activity of assemblies of millions of neurons. When enough neurons fire at roughly the same time, they create a big-enough electrical potential to be measured at the surface of the head.\nIn music-related research, it’s most common to look at the electrical activity that’s evoked in response to a particular sound. These are so-called event-related potentials (abbreviated ERPs). ERPs have characteristic signatures that can indicate, for example, whether a person noticed or paid attention to a given sound. These signatures have proved particularly useful in answering questions about what infants are able to hear or distinguish. For example, research by Laurel Trainor and her colleagues has determined when infants are able to recognize atonal violations of tonal melodies. Infants can’t talk about what they hear, but their EEG responses can give important clues about how they’re experiencing the sounds.\nAn important class of metabolic measures is to be found in hormone levels. Hormones are chemical messengers, commonly transported by the blood that influence cell behavior in many ways. Examples of hormones include epinephrine and norepinephrine (also known as adrenaline and noradrenaine), dopamine, serotonin, oxytocin, prolactin, cortisol, histamine, estrogen, testosterone, insulin, as as others. Some of these hormones (like testosterone) can be measured in saliva. So we can have a participant simply spit into a small cup, or use a mouth swab. Another common technique has the participant chew on a small absorptive plastic device.\nOther hormones can be measured only by taking a blood sample. Depending on the sensitivity of the assay method, the researcher may need only a small dab of blood (that can be collected from a pin-prick), or a larger sample of blood that would require a professional nurse to do a blood-draw.\nSome hormones are present only in the brain. Unfortunately, these hormones can be measured only by examining the cerebro-spinal fluid — which is gathered through a spinal tap. This procedure is much too invasive to be used for casual research purposes, like studying music.\nAn example of observing changing hormone levels in response to music is the work of Hajime Fukui who found that testosterone levels are lower when people listener to their preferred music.\n In the past decade, it’s become increasingly common to use brain scanners in music research. At least four types of scanners can be distinguished. Functional Magnetic Resonance Imaging (or fMRI) can be used to measure how much oxygen is being used in different parts of the brain. Areas of the brain that are the most active use more oxygen. The localized blood volume in that area changes rapidly, and fMRI methods can be used to pin-point these places when a person is engaged in different tasks. MRI machines are noisy — producing a loud intermittent banging sound that limits their use in auditory tasks. The machines are bulky and expensive.\nPositron Emission Tomography (or PET) involves the injection of mildly radio-active substances into the blood-stream. Depending on the substance that is injected, PET can provide very useful indications of where in the brain a particular neurochemical is released or congregates. Using this technique, for example, Varlery Salipoor and her colleagues were able to show that listening to chill-inducing music released dopamine in regions of the brain associated with the experience of pleasure. PET is quite invasive since participants are injected with radio-active substances. The preparation of these materials by an experienced chemist makes PET especially expensive.\n A third technique is MagnetoEncephalography (or MEG). Like EEG, this method traces the electrical activity of the brain. However, it monitors magnetic information rather than voltage — giving much better resolution and is able to provide better information about activity going-on deeper in the brain. Compared with fMRI and PET, MEG has an especially good temporal resolution, resolving electrical events in the brain with a precision around 10 milliseconds. Since MEG scanners measure magnetic fields, the scanning must take place in a magnetically shielded room. In addition, liquid helium is used to cool the machine, which raises the cost significantly.\nA fourth method is functional Near-Infrared Spectroscopy (or fNIRS). Infrared light is able to penetrate through the scalp and skull, and for some distance into the cortex. The reflected light is useful for detecting changes in blood hemoglobin. Like fMRI, fNIRS can be used to infer oxygen uptake in regions of the brain associated with neural activity. In addition, NIRS is much more portable than fMRI, PET, or MEG machines. Some manufacturers provide wireless instruments that allow researchers to study freely moving people. However, due to the absorption of light, NIRS can only be used to scan the outer cortical tissues of the brain. It is unable to measure subcortical structures. NIRS is especially effective for imaging the brains of infants. The technique is non-invasive, and the thin skulls of infants allows the light to penetrate more deeply into the neural tissue.\n\n\n5. Self-Report\nWithout question, the most common behaviors used in research is the self-report. Self-report is the label given to any behavior in which we simply ask participants something. An obvious example of a self-report is when we ask someone to indicate their age on a questionnaire. In these cases, we rely on special knowledge possessed by the participant.\nSelf-report observations can be collected in many ways, including both formal and informal interviews. For example, we might ask a child what songs she knows, how long she’s been studying an instrument, or what she thinks of her music teacher.\nQuestionnaires and surveys all involve self-report. These can be done with pencil and paper, through brief interviews, or using electronic media such as having people answer questions via the web.\n Many self-report behaviors involve asking someone to offer an opinion. IQ tests and personality tests — all rely on self-report. Often responses are structured, so that the participant simply selects one of a set of predefined answers. For example, a statement may be presented, and the participant is asked to indicate the degree to which they agree with it: strongly-agree, agree, agree somewhat, undecided, disagree somewhat, disagree, or strongly disagree. In music, it’s common to ask which of two sounds is more dissonant, or more memorable, or more whatever.\nThese are referred to as forced-choice responses. If only two choices are provided, the mode of responses is commonly known as a two alternative forced choice — which is frequently abbreviated 2AFC. Responses might involve typing on a computer, pointing-and-clicking, or pressing a button of some sort.\nSelf-reports include elicited statements and introspective reports — such as asking a performer to describe (for example) how he or she begins practising a new musical work.\nApart from interviews, conversations, surveys or questionnaires, responding by pressing a button is also typically regarded as a form of self-report.\nSelf-report is commonly used in research, even when there are more objective methods available. For example, rather than asking a person their age, we might follow-up by using government records to obtain a copy of their birth certificate. But in the majority of studies, the researcher has little reason to doubt the accuracy of the information provided by the participant.\nNevertheless, there are common situations where participants do not accurately self-report. For example, in self-report, people often claim to be taller than they are, and it’s very common for people to mis-report their weight. When accurate data is needed, the researcher may choose to perform objective measurements.\nThe biggest advantage of self-reports is that they are often the easiest kind of data to collect. However, they are also easily confounded by the views, the ideas or the beliefs of the participant. These beliefs may or may not be accurate. For example, a musician might intellectually conclude that there’s nothing inherently sad about the minor chord. Our musician might even claim that he or she doesn’t hear the minor chord as having any sad connotations. In many cases, we should simply believe what a person says. But sometimes people deceive themselves — and so what is said doesn’t necessarily accurately reflect what they’re experiencing.\n\n\nImplicit Measures\nIn these sorts of cases it’s useful to use a method in which the person’s beliefs are sidelined. One of the best ways to do this, is by employing so-called implicit measures. A simple example is provided by the affective priming method. Frank Ragozzine carried out a simple experiment where he flashed words on a screen and asked participants to respond as quickly as possible to whether the word was a Happy word or a Sad word.\nFirst of all, this task is easy. The words are very clearly happy or sad. For example, sad words might include glum, down, sorrow, blue and blah. Happy words might include smile, jolly, pleased, jumping and sunny. The participant simply had to press one of two buttons as quickly as possible — either the happy-word button or the sad-word button.\nNow consider what happens when we play either a major or minor chord immediately before the word appears on the screen. What Ragozzine found was that playing a major chord improved the reaction speed for happy words but reduced the speed for sad words. At the same time, he found that playing a minor chord reduced the speed of reaction for happy words and improved the reaction speed for sad words. In other words, the chords facilitated performance when they conformed to the major-happy or minor-sad conventions — but they interferred with performance when the didn’t match the major-happy or minor-sad conventions. What’s nice about this method is that the task is simply too fast for people to give any thought. We can test people from different cultures and different backgrounds and see how they respond — without having to rely exclusively on what they say or claim.\nIn general, researchers prefer these implicit observations over explicit measures. Implicit methods reduce the impact of what a person believes — and instead focuses on how they behave. The most revealing kinds of behaviors are those that are spontaneous and unconscious. When a person taps their foot or smiles, it’s likely that they’re enjoying the music — whatever they might say. Spontaneous and implicit measures have been used, for example, to determine how prejudiced a person is. Few people would ever say that they are prejudiced against people of African descent, or women or foreigners. However, various implicit association tasks can be quite revealing about a person’s unspoken dispositions. Our actions can betray attitudes that our conscious selves might find quite uncomfortable.\nAs researchers, then, we’re always on the look-out for spontaneous, easily observable behaviors that are not strongly regulated by conscious thought.\nI recall one day, casually listening to a recording by the musical humorist Peter Schickele — better known as P.D.Q. Bach. It was a live recording — and of course there was laughter. Now music-induced laughter isn’t a common behavior. But I remember being so struck by the behavior itself: laughter is an easily observable behavior that’s quite spontaneous and not strongly regulated by conscious thought.\nThat observation led to a major study of musically-evoked laughter. With Joy Ollen, for example, we carefully studied 640 instances of audience laughter in response to musical jokes.\nAlthough it might seem tangential to more commonplace musical experiences, we learned a lot from that research.\nSo once again, although self-report is perhaps the most important source of behavioral observations for studying music, there are times when the researcher needs to be suspicious of the capacity of participants to accurately introspect and honestly report what they’re experiencing.\nA useful check on observations from self-report, is to look for converging evidence using implicit methods where the conscious thoughts and beliefs of the participant are sidelined.\n\n\n6. Artifactual Behaviors\nA final category of behaviors might be called artifactual behaviors. People create things: in cultures all over the world we find all kinds of different musical instruments; musicians also make recordings — in lots of different formats; people make music videos, they take photographs, they write in diaries and blogs, they write program notes, they even write lyrics down on slips of paper.\nIn the case of music, some of the most important artifacts include sound recordings and musical scores. Scores and recordings are the familiar starting points for many forms of traditional music scholarship — especially the work of music theorists and scholars doing music analysis.\nWhen approached systematically, empirical observations rely on the usual quantitative measures. A scholar might count the number of instances of a certain event — like the number of Neapolitan chords in some corpus. Even simple counts — like the number of notes or the number of measures in a work can sometimes prove useful.\n Historical musicologists make use of a variety of techniques for making sense of manuscripts. For example, paying close attention to the hand-writing in a manuscript might help to resolve whether two manuscripts were written by the same person, or by two different people.\nIt’s often helpful to measure or count features as a way of helping to resolve a question or conjecture. What is the average angle of the stems compared with the horizontal staff. Is there a slight leaning to the left or to the right? How long are the stems? What kind of cursive style is used for the textual underlay — and so on. Tallying-up a series of physical measures from a manuscript can help provide converging evidence for a particular historical interpretation.\nConventionally, analysis might involve interpreting the harmonies in a score and perhaps relating certain harmonic progressions to different styles, genres or periods.\nIn recent decades, many musical scores have become available online, and so many measurements can be automated with the assistance of computer software.\nA simple example here might be the so-called melodic arch. Musicians long ago observed an apparent tendency for melodic phrases to rise upward and then fall downward forming a sort of arch. Think of “My Bonnie Lies Over the Ocean” or “Somewhere Over the Rainbow.” In each case, the phrases tend to ascend at the beginning and descend towrd the cadence. But no sooner do you think of this — than all kinds of exceptions come to mind. So, for example, the song “Joy to the World” and the American national anthem both begin high — dropping downward — and then rise upward toward the end. So is there any merit to the notion of a melodic arch? \n The three accompanying graphs are from Huron (1996). The graphs show what happens when the pitches for a large number of melodic phrases are averaged together. The three graphs illustrate phrases of 7-, 8- and 9-notes in length. In each graph, the first plotted point represents the average pitch height (in semitones above middle C) for all the first notes in the phrases. The second point represents the average pitch height for the seconds notes in the phrases, and so on.\n Although we can identify plenty of individual exceptions, on average, it is indeed the case that for many genres of Western music, the melodic arch is more than a figment of our imaginations. In general, there is a tendency for melodies to ascend and then descend over the course of individual phrases.\nAs we’ve seen, there are innumerable artifacts arising from human behavior that can be measured or tabulated. These include scores, sound recordings, and even from musical instruments. All of these can provide useful observations — either for exploratory research or for hypothesis-testing.\n\n\nReferences:\nUlf Dimberg (1987). Facial reactions and autonomic activity to auditory stimuli with high and low intensity. Psychophysiology, Vol. 24, p. 586.\nUlf Dimberg (1989). Perceived unpleasantness and facial reactions to auditory stimuli. Uppsala, Sweden: Uppsala Psychological Reports, No. 414.\nHajime Fukui (2001). Music and testosterone: A new hypothesis for the origin and function of music. Annals of the New York Academy of Sciences, Volume 930, pp. 448-451.\nDavid Huron (1996). The melodic arch in Western folksongs. Computing in Musicology, Vol. 10, pp. 3-23.\nOlaf Post (2011). “The way these people can just listen!”: Inquiries about the Mahler tradition in the Concertgebouw. PhD Dissertation, Columbia University Department of Music.\nFrank Ragozzine (2011). Cross-modal affective priming with musical stimuli: Effect of major and minor triads on word-valence categorization Journal of ITC Sangeet Research Academy, Vol. 25, pp. 8-24.\nMatthew Salganik, Peter Dodds & Duncan Watts (2006). Experimental study of inequality and unpredictability in an artifical cultural market. Science, Vol. 311 (February 10, 2006), pp. 854-856.\nValorie Salimpoor, Mitchel Benovoy, Kevin Larcher, Alain Dagher & Robert Zatorre (2011). Anatomically distinct dopamine release during anticipation and experience of peak emotion to music. Nature Neuroscience, Vol. 14, pp. 257-262.\nJohn Sloboda (1985). The Musical Mind. Oxford: Oxford University Press.\nWeaver (1943). [On eye movements while reading music.]"
  },
  {
    "objectID": "emp_methods_workshop/remarks1.html",
    "href": "emp_methods_workshop/remarks1.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Even if you have no interest in conducting formal empirical research, my hope is that you will benefit from learning how to critically read and evaluate any empirical study.\nApart from these explicit learning objectives, I should confess to a personal ulterior motive.\nAs an undergraduate student, I learned from my music & arts professors to use the word “empirical” as a term of derision.\nSome people are surprised to learn that I did my masters thesis on the work of Theodor Adorno — Frankfurt School of Critical Theory.\nAdorno would have regarded himself as the embodiment of the antithesis of empiricism. And there are many modern music scholars who hold similarly dim views of empirical scholarship.\nPresumably, everyone in this room is present because of some abiding interest in empirical research.\nEncountering people of like mind is always reassuring.\nBut it is important to recognize that empirically-oriented music scholars are definitely in the minority, and that much of the broader musicological community is quite skeptical.\nSome of the skepticism is unjustified and arises from widespread misconceptions in the arts communities about formal empirical research methods.\nAt the same time, some of the skepticism is justified. Some empirically-oriented music scholars have exhibited a remarkable naivitee in presenting their work, and other empiricists have used insulting or intemperate language when critiquing non-empirical scholarship.\nApart from empirically-oriented arts scholars, scientists add fuel to the fire when they make untenable claims, or when they show a dismissive attitude toward traditional hermenuetic and other methods common in arts scholarship.\nWhen it comes methodology, everyone has something to learn. There is no discipline that has established the one true method.\nI don’t think we need to replace traditional music scholarship by more empirical approaches. Mostly, I think that there are lost opportunities to conduct better, more insightful research.\nSo what do I hope you get out of these five days?\nIt’s not simply the case that I hope you’ll do more rigorous, compelling, insightful, and inspiring research. I also hope you’ll act as emmissaries — bridging a cultural divide that shouldn’t exist in the first place, but which is harmful to fruitful dialog about how best to conduct research — whether we’re talking about the sciences or the arts."
  },
  {
    "objectID": "emp_methods_workshop/sampling.html",
    "href": "emp_methods_workshop/sampling.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Sampling\n\nThe goal of sampling is to be able to make accurate claims about some “population” even though you have only examined a subset of that population. Sampling is carried out only for practical reasons. If researchers had infinite resources, we would only study entire populations rather than samples. Some people believe that making generalizations from small samples is inherently untenable. However, statisticians have formally demonstrated that sampling can provide a very effective way of making inferences about a much larger group. This is also borne-out in practice: from a small sample of blood, a medical technician can often infer a great deal about the health of a patient. The technician doesn’t need to take all of your blood in order to make such inferences.\nPopulation: everything (or everyone) that you’re interested in. A population is whatever you want it to be.\n\ne.g. all the world’s people\nall the world’s people including living and deceased\nall Western-enculturated people\nall people who enjoy listening to music\nall clarinet players\n\nA “population” does not refer only to people: Other examples:\n\nall of the music written by Vivaldi\nall solo flute music (both with and without accompaniment)\nall music in the minor mode\nall of the jazz scores available in the New York Public Library\nall performances of Rachmaninov’s 2nd piano concerto\nall recordings of Rachmaninov’s 2nd piano concerto\nall commercially released recordings of Rachmaninov’s 2nd piano concerto\nall the commercial recordings of Rachmaninov’s 2nd piano concerto accessible via the web\n\nSample: a subset of the population that you hope closely resembles the population as a whole.\nA sample is said to be representative when the property of interest is identical in both the sample and the population.\nA sample is said to be biased when the property of interest differs between the sample and the population.\n\nDefining Your Population\nYou can’t sample a population unless you have a clear idea of what constitutes the population of interest. It often requires careful thought to identify the appropriate population. Suppose, for example, that you are a political pollster. Your aim is to predict the likely election results for a national election in Denmark. What, precisely, is the population you are interested in?\n\nAll Danish citizens?\nAll people living in Denmark?\nAll people living in Denmark eligible to vote?\nAll people eligible to vote in Danish elections?\nAll people likely to vote in Danish elections?\n…\n\nSampling method: the way you recruit or assemble your sample. When your population consists of people, sampling methods might include soliciting information by telephone (telephone sampling), street sampling, mail sampling, web sampling, classroom sampling, concert sampling, etc. You might hire a professional pollster to carry out a sophisticated sampling method. (Pollsters know more about sampling than any other group of professionals.)\nSampling bias: when the sampling method introduces differences that cause the sample not to be representative. We try to avoid or minimize sampling bias.\nWhen conducting a telephone survey, a pollster may be tempted to ask to speak to a respondent’s spouse. However, spouses are likely to share many things in common (such as political views) so the sampling method will introduce a bias.\nThe only way to eliminate sampling bias is by sampling the entire population. This is done sometimes (as in national census research). It is also possible when the population is relative small. For example, since Brahms published only three string quartets, it may be possible to make fully accurate summary comments by examining all three quartets. In most research however, it is not practical to examine an entire population.\nIn street sampling, bias can be introduced because the pollster may be attracted to approach certain people and not others. For example, a pollster may be more likely to approach someone who is smiling. This will potentially bias the sample toward “friendly” people. Also, curious people may hang around hoping that the pollster will ask them. This may also introduce bias. A common technique is to count a certain number of passers-by. For example, after someone has completed the survey, count ten passers-by before approaching the next person you encounter.\nThe key to good sampling is to control for those forms of bias that would obviously skew the results, and to be aware of the how your sample might fail to be representative.\n\n\nSampling Methods\nThere are many sampling methods. Here we identify six common methods.\n\nConvenience Sampling. A convenience sample simply takes advantage of whatever might be available. For example, a sample of organ music by Gabriel Fauré might simply consist of all of the scores available in a music library. Similarly, we might stand on a street corner and ask whoever passes by to answer questions on a survey. In the OSU School of Music, we have a (convenience) subject pool consisting of all the students in second year aural skills courses. The School of Music’s subject pool represents Convenience samples are nearly always biased in some way, but they are easy to assemble.\nIt is sometimes helpful to collect additional information regarding convenience samples in order to help identify ways that the sample is biased. For example, in the case of the OSU School of Music subject pool, we have collected basic demographic and other information from this pool to help us understand how this group of people differs from the general population.\nSimple Random Sampling. Suppose we want to know about musical instrument sales in the City of Nashville. We could use the telephone directory (www.yellowpages.com) to identify all of the shops within the city boundaries that sell musical instruments. Perhaps we discover that there are 131 retailers. From this list, we might randomly select 25 retailers in order to carry out our survey.\nRandom sampling might be done by selecting slips of paper from a bowl, rolling dice, using a printed random number table (often provided in an Appendix of a book on statistics), or using a computer-programmed random generator.\nSuppose we have a video recording of an audience listening to a concert. Our hypothesis is that members of the audience “figet” more when listening to Schoenberg compared with Ravel. The video camera allows us to see 150 audience members and the recording lasts 40 minutes. Counting the figets for each audience member over the entire recording would require too much work. To simplify the problem, we might randomly select 25 of the 150 audience members. In addition, instead of coding data for the entire video, we might count the number of figets during twenty randomly selected 10-second segments: ten 10-second segments from the Schoenberg work and ten 10-second segments from the Ravel. Here we are randomly sampling from the total visible audience, and also randomly sampling from the total length of the video recording.\nSystematic Sampling. A common alternative to simple random sampling is to employ a systematic sample. For example, in a study involving score-based analysis, we might sample every 50th measure. Similarly, we could sample CD recordings from a large library collection by using a ruler to select the CDs that occur every 25 cm.\nSuppose that we have a questionnaire we want to distribute to people who attended a concert. There might be 500 audience members, but we have only 50 surveys to distribute. One approach would be to distribute the questionnaires to the first 50 people leaving the concert hall. Notice, however, that this might bias our sample to people who are in a hurry, or people who didn’t like the concert and are eager to leave. A better approach would be to hand-out our survey to every tenth person as they exit from the concert hall.\nStratified Sampling. Many populations exhibit sub-populations. For example, in music, we can identify different styles, like pop, folk, jazz and classical. We can also identify different genres, like vocal, instrumental, and mixed (vocal+instrumental) music. Similarly, we might expect that music-related behaviors might differ for infants, children, adolescents, adults and elderly. When we have reason to suspect that difference in sub-populations might influence the results, it is common to sample in such a way to ensure that each of the main sub-populations are represented. In Post and Huron (2009) for example, we were interested in common-practice era tonal classical music. So we decided to use a stratified sample consisting of music from three periods: Baroque, Classical and Romantic. Our overall sample consisted of equivalent numbers of works from each of these historical eras.\nQuota Sampling. A type of stratified sampling in which sub-samples are weighted according to their prevalence in the population. For example, in the general population, 51% are female and 49% are male. So in our sampling, we might want to ensure that 51% of our sampled individuals are female. Suppose that we find that 52% of instrumentalists are most accomplished on guitar, 33% are most accomplished on keyboards, 12% on flute, 9% on trumpet, 8% on violin, etc. In quota sampling, we would aim to sample the same proportions for each instrument. We might use a commerical music catalogue to determine the ratios of different types of music. For example, we might note that 60% of the entries in a catalogue are classified as “pop,” 25% of the entries are deemed “rock,” 10% jazz, and 5% classical. A quota sample would aim to duplicate these proportions.\nCollister and Huron (2008) were interested in the intelligibility of vocal lyrics. We collected a sample of 78,000 words from lyrics. We found that 81% were monosyllabic, 16% were bisyllabic, and 2% were trisyllabic. In testing the intelligibility of individual words in a sung context, we employed the same proportions of monosyllabic, bisyllabic, and trisyllabic words.\nMatched Random Sampling. A way of linking members from two or more samples. For example, a study might involve matching each professional musician with an amateur musician who plays the same instrument. One might pair a randomly selected work in the major mode with a randomly selected work by the same composer in the minor mode.\nSometimes, the matching is done within each participant. For example, we might compare the milk produced by each cow with the milk produced by the same cow after music is introduced to a dairy barn. When the results for a single individual are compared across conditions, this is also referred to as a “repeated measures” design.\n\n\n\nWestern Sampling Bias\nEthnomusicologists have rightly criticized music psychologists for assuming that experiments done with Western-enculturated participants necessarily apply to all the world’s peoples. If ethnomusicologists ran the psychology journals, every article would begin with a warning analogous to those found on cigarette packages:\n“The ideas expressed in this article are those of Western-enculturated researchers who have studied only a handful of Western-enculturated subjects, and who brazenly presume (though they never explicitly state so) that the results of this study somehow generalize to people in other cultures.”\nIt is impractical to suppose that every musical study can be done with participants drawn from all over the world. Research can be done only if it is practical. It is reasonable to rely on convenience samples for research, but it is not reasonable to assume that a convenience sample is representative of the entire world. It is important in any study to make a clear statement of the sample, and to identify the limitations of any sample with regard to the presumed population of interest.\n\n\nReferences:\nLauren Collister & David Huron (2008). Comparison of word intelligibility in spoken and sung phrases. Empirical Musicology Review, Vol. 3, No. 3, pp. 109-125.\nOlaf Post & David Huron (2009). Music in minor modes is slower (except in the Romantic Period). Empirical Musicology Review, Vol. 4, No. 1, pp. 1-9."
  },
  {
    "objectID": "emp_methods_workshop/average.html#measures-of-central-tendency-mean-median-mode",
    "href": "emp_methods_workshop/average.html#measures-of-central-tendency-mean-median-mode",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "Measures of Central Tendency: Mean, Median, Mode",
    "text": "Measures of Central Tendency: Mean, Median, Mode\n\n\nMean (or average)\nQ. What is an average?\nA. The sum of all values divided by the number of values.\nQ. Why calculate an average?\nA. To reduce the effect of noise when estimating a population norm.\nCalculating an average can sometimes allow you to see a pattern that is otherwise invisible.\nQ. When can I calculate an average?\nA. Two conditions are necessary: (1) All of the values have been drawn from the same population. (2) The values are normally distributed.\nQ. How do I know whether the values are normally distributed?\nA. Graph the values and see whether the values cluster in a single region, or in two or more regions.\n(There are also formal statistical methods that can be used to estimate whether a set of values all belong to a single population.)\n\n\nMedian\nQ. What is a median?\nA. The middle value of an ordered list of numbers.\nQ. When is it appropriate to calculate a median?\nA. When all of the values are drawn from the same population. The values may or may not be normally distributed. When the distribution is not normal, the median is preferred over the mean as a representative measure of central tendency\n\n\nMode\nQ. What is a mode?\nA. The mode is the most commonly occurring value in some set.\nQ. When is it appropriate to calculate a mode?\nA. When all of the values are drawn from the same population. The values may or may not be normally distributed. The mode is the preferred measure of tendency when the values are are nominal. For example, in Western art music, the most commonly occurring key is G major. That is, G major is “mode” (in the statistical sense) for the set of all music keys. In an orchestral ensemble, the mode instrument is the violin."
  },
  {
    "objectID": "emp_methods_workshop/big_ideas.html",
    "href": "emp_methods_workshop/big_ideas.html",
    "title": "Seven Big Ideas",
    "section": "",
    "text": "Only logicians and mathematicians can talk about “proof.” Truth cannot be established by observing the world. Any set of observations can be interpreted in more than one way (Duhem, 1906).\nIn empirical work, conclusions should be expressed as follows:\nThe results are consistent with the view that … The observations are consistent with the theory that …\nWords like “establish,” “confirm” or “prove” are to be avoided. Even the word “supports” is deceptive. If necessary, use words like “suggest” or “imply.”\nOur study suggests that … The result of our experiment implies that …\nIn general, the empirical researcher’s most useful phrase is: “consistent with.” Get in the habit of regularly saying “consistent with.”\nJust because we can’t prove anything doesn’t mean we aren’t interested in truth. In fact, the pursuit of truth is one of the main motivations for people who engage in research.\nWe are not in the business of proving something to be true. We would love to know the Truth (if that exists). But even if we had access to the Truth, we could never be sure that it was indeed true.\nOur first slogan reminds us of the motivation, and simultaneously tells us that the Truth is not accessible to us:\nSlogan: Motivated by truth, with no hope of proof.\n\n\n\n\n\n\nIn research, we invite the world to speak to us. If the purpose of research is to learn, then we must be prepared to learn that we are wrong. The purpose of research is not to confirm what we already believe (although this is an understandable motivation for why some people might engage in research).\nSince scholarship is a rhetorical enterprise, it is appropriate to consider what will convince other people that an idea has merit. If we ourselves are unwilling or reluctant to entertain the idea that we are wrong, then others will be resistant to our ideas.\nAny set of observations is consistent with innumerable theories. So showing that the evidence is consistent with some theory isn’t a very compelling argument. In formal empiricism, researchers follow a different rhetorical strategy: Instead of trying to “prove” your theory, try to make your theory fail. Good research chronicles sustained efforts to refute your own ideas. If you tempt failure, your audience will be more impressed.\nResearch is often motivated by our intuitions, hopes, and (sometimes) secret beliefs. Without these motivations, we wouldn’t have the energy to do all the work involved in research. Even if you have no ulterior motive, other researchers may think that you have an ulterior motive. You will convince your most ardent critic by displaying a readiness to allow your ideas to fail.\nSlogan: The best research invites failure.\n##Testing Predictions\n\n\n\nSo how do we invite failure? If a theory is good, then we should be able to make a prediction about the future. Instead of explaining or interpreting what has been observed, we predict what will be observed. In making a prediction, we stick our neck out. The prediction can fail. An empirical test is a prediction.\nBiblical Adage: You can recognize a false prophet by his/her false prophecies. Empirical Test: You can recognize a false theory by its false predictions.\nAs noted earlier, science is a form of rhetoric — a form of argument or persuasion. Audiences seem most persuaded when someone makes an improbable prediction about the future that is then borne-out. That is, people are impressed by accurate prophecy. The rhetorical power of science comes not from scholars assembling evidence, but from scholars testing predictions.\nSlogan: We invite failure by testing predictions.\n\n\n\n\n\n\nWe’ve already learned that good research invites failure: good research gives the world a voice, and allows the world to tell us that we’re wrong. But how do we know when the accumulated evidence is sufficient to admit defeat? At what point do we recognize failure?\nOne observation may be contrary to our prediction, but is that sufficient to reject the idea? How about one contrary observation out of 100 observations? One out of fifty observations? What proportion of our observations must be contrary to our prediction before we accept that the world is inconsistent with it?\nHuman psychology is the principal obstacle here. Our natural tendency is to want our predictions to work. We might suppose (say) that 15-20 contrary observations would be bad for our idea. So what if we make 16 contrary observations? You might well think that, really, 16 is on the low side of our 15-20 range, so we shouldn’t necessarily discard our idea. In the face of contradicting evidence, research shows that people attempt to discredit the evidence, and rationalize and defend their views (Ariely, 2010; Kahneman, 2012). Psychologically, we’re just not built to easily admit when we’re wrong.\nThe way to overcome our natural disposition to avoid admitting failure is to establish a criterion before we begin our work. We need to draw a line in the sand. We need to say something like “15 or more contrary observations and I’ll admit defeat.” Such a strict line will inevitably seem arbitrary. (“Is 16 really so different from 15?”) In fact, the line IS arbitrary. But if we don’t set some unambiguous criterion then we will leave ourselves open to all sorts of rationalizations and defenses. If we are to invite failure, we must be clear to define what failure means. By drawing a line, we allow the world to force us to recognize defeat.[1] If we don’t draw a line, we won’t simply waffle: we will assume that we are right — but that the evidence is weak. We need to give the world an opportunity to change our minds.\nIn empirical research, this line in the sand is called the confidence level. The confidence level is expressed as a percentage. We might choose a confidence level of 80%, or a more stringent confidence level of (say) 99%. In empirical research the most commonly chosen confidence level is 95%. However, it’s important to recognize that this level is chosen by the researcher. As we will see later, the choice of confidence level is determined by the moral implications of making a mistake. If the consequences of being wrong are especially bad (as in some medical research), then we may want to choose an especially high confidence level (like 99.999%).\nThe “confidence level” is a technical concept that we’ll define in greater detail later. (It’s not simply the percentage of correct predictions.) For now, it is important to understand four things about confidence levels: (1) The confidence level defines when the researcher recognizes defeat. (2) The confidence level is an arbitrary line drawn by the researcher. (3) Since the researcher gets to draw the line, it is essential that the line is drawn before the work begins. (Of course, it is cheating to draw the line after the observations are made!)\nNotice that drawing a line means that we must have some way of determining on which side of the line the observations lie. In order to do this, we have to measure or count things. Because of this, empirical research is often called quantitative research. As we’ll emphasize later, the foremost reason for making measurements is to let the world tell us that our idea is wrong. So the fourth thing to note about confidence levels: (4) Having establishing a confidence level, the researcher must use quantitative measures in order to determine whether the observations satisfy the confidence level criterion.\nOur slogan doesn’t mention confidence level, but that’s what it refers to:\nSlogan: We recognize failure by drawing a line in the sand.\n[1] Actually, even when the research “fails” — researchers will commonly still believe the idea (or variation of it) is right. By establishing a prior criterion, the researcher will at least publicly admit that the prediction failed, even if the researcher is still not convinced.\n\n\n\n\nThe statement “All swans are white” can never be confirmed because you could never be sure that you have observed all swans (David Hume, 1748). However, the statement “All swans are white” can fail by observing a single non-white swan. Refutation is easier than confirmation (Karl Popper, 1934).\nModern science in a nutshell: Tempt failure by trying to show that a set of observations is not consistent with predictions arising from your hypothesis. If this test fails, then you can say that “the observations are consistent with the hypothesis.”\nIn short, our aim is not to be right; instead, our aim is more modest — not to be wrong. Or, expressed as our slogan:\nSlogan: Aim not to be right, but to be not not right.\n\n\n\n\n\n\nIf an abstract idea correctly describes the world, then we should be able to see evidence of the idea in the concrete organization of the world.\nTheories are abstract ideas. They can be tested only by making predictions that have observable consequences. If a theory only predicts things that can’t be observed, then the theory can’t be tested. In order to test a theory, we need to predict things that can be observed. Transforming abstract ideas into concrete observations is called operationalizing.\nFor example, a theory might predict that listening to a certain musical work will make listeners feel embarrassed. But how can we observe embarrassment? Some people blush when embarrassed, but people can feel embarrassed without blushing. Similarly, a person might become redfaced simply because he/she is feeling hot. One simple approach is to ask listeners whether they feel embarrassed. That is, we might operationalize “embarrassment” as “any introspective report in which the listener claims to feel embarrassed.” Notice that operationalizations are imperfect approximations of theoretical concepts.\nSlogan: Test hypotheses by operationalizing terms.\nAll concepts are inherently enigmatic and fuzzy. Terms like “romanticism” or “jazz” can never be pinned down. Moreover, even terms we think of as more basic — such as “guitar” or “melody” or “listen” or “note” — prove elusive. It is impossible to provide comprehensive definitions or grasp the essence of some concept.\nThe belief that there is some true essence of things is referred to as essentialism. The dangers of essentialism are most evident when applied to people (Fuss, 1989). At different times and places, different definitions have been offered as to what it means to be “human,” or “civilized,” or “feminine,” or even “musician.” No list of properties will capture the concept, and the definitions are frequently formulated with self-serving political motiviations.\nIn practical terms, we cannot avoid speaking without using terms in ways that imply some kind of essentialism. That is, we regularly talk about things we ultimately cannot grasp. As we’ve seen, when carrying out empirical research, we must operationalize terms so as to allow us to make clear observations.\nIn empirical research, we are forced to approximate or estimate concepts through operational definitions — but don’t confuse the operational definition with the concept itself. For example, for the purposes of some study we might operationally define a melody to be “a continuous sequence of successive pitches.” No matter how successful the research, do not then turn things around and claim that the definition of melody IS “a continuous sequence of successive pitches.” Concepts will always remain fuzzy and questionable — “contested” (Gallie, 1956). Although research forces use to operationalize concepts, we must not believe that the operationalization has captured the essence or truly defined the concept. In operationalizing concepts, we must take care to avoid essentializing them.\nSlogan: Operationalize, but don’t essentialize.\nBy way of illustration, consider a study contrasting music for xylophone and marimba carried out by Michael Schutz and his colleagues (2008). Organologists (people who study musical instruments) will tell you that there is no clear dividing line between a xylophone and a marimba. Both instruments involve wooden bars struck by mallets. In order to carry out their research, Schutz and his colleagues needed some way to distinguish the repertoires for the “two” instruments. How do you know that a piece of music is truly “marimba music?” Their solution was simply to call “marimba music” any piece listed on the Percussion Arts Society’s website that classified the work as “marimba music” (and similarly for “xylophone music”). In short, they sidestepped the question of what is “truly” the difference between a marimba and a xylophone by relying on an independent source for a useful (though imperfect) distinction. No one should believe that the Percussion Arts Society has some ideal and flawless knowledge that distinguishes between the two instruments. By using the Percussion Arts Society listings, Schutz et al. operationalized the terms marimba and xylophone without essentializing them.\n##Control\n\n\n\nMost people who catch a cold feel better in three or four days. Suppose you catch a cold and take a drug. You feel better in three or four days. Did the drug contribute to your recovery?\nIf you always took this drug whenever you caught a cold, how would you ever know whether it was useful or useless?\nWhen making observations, ask yourself “What would one normally see without this change or intervention?”\nIn order to determine whether a drug helps you recover from a cold, you must compare the effect of taking the drug with what would happen if you didn’t take the drug.\nEach time you get a cold, count the number of days before you recover. Take the drug only every second illness. If the drug is effective for you, then it should, on average, shorten the duration of the colds when you took the drug compared with the colds when you didn’t take the drug. In this research, you will compare the results for the treatment condition against the control condition.\nWe learn things only by comparison. All empirical research involves comparing two or more situations, measurements, or conditions. In your research, ask yourself what you are comparing? If you’re not comparing something, then you need to rethinking your project.\nSlogan: Compare, compare, compare.\n\n\n\nDan Ariely (2010). Predictably Irrational: The Hidden Forces That Shape Our Decisions. New York: Harper Collins.\nPierre Duhem (1906). La Théorie Physique: Son Objet, Sa Structure. Paris: Chevalier & Riviére. Translated as: The Aim and Structure of Physical Theory. Princeton: Princeton University Press. [Duhem was a French physicist. This book is the classic statement that scientists never prove anything, and that in empirical research we never know the Truth. His claim that science never establishes truths was accepted by philosophers of science long before the advent of Postmodernism.]\nDiana Fuss (1989). Essentially Speaking: Feminism, Nature, and Difference. New York: Routledge.\nWalter Gallie (1956). Essentially contested concepts. Proceedings of the Aristotelian Society, Vol. 56, pp. 167-198.\nDavid Hume (1748). An Enquiry Concerning Human Understanding. London: A. Millar.\nDaniel Kahneman (2012). Thinking, Fast and Slow. New York: Farrar, Straus and Giroux.\nKarl Popper (1934). Logik der Forschung. Vienna: Springer. Translated as: The Logic of Scientific Discovery. (1959).\nSchutz, M., Huron, D., Keeton, K. & Loewer, G. (2008). The happy xylophone: Acoustic affordances restrict an emotional palate. Empirical Musicology Review, Vol. 3, No. 3, pp. 126-135."
  },
  {
    "objectID": "emp_methods_workshop/prophecy.html#the-rhetoric-of-science",
    "href": "emp_methods_workshop/prophecy.html#the-rhetoric-of-science",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "The Rhetoric of Science",
    "text": "The Rhetoric of Science\n\n\nThe Rhetoric of Science\nScholarship is a form of rhetoric. The principal products of research are written documents and spoken presentations.\nMost of the books sold in bookstores are classified as fiction. Fiction writers tell us that they make up their stories; the stories they write are products of their imaginations. Other materials are sold under the category non-fiction and this includes scholarship of all sorts, including science. On what basis can anyone make a distinction between fiction and non-fiction?\nIf you pick up a copy of a professional journal, such as the Journal of Experimental Biology, what evidence do you have that the articles it contains are not also works of fiction? How do we know that the authors haven’t simply made it all up? On what basis would one ever be justified in thinking that an article in JEB is more believable or credible than (say) a chapter in the Anthology of Science Fiction and Fantasy?\nAll researchers are story-tellers of a certain sort. All knowledge claims (scientia) are narratives. Science is a form of story-telling, a form of rhetoric. But that doesn’t mean that science doesn’t have special properties as a narrative form.\nSuppose you wanted to tell a really good story. What makes a story truly captivating? Not all stories are equally compelling, and if we aim to be good story tellers, it is helpful to understand what makes some stories really grab the human mind. It is helpful to know something about different forms of rhetoric.\n\n\nProphecy\nIn 1494, Christopher Columbus embarked on his second voyage to the Americas. On May 5th of that year he landed on the island now known as Jamaica. He told the local native Jamaicans that in two days time, the moon would rise up and swallow the sun — plunging the midday into darkness. (In short, he predicted a solar eclipse.) When the eclipse did in fact take place, the native Jamaicans were dumbstruck: they were deeply impressed by Columbus’ apparent prophetic abilities.\nThere are only three way to account for the accuracy of any prediction. Either:\n\nThe person has some sort of direct communication channel to God.\nThe person’s prediction was just darn lucky. Or\nThe person is in possession of a useful theory of the natural or social world that makes it possible to produce such predictions with a high likelihood of success.\n\nThroughout history, the most powerful form of rhetoric has been prophecy. People have been most impressed, when someone made a prediction that was then borne-out. In most of the world’s religions, prophetic texts are among the most revered documents.\nOf course, some predictions are more impressive than others. Suppose I predicted that tomorrow it will rain in Seattle. I doubt anyone would be impressed if it did, in fact, rain (even if the official weather forecast didn’t call for precipitation). The reason why this prediction would be less impressive is that there is a high probability of simply guessing correctly. That is, it is much more likely that my accurate prediction was “just darn lucky” in the case of predicting rain in Seattle compared with Christopher Columbus correctly predicting a solar eclipse.\nSo why might people respect or even revere science more than (say) novels or poetry? The answer lies in the mind’s disposition to be impressed by prophecy. Science provides compelling stories because it relies on the rhetorical power of prophecy. Its persuasive power lies in improbable predictions. Christopher Columbus could have told the Jamaicans a story about how, in the past, he had seen “the moon swallow the sun.” They might have thought this story was interesting and entertaining. But instead, Columbus told them a story about the future — a story that (improbably) came to pass. A future-tense story is much more impressive than a past-tense story. Yes: science is a form of narrative story-telling, but it commands special interest and respect, because it draws on the most powerful of all known rhetorical devices.\nOur second slogan reminds us that science is a form of rhetoric, and that it’s rhetorical strength comes from making predictions:\nSlogan: The rhetoric of science is the rhetoric of prophecy.\n\n\nPrediction vs. Explanation\nA common view is that science is an explanatory discipline; science aims to help us understand phenomena. This is only partially correct; it misses the main point. There are many other human activities that provide explanations for phenomena. History explains past events. Aesthetics explains art. Religion explains the meaning of life. Poetry explains the human heart. In each of these cases, people may draw great satisfaction from the explanations offered. Who is to say which explanations are True in any ultimate sense?\nThe key difference between science and most other intellectual activities is not explanation but prediction. Scientific explanations are used to make predictions; when the predictions derived from these accounts are borne-out, people tend to ascribe greater credence to the originating explanation. It is prediction that is key. Science is a community of scholars who hold each other to a methodological commitment to making and testing predictions.\nWe will never know for certain whether any scientific explanation is True. What makes scientific narratives compelling to human minds is not their truth value, but their prophetic value. When people say that science is just another form of story-telling, the implication is that it is no different than other forms of story-telling. However, some forms of story-telling are more compelling than others. Science is perhaps the most engaging form of story-telling human minds have yet discovered."
  },
  {
    "objectID": "emp_methods_workshop/goals.html",
    "href": "emp_methods_workshop/goals.html",
    "title": "Goals",
    "section": "",
    "text": "Our goals are:\n\nTo communicate the main concepts and techniques in modern empirical research.\nTo provide practical experience in research skills, including posing research questions, formulating and operationalizing hypotheses, designing experiments and questionnaires, and analyzing data.\nTo provide practical research advice.\nTo expose participants to examples of empirical music studies through lectures and readings.\nTo build critical skills when reading and interpreting empirical research studies — identifying both strengths and weaknesses.\nTo stimulate participants’ creative imaginations in posing and pursuing musical questions.\nTo provide sufficient background so that participants will feel confident in beginning their own program of empirical music research.\nTo identify resources for continuing education in empirical musicology.\nTo offer a philosophical framework for understanding how empirical methods can be contextualized within traditional concerns in the arts and humanities."
  },
  {
    "objectID": "emp_methods_workshop/freelist_instructions.html",
    "href": "emp_methods_workshop/freelist_instructions.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Task\nI’m going to ask you to make a list of a certain sort. You will be given just sixty seconds to perform this task. Your aim should be to make the list as long as you can in the short time provided. Wait for verbal instructions before you start.\nVERBAL INSTRUCTION: In the next 60 seconds, I want you to list as many composer’s names as you can. Go.\nAfter the alotted time has elapsed, call for a stop.\nFirst, check to see if you have any duplicated names; if so, scratch out the second occurrence of that name.\nHave participants number the names beginning at the end of the list with the number 1.\nDid anyone get as many as twenty names? (That would be extraordinary.) Who got the most number of names (congratulations to the winner!).\nAsk for a volunteer. Have the assistant tally the total numbers for all names mentioned more than once: E.g., Beethoven: 20+21+13+18+…; etc. Probable:\n\nBeethoven\nMozart\nBach\nHaydn\nSchubert\nBrahms\nMendelssohn\nHandel\nChopin\nLiszt\nMahler\nTchaikovsy\nRavel\nStravinsky\nWagner\nVivaldi\nSchumann\nDebussy"
  },
  {
    "objectID": "emp_methods_workshop/hearing_screening_coding.html",
    "href": "emp_methods_workshop/hearing_screening_coding.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Instructions\nThis screening inventory was developed by Stanley Coren and Ralph Hakstian of the University of British Columbia. The inventory is described in: Corren, S. & Hakstian, A.R. (1992). “The development and cross-validation of a self-report inventory to assess pure-tone threshold hearing sensitivity. Journal of Speech & Hearing Research, Vol. 35, No. 4, pp. 921-928. This inventory is for internal use only in the Cognitive and Systematic Musicology Laboratory.\n\n\nScoring instructions:\nResponses are scored:\n\n1 for “Never,”\n2 for “Seldom,”\n3 for “Occasionally,”\n4 for “Frequently,”\n5 for “Always”\n1 for “good”\n2 for “average”\n3 for “slightly below average”\n4 for “poor”\n5 for “very poor”\n\nThe total score is simply the sum of the 12 responses. (Note: Items 2, 3, 4, 7, and 8 are reverse-scored.)\n\n\nScore Interpretation:\n\nscores from 12 to 27 are considered normal\n28 to 36 indicates slight hearing impairment\n37 to 60 indicates major hearing loss"
  },
  {
    "objectID": "emp_methods_workshop/origins.html",
    "href": "emp_methods_workshop/origins.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Philosophers of science make a distinction between the context of discovery and context of legitimation (e.g. Duhem, 1905).\nTraditionally, we don’t care where an idea comes from: E.g. Kukelé’s discovered the (ring) structure of benzene from a daydream about a snake swallowing it’s tail.\nTraditionally, methodologists focus on the context of legitimation and ignore context of discovery. For traditional methodologists the main focus is on the truth status of some claim, not where the claim comes from.\nIn recent years, social scientists have used the terms exploratory and confirmatory (instead of discovery and legitimation). The aim of exploratory research is to use empirical observations to formulate new theories or hypotheses. The aim of confirmatory research is to test a theory.\nNotice that theories in exploratory research emerge post hoc, whereas in testing-oriented research, the theories are a priori.\n(Huron doesn’t like either of the terms “legitimation” or “confirmatory” since we never “legitimate” or “confirm” an idea. Currently, in social sciences methodology, the terms “exploratory” and “confirmatory” research are popular. The terms “testing” or “prophetic” would be appropriate instead of confirmatory. The terms post hoc and a priori are perhaps clearest.)\nNotice there are an infinite number of theories and hypotheses. Obviously, we can’t test them all. Which should we test first? The order in which we address theories/hypotheses implies some prioritizing.\nIt is entirely legitimate to ask why we should give priority to testing one theory or hypothesis over another. We should ask “whose” theory? “whose” hypothesis?\nWe could well imagine that white English-speaking males have their own preoccupations, and so their research tends to reflect the concerns of white English-speaking males.\nBy way of example, Western medical research is strongly biased toward dealing with diseases endemic in the developed West—such as heart disease, cancer, and diabetes. However, the most common debilitating disease in the world is malaria. A quarter of a billion people are infected with the malaria parasite each year, and the resulting infection is permanent.\nWe need to guard against researcher bias—not simply in how we observe and interpret phenomena—but also in the choice of the phenomena we elect to investigate.\nWhile it is understandable that researchers will tend to focus on issues that touch them personally, there is much to be said for taking a broader perspective.\nSince historically, methodologists regarded the context of legitimation as much more important than the context of discovery, it followed that testing or confirmatory research was held in high esteem and exploratory research was held in low esteem.\nHowever, one of the most important functions of exploratory research is to alert us to new phenomena, different ways of thinking about a phenomenon, and ultimately promising theories and hypotheses.\nNon-quantitative research methods like reconnissance, descriptive, ethnographic fieldwork, etc. offer important tools for reducing egocentric bias related to theories and hypotheses. The medical researcher who casts a wide net is more likely to recognize the importance of malaria—even if the researcher has never encountered a person who suffers from malaria.\nSo here is Huron’s interpretation of the argument between quantitative and qualitative research: We have something to learn from both qualitative and quantitative approaches. As traditionally practised, both approaches have their blind spots.\nQuantitative research:\n\nResearch driven solely by a priori hypothesis testing can blind researchers to novel insights and relationships which await discovery by vigilant observers. Quantitative research doesn’t pay sufficient attention to the creative discovery of new ideas.\nConstant reliance on a strictly quantitative approach can lead researchers to become poor observers.\nRarely recognizes the existence of egocentric bias in the choice of hypotheses or theories to test.\nFinally, things that are difficult to measure are in danger of being ignored, or lead to claims that they don’t exist. (This is known as the positivist fallacy, which we’ll discuss further later).\n\nQualitative research:\n\nTends to ignore hypothesis testing so the conclusions are less reliable.\nDouble-use data is rampant in qualitative research.\nSince most qualitative research doesn’t invite failure, researchers tend to become overconfident (they are never wrong). Because the researcher is never wrong, the researcher tends to place excessive trust in his/her intuitions.\n\nWe need both approaches: exploratory methods to expand our horizons and alert us to ideas or phenomena we haven’t considered, and testing-prophetic methods to keep us honest and humble.\nThere is room for a “division-of-labors” approach, where one group of researchers specialize in exploratory research (generating lots of interesting ideas and theories to be tested), while a second group of researchers engage in solely confirmatory research (where ideas are formally tested). In fact, this division of labors is evident in modern physics. However, it is probably better that researchers are trained and become experienced in both approaches. Each approach helps empower the researcher to do good things.\n\n\nReferences\nScott E. Page (2007). The Difference: How the Power of Diversity Creates Better Groups, Firms, Schools, and Societies. Princeton: Princeton University Press."
  },
  {
    "objectID": "emp_methods_workshop/explore_then_test.html#explore-then-test-approach",
    "href": "emp_methods_workshop/explore_then_test.html#explore-then-test-approach",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "Explore-then-Test Approach",
    "text": "Explore-then-Test Approach\n\nOne of the strongest research strategies combines an exploratory study with a subsequent study that tests a resulting hypothesis. The exploratory study is used to acquaint the researcher with the phenomenon, and to inspire the researcher to formulate an interpretation or explanatory theory. This theory is then used to generate one or more conjectures. One of the conjectures is refined to one or more testable hypothesis, and these hypotheses are then tested using either a correlational method or an experimental method. We can refer to this research strategy as the explore-then-test approach.\nOf course one cannot use the same data both to inspire the hypothesis and to test the hypothesis (i.e., double-use data). A new set of data is required in order for the test to be truly a priori.\n\nThe Reserved Dataset\nSuppose that a researcher has administered an exploratory survey questionnaire to 100 people. Examining the surveys, the researcher observes certain correlations. For example, the researcher might discover that people who enjoying listening to sad music score high on questions measuring the personality trait known as “openness.” Notice that the research employs an exploratory correlational approach, and that the discovery is “post hoc.”\nHaving made this observation, the conscientious researcher would then go ahead and test it. The hypothesis is that “people who enjoying listening to sad music will score high on the openness personality trait.” Our researcher might form a new questionnaire (or even reuse the earlier questionnaire) to collect new data. With this new data, the hypothesis can now be tested as an a priori hypothesis.\nIn many circumstances, returning to collect a second set of data can prove to be inconvenient or costly. For example, an ethnomusicologist may have difficulty obtaining funding to return to a remote location for a second round of data collection. In these situations, a clever strategy might make use of the so-called reserved dataset also known as held-out data.\nSuppose that our researcher had originally collected survey questionnaire data from 200 people (rather than 100). Before examining any of the data, the researcher randomly selects 100 surveys and stores them out-of-reach. (The researcher simply avoids looking at these 100 surveys.) This will be the “reserved dataset” or “held-out data.#148; Examining the remaining surveys, the researcher observes the correlation between enjoyment of sad music and”open” personalities. As before, this is a “post hoc” discovery. It would be nice to test this as a proper a priori hypothesis. In effect, the reserved dataset is equivalent to gathering questionnaire data for 100 new participants. (It’s like collecting the data before we need it.) So the researcher can then test the hypothesis using the held-out dataset.\nNotice that the held-out dataset allows the researcher to carry out an exploratory study (whose purpose is to inspire theories and conjectures), followed by proper a priori hypothesis testing, while collecting data only once. This method is especially valuable when the researcher knows that it will be difficult to arrange a second round of data collection."
  },
  {
    "objectID": "emp_methods_workshop/perttu.html#a-quantiative-study-of-chromaticism---perttu-2007",
    "href": "emp_methods_workshop/perttu.html#a-quantiative-study-of-chromaticism---perttu-2007",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "A Quantiative Study of Chromaticism - Perttu (2007)",
    "text": "A Quantiative Study of Chromaticism - Perttu (2007)\n\nRead Perttu (2007). A quantitative study of chromaticism: Changes observed in historical eras and individual composers. Empirical Musicology Review, Vol. 2, No. 2, pp. 47-54.\nAnswer the following questions:\n\nWhat kinds of studies are these? Exploratory, descriptive, correlational, experimental, pilot, modeling?\nPerttu never formally states any hypotheses, but they are implied. What is the hypothesis for the first study? The second study? The third study?\nFor Study #1, what is the hypothesis?\nFor Study #1, what is the hypothesis?\nFor Study #1, what is the population?\nFor Study #1, what is the sample?\nWhat sampling method(s) does Perttu use for the first two studies? Convenience, simple random, systematic, stratified, quota? What sampling method(s) does Perttu use for the (third) Mozart study?\nWhat exclusionary criterion were used in the sampling?\nIn what ways does the sample differ from the population?\nWhy sample only one note from each work?\nWhat are the theoretical terms in the hypothesis?\nHow are the theoretical terms operationalized?\nWhat two pieces of information were collected for each sampled theme?\nWhat bias is associated with the right-hand margin when sampling from notated scores?\nIs Study #1 an experimental or correlational study?\nIs the conclusion for Study #1 properly phrased?\nIs this grandmother research?\nIf someone were to repeat this study, what changes would you recommend?"
  },
  {
    "objectID": "emp_methods_workshop/ceiling.html#judging-performance-quality-sight-versus-sound",
    "href": "emp_methods_workshop/ceiling.html#judging-performance-quality-sight-versus-sound",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "Judging Performance Quality: Sight versus Sound",
    "text": "Judging Performance Quality: Sight versus Sound\n\nIn 2013, Chia-Jung Tsay published an article entitled “Sight over sound in the judgment of music performance” in the Proceedings of the National Academy of Sciences (Vol. 110, No. 36, pp. 14580-14585). The full article can be accessed at: www.pnas.org/content/110/36/14580.full\nIn general, Tsay carried out a series of seven experiments. For all seven experiments, Tsay made use of brief (six-second) excerpts from performances from 10 major music competitions. The excerpts were randomly selected from full performances by the three finalists in each competition. Thus the stimuli involved 30 different performers. In all seven experiments, participants were asked to judge which of three finalists was the actual winner of the competition. Some of the excerpts consisted of sound only; other excerpts consisted of video only (no sound); yet other excerpts included both sound and video. In experiments 1, 2, and 3, Tsay made use of non-expert listeners. In experiments 4, 5, 6 and 7, Tsay made use of professional musicians who were experienced in judging music competitions. Below are quoted extracts from the published article that summarize the results:\nABSTRACT: “Social judgments are made on the basis of both visual and auditory information, with consequential implications for our decisions. To examine the impact of visual information on expert judgment and its predictive validity for performance outcomes, this set of seven experiments in the domain of music offers a conservative test of the relative influence of vision versus audition. People consistently report that sound is the most important source of information in evaluating performance in music. However, the findings demonstrate that people actually depend primarily on visual information when making judgments about music performance. People reliably select the actual winners of live music competitions based on silent video recordings, but neither musical novices nor professional musicians were able to identify the winners based on sound recordings or recordings with both video and sound. The results highlight our natural, automatic, and nonconscious dependence on visual cues. The dominance of visual information emerges to the degree that it is overweighted relative to auditory information, even when sound is consciously valued as the core domain content.”\n“Experiment 1: Core Beliefs About Music. Suppose that you have the chance to win cash bonuses if you can guess who won a live music competition. You may choose the type of recording you think would give you the best chance at winning the prize. You can select sound recordings, video recordings, or recordings with both video and sound. Which recordings do you choose? In experiment 1, participants were asked to make exactly that decision and bet their study earnings on their choices.”\n“As expected, 58.5% chose the sound recordings, significantly more so than the 14.2% who chose video recordings, X2(df=1, n=77)=28.89, p<0.001. Despite a ‘tax’ levied on selecting the recordings with both video and sound, 27.4% still chose those recordings, a significantly larger proportion than those who chose the video recordings, X2(df=1, n=44)=4.46, p=0.035. People have the intuition that sound is a more revealing channel of information in the domain of music and that recordings with both visual and auditory output offer additional and more relevant information that better approximates the conditions under which the original expert decisions were made.”\nNotice that when selecting which of three finalists is the winner, a chance level of performance would be 33% correct.\n“In experiment 2, novice participants were presented with both video-only and sound-only versions of 6-s clips of the top performances from international competitions. Although 83.3% of participants reported that the sound mattered most for their evaluation of music performance, these same participants were significantly more likely to identify the winners when they were presented with only the visual components of the performances, t(105)=12.07, p<0.001 … (Fig. 1). The … effect held across all 10 competitions, t(9)=4.37, p=0.002. Indeed, with silent video-only recordings, participants were significantly above chance (52.5%), t(105)= 10.90, p<0.001. With sound-only recordings, they were significantly below chance (25.5%) at identifying the winners, t(105)=-5.23, p<0.001.”\n[Experiments 4 and 5 made use of expert musicians who] “had participated in and judged competitions and are familiar with how professional judgment is determined.”\n“In experiment 4, 96.3% of domain-expert participants reported that the sound mattered more for their evaluations, X2(df=1, n=27)=23.15, p<0.001. Despite musicians’ training to use and value sound in their evaluations, only 20.5% of experts identified the winners when they heard sound-only versions of the recordings, t(34)=-6.11, p<0.001. However, 46.6% did so upon viewing silent video clips, t(34)=4.05, p< 0.001. Those with video-only stimuli performed significantly better, compared with those who heard sound-only stimuli, t(34)=5.89, p<0.001 …”\nIn the subsequent experiments, Tsay attempted to identify which visual features are most influential in rendering judgments. He compared simple images of the performer (that would indicate race, sex, age, physical attractiveness) with modified video that obscured these features, but left the motion and physical gestures intact.\n“The importance of dynamic visual information to professional judgment was further established through two supplementary experiments. Although demographic cues such as race and sex have been associated with various capabilities (Bertrand & Mullainathan, 2004; Pager, et al. 2009), such as the quality of musicianship (Goldin & Rouse, 2000)—and although the many advantages of physical attractiveness have been documented (Dion, Berscheid & Walster, 1972), from hiring (Beehr & Gilmore, 1982) to income (Biddle & Hamermest, 1998)—these static visual cues did not significantly impact professional judgment in these competitions.”\n“Passion had considerable impact on the professional judgment of quality when it was visible; through silent videos, those selecting ‘the most passionate contestant’ identified the actual winners at rates significantly higher than chance (59.6%).”\nQuestion for discussion: What do you make of Tsay’s conclusion? I.e., “People consistently report that sound is the most important source of information in evaluating performance in music. However, the findings demonstrate that people actually depend primarily on visual information when making judgments about music performance.”"
  },
  {
    "objectID": "emp_methods_workshop/ethics.html",
    "href": "emp_methods_workshop/ethics.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Research Ethics (Lecture notes)\n\n\nNazi Medical Experiments. Nazi doctors: the only direct measures of blood pressure.\nNuremberg Trials. In the aftermath of the second World War, 23 doctors were put on trial at Nuremberg. Unlike the other Nuremberg trials, only the US was involve in prosecutions.\nNuremberg Code. Ten points. For our purposes, three main points: The first point is that voluntary consent of the human subject is absolutely essential. Second: that physical and mental suffering should be minimized. And third: The degree of risk to be taken should never exceed that determined by the humanitarian importance of the problem to be solved by the experiment.\nTuskegee syphilis experiment. Was conducted in Macon County, Alabama between 1932 and 1972. Was an official U.S. government experiment conducted by the U.S. Public Health Service. Involved 600 African American sharecroppers - 399 had contracted syphilis before the study began, and another 201 were recruited as controls. As a recruitment incentive, the men were provided with free medical care, meals, and free burial insurance. None were told that they had syphilis. By 1940, medical science had well established that penicillin is an effective treatment for syphilis. Nevertheless, the researchers wanted to chronicle the natural progression of the disease. None of the men were ever treated. The experiment stopped in 1972, only because of a whistleblower. Notice that the research was government sponsored, and continued 25 years after the establishment of the Nuremberg Code.\nHelsinki Declaration (1964). Several revisions (the most recent, 2013). Major revision in 1974 — partly instigated by public revelation of the Tuskegee syphilis experiment.\nJapanese Chemical Weapons Experiments in Manchuria. None of the Japanese researchers involved were prosecuted. The U.S. government made a deal with the Japanese: they would not prosecute if the Japanese turned over all of their research results to the U.S. military.\nResearch Fraud. Apart from treatment of human and animal participants, research ethics also embraces other aspects of research conduct. Many cases of research fraud including (1) making up data (or tampering with data) [recent case of Prof. Mark Hauser at MIT - he lost his job]; (2) stealing or failing to acknowledge origin of research ideas; (3) improper authorship attribution [for multi-authored papers, many science journals now require a letter or document describing in detail which author was responsible for which aspect of the research; some journals publish this information as a footnote in the published article.]\nWhat constitutes research with human subjects? The regulations were drawn up by people in medicine, with no consideration for arts and humanities scholarship. E.g., interviewing a person [My experience with Johan Sundberg’s visit to OSU. Applied for exemption 2 weeks before visit; wasn’t processed in time for the interview.] E.g.2: Examining past (non-public) historical records of anyone who is still living.\nEducational Exemption. Educators were savvy and got an exemption. Any research related to curriculum development or educational testing is exempt from any human subjects review.\nThe Arts and Humanities are not impervious to the Dark Side. Charge of plagarism: if unfounded, has a negative impact on a composer’s reputation; if the composer is alive, then the moral consequences are amplified. E.g., the “dark days” of Comparative Musicology - the Great Chain of Being. Music as representative of different stages in human development: hunter/gathers, agrarian/tribal, urban/democratic.\nTake-home Message. It is important to treat people well."
  },
  {
    "objectID": "emp_methods_workshop/types_of_studies.html",
    "href": "emp_methods_workshop/types_of_studies.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "An example of a correlational study is an article by Huron and Ommen (2007) on syncopation in American popular music. We simply selected recordings from the period 1890 to 1939 and counted how often syncopations occur. The study began with no hypothesis to test (although later we did use the collected data to test two hypotheses).\nMore commonly, models are rendered as computer programs. Later, we will examine a computer model of a B-flat valve trumpet/performer interaction (Huron & Berec, 2009)."
  },
  {
    "objectID": "emp_methods_workshop/hypotheses_good_bad.html#hypotheses---good-and-bad",
    "href": "emp_methods_workshop/hypotheses_good_bad.html#hypotheses---good-and-bad",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "Hypotheses - Good and Bad",
    "text": "Hypotheses - Good and Bad\n\n\nDiscovery vs. Justification\nTraditionally, philosophers of science have made a distinction between the context of discovery and the context of justification (Popper, 1935; Reichenbach, 1938). According to this view, scientists shouldn’t care where an idea comes from. The important issue is whether the idea is justifiable or not—that the idea survives experimental testing. A hypothesis might come from a careful theoretical calculation, a religious text, a drug-induced vision, or last night’s dream. It simply doesn’t matter where a theory, conjecture, idea, or hypothesis comes from.\nA classic historical example of the irrelevance of the context of discovery is Friedrich Kekulé’s discovery of the chemical structure of benzene. Throughout the first half of the 19th century, benzene had thwarted chemists’ best efforts to understand its structure. One night, Kekulé had a dream of a snake swallowing its tail. Inspired by his dream, he began a series of experiments that ultimately established that benzene is structured as a circular ring consisting of six carbon atoms. From the scientific perspective, it doesn’t matter that Kekulé got the idea from a dream. He could have be inspired by reading his horoscope—for all scientists care. The important point is that he carried out a series of experiments that ultimately justified the idea.\nTraditionally, empiricists are taught not to care where hypotheses come from. Good research centers on the testing of hypotheses. In conventional philosophy of science, what counts is the context of justification, not the context of discovery.\nIn more recent years, scholars have become aware that one shouldn’t have such a cavalier attitude towards hypotheses.\n\n\nBetter and Worse Hypotheses\nRecall that a hypothesis is a testable conjecture. It is a claim or prediction that can, in principle be compared to existing or future observations. Not all hypotheses are equally good. One might claim, for example, that the moon is made of green cheese. A multi-billion-dollar mission might be carried out, sending a remote vehicle to the moon in order to test this hypothesis. Clearly, this would be a waste of time, effort, and money. Compare the green-cheese hypothesis to one that geophysicists consider more interesting: that the moon was once part of the earth, and that it broke away in a catastrophic collision with another large celestial object. This is a hypothesis that geophysicists regard as more worthwhile.\nSince testing hypotheses is time-consuming and expensive, researchers ought to be picky about which hypotheses they test. Ideally, researchers should focus on the most important hypotheses first. There are at least two issues involved here. First, how do we prioritize some hypotheses over others? Second, how do we encourage the creative process by which researchers come up with good hypotheses?\n\n\nPrioritizing Hypotheses\nConsider first the evaluative question: How do we evaluate the potential worth of different hypotheses? The following list of five evaluative criteria is intended to be suggestive rather than exhaustive:\nPlausible. Good hypotheses should be plausible. There is little point testing hypotheses (like the green-cheese hypothesis) that we a priori consider unlikely.\nBeneficial. Good hypotheses are morally or aesthetically pertinent. Depending on the results of the research, the hypothesis may have repercussions for human health or wellbeing.\nCritical. Good hypotheses help distinguish between two or more competing theories. A so-called “critical experiment” is an experiment that pits two theories directly against each other. If theory A is right, then the results should be +X; if theory B is right, then the results should be –X.\nInteresting. Good hypotheses are compelling in some way. Depending on the results of the research, the hypothesis may be transformative in how people think about themselves and the world.\nFundamental. In many cases, nearly everyone relies on an assumption that has never been tested. Depending on the results of the research, if a commonplace assumption is found to be wrong, this would potentially have major repercussions.\n\n\nCreating New Hypotheses\nSo how might a researcher generate new hypotheses? In particular, how might a researcher generate new hypotheses of the highest value? As with all creative enterprises, discovery is an imperfect art. One can only offer suggestions, such as the following tips:\n\nKeep abrest of contemporary research.\nConverse regularly with other scholars.\nRead widely. Include materials beyond your discipline. Take notes.\nSeek out new personal experiences, such as through travel.\nGet in the habit of thinking cross-culturally and historically. Try to imagine what people in other times and places would want to say to you. Read old literature with an eye to possible forgotten ideas.\nDevelop curiosity about things other people do that you don’t understand. Listen broadly to the experiences of others.\nKeep a research diary. Maintain an intellectual life.\n\nWe’ve left the most important advice for developing good hypotheses for last:\n\nMake use of exploratory research methods.\n\n\n\nReferences\nErnest Nagel and Morris R. Cohen (1934). An Introduction to Logic and Scientific Method. New York: Harcourt & Brace.\nKarl Popper (1935). Logik der Forschung. Vienna: Julius Springer Verlag. Translated as The Logic of Scientific Discovery. London: Hutchinson, 1959.\nHans Reichenbach, (1938). On Probability and Induction, Philosophy of Science, Vol. 5, no. 1: 21-45. Reprinted in S. Sarkar, ed. Logic, Probability and Induction. New York: Garland, 1996."
  },
  {
    "objectID": "emp_methods_workshop/interview_trans.html",
    "href": "emp_methods_workshop/interview_trans.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Interviews in Translation\n\nIn carrying out interview research, we are often trying to cast a wide net so as to include voices that are not always heard. In seeking a diversity of viewpoints, your research is likely to take you to individuals or communities where you don’t speak the language (and they don’t speak your’s). Ideally, you would make the long-term investment to learn the language, but that’s not always practical. Consequently, you may need to rely on a translator to help conduct the interview.\nIn the world of languages translators are people who take a written text and render a parallel text in another language. The word interpreter is used to identify people who render the spoken word into another language. In the case of live interviews, you will make use of an “interpreter” rather than a “translator.”\nVery few people are trained as professional interpreters. (It is something of a grueling occupation that involves especially rigorous training.) In field research, it is much more likely that you will simply find a bilingual individual who speaks both a language you know and the language of the people you want to interview. Unfortunately, these “convenience interpreters” raise all sorts of methodological issues.\nSuppose your goal is to interview Maria, a Quechua-speaking Peruvian indian. You don’t speak Quechua, but fortunately, you have befriended Jorge, who is a native Quechua speaker who also has fair-to-good English skills. When you spend time in the field, you will inevitably form your closest friendships with those people you most easily communicate with. If Jorge is the only Quechua speaker in the village who speaks English, you are sure to rely a great deal on him.\nIn such situations, much of your understanding of the local culture will be filtered through Jorge’s eyes. On the one hand, Jorge is likely to have some very useful insights into his own culture. On the other hand, Jorge is likely to filter what other people say in accordance with what he thinks they mean, or what he thinks they should be saying, or more importantly (even if unconsciously), what he thinks you’re looking for or want to hear. Remember, if Jorge speaks the best English, this means he is also likely to be the person in the village who is most outward looking, and probably the most favorably disposed to Western culture. By definition, this makes him not representative of the people in his village.\nBefore the interview, take time to explain to Jorge what you are doing. Tell him you very much value his views. But when interviewing other people, you expect that they will have different viewpoints that you truly want to hear. Even if Jorge thinks the interviewee is wrongheaded, you want to know — insofar as possible — what the interviewee has said. Tell Jorge that he will be of greatest value to your research if he simply does his best to translate Maria’s Quechua into English without adding or subtracting anything. Also tell him that, after the interview is over, you’d like to hear his particular take on what Maria said. For example, if Jorge has background knowledge that will help make sense of what she said, you’d like to hear that. Similarly, if Jorge thinks she was exaggerating or lying, or not telling you the whole story, you’ll certainly welcome his input.\nWhen actually conducting an interpreted interview eye contact and body language are of paramount importance. As an interviewer, your most important rule is to keep your eyes and attention focused on the interviewee (Maria) — not on the interpreter (Jorge). You want to ensure that Maria is the center of attention. Through your body language, you want to indicate that this is a conversation between you and Maria. With untrained interpreters, it is very easy for the conversation to degrade into long interactions between you and the interpreter, or between the interviewee and the interpreter. If you can, teach your interpreter how to use his or her eyes to ensure that the conversation is between you and the interviewee. A good interpreter should not simply look at whoever is talking. Instead, a good interpreter will cast their eyes back-and-forth between the interviewer and interviewee. This will subtley tell Maria that she is talking to you, not to Jorge.\nWhere possible, always record the conversation. Even if you have no other way of translating the conversation, listening to the recording after the interview is over can alert you to potential problems. Be vigilant for interactions that don’t seem right. For example, pay attention to disparities between the durations of answers and translations. Suppose you ask the question “Are you married?” Maria’s response might be 20 seconds in duration, yet Jorge’s translation might be a brief three seconds: “Yes, she’s married to Simón.” Conversely, if Maria gives a short response, and Jorge’s translation is somewhat long, this should also be cause for suspicion. If you are suspicious, isolate a particular sound bite, play it to Jorge, and ask him what precisely did Maria say here. This exercise will also help Jorge learn how to become a better interpreter. Of course, practice helps a lot. The more experience you have with your interpreter, they better he/she will become.\nGiven the problems with untrained interpreters, you should endeavor to try to access the information through several different methods. One approach is simply to schedule a repeat interview. If there is more than one person with whom you share a common language, you might seek input from another source, or employ a second interpreter. Remain vigilant for other sources of information that might converge or confirm findings in your interviews."
  },
  {
    "objectID": "emp_methods_workshop/anecdotes.html",
    "href": "emp_methods_workshop/anecdotes.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Anecdotal Evidence\n\nIn 1966, Peter Wason published an article showing a peculiar aspect of human reasoning. He devised the following task. Various cards are presented: each card has a number written on one side and a letter written on the other. There is a rule: any card having a vowel on one side has an even number on the other. Now suppose you are looking at four cards with the following visible characters:\nA\nB\n4\n7\nWhich of the four cards should you turn over in order to check whether the rule has been followed? (It’s important to try this task before continuing to read.)\nThe correct answer is: turn over A and 7. You need to turn over A to make sure there is an even number on the back. You need to turn over 7 in order to ensure that it doesn’t have a vowel on the back.\nNow consider the following variant on the task. In this case, the cards now contain different information. On one side of the card is the age of a person at a bar. The other side of the card identifies what it is that the person is drinking. Once again, there is a rule: anyone drinking alcohol must be 21 years of age or older. Your task is to identify which cards to turn over in order to check whether the rule is followed:\nbeer\ncola\n16\n25\nMost people find this problem trivial. You need to check the age of the beer-drinker, and you need to check what the 16-year-old is drinking.\nWhat’s important is that the two problems are logically identical, but people have a much easier time understanding (and correctly solving) the second problem compared with the first. Why?\nIn 1977, Roger Schank and Robert Abelson published an influential book entitled Scripts, Plans, Goals and Understanding. In that book they reviewed research in social psychology showing that human knowledge and understanding are deeply rooted in stories and storytelling. Research persistently shows that people are not interested in numbers. We are interested primarily in other people and in hearing about the experiences of other people. The second version of the Wason card task is easier to solve because the problem is less abstract and more more story-based.\nCharities have learned the importance of story-telling. For example, many charities (Oxfam, Save The Children, CARE, UNICEF, Mercy Corps, etc.) are involved in trying to alleviate hunger and starvation. In their advertising, these charities have learned to avoid abstract appeals. A charity appeal that notes that one million people are currently experiencing starvation in Somalia has surprisingly little effect. Video footage of a single starving child (who is given a name) is far more effective than video footage showing a sea of starving people in refugee camps.\nIt is easier for people to comprehend concrete examples rather than abstract ideas. This lesson also applies to music research. A musical concept is best conveyed by offering a musical example rather than presenting an abstract generalization. Although a music theorist may appreciate a formal statement, most theorists prefer some good musical examples that illustrate the idea. The example will be especially compelling if the specific musical work is well-known and so easily recalled.\n\nStories\nThe research suggests that narratives involving individual people, examples, or events are easier to understand than abstract principles or large numbers. At the same time, human-oriented narratives are more memorable. We remember anecdotes about people better than statistical facts (Schank & Abelson, 1977). When people gather together, they tend to relay stories; these stories include not only first-hand experiences, but also the recounting of stories we have heard from other people.\nPeople find People magazine much more compelling to read than reports from the U.S. Bureau of Labor Statistics. There are compelling human concerns behind labor statistics, but because these numbers are faceless, people don’t find them interesting.\nSchank and Abelson have noted that people’s memories of essential facts are indexed in the brain around stories. If we are presented with a fact that is accompanied by a story, the research shows that we are more likely to remember it. However, when we are presented with facts without a story, that fact enters short-term memory, but typically never makes it into long-term memory.\nOn the pacific island of Tongatapu, I spent considerable time listening to local radio broadcasts. From my field notes, I know that my tallies suggest that the most popular music on the island was reggae. However, I don’t remember any of the specific tallies, or any of the specific recordings I heard on the radio. What I do recall is asking a taxi driver what his favorite music was: He said “reggae.” I recall the vehicle, the driver, and even the specific road on the island where he responded to my question. But I don’t even remember any of the times I spend monitoring the radio or making notes on broadcast statistics.\n\n\nThe Utility of Stories\nThere are two places where anecdotes have a role in research. First, anecdotes are a legitimate part of exploratory research. Anecdotes can provide useful starting points for formulating theories which can then be tested using formal empirical methods. Second, anecdotes provide memorable illustrations or examples — but only when the empirical research supports the general lesson illustrated by the anecdote.\nBecause narratives can be quite memorable, we should always try to make use of narratives in our teaching. Let the research speak, and then cement the lesson by adding an appropriate anecdote. Journalists love to do this. Even when a journalist is reporting research — say, about sleeping — they will typically try to personalize the article by starting with an individual case or a specific anecdote: “For years, Jill Johnson had suffered from chronic sleep problems ….” The above anecdote regarding reggae on Tongatapu is a concrete example.\n\n\nProblems\nThere are at least four problems with anecdotes. First, anecdotes can be mentally too compelling. A good anecdote will trump reasoned argument. Anecdotes are easily recalled when they go against expectations. For example, some people have died in automobile accidents because they got tangled in a seat belt. However, seatbelts save many more lives than they end. One or two ancedotes can have an enormous impact on people’s willingness to wear seatbelts. Unfortunately, ancedotes commonly trump research statistics.\nThe second problem with ancedotes is that they are not systematically or randomly sampled and so are rarely representative. The most successful book ever published on music and the brain is Oliver Sacks’s Musicophilia. Each chapter describes a different case study — a single person — and how brain damage influenced their relationship with music. For example, Sacks describes an unforgetable case of a man who became a passionate performer after being struck by lightning. The story is fascinating, but it is hardly representative. Being struck by lightning rarely changes one’s appreciation for music. Sacks is the first to admit that he is not a researcher or a scholar. He modestly notes that “I’m really just a neurological story-teller.”\nA third problem is that anecdotes tend to attract similar anecdotes. In ordinary conversation, for example, if a person relays a positive anecdote about Sam, the story is likely to evoke further positive anecdotes about Sam. Conversely, if someone says something negative about Sam, the comment is likely to evoke further negative anecdotes about Sam. This tendency for stories to elicit similar stories is known as the bandwagon effect. The bandwagon effect exaggerates a given view and makes it seem more commonplace than is the case — that is, ancedotes tend to amplify sampling bias.\nA fourth problem is that anecdotes are often volunteered rather than elicited. Often, people have an underlying motive or goal in telling the story. The story-teller is likely to have an “agenda” or a particular point-of-view that is being actively promoted by telling one story rather than another.\nIn light of the sampling problems related to anecdotes, haphazard collections of anecdotes should not be used as data for testing a theory or hypothesis. Our slogan reminds us to be wary of anecdotal information:\nSlogan: The plural of anecdote is not data.\nBy way of summary, anecdotes play an important role in exploratory research (the context of discovery): many good theoretical insights arise from the stories people tell about their experiences. Anecdotes also serve as invaluable memory aids for illustrating particular ideas — ideas that are supported by solid empirical evidence. However, anecdotes should play no role in the context of justification. Due to the haphazard manner in which most anecdotes are collected, they are rarely representative.\n\n\nReferences\nOliver Sacks (2007). Musicophilia: Tales of Music and the Brain. New York: Alfred A. Knopf.\nRoger Schank and Robert Abelson (1977). Scripts, Plans, Goals and Understanding. New York: Wiley.\nRoger Schank and Robert Abelson (1995). Knowledge and memory: The real story. In Robert S. Wyer Jr. (ed.), Knowledge and Memory: The Real Story. Hillsdale, NJ: Erlbaum, pp. 1-85.\nPeter Wason (1966). Reasoning. In Foss, B. M. New Horizons in Psychology. Harmondsworth: Penguin."
  },
  {
    "objectID": "emp_methods_workshop/cleverHans.html",
    "href": "emp_methods_workshop/cleverHans.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Clever Hans\n\nHow likely is it that a horse could read German fluently and do advanced mathematics? The story of Clever Hans provides one of the most illuminating lessons of what can go wrong with research. Interestingly, there is a musical connection in this story. The German government hired Carl Stumpf—one of the early famous music psychologists—to lead the commission of inquiry. Despite detailed investigation, Stumpf failed to recognize what was going on.\n \n[From Wikipedia:] “Hans was a horse owned by Wilhelm von Osten, who was a mathematics teacher, an amateur horse trainer, phrenologist, and something of a mystic. Hans was said to have been taught to add, subtract, multiply, divide, work with fractions, tell time, keep track of the calendar, differentiate musical tones, and read, spell, and understand German. Von Osten would ask Hans,”If the eighth day of the month comes on a Tuesday, what is the date of the following Friday?” Hans would answer by tapping his hoof. Questions could be asked both orally, and in written form. Von Osten exhibited Hans throughout Germany, and never charged admission.”\n“Due to the large amount of public interest, the German board of education appointed a commission to investigate von Osten’s scientific claims. The psychologist Carl Stumpf formed a panel of 13 people, known as the Hans Commission. This commission consisted of a veterinarian, a circus manager, a Cavalry officer, a number of school teachers, and the director of the Berlin zoological gardens. This commission concluded in September 1904 that no tricks were involved in Hans’ performance.”\n“The commission’s work was continued by Oskar Pfungst, who tested the basis for these claimed abilities by: Isolating horse and questioner from spectators, so no cues could come from them, using questioners other than the horse’s master. By means of blinders, varying whether the horse could see the questioner Varying whether the questioner knew the answer to the question in advance. Using a substantial number of trials, Pfungst found that the horse could get the correct answer even if von Osten himself did not ask the questions, ruling out the possibility of fraud. However, the horse got the right answer only when the questioner knew what the answer was, and the horse could see the questioner. He observed that when von Osten knew the answers to the questions, Hans got 89 percent of the answers correct, but when von Osten did not know the answers to the questions, Hans only answered six percent of the questions correctly.”\n\nClever Hans Effect: An experimenter may inadvertently provide non-verbal (or verbal) cues as to what they hope will happen. People can be even more tuned-in than a horse in reading body language."
  },
  {
    "objectID": "emp_methods_workshop/correlational_study2.html",
    "href": "emp_methods_workshop/correlational_study2.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "The Correlational Study\n\nCorrelational studies aim to identify linkages or relationships between things. We say two things are correlated when there is some sort of connection or association between them. For example, music with a fast tempo tends to be louder than music with a slow tempo. Although there are exceptions to this, in general, there is a correlation between tempo and dynamic level.\nA correlational study involves collecting at least two different sets of measurements, and determining whether there is any relationship between the two sets. When we suspect that two measures are correlated, we should plot the measures, with one measure on the horizontal axis, and the other measure on the vertical axis. For example, we might plot the average dynamic level against the average tempo. Each point will represent a single musical work.\n\nThe degree of correlation can be expressed using a numerical correlation coefficient. Correlation coefficients range between -1 and +1. A correlation of +1 means a perfect positive correlation: as one measure increases, the corresponding measure also increases. Conversely, a correlation of -1 means a perfect negative correlation: as one measure increases, the corresponding measure decreases.\nAn example of a strong positive correlation would be +.89. An example of a moderate positive correlation would be +.65. An example of a weak positive correlation would be +.21. Similarly, strong, moderate, and weak negative correlations would simply reverse the sign from positive to negative. If there is little or no meaningful relationship between two sets of measures, the correlation will tend to be near zero—or where the correlation coefficient (of whatever value) is deemed not to be statistically significant. (Refer to our later discussion.)\nA common type of correlational study is the survey (although many surveys are descriptive or measurementive rather than correlational). For example, a survey might reveal that people with high incomes are more likely to prefer jazz than country music, or that social conservatives are less likely to enjoy sad music. Once again, correlational studies say nothing about causation: they simply suggest that certain relationships exist.\n\nCorrelational studies may or may not be hypothesis-driven. When there is no a priori hypothesis, the study is said to be an exploratory correlational study.\nCorrelational studies always involve at least two sets of measurements.\nA common type of correlational study is the survey or questionnaire.\n\n\nAn Example\nBoth formal field observations and formal experiments have shown that males tend to prefer slower dance tempos than females. What accounts for this sex-related difference in preference? One possibility is that males and females prefer different styles of music. For example, women are more likely to prefer (generally fast) dance music, whereas men are more likely to prefer (slower) rock music. However, the difference in tempo preference is evident even when women and men are allowed to “tune” their own preferred dance tempo using a drum machine programmed to produce a simple back beat rhythm of alternating bass and snare drums.\nSofia Dahl and David Huron (2007) considered an alternative hypothesis that dance tempo might be correlated with body morphology. Dancing is essentially stylized bouncing, and like any oscillating system, the optimum rate of bouncing might be expected to depend on physical aspects of the moving body—such as height and weight.\nAccordingly, Dahl and Huron had participants tune a drum machine to their preferred dance tempo, and then afterwards made a series of morphological measures, including the person’s height and weight. The graph below shows a scatterplot for 30 participants. Each point plots the preferred dance tempo (in beats per minute) against the average length of the persons’s two legs (in centimeters). As can be seen, there is a general trend downward: people with longer legs tend to prefer slower tempos. The correlation coefficient is a moderate -.67.\n\nBody weight was found to produce a somewhat weaker correlation of -.38. That is, heavier people tend to prefer slower dance tempos.\n\n\nReference\nSofia Dahl & David Huron (2007). The influence of body morphology on preferred dance tempos. In: Proceedings of the International Computer Music Conference. Copenhagen, Denmark, Vol. 2, pp. 1-4.\nDahl, S., Huron, D., Brod, G., & Altenmüller, E. (2014). Preferred dance tempo: Does sex or body morphology influence how we groove? Journal of New Music Research, pp. 1-10."
  },
  {
    "objectID": "emp_methods_workshop/index.html",
    "href": "emp_methods_workshop/index.html",
    "title": "Methods in Empirical Music Research A Workshop for Music Scholars",
    "section": "",
    "text": "Welcome & Introduction\n\n(Introductory comments)\nGenerals Aims & Preview\nWorkshop flier\nLearning objectives\n(Comments following learning objectives)\n\nEmpirical Research\n\nTypes of knowledge (powerpoint presentation #1)\nSeven big ideas\n\nMotivated by truth, with no hope of proof.\nThe best research invites failure.\nWe invite failure by testing predictions.\n\nA line in the sand\n\nWe recognize failure by drawing a line in the sand.\n\nRefutation is easier than confirmation\n\nAim not to be right, but to be not not right.\n\nOperationalizing\n\nTest hypotheses by operationalizing terms.\nOperationalize, but don’t essentialize.\n\nComparison\n\nCompare, compare, compare.\n\nThe rhetoric of science (video - 8 minutes)\n\nThe rhetoric of science is the rhetoric of prophecy. “Science is a narrative activity, conducted by a community of scholars who hold each other to a methodological commitment to making and testing predictions.”\n\nReview the first 9 slogans: Quiz #1\nGroup Task #1: What’s worth knowing? An audience with God\nQuestions, conjectures, hypotheses and theories\nGroup Task #2: Question, theory or hypothesis? (Answers)\nGrandmother research\nThe quantitative/measurement obsession\nGroup Task #3a: Obvious theories - Part 1\nGroup Task #3b: Obvious theories - Part 2\n(Group Task #3 Debriefing) Obvious theories: Hindsight bias\n\nHindsight is 20/20.\n\nGrandmother research revisited\nTwo forms of reductionism\n\nIn research, reductionism is a method, not a belief.\nDon’t try to explain the whole world at once.\n\nEpistephobia\nTypes of failure (powerpoint presentation #3)\nReview (first 12) slogans\n\nTypes of Empirical Studies\n\nTypes of empirical studies (powerpoint presentation #5)\nGroup Task #4: Types of studies (Answers/Discussion)\nGeneralizing versus universalizing (video - 10 minutes)\n\nGeneralize, but don’t universalize.\n\nExploratory studies\nReactivity (powerpoint presentation #7)\nMeasurement studies\nHypothesislessness\n\nAvoid chronic hypothesislessness.\n\nGroup Task #5: Operationalize the following hypotheses\nSyncopation: From question to hypothesis (powerpoint presentation #8)\nOpinions as operationalizations\nDouble use data\n\nBeware of the post hoc theory.\n\n(Exploratory & confirmatory: Contexts of discovery and legitimation - lecture)\nExplore-then-test Approach\nExplore-then-test example - Part 1 (What kind of study is this? What should happen next?)\nExplore-then-test example - Part 2 (Why make use of a robot rather than a human?)\nFrom question … to protocol.\n\nFrom question to theory to conjecture to hypothesis to protocol\n\nReview (first 16) slogans\n\nHomework\n\nHomework 1: Reading Guide #1: Lancashire & Hirst (2009)\nReading #1: Lancashire & Hirst (2009)\nHomework 2: Individual Task #6: From question to theory"
  },
  {
    "objectID": "emp_methods_workshop/multiple_tests.html",
    "href": "emp_methods_workshop/multiple_tests.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Multiple Tests, File Drawer Effect, & Positive Results Bias\n\n\nMultiple Tests\nIn empirical research it’s very common for researchers to use the 95% confidence level—that is, where the significance level (alpha) is .05. If our statistical test produces a p value of .05 or lower, then we can reject the null hypothesis and conclude that the results are consistent with our research hypothesis.\nNotice that a significance level of .05 means that the researcher accepts a 1 in 20 chance of making a Type I error—of claiming something to be true or useful or knowable, when it is in fact false, or useless or unknowable. Now suppose you are browsing through a printed issue of a scholarly journal. The journal contains 20 empirical research articles. Each article reports positive results in testing a single hypothesis. Moreover, suppose that each article reports a p value of .05, and that all of the researchers used a 95% confidence level.\nThere are twenty significance results published in this issue. Since each of the 20 researchers accepted a 1 in 20 chance of making a Type I error, and since there are 20 statistical tests, on average (just by chance), we would expect one of the articles in the issue to contain spurious results. Just because a study is published, doesn’t make it believable!\nNow suppose you were reading a single published article. The article reports statistical tests for 20 hypotheses: the researcher reports that 4 of the hypotheses are statistically significant at the 95% confidence level. Once again, on average, we would expect 1 in 20 tests to reach statistical significance, merely by chance. In other words, the likelihood is that 1 of the 4 positive results reported in the article is spurious.\nNow suppose that I run an experiment, whose results turn out to be negative. I know that negative results might arise for all sorts of reasons. Perhaps the reason why it didn’t work is because of poor equipment calibration. I carefully re-calibrate the machine and repeat the experiment. Still, it doesn’t work. The participants report that they find my experiment very boring, and I begin to suspect that my data are less than ideal because the participants are not fully motivated. I decide to cut the number of stimuli in half and compensate for the loss of data by doubling the number of participants. Once again, I re-run the experiment, and once again I end up with negative results. At this point I am suspicious about the quality of my participants. I would like to use better musicians, so I administer a preselection test that allows me to gather data from the subjects who are the most musical. Once again the results turn out to be negative.\nWith much hard work over several months, I continue to refine my experiment, removing sources of error, aiming for greater reliability in responses, and so on. Finally, after the 20th attempt, I get positive results at the .05 significance level. Hurray! My hard work has finally paid off!\nOf course, repeating any experiment 20 times means that, on average, you are likely to get a spurious positive result if your confidence level is 95%. My efforts to “improve” the quality of my experiment may, or may not, have done anything. Just running the experiment multiple times increases the likelihood of producing a spurious positive result.\nIn all of the above cases, we encounter a problem known as multiple tests: each time you carry out a test, you increase the likelihood of generating a spurious result.\nMultiple tests may arise simply by repeating an experiment, or by running several versions of an experiment, or by “tinkering” with various criteria (such as excluding outlier data points) when analyzing your data.\n\n\nControlling Multiple Tests\nThere are four ways to control for multiple tests:\n\nlimiting the number of tests\nincreasing the confidence level\ncorrecting for the number of tests, and\nconverging evidence\n\nThe first way control for multiple tests is to limit the number of tests you perform. Resist the temptation to test many hypotheses. Focus on the most important hypothesis. In general, one should regard one’s data as a finite resource rather than an infinite resource. A data set is much like a battery or a basket of food: each time you use it, you effectively “consume” some of it. A single data set can effectively “wear out” due to multiple tests.\nA second approach for dealing with multiple tests is to increase the confidence level. If you choose the 99% confidence level, then, on average, only 1 of 100 tests will produce a spurious result. If you predefine you significance level at .001, then, on average, only 1 of 1,000 tests will be spurious. Notice that if you raise the confidence level, then you will probably have to collect much more data—or you will only find positive results for phenomena that exhibit very large effect sizes.\nA third approach allows you to carry out more than one test without having to collect additional data and without having to change the confidence level. In this case, you carry out a mathematical correction for multiple tests. There are several correction methods but the simplest to use is known as the Bonferroni correction. Suppose we want to carry out five tests. Moreover, we want each of the five tests to be done using the 95% confidence level. For a single test, we would look for p values of .05 or less. Suppose that our five hypotheses produce p values of (1) .05, (2) .005, (3) .2, (4) .10, and (5) .01. Without correcting for multiple tests we might conclude that hypotheses 1, 2, and 5 can be accepted. Since we have carried out five tests, the Bonferroni correction would require that we multiply each of the p values by 5. This results in the following “corrected” values: (1) .25, (2) .025, (3) 1.0 (4) .50, and (5) .05. Having corrected for multiple tests, we would conclude that only hypotheses 2 and 5 could be accepted at the 95% confidence level.\nIn summary, the Bonferroni correction involves multiplying each of the p values by the number of tests you perform. If you perform 10 tests, then multiply your p values by 10 before you determine whether they are smaller than your a priori α level.\nThe fourth approach to dealing with multiple tests is to collect additional data, preferrably using data from independent research. In short, we aim for converging evidence through replication studies. (Refer to the course document on converging evidence.)\nOur slogan simply reminds us to be vigilant for the problem of multiple test, and if present, to correct for the problem:\nSlogan: Correct for multiple tests.\n\n\nInvisible Multiple Tests\nMultiple tests are not always immediately apparent. Sometimes we need to be vigilant to recognize situations where multiple tests are occurring.\nResearch reporting positive results is 2-3 times more likely to get published than research reporting negative results. Moreover, research indicates that researchers are less likely to submit negative reports for publication. This phenomenon is referred to as positive results bias.\nSuppose that there is a popular theory—let’s call it Theory A. Researchers in the field think Theory A has a lot of merit: it is conceptually beautiful and makes intuitive sense. A young scholar named Alice is interested in Theory A. She does a literature search and is surprised to find that no experiment has been published testing Theory A. Alice decides to carry out a pertinent experiment. She designs and carries out an experiment, but is disappointed when she gets negative results. She wonders about her design. There are a number of improvements that she might make, but she is discouraged from continuing. Alice abandons the project, takes her data and places it in the bottom drawer of her filing cabinet.\nAnother scholar, Bill, has been thinking about Theory A. Bill’s dissertation research had assumed that Theory A was true and he’s always been a little uneasy about this assumption. Everyone thinks Theory A is uncontroversial, but Bill’s literature search fails to find any experiment testing Theory A. Bill decides to carry out an experiment with the hope that he can find evidence consistent with Theory A. He designs and carries out an experiment, but is disappointed when he gets negative results. He submits a manuscript to a professional journal, but the reviewers are skeptical. Theory A is conceptually beautiful and makes intuitive sense. There are several ways in which Bill’s experimental method could be improved. The editor of the journal requests a “revise and resubmit.” Bill makes some changes to his method and runs the experiment again. Still, he gets negative results. He knows the journal reviewers will be unhappy that he didn’t implement all of the (sometimes impractical) changes they suggested. Discouraged, he places his manuscript in the bottom drawer of his filing cabinet and goes on to other projects.\nAlice and Bill are not alone. Over the years, several scholars attempt to test Theory A without success. There is a good reason why all of these experiments end in failure: Theory A is, in fact, wrong. All of the negative results are telling us something, but because of a positive results bias, the results are hidden from view. This is the file drawer effect.\nWhile the file drawer effect is pernicious, the situation can sometimes get even worse. Consider how the story might continue …\nMany years later, Zack decides to test Theory A. Zack is not aware of the many other tests that have been done that all ended in failure. Zack carries out the experiment and is thrilled to get positive results at the .05 significance level. Zack immediately submits the results to a prestigious journal. The journal reviewers are delighted to see a study that “confirms” a widely loved Theory A, and the journal Editor is thrilled to publish an article what will surely become frequently cited. There is only one problem. In reality, Theory A is wrong, and Zack has had the misfortune of getting spurious results. Repeat an experiment enough times, and one is bound to get spurious statistically significant results. Dozens of previous experiments resulted in negative results. Zack’s experiment is really part of a pattern of multiple testing: the p value in Zack’s experiment ought to be corrected for the many earlier tests of Theory A. But Zack (and everyone else) is unaware of all of the other tests that have been carried out by other researchers. The file drawer effect has become a version of the multiple tests problem—one that is aggrevated by positive results bias.\nIn the bad old days, pharmaceutical companies would sometimes engage in clandestine multiple tests. When a company develops a drug, they need to carry out clinical trials in order to establish whether or not the drug is safe and effective. The data they collect is then sent to the Food and Drug Administration who will approve (or deny) the sale of the drug. So-called “Phase 1” trials are carried out simply to determine whether the drug is safe and doesn’t have onerous side-effects. There is no incentive for drug companies to cheat on Phase 1 trials because an unsafe drug will ultimately lead to expensive lawsuits. The temptation to cheat happens in later phases, where the aim is to establish whether the drug is effective—that is, whether the drug has any beneficial effects. If the drug is safe, but ineffective, the company can still make considerable profit because people want to believe it is useful. In several historical cases, the pharmaceutical company would commission multiple clinical trials. For example, perhaps 10 clinical trials would be started. After the trials were completed, however, the pharmacetical company would inform the FDA about the results of only 3 or 4 trials. They would simply fail to report results suggesting that the drug was ineffective.\nThese sorts of deceptions are now impossible because the various regulatory agencies (like the European Medicines Agency and the U.S. Food and Drug Administration) do not accept any clinical data unless the clinical trial is first registered. That is, pharmacetical researchers must report their intention to start a clinical trial before they start. The various drug and medicines agencies follow up each experiment in order to determine whether the results are negative or positive.\nSometimes multiple tests are not recognized by a researcher. Suppose, for example, you have a set of sound stimuli in which you are interested in the effect of a certain manipulation on perceived emotional content. You ask listeners to judge a number of different affective states: happy, ecstatic, sad, grief, tender, aggressive, dramatic, relaxing, inspiring, contemplative, etc. If you test each affect separately, then you will have to correct for multiple tests.\nInexperienced researchers will sometimes go on a “fishing expedition” in which they test many different hypotheses against the data they have collected. For example, the researcher may test 30 or 40 hypotheses. However, only two or three hypotheses produce significant results. They then craft a paper “testing” these two or three hypotheses without ever mentioning that they tested a large number of other hypotheses. We might call these unreported tests.\nIncidentally, it is okay for researchers to engage in such “fishing expeditions” if the activity is reported as exploratory research rather than hypothesis testing research. For example, you might carry out 30 tests on a set of data, and find two (seemingly) statistically significant results. Given the number of tests carried out, there is a good chance that either one or both of these results are spurious. However, you might be sufficiently intrigued to carry out a separate study whose purpose is explicitly to test one or both of these hypotheses. In other words, we can “mine” a data set for new ideas, that are then independently tested (i.e., explore-then-test).\n\n\nCommunity Fears\nConsider the hypothetical town of Kleinburg (population 2,000). Kleinburg is a happy place until one day, an administrator in the local hospital notices that there seems to have been an unusually large number of cases of liver cancer. Looking up statistics at the National Institutes for Health, the administator is shocked to discover that Kleinburg has an incidence of liver cancer eight times higher than the national average and has the highest rate in the state. Soon the whole town is alive with gossip and suspicion. Attention soon gravitates to the natural gas plant on the edge of town.\nThere may be reason for concern. Or there may not be. In any room full of people, someone has to be the tallest person. Someone will have the longest hair, the darkest eyes, and the biggest feet. Similarly, in any country, by definition there will be some town or municipality that has the highest rate of heart disease. Another town will have the highest rate of gall stones, and another will have the highest incidence of ingrown toe-nails. Different places will have the highest rate of insurance fraud, bank imbezzlement, copper wire theft, color blindness, and jaywalking. There are thousands of bad things that can happen, and the fact that one town is the worst of many is to be expected. What then, do we make of the fact that the incidence of liver cancer in Kleinburg is eight times the national average?\nIn a group of ten people, one may well find a person who weighs three times as much as the lightest person. But if we expand the sample to 1,000 people, then one is likely to find a person who weighs eight times as much as the lightest person. There are 2,155 towns and cities in Germany, with many thousands more small villages and hamlets. Simply by natural chance variation, one may expect to find a municipality that has a liver cancer rate that is eight times the national average. The probability of this increases as the size of the town gets smaller. In a hamlet consisting of 50 people, a single case of liver cancer represents a stunning 2% rate.\nIn effect, statistics for towns and villages amount to multiple tests. With enough places in a country, natural variation is bound to result in some (seemingly) highly suspicious coincidences. The greater the number of observations, the greater the likelihood of finding something that might imply that something truly bad is going on. If we know that something has a probability of only occuring once in 10,000, then that something has probably happened in one of the 10,000 villages in Germany.\nSo what are we to make of the liver cancer incidence in Kleinburg? Statisticians know to look at other factors. If the natural gas plant is releasing carcinogens, then one would expect to see other cancers, not just liver cancers. What is the incidence of stomach cancer, bone cancer, leukemia, and other cancers in Kleinburg? If the incidence of other cancers are similarly elevated then there is a greater likelihood that something bad is indeed going on. However, if the incidence of liver cancer is the only elevated observation, then it is mostly likely a simple consequence of a small sample of people in a large number of villages.\nIt rarely occurs to people that some town will contain people with bigger feet than any other town in the country. The reasons for these differences may be quite uninteresting. Big numbers increase the likelihood of observing something bizarre. Small numbers increase the likelihood of observing large differences between samples.\n\n\nReferences\nN.B. For a musical example of the file drawer effect, see Keith Mashinter (2006). Calculating sensory dissonance: Some discrepancies arising from the models of Kameoka & Kuriyagawa, and Hutchinson & Knopoff. Empirical Musicology Review, Vol. 1, No. 2, pp. 65-84."
  },
  {
    "objectID": "emp_methods_workshop/huron-slogans.html",
    "href": "emp_methods_workshop/huron-slogans.html",
    "title": "Huron’s Research Slogans",
    "section": "",
    "text": "SOME SUPPLEMENTARY SLOGANS:\n\nSplurge to converge.\nSeek converging evidence. The most compelling theories are ones that have been tested in a variety of ways. Pursue alternative ways of operationalizing the terms of a hypothesis. Test different musical samples and different groups of participants.\nBe skeptical, but not nihilistic.\nBe skeptical, but don’t let your skepticism paralyze you from doing research.\nOver-deliver and Under-conclude.\nAim for modesty and competence: over-deliver on evidence while understating your conclusions. Offer more evidence than your readers think necessary, and express your conclusions in a more tentative or circumspect manner than your readers expect.\nTriumphalism is unscholarly.\nWhat most displeases critics of empirical research is the smug, gloating, self-satisfied, triumphalism of science. Aim for modesty, even as you make your points. Remind critics that most of the unguarded and triumphalism language is the product of journalists, populists, and other non-researchers. The strength of empirical methodology is precisely that its practitioners choose methods that enforce intellectual humility.\nCollaborate.\nMusicians understand the value of group effort. The same thing holds with research. You can’t be a known-it-all, and you shouldn’t pretend to be. Seek out one or more collaborators — people who have similar interests, and (hopefully) different skills. You will learn more from working with someone else than from some book. Make sure you bring something to the table — something that makes it attractive for other people to work with you.\nIt’s never too late to quit.\nSome problems are just plain hard. Some projects drag on far too long. Patience, dedication and stamina can be admirable qualities, but they can also stand in the way of using your time productively."
  },
  {
    "objectID": "emp_methods_workshop/qual_quant.html",
    "href": "emp_methods_workshop/qual_quant.html",
    "title": "Qualitative AND Quantitative",
    "section": "",
    "text": "At this point it should be clear that both qualitative and quantitative research methods hold advantages and disadvantages.\nQualitative (and exploratory methods generally) are essential to good research. Especially when your investigations take you into new or unfamiliar territory (such as spending time in an unfamiliar culture) qualitative methods have much to offer. In new circumstances we need to be vigilant for things we haven’t considered. We need to be open, allowing the new environment or phenomenon to shape our thinking. Although our perceptions are inevitably limited by our existing preconceptions, we still need to make an effort to give primacy to “grounded” observations. The good scholar is the acute observer. A singular focus on quantitative a priori hypothesis testing might be wrong-headed in many situations.\nParticipant-observation, structured and unstructured interviews, grounded theory, phenomenological introspection, and other qualitative methods can lead us to important insights. These insights may even reshape the research questions. Although we may have started with a particular pre-existing agenda, we need to be prepared that the situation might render that agenda irrelevant or a low priority. A volunteer might enter a particular community with the aim of (say) enhancing prenatal health care. But the situation might cause a realignment of priorities: one might discover that domestic violence is a more pressing problem, and might be the principal impediment to improving prenatal care.\nAt the same time, qualitative research methods have their limitations. When theories are formed after the evidence is assembled, there is the danger of double-use data. When asked what evidence do you have in support of a theory, it’s not legimate to point to the same observations you used to form the theory in the first place.\nMore problematic is that an exclusive reliance on qualitative or exploratory methods means that theories are never tested. When we rely exclusively on qualitative methods, we are never inviting failure. When we are never shown to be wrong, the inevitable consequence is that we become over-confident in ourselves. By contrast, the whole quantitative enterprise is oriented to minimizing Type I errors—the errors where we thing our wrong theories are right.\nFinally, qualitative research tends to rely excessively on case studies or anecdotal evidence. As we have seen, there is a rhetorical and educational value to anecdotes, cases, or examples. However, whenever people give examples, we tend to gravitate to our BEST examples, not our most representative or TYPICAL examples. Examples, cases, or anecdotes hold a rhetorical power that exceeds their informativeness.\nAs we noted earlier, quantitative methods are important for two reasons. First, measuring things can help us discover phenomena that might otherwise be invisible. Second, measuring or counting can make it clear when we are wrong. Counting and measuring are useful tools, like a magnifying glass or a stethoscope.\n\n\n\nMuch of the rhetorical friction between qualitative and quantitative methods arise from issues of identity. Methods should be regarded as tools, not badges of identity. A person should not think of him/herself as either a qualitative or quantitative researcher. Nor should people identify themselves as a “empirical” researcher. Resist the temptation to link your identity to a method. Instead, think of yourself as a scholar who is conscientious about choosing the most appropriate method for whatever project you are pursuing. The person who identifies with a particular method is probably someone who is only familiar with one method. As the saying goes: “For the person with a hammer, all the world looks forever like a nail.”\n\n\n\nDavid Huron (1999). The 1999 Ernest Bloch Lectures. Lecture 3. Methodology: The New Empiricism: Systematic Musicology in a Postmodern Age."
  },
  {
    "objectID": "emp_methods_workshop/epistephobia.html",
    "href": "emp_methods_workshop/epistephobia.html",
    "title": "Epistephobia",
    "section": "",
    "text": "“… he that increaseth knowledge, increaseth sorrow.” - Ecclesiastes 1:18\n\n\n\nAttempts to account for sublime experiences of music will inevitably be disappointing because they trivialize the profound. For example, biologists tell us that “love” is nature’s way of promoting pair-bonding and procreation. This may be biologically true, but it captures none of the beauty, power, and deep meaning we experience through love.\nIt is perfectly reasonable for a person to say “I don’t want to know about that.” “I don’t want to know how music works.”\nHowever, curiosity is also a deeply human characteristic. It is perfectly reasonable for a person to say “I want to know about that.” “I want to know how music works.”\nA common complaint is that science contributes to the “demystification” of the world. That is, knowledge makes the world less rich, less compelling, less magical, or less beautiful.\nHowever, consider the case of modern astronomy. In ancient times, people looked up into the sky and saw all sort of mythical objects and creatures. They thought that the movement of the planets affected human behavior on earth. Modern astronomy seems to have discarded these colorful and rich ideas. But modern astronomy replaced the stories of the heavens with a different set of stories: of enormous objects at unfathomable distances, of quasars and galaxies, of black holes and big bangs. Can we really say that the stories provided by modern astronomy are less compelling, magical, or beautiful than Medieval beliefs about the heavens?\nAnother point of view comes from the world of theatrical magic. The famed broadway magician, Doug Henning, suggested that only the magician gets to see the true magic. Theatrical magic involves ordinary manipulation of physical objects, but done in a way that leaves viewers astonished. For the magician, there is nothing mystical about the physical objects or their manipulation: the real mystery resides in the fact that viewers can be astonished. Similarly, music is just sound — the physical jiggling of air molecules. What is astonishing is not the sound, but what it evokes in listeners. Part of understanding the magic of music, is realizing the extraordinarily pedistrian nature of sound. Like the magician, stripping away the mystification may be the first step in allowing us to see the true magic of music.\nFinally, if you are worried that knowledge might somehow destroy your love of music, you may find consolation in the idea that nearly all theories are wrong. Even with the best research efforts, musical probably doesn’t work the way we think it works.\nIf you are truly afraid that knowledge might spoil your experience of music, then don’t engage in music research."
  },
  {
    "objectID": "emp_methods_workshop/theory_hypothesis.html",
    "href": "emp_methods_workshop/theory_hypothesis.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Identify whether the following statements are questions, theories, conjectures, hypotheses or none.\n\nWhy do performers tend to slow down at the ends of phrases?\nBeethoven’s metronome markings are too fast because his metronome was broken.\nMuch of the popularity of World Music is due to commercial exploitation.\nHow is it that listening to music can sometimes cause shivers to run up-and-down your spine?\nThe language we use shapes the way we think.\nBrahms uses a lot of hemiolas in his music.\nBrahms liked hemiolas.\nAfricans have a better sense of rhythm than Europeans.\nThe music of Carl Nielsen echoes the spirit of the Danish people.\n“Musick has charms to sooth the savage breast.”\nRaag Shree sounds sad to experienced Indian listeners.\nThe music of the Lakota has been influential primarily because of frequent portrayals of Plains Indians in Hollywood films.\nThe purpose of our research is to study the relationship between music and ritual."
  },
  {
    "objectID": "emp_methods_workshop/feedback1.html",
    "href": "emp_methods_workshop/feedback1.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Feedback - Day 1\n\n\nFirst, tell us about the speed or pacing of the workshop:\n\ntoo slow\na bit slow\nabout right\na bit fast\ntoo fast\n\nSecond, tell us about the level of difficulty. Overall, I find the workshop material:\n\ntoo easy\na bit easy\nabout right\na bit challenging\nsomewhat challenging\ntoo hard\n\nHow useful was each group activity? (Circle 1, 2 or 3 stars — where three stars is best.)\n\n★ ★ ★ What’s worth knowing? An audience with God.\n★ ★ ★ Classifying statements as questions, theories, conjectures or hypotheses.\n★ ★ ★ Hindsight bias - interpreting results from the American Soldier study\n★ ★ ★ Types of empirical studies - Descriptive, Correlational, Experimental, etc.\n★ ★ ★ Operationalizing: e.g. Women like to dance more than men.\n\nOverall, I found the group activities:\n\nof limited use\nokay\nuseful\nvery useful\n\nTell us the most important thing you learned today.\nOf all the material we covered today, the least useful topic was:\nGeneral suggestions (on reverse):"
  },
  {
    "objectID": "emp_methods_workshop/reactivity.html",
    "href": "emp_methods_workshop/reactivity.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Reactivity\n\nThere is a quirky and charming Norwegian film from 2003 entitled Kitchen Stories. The story follows an employee of the fictitious Swedish Home Research Institute. The researchers are interested in designing more efficient kitchens. Having studied Swedish housewives, the researchers are now studying Norwegian bachelor farmers. Before dawn, the researcher (Forke) takes up his observation position, perched on a high chair in the corner of the kitchen. Over the course of the day, he records each time his observee (Isak) enters the kitchen, and makes detailed notes of each action — such as reaching for the salt shaker. Forke and Isak are under strict instructions not to converse or otherwise interact with each other. As you can imagine, the film’s dry humor derives from the ridiculousness of the situation.\n\nAn important question in any observation is how the presence of the observer changes the behavior of the observed. This is the problem of reactivity. A situation is said to have high reactivity when the observer has a major influence on the behavior of the observed. In the film Kitchen Stories, Isak begins eating his meals outside of the kitchen so that he is no longer under constant surveillance by Forke.\n\nAn example of high reactivity in music research can be found in the work of ethnomusicologist Marc Perlman. In the process of learning Indonesian music, Perlman continually pestered his master teachers with “why” questions. His teachers were quite aware of the Western tradition of “music theorizing.” They were also aware that many traditional Indonesian musical practices were done with little in the way of theoretical justification or explanation. Possibly under the influence of Western ideas, Perlman’s teachers also viewed “theory” as a desireable and prestigious intellectual enterprise. Over the decades, Perlman observed and chronicled the emergence of a “music theory” among a handful of respected musicians in Indonesia. Moreover, Perlman was aware that the development of these explanatory frameworks was, at least in part, a response to the very questions posed by ethnomusicologists (including himself) about the meaning, reasons, or explanations for various Indonesian practices. The sorts of questions asked by ethnomusicologists are likely to have been a formative impetus for the development of Indonesian music theory. In his award-winning book, Perlman (2004) documents the emergence of music theorizing in Indonesia.\n\nHawthorne Effect\nReactivity effects can be remarkably subtle yet important. A classic example is the famous Hawthorne Effect. The Hawthorne Effect is named after a manufacturing plant owned by Western Electric. In the 1920s and 1930s Western Electric was interested in the effect of workplace changes on productivity. Over the course of five years, they tried many changes: improving the lighting, rearranging furniture or equipment, improving cleanliness, etc. Each time, they observed an increased productivity (which fell back to normal after a month or so). As a control, they also changed the workplaces back to their original conditions — reducing the lighting, reducing cleanliness, replacing the furniture, etc. With each change, however, they observed the same increase in productivity for a short period. That is, reducing the lighting was as good as improving the lighting in terms of productivity increase. It turned out that the workers were simply reacting to the fact that they were being observed by a seemingly interested, even sympathetic, management. The workers were reacting principally to the attention of their bosses, rather than reacting to the specific changes made in their workplaces.\n\n\nHistorical Reactivity\nReactivity can also be present over long stretches of history. A young historican, for example, might spend time carefully examining the diary of a recently deceased composer. Surely, we might think, the observations made by a modern historian have no effect on what a dead composer wrote! Unfortunately, things are not so simple. Diaries are often written with a future audience in mind. When a living composer today writes in a journal or diary, he or she may very well have one eye on the future and how future scholars may view his or her actions, thoughts and emotions. Diaries are not documents that simply report facts. Diaries are shaped, at least in part, by how the writer wants to be remembered. Even when writing nominally private notes, a person may still imagine the presence of an unseen observer — looking over his or her shoulder.\nThis does not mean that the effects of reactivity cannot be minimized. A historian might find important clues in shopping lists, tax returns, wills, and other documents that a historical figure might never have imagined would be examined. At the moment, people are largely unaware of the volume of personal data being collected about them from their activities on the Internet. In the future, it is quite likely that these records will become available to historians. When this happens, public figures are apt to become far more circumspect about their behaviors on the Internet. That is, their behaviors will be shaped, at least in part, by how they want to be portrayed. A common reactivity problem in interviews and surveys arises when an interviewee is especially eager to be perceived as helpful.\nIn a later discussion, we’ll consider reactivity problems in more detail. We’ll consider eight techniques for minimizing or controlling their effects.\n\n\n\nReferences\nMarc Perlman (2004). Unplayed Melodies: Javanese Gamelan and the Genesis of Music Theory. Berkeley: University of California Press."
  },
  {
    "objectID": "emp_methods_workshop/glossary.html#glossary-of-terms-for-asl-interpreters",
    "href": "emp_methods_workshop/glossary.html#glossary-of-terms-for-asl-interpreters",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "Glossary of Terms for ASL Interpreters",
    "text": "Glossary of Terms for ASL Interpreters\n\nDAY 1\ntypes of knowledge: empirical knowledge (knowledge from observation), inductive knowledge (knowledge from experience), deductive knowledge (knowledge from logic or rational thought), intuitive knowledge\na line in the sand\nrefutation, confirmation\n“not-not-right” (as opposed to “right” or “wrong”)\noperationalization (an estimate measurement)\noperationalize (creating an estimate measurement)\nessentialize (the mistake of treating an operational definition as though it is real)\ncontrol (in experiments, the group that doesn’t receive any treatment)\nmicrofinance\ndistinguish: question, conjecture, hypothesis, theory\ngrandmother research (research whose result is obvious)\nreductionism (2 types: philosophical reductionism, methodological reductionism)\nepistephobia (fear of knowledge)\nseven types of empirical studies: reconnaissance study, descriptive study, measurement study, correlational study, experimental study, meta-study (a study of studies), and modeling study\nexploratory studies\nhypothesislessness (the state of being without a hypothesis)\ndouble-use data (data that is used both to inspire the hypothesis and then as evidence in support of the hypothesis)\nconfirmatory studies (studies that test a hypothesis)\ncontext of discovery (how we came across some knowledge)\ncontext of legitimation (how we defend or rationalize some knowledge claim)\nexplore-then-test approach\nDAY 2\nmultiple operationalizations\nformal observation\nexperiment\ncorrelation; correlational study\ncausation\nthird variable problem\nsampling\nWEIRD sampling (Western, Educated, Industrialized, Rich, Democratic)\ndata independence\nbiased sampling\nrandom\nsample size\nlaw of big numbers; law of small numbers\neffect size\nregression-to-the-mean (the tendency for extreme values to be followed by moderate values)\nbehavioral data\ndependent measure\nindependent measure\nimplicit responses\nexplicit responses\nreactivity\ndemand characteristics (when an experiment becomes invalid because of what participants believe is the purpose of the experiment, right or wrong)\nClever Hans (the name of a smart German horse)\ndebriefing\nmeasurement scales (four types:) Nominal scale, Ordinal scale, Interval scale, Ratio scale, (We use the acronym “NOIR” as a reminder of the four types.)\nquestionnaire\nDAY 3\nmeasurement\nFermi question (names after physicist Enrico Fermi)\ndescriptive statistics (statistics used to describe some phenomenon)\ninferential statistics (statistics used to test some hypothesis)\ncentral tendency\nmean, median, mode (mean=average, median=middle value, mode=most common value)\nvariance\nprobability\nnull hypothesis\nconfidence level (as distinct from:) confidence interval\nsignificance level\nstatistical test\nstatistical significance\nChi-square test\ncritical value\nPearson’s r\nspurioius correlation\np\nmultiple tests (when many statistical tests are conducted - a bad idea)\nBonferroni correction (a way of correcting for multiple tests)\nfile drawer effect (when research results remain unpublished)\npositive results bias (tendency for journals to only publish positive results)\nresearch registry (a public online registry where research declare their intentions and methods in advance of conducting the research)\napplied statistics\npopulation density\nregression analysis\ncluster analysis\nmulti-dimensional scaling (abbreviated MDS)\nmodeling (\nB-flat trumpet\nFoote Novelty (a method for characterizing novelty)\nDAY 4\ntest-retest reliability\nintersubjective reliability (a measure of the degree to which research participants behave in a similar way)\nbetween-subject (a research design in which different participants received different treatments)\nwithin-subject (a research design in which individual participants receive all of the different treatments)\noutliers (extreme data values)\nexperimental design (how an experiment is set up)\nexperimental paradigm (different types of experiments)\nqualitative methods\nparticipant-observation\ninterviews\nexploratory open interview\nGrounded Theory\nanecdotal evidence\nfreelisting (task involving making lists)\npile sorting (task involving sorting items into groups or categories)\nqualitative research, quantitative research\nfloor effect (happens when a task is too difficult),\nceiling effect (happens when a task is too easy)\ndistractor task (intended to reduce the likelihood that a research participant figures out the purpose of an experiment)\nprimacy (first thing encountered)\nrecency (last thing encountered)\ncitations (references)\nquotations\nreviewer (someone who reviews submitted journal manuscripts)\npre-reviewing (when journals review submitted manuscripts whose research results are withheld from the reviewers; journals decide to accept or reject manuscripts solely on the basic of methodology, and without knowledge of the results)\nreserved data set (portion of data that is set aside and not examined)\nconverging evidence (when different methods all produce the same results)\nreplication crisis (recent recognition that many classic experimental results are wrong)\nInstitutional Review Boards (IRB)\nsubject pool (captive group of people used in experiments)\nresearch assistant\ncollaboration\nSome Musical Terms:\nsyncopation (a disrupted rhythm)\ntempo (musical speed from fast to slow)\norchestration (the choice of concurrent combinations of various instruments)\nClassical period (European art music from roughly 1730-1820)\nRomantic period (European art music from roughly 1780-1910)\ndynamics (in musical: the variability of loudness from quiet (pianissimo) to loud (fortissimo))"
  },
  {
    "objectID": "emp_methods_workshop/question_to_theory.html",
    "href": "emp_methods_workshop/question_to_theory.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Task\nIn your group, discuss the questions and theories developed by individual members in the previous exercise (“From Question to Theory”). Choose one of the theories and develop two or three conjectures: If the theory were correct, what might you expect to observe or predict? Choose one of your conjectures and write it as a single sentence.\nIf there is time, refine your conjecture into a hypothesis. Once again, consider two or three possibilities, before deciding on a single one. Write your hypothesis as a single sentence, in the form of a claim.\nIf there is still time, operationalize the terms in your hypothesis so you produce a protocol.\n\n\nRationale\nThis exercise provides practice in linking high-level theorizing to concrete empirical research procedures."
  },
  {
    "objectID": "emp_methods_workshop/questionnaire_task.html",
    "href": "emp_methods_workshop/questionnaire_task.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Task\nDesign a questionnaire whose purpose is intended to test the following hypothesis:\nH1. People who enjoy risk-taking are more likely to prefer loud music.\nOperationalize the terms, and create a series of questions you think might help test the hypothesis.\n\n\nSample\nHow might you distribute your questionnaire?\n\n\nRationale\nThis task applies several lessons in operationalizing terms, minimizing demand characteristics, and designing questionnaires."
  },
  {
    "objectID": "emp_methods_workshop/slogans.html",
    "href": "emp_methods_workshop/slogans.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Huron’s Research Slogans\n\n\nMotivated by truth, with no hope of Proof\nThere is no inductive proof. We are not in the business of proving something to be true. We would love to know the truth (if that exists), but we understand that we could never be sure of the truth, even if we had it. The best we can hope for is that what we observe is consistent with our theories.\nThe best research invites failure.\nGive the world an opportunity to tell you that you’re wrong. (This is the essence of good research.)\nWe invite failure by testing predictions.\nTest an idea by making a prediction, and then determine whether the observations are consistent with the prediction.\nWe recognize failure by drawing a line in the sand.\nIn order to make failure obvious, establish a criterion in advance that says, “If the evidence doesn’t cross this line, then I’ll admit failure.” In statistics, the line is referred to as the confidence level.\nAim not to be right, but to be not not right.\nInstead of establishing The Truth, our more modest aim is to be not obviously wrong. When our observations turn out to be consistent with our hypothesis, we don’t claim that we are right; instead the observations suggest that our hypothesis may not be wrong.\nTest hypotheses by operationalizing terms.\nTranslate all of the terms in a hypothesis into concrete things you can observe or measure. We can’t directly measure concepts like “sadness.” We have no choice but to measure things using imperfect rulers.\nOperationalize, but don’t essentialize.\nAll concepts are inherently enigmatic and fuzzy. Terms like “melody,” “listen” or “note” can never be pinned-down. It is impossible to provide comprehensive definitions or grasp the essence of some concept. We are forced to approximate or estimate concepts through operational definitions — but don’t confuse the operational definition with the concept itself, and don’t assume that all concepts are “real.”\nCompare, compare, compare.\nContrast a “treatment” condition with one or more “control” conditions.\nThe rhetoric of science is the rhetoric of prophecy.\nPeople are most impressed when someone accurately foretells the future. Science is a form of rhetoric whose persuasive power resides in the testing of predictions. The rhetorical power of science comes not from scholars assembling evidence, but from scholars testing predictions.\nHindsight is 20/20.\nMost things seem obvious in retrospect (hindsight bias). When the results aren’t obvious, humans are enormously gifted at coming up with explanatory accounts. We can make up a story for just about any set of data. Post hoc theories don’t have the same plausibility as a priori theories. The true test is making up the story first (i.e., prediction)! Prefer theorizing first, then collect (or examine) your data.\nReductionism is a method, not a belief.\nWe simplify problems, not because we believe problems to be simple, but because we believe problems to be complex. Restricting our gaze is a useful strategy for discovery.\nDon’t try to explain the whole world at once.\nManipulate one variable at a time. Seek simplicity, even as you distrust it.\nGeneralize, but don’t universalize.\nWhen presenting your results, frame them narrowly rather than broadly.\nAvoid chronic hypothesislessness.\nExploratory and descriptive studies are important, but you can’t invite failure without testing predictions.\nBeware of the post hoc theory.\nThe scholar who only offers theories after looking at the evidence is a scholar who is never wrong. Post hoc theorists don’t allow the world to tell them when their ideas are problematic.\nFrom Question to Theory to Conjecture to Hypothesis to Protocol.\nStart with a question, propose an explanatory theory, derive a conjecture, refine the conjecture into a hypothesis, then operationalize the terms of the hypothesis into a protocol. The protocol provides an action plan for how to carry out the research.\nNo causation without manipulation.\nCausality cannot be inferred unless you manipulate one of the variables. The Experiment is the only type of study in which it is possible to infer causality. Correlational studies don’t allow us to discount the possibility of a “3rd variable.”\nDon’t get stuck with sticky data.\nSeek data independence. Ideally, each piece of data should be gathered from a different source. (Collecting independent data is another way of minimizing the effect of unknown third variables.)\nThe law of large numbers does not apply to small numbers.\nPay attention to sample sizes. The smaller the sample size, the greater the variability.\nAlways debrief.\nListen carefully to what people say about their experiences. Look for ways in which participants are misunderstanding the instructions. Be vigilant for possible demand characteristics — where the participant forms an idea about the experiment that confounds the results.\nMake friends with a statistician.\nBefore you collect any data, talk with a statistician. Describe what you are planning to do and listen carefully to the advice. Statistical consultants are thrilled when people come and talk with them before the data are collected. Take advantage of their expertise.\nCorrect for multiple tests.\nEach statistical test increases the likelihood of making a Type I (false positive) error. Repeating an experiment also increases the probability of a Type I error.\nIf you torture the data long enough it will confess to anything.\nIt’s not true that “statistics can prove anything,” but it is possible to manipulate data in ways that deceive yourself and others. Create a data analysis plan. Do not exclude outliers, normalize data, set conditions for excluding participants, or introduce post hoc tests without some principled prior reasoning. Be honest in reporting the analyses you carry out. Don’t hide multiple tests.\nThe plural of anecdote is not data.\nAnecdotes are not randomly sampled and so are not representative. Anecdotes are easily recalled when they go against expectations. For example, some people have died in automobile accidents because they got tangled in a seat belt — even though seat belts save many more lives than they end. Be wary of anecdotal or informal evidence.\nRandomize stimuli for each participant.\nMinimize the effect of primacy (the first thing heard) and recency (the last thing heard) by providing a different order for each participant.\nWrite the paper first.\nBegin by imagining what story you hope to tell. Writing down your story (in detail) will help you see what work is needed in order to make the story compelling. Most importantly, writing a detailed story first better invites failure.\nNurture your passion.\nYour most precious resource in doing research is your own passion. Have fun. Don’t squander your enthusiasm — feed your passion.\nMake time to think.\nIf you are always engaged in “busy work” then you are not a scholar. The essence of the contemplative life is leisure. Schedule time to do nothing but think. Chronicle your thoughts using a research diary. Busy work makes you tired, but thinking makes you smarter.\nChoose projects from a long menu.\nDon’t start with the first project you think of. Make a list of at least 10 projects before you begin. From your list, choose the project that looks to be the most exciting (nurture your passion), the most likely to produce positive results, and the one that appears to be the quickest to do.\nGrow your projects.\nSince research can fail, it is better to fail with small projects than with big projects. Big failures will dampen your passion. Start with small projects, and then pursue bigger projects as you gain experience and confidence.\nKnowledge is communal.\nScholarship is a group activity. Read. Learn from the experiences of others. Ask questions. Read. Don’t hoard the fruits of your own labors. Share your own experiences. Read.\nThere are no competitors, only collaborators.\nResist the temptation to view other researchers as competitors. We all benefit from each others’ efforts. Treat everyone as a teacher from whom you can learn, and as a student you can nurture.\nDon’t be afraid to add your own slogans.\nIn doing research, we learn not just about the topic of interest, but we learn better ways of doing research. Be prepared to make mistakes. Keep abrest of the latest literature on methodology, and be viligant for insights you may be able to contribute.\n\nSOME SUPPLEMENTARY SLOGANS:\n\nSplurge to converge.\nSeek converging evidence. The most compelling theories are ones that have been tested in a variety of ways. Pursue alternative ways of operationalizing the terms of a hypothesis. Test different musical samples and different groups of participants.\nBe skeptical, but not nihilistic.\nBe skeptical, but don’t let your skepticism paralyze you from doing research.\nOver-deliver and Under-conclude.\nAim for modesty and competence: over-deliver on evidence while understating your conclusions. Offer more evidence than your readers think necessary, and express your conclusions in a more tentative or circumspect manner than your readers expect.\nTriumphalism is unscholarly.\nWhat most displeases critics of empirical research is the smug, gloating, self-satisfied, triumphalism of science. Aim for modesty, even as you make your points. Remind critics that most of the unguarded and triumphalism language is the product of journalists, populists, and other non-researchers. The strength of empirical methodology is precisely that its practitioners choose methods that enforce intellectual humility.\nCollaborate.\nMusicians understand the value of group effort. The same thing holds with research. You can’t be a known-it-all, and you shouldn’t pretend to be. Seek out one or more collaborators — people who have similar interests, and (hopefully) different skills. You will learn more from working with someone else than from some book. Make sure you bring something to the table — something that makes it attractive for other people to work with you.\nIt’s never too late to quit.\nSome problems are just plain hard. Some projects drag on far too long. Patience, dedication and stamina can be admirable qualities, but they can also stand in the way of using your time productively."
  },
  {
    "objectID": "emp_methods_workshop/significance.html",
    "href": "emp_methods_workshop/significance.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Statistical Significance\n\nThe word “significant” has a special meaning in statistics. It does not mean that something is important. Instead, it means that the observed results are unlikely to have occurred by chance. The word “unlikely” is important: “Statistically significant” doesn’t mean mean the results didn’t occur by chance. It just means that the results are unlikely to have occurred by chance.\nSuppose we have a coin. We suspect that the coin might be biased — it is possible that it has two “heads” or two “tails.” How many coin tosses would we need to observe in order to test our suspicion? Before we answer this question, we must draw our line in the sand. We need to do this a priori — before we make any observations. We draw the line by setting the confidence level. The confidence level can be any value, but two are conventionally used in empirical research — the 95% confidence level, and the 99% confidence level. Suppose we choose the 95% confidence level. This level corresponds to a significance level of 0.05. That is, we would achieve the 95% confidence level if the probability of our observations falls below 0.05.\nNow we state our hypothesis:\nH1. This coin is not fair.\nOur null hypothesis reverses the formulation:\nH0. This coin is fair.\nWe want to observe a series of coin tosses, and test whether the observed outcomes are consistent with the hypothesis that the coin is fair.\nWe begin with our first observation. We toss the coin once and it comes up heads. The probability of this result is 0.5. Now we toss the coin a second time, and again it comes up heads. Once again, the probability of this result is 0.5. However, the probability of two consecutive tosses coming up heads is 0.5 × 0.5 = 0.25. We toss the coin a third time, and again it comes up heads. The probability of three consecutive heads is 0.5 × 0.5 × 0.5 = 0.125. When the fourth toss comes up heads, the probability is now 0.5 × 0.5 × 0.5 × 0.5 = 0.062. Finally, a fifth toss comes up heads. The probability of five consecutive heads is 0.031. Notice that this probability is lower than 0.05.\nAt this point, the observations have reached the 95% confidence level. We can now conclude that the observations are not consistent with the hypothesis that the coin is fair. (Notice that we haven’t proved the coin is unfair.) More precisely, we can claim that\nThe observations are not consistent with the hypothesis that the coin is fair at the 95% confidence level.\nWe now have reason to discard the null hypothesis.\nHaving discarded the null hypothesis, we can now infer that the results are, instead, consistent with the original research hypothesis. We say that\nThe observations are consistent with the hypothesis that the coin is unfair at the 95% confidence level.\nWe can say that the results are statistically significant.\n\nNotes\n\nStatistical significance is mathematically related to, but not the same as, the confidence level. A confidence level of 95% correspondents to a significance level of 0.05. A confidence level of 99% correspondents to a significance level of 0.01.\nStatistical significance is represented by the lower-case Greek letter alpha (α). When we set the confidence level (and therefore the significance level), it is common to say we have “set alpha to …”\nA result is said to be statistically significant when the probability of the observations for the null hypothesis is lower than the significance level.\nUnlike everyday speech, the word “significant” in statistics is not the same as importance. Just because a result is statistically “significant” doesn’t mean that it is important.\nRecall that the confidence level is a way of “drawing a line in the sand.” The purpose is to make it clear when our idea has failed. In statistical tests, statisticians don’t pay attention to how close the results are to the line. Our sole criterion is “which side of the line” the observations fall. A set of observations that exhibit a p value of 0.00001, is not “more significant” than 0.01. However, if we set an a priori significance level of 0.00001, then positive results would mean we have achieved a higher confidence level. We’ll have more to say about this later."
  },
  {
    "objectID": "emp_methods_workshop/index2016.html",
    "href": "emp_methods_workshop/index2016.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Materials Copyright © 2016 by David Huron\n\nMethods in Empirical Music Research A Workshop for Music Scholars\nADVICE: From past experience, DAY 4 and DAY 5 are now the weakest. The highlight of DAY 4 is “Writing The Paper First.”\nApart from the breaks, it may be useful to have a 5th-inning stretch from time-to-time.\nDAY 3 (Statistics) is now in pretty good shape — although it is essential to review the material thoroughly before teaching. With regard to statistics, there are four tasks awaiting. First, I need to add a powerpoint/handout on measuring variance. Second, I need to add z-scores/standard scores. This can be illustrated by my work on the interval “preference” in J.S. Bach. Third, I really ought to have a section on measuring effect size. Measuring Cohen’s d is very straightfoward. There is a very nice website with an excellent dynamic demonstration at: http://rpsychologist.com/d3/cohend/ Moreover, a segment on measuring variance would set the stage for measuring Cohen’s d. Fourth, I need to complete my handout on “Some advanced statistics (Introduction).”\nDAY 5: Running subjects can be excessively slow if there are a lot of participants. Next time, consider running subjects in the Ethno Lab and 504 so things move more quickly. On DAY 5, it is appropriate to have lunch with the participants. On DAY 5, one of the students brought cookies (home-baked snacks). I should do that on DAY 5 — was well appreciated on the final day. On DAY 5, I stayed until 6PM to converse with participants. People really appreciated that.\n\nDAY 1\n\n\n[Title page]\n[Table of contents]\n\n\nWelcome & Introduction\n\nIntroductory lecture\n\nGenerals Aims & Preview\n\nWorkshop flier\nCourse Syllabus\nLearning objectives\n\nEmpirical Research\n\nTypes of knowledge (powerpoint presentation #1)\nSeven big ideas\n\nMotivated by truth, with no hope of proof.\nThe best research invites failure.\nWe invite failure by testing predictions.\n\nA line in the sand\n\nWe recognize failure by drawing a line in the sand.\n\nRefutation is easier than confirmation\n\nAim not to be right, but to be not not right.\n\nOperationalizing\n\nTest hypotheses by operationalizing terms.\nOperationalize, but don’t essentialize.\n\nComparison\n\nCompare, compare, compare.\n\nThe rhetoric of science (video - 8 minutes)\n\nThe rhetoric of science is the rhetoric of prophecy. “Science is a narrative activity, conducted by a community of scholars who hold each other to a methodological commitment to making and testing predictions.”\n\nReview the first 9 slogans: Quiz #1\nGroup Task #1: What’s worth knowing? An audience with God\nQuestions, conjectures, hypotheses and theories\nGroup Task #2: Question, theory or hypothesis? (Answers)\nGrandmother research\nThe quantitative/measurement obsession\nGroup Task #3a: Obvious theories - Part 1\nGroup Task #3b: Obvious theories - Part 2\n(Group Task #3 Debriefing) Obvious theories: Hindsight bias\n\nHindsight is 20/20.\n\nGrandmother research revisited\nTwo forms of reductionism\n\nIn research, reductionism is a method, not a belief.\nDon’t try to explain the whole world at once.\n\nEpistephobia\nTypes of failure (powerpoint presentation #3)\nReview (first 12) slogans.\n\nTypes of Empirical Studies\n\nTypes of empirical studies (powerpoint presentation #5)\nGroup Task #4: Types of studies (Answers)\nGeneralizing versus universalizing (video - 10 minutes)\n\nGeneralize, but don’t universalize.\n\nExploratory studies\nReactivity (powerpoint presentation #7)\nMeasurement studies\nHypothesislessness\n\nAvoid chronic hypothesislessness.\n\nGroup Task #5: Operationalize the following hypotheses\nSyncopation: From question to hypothesis (powerpoint presentation #8)\nOpinions as operationalizations\nDouble use data\n\nBeware of the post hoc theory.\n\nExploratory & Confirmatory: Contexts of discovery and legitimation (lecture)\nExplore-then-test Approach\nExplore-then-test example - Part 1 (What kind of study is this? What should happen next?)\nExplore-then-test example - Part 2 (Why make use of a robot rather than a human?)\nFrom question … to protocol.\n\nFrom question to theory to conjecture to hypothesis to protocol\n\nReview of (16) slogans\n\nHomework\n\nHomework 1: Reading Guide #1: Lancashire & Hirst (2009)\nReading #1: Lancashire & Hirst (2009)\nHomework 2: Individual Task #6: From question to theory"
  },
  {
    "objectID": "emp_methods_workshop/quiz2_answers.html",
    "href": "emp_methods_workshop/quiz2_answers.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Quiz #2: Understanding the Research Slogans (ANSWERS)\n\nWrite your answers here:\n\n\n\n1-J\n\n\n2-G\n\n\n3-F\n\n\n4-O\n\n\n5-P\n\n\n6-A\n\n\n7-B\n\n\n8-I\n\n\n9-E\n\n\n10-H\n\n\n11-K\n\n\n12-M\n\n\n13-L\n\n\n14-C\n\n\n15-D\n\n\n16-N"
  },
  {
    "objectID": "emp_methods_workshop/romanticism.html",
    "href": "emp_methods_workshop/romanticism.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "From Classicism to Romanticism\n\nA more sophisticated use of clustering is found in a study by Katelyn Horn and David Huron (2012). The motivation for this study came from earlier studies that implied that the minor mode is used differently in the 19th century compared with other centuries. As you might expect, music in the minor mode is typically associated with a slower tempo and with a quieter dynamic level. However, in the 19th century that association reverses. On average, in the 19th century, music in the minor mode is faster and louder than music in the major mode (Post & Huron, 2009). In light of this work, Horn and Huron decided to investigate how music changes from the 18th century and the 19th century. Specifically, they focused on the period between 1750 and 1900. This period corresponds to the presumed shift from the so-called Classicism of the 18th century to the Romanticism of the 19th century.\nHorn and Huron assembled a stratefied random sample containing 750 musical works. They sampled 250 works from each of three 50-year periods: 1750-1800, 1800-1850, and 1850 to 1900. In order to increase data independence, they sampled no more than two works written by any given composer.\nIn order to avoid sampling from just the beginnings of works, they devised a procedure that allowed a sample to be selected from throughout the work. Each work or movement was conceptually divided into “sections.” The beginning of a section was operationally defined as either (1) the beginning of the work, (2) immediately following a mid-work double barline, (3) immediately following a repeat sign, or (4) immediately following a new tempo marking or a change of meter. Having split a work or movement into a sequence of “sections,” they then randomly selected one section from each work. Using this sampling method, they avoided biasing the sample to how music begins.\nEach randomly-selected section was coded according to five properties: mode, dynamic level, tempo, articulation, and date. With regard to mode, they categorized each sampled section as either “obviously major,” “obviously minor,” or “not obviously major or minor.” With regard to dynamic level, passages were coded according to the notated dynamic markings — such as pianissimo, mezzo piano, or triple forte. These markings were treated as an ordinal scale ranging from ppp, pp, p, mp, … to fff. The figure below shows that the most common dynamic marking for the 750 works is piano (p).\n\nWith regard to tempo, they made use of standard Italian tempo terms. Specifically, they made use of 19 common Italian terms that were independently ordered from slow to fast. The figure below shows the percentage of occurrence for the different tempo terms found in the sample of 750 works. The most common dynamic marking is allegro.\n\nWith regard to articulation, passages were coded according to the prevailing texture in the first 4-8 measures of the sampled section. This was a subjective evaluation on the part of the researchers. Passages were coded as one of five possibilities: very staccato, generally staccato, balanced or unclear, generally legato, or very legato. The figure below shows the percentage of occurrence for the different articulations. In general, legato is more common than staccato.\n\nHaving coded the sampled sections for their mode, dynamic, tempo, and articulation, several cluster analyses were carried out. First, all of the data from all 750 passages spanning 1750-1900 were analysed. The resulting dendrogram is shown below.\n\nThe short lines at the bottom of the dendrogram are the “leaves” — each line representing one of the 750 sampled passages. The numbers along the bottom identify specific passages (labelled by number).\nRecall that cluster analysis does not tell you the “meaning” or origin of the groups. It simply tells you that there are statistically pertinent natural groupings that arise depending on the way you define similarity. All of the labels on the figure were added by the researchers. They represent efforts to interpret the meaning of each cluster and sub-cluster. For example, at the highest level, the tree exhibits two broad clusters. By examining the individual passages identified in each of the two clusters it is possible to interpret the meaning of this split. Roughly 98 percent of the musical passages in the right-most group were coded in the major mode (with 2 percent coded as ambiguous modality), whereas roughly 90 percent of the passages in the left-most group were coded in the minor mode (with 10 percent coded as ambiguous). In other words, the top-level clusters appear to represent the distinction between major- and minor-mode passages. Below this, both the major and minor clusters split into two broad categories. Once again, an examination of the individual passages suggests that the subdivisions represent the distinction between loud and quiet passages.\nAll of the clusters, of course, arise from combinations of the four musical features: major/minor mode, loud/quiet dynamic, fast/slow tempo, and staccato/legato articulation. By examining the individual musical passages, it is possible to identify the characteristics of each cluster. For example, the left-most cluster arises from the combination of loud, fast, staccato, and major-mode music. The cluster immediately to the right represents the combination of quiet, fast, staccato, and major-mode music. And so on.\nAs in the case of Johnson’s analysis of orchestral clusters into Standard, Power, and Color instruments, we are free to attempt to interpret the clusters here. For example, Horn and Huron applied the label “Joyful” to the major-loud-fast-staccato passages, and the label “Light/Effervescent” to the major-quiet-fast-staccato passages. It is important to recognize that these labels are post-hoc interpretive labels. The clusters have some objective basis in the data, but the labels are subjective impositions. Another researcher might disagree with these characterizations or offer alternative ways of interpreting the clusters. Nevertheless, the interpretive labels are useful ways of referring to the groups arising from the cluster analysis.\nIn interpreting the results, Horn and Huron made the following interpretations:\n\n\n\n“Joyful”\nmajor\nloud\nfast\nstaccato\n\n\n“Light/Effervescent”\nmajor\nquiet\nfast\nstaccato\n\n\n“Regal”\nmajor\nloud\nslow\nlegato\n\n\n“Tender Lyrical”\nmajor\nquiet\nslow\nlegato\n\n\n“Passionate”\nminor\nloud\nfast\nstaccato\n\n\n“Sneaky”\nminor\nquiet\nfast\nstaccato\n\n\n“Sad/Relaxed”\nminor\nquiet\nslow\nlegato\n\n\n“Serious”\nminor\nloud\nslow\nlegato\n\n\n\nHaving carried out a cluster analysis for all 750 works, Horn and Huron then carried out cluster analyses according to each of the 50-year periods. Below is the resulting dendrogram for the earliest period, 1750 to 1799.\n\nA hundred years later (1850 to 1899), we can see that the musical samples produce a somewhat different clustering.\n\nThe table below summarizes the changes in clusters across the 150-year period of the study. The biggest change occurs with Light/Effervescent music. In the late 18th century, this represents the biggest cluster accounting for nearly 40% of musical passages. By the end of the 19th century, Light/Effervescent has virtually disappeared according to the sample. A second major change occurs with Sad/Relaxed music. Over the three 50-year periods, the amount of Sad/Relaxed music triples from 7% to 21%. Passionate music doubles (from 4% to 8%), but more importantly, Tender/Lyrical music doubles (from 22% to 44%).\n\nIn general, the results of the cluster analysis appear to be consistent with conventional musical interpretations regarding the historical change between the so-called Classical and Romantic periods. Light/Effervescent music which is so common in the Classical style nearly disappears. Joyful music declines slightly. Instead, there is a large increase in Tender/Lyrical music, accompanied by lesser increases in Sad/Relaxed, and Passionate musics.\n\nReferences\nKatelyn Horn and David Huron (2012). Major and minor: An empirical study of the transition between Classicism and Romanticism. In E. Cambouropoulos, C. Tsougras, P. Mavromatis & K. Pastiadis (editors), Proceedings of the 12th International Conference on Music Perception and Cognition. Thessaloniki, Greece: ESCOM, pp. 456-464.\nOlaf Post and David Huron (2009). Music in minor modes is slower (except in the Romantic Period). Empirical Musicology Review, Vol. 4, No. 1, pp. 1-9."
  },
  {
    "objectID": "emp_methods_workshop/feedback2.html",
    "href": "emp_methods_workshop/feedback2.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Feedback - Day 2\n\n\nFirst, tell us about the speed or pacing of today’s workshop:\n\ntoo slow\na bit slow\nabout right\na bit fast\ntoo fast\n\nSecond, tell us about the level of difficulty. Overall, I found today’s workshop material:\n\ntoo easy\na bit easy\nabout right\na bit challenging\nsomewhat challenging\ntoo hard\n\nHow useful was each group activity? (Circle 1, 2 or 3 stars — where three stars is best.)\n\n★ ★ ★ From theory to conjecture.\n★ ★ ★ Sampling approaches.\n★ ★ ★ Sampling issues.\n★ ★ ★ What is Random?\n★ ★ ★ Are Dynamics Asymmetrical?\n★ ★ ★ What kind of measurement scale?\n★ ★ ★ Questionnaire design.\n\nOverall, I found today’s group activities:\n\nof limited use\nokay\nuseful\nvery useful\n\nTell us the most important thing you learned today.\nOf all the material we covered today, the least useful topic was:\nGeneral suggestions (on reverse):"
  },
  {
    "objectID": "emp_methods_workshop/research_research.html",
    "href": "emp_methods_workshop/research_research.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Research Research\n\nThere is no recipe for doing research; there is no standard plan or omnibus template. Instead, there are some commonly used techniques, such as cluster analysis, pile-sorting, and chi-squared tests. But these techniques don’t tell you how to do research. Research involves a lot of different activities, and new approaches are constantly appearing.\nWhat we call “research methodology” is primarily a bunch of pointers — a bunch of do’s and dont’s. Actually, most of the important research advice is in the form of dont’s: avoid multiple tests, avoid essentializing terms, don’t try to explain the whole world at once, avoid chronic hypothesislessness, avoid sticky data, beware of post-hoc theorizing, etc.\nWhen we do research, with luck, we learn some new things about our field of interest. But doing research also teaches us about research itself.\nIn pursuing your research, be prepared to make mistakes. Even if you avoid all of the pitfalls discussed in this course, there will be new, not-yet-identified pitfalls waiting to be discovered. It may be that your most important contribution to scholarship will be identifying yet another methodological pothole to be avoided.\nOur slogan reminds us that learning about research is itself an ongoing enterprise:\nSlogan: Don’t be afraid to add your own slogans."
  },
  {
    "objectID": "emp_methods_workshop/experiment.html",
    "href": "emp_methods_workshop/experiment.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "The Experimental Study\n\nWhat distinguishes an experiment from all other kinds of studies is that the experimenter manipulates the world. Rather than being just a passive observer, the experimenter intentionally makes some change and then observes the effect of the change.\nConsider an experiment carried out by Céline Jacob, Nicolas Guéguen and Gaëlle Boulbry (2010). They wanted to know whether songs with kindly lyrics would tend to encourage people to behave in a more kindly manner. Before conducting their experiment, they first had listeners identify songs with kindly, empathetic, or prosocial lyrics. They also had listeners identify songs with comparatively neutral lyrics. They then matched the songs according to preference or liking. So the songs with the kindly lyrics weren’t rated by listeners as more or less enjoyable than the songs with the more neutral lyrics.\nOver a six-week period, Jacob et al played the prosocial and neutral music in alternating sessions in a restaurant located along the Breton coast in France. During each session they calculated the amount of tips left by restaurant patrons. They found that when music with prosocial lyrics was played, patrons left significantly better tips than when music with neutral lyrics was played.\nThe variable manipulated by the experimenter is called the independent variable (or independent measure). The variable observed by the experimenter is called the dependent variable (or dependent measure).\nIn the case of the restaurant study, the manipulation was the type of music played. That is, the independent variable was the degree of prosocial content in the song lyrics. The observed variable was the percentage tipping. That is, dependent variable (or dependent measure) was tipping percentage.\nIn the simplest experimental design, there are two values given to the independent variable. These values might be music versus no-music; heavy-metal versus reggae, loud versus quiet, consonant versus dissonant, in-fashion versus out-of-fashion, or, (in the Jacob et al experiment), prosocial lyrics versus neutral lyrics. Do kindly lyrics encourage people to behave in a more kindly manner? Evidently, the answer is “yes,” at least when it comes to French songs played in a certain Breton restaurant.\nIn a basic experiment, observations will be made for two groups: the “treatment” group and the “control” group. A more elaborate experiment might have several different “treatment” levels or treatment conditions.\nIn a correlational study, we are often interested in the relationship between two variables, but we don’t directly manipulate one of the variables. Instead, we simply observe the pre-existing relationship between the two variables. In correlational studies, there is no independent variable that we change.\n\nReference\nCéline Jacob, Nicolas Guéguen & Gaëlle Boulbry (2010). Effects of songs with prosocial lyrics on tipping behavior in a restaurant. International Journal of Hospitality Management, Vol. 29, pp. 761-763."
  },
  {
    "objectID": "emp_methods_workshop/program.html",
    "href": "emp_methods_workshop/program.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Methods in Empirical Music Research A Workshop for Music Scholars\n\nDAY 1\n\n\nWelcome & Introduction\n\nGenerals aims & Preview\nLearning objectives\n\nEmpirical Research\n\nTypes of knowledge\nSeven big ideas\nA line in the sand\nRefutation and confirmation\nOperationalizing\nComparison\nThe rhetoric of science (video - 8 minutes)\nReview\nGroup Task #1: What’s worth knowing?\nQuestions, conjectures, hypotheses and theories\nGroup Task #2: Question, theory or hypothesis?\nGrandmother research\nThe quantitative/measurement obsession\nGroup Task #3: Generating theories, Hindsight bias\nGrandmother research revisited\nTwo forms of reductionism\nEpistephobia\nTypes of failure\nReview\n\nTypes of Empirical Studies\n\nTypes of empirical studies\nGroup Task #4: Types of studies\nGeneralizing versus universalizing (video - 10 minutes)\nExploratory studies\nReactivity\nMeasurement studies\nHypothesislessness\n\nMotivations and Theories\n\nGroup Task #5: Operationalize the following hypotheses\nSyncopation: From question to hypothesis\nOpinions as operationalizations\nDouble-use data\nExploratory & Confirmatory: Contexts of discovery and legitimation\nExplore-then-test approach\nFrom question … to protocol.\n\nWrap-up\n\nReview\nHomework\nFeedback Day 1\n\n\n\nDAY 2\n\n\nReview\n\nGroup Task #7: From question to theory to conjecture to hypothesis to protocol\nFormal observation\nThe Experiment\nThe Correlational Study\nCorrelation and causation\nThe third variable problem\n\nSampling\n\nSampling\nGroup Task #8: Sampling approaches\nWEIRD sampling\nData independence\nBiased sampling\nGroup Task #9: Sampling issues\nGroup Task #10: What is random?\nHomework review: Review questions and discussion for Lancashire & Hirst (2009)\nSampling - A practical problem\nRandomized controlled studies\nSample Size: Law of big numbers\nSample Size: Law of small numbers\nEffect size\nRegression-to-the-mean\nRegression-to-the-mean in music\nReview\nGroup Task #11: Are musical dynamics asymmetrical?\n\nBehavioral Data\n\nDependent and independent measures\nImplicit vs. explicit responses\nDemand characteristics\nDer kluge Hans\nMeasurement scales\nGroup Task #12: What kind of measurement scale?\nDesigning questionnaires\nGroup Task #13: Design a questionnaire\nTypes of interviews\nInterviews in translation\nGroup Task #14: Constructing an interview guide\n\nExperimental Design\n\nExperimental paradigms\nReconnaissance study\nLife-sample studies\nDevelopmental Study\nLongitudinal Study; Cross-sectional Study\nPreferential Looking Paradigm\nPre-test, Post-test Design\nCounterbalanced Design\nReaction-time Experiment\nWithin-Subjects Design; Between-Subjects Design\nProbe-Tone Study\nImplicit Association Tasks (IAT)\nTwo-Alternative Forced Choice (2AFC) experiment\nMethod of Adjustment\nTelephone paradigm\nCatch-Recatch Method\nTwo-Experiment Ploy\nPhysiological Study\nEvoked Response Potential (ERP) study\nMultiple Baseline Design\nFactorial Design\n\nWrap-up\n\nReview\nFeedback Day 2\nHomework: Types of behaviors (video - 32 minutes)\nHomework: The Exploratory Open Interview (video - 45 minutes)\n\n\n\nDAY 3\n\n\nIntroduction to Statistics\n\nPreview\nWhy measure? How to measure anything\nFermi questions\nDescriptive statistics\nMeasures of central tendency\nMeasures of variability\nGroup Task #15: Descriptive statistics\nStandardized scores\nInferential statistics\nProbability, The null hypothesis, confidence and significance levels, statistical tests\nConfidence level and confidence interval\nCalculating confidence interval from confidence level\nStatistical significance\nChi-square test\nGroup Task #16: Chi-square tests\nGroup Task #17: Funeral marches in F minor: Hypothesis\nPublishing practice\nStatistical tests - General remarks\nInterpreting p\nMultiple tests, Bonferroni correction, file drawer effect\nDemonstration of correlation\n\nApplied Statistics\n\nHeroes and villains in opera\nPopulation density and preferred musical tempo\nComparing Chinese and German folksongs\n\nSome Advanced Analytic Techniques\n\nAdvanced analytic techniques\nCluster analysis: Patterns of Orchestration Cluster analysis: From Classical to Romantic\nFactor analysis, Principal Components Analysis\nMulti-dimensional scaling (MDS): Comparing medieval modes\nModeling - The case of the B-flat trumpet\nFoote novelty\nData analysis\nReliability and validity\nHomework review: Review questions and answers for Perttu (2007)\n\nWrap-up\n\nReview\nHomework\nFeedback Day 3\n\n\n\nDAY 4\n\n\nReview\nSome Useful Tools\n\nOutliers\nTest-retest reliability\nIntersubjective reliability\nTests, instruments, and batteries\nHearing screening inventory\nOllen Musical Sophistication Index (OSMI)\nGoldsmiths Musical Sophistication Index (GoldMSI)\nHumdrum Toolkit; Music21; Verovio\nResearchmatch.com\n\nQualitative Methods\n\nIndividual differences / Cultural differences\nAnecdotal evidence: Uses and abuses\nHypotheses - good and bad\nGroup Task #18: Creativity exercise\nQualitative AND quantitative\nSome thoughts regarding context\nParticipant-observation method\nDiary methods\nAutoethnography - Sudnow’s jazz improvisation studies\nContent analysis\nFreelisting and pile sorting\nGrounded Theory\n\nThe Experience of Research Participants\n\nFloor and ceiling effects\nUse of distractor tasks\nPrimacy and recency\nDebriefing participants\n\nReading and Writing Research Articles\n\nWriting an empirical paper\nCitations and quotations\nCritical reading of empirical articles\nWriting the paper first\nGroup Task #19: Writing a draft paper\nPre-reviewing\nEconomics of research Cheaper/faster/better dependent measures\nHow to review submissions\nHow to revise a manuscript in response to reviewers’ comments\nHow to criticize the work of others\n\nPotpourri\n\nWhy experiments fail\nReserved data sets\nConverging evidence\nMethodology as pothole markers\nGroup Task #20: Methodological potholes quiz\nGroup Task #21: Creative thinking about doing research\n\nParticipant Presentations\n\nGroup Task #22: Presentation & discussion of individual participant research programs\n\nResearch Practices\n\nGenerating stimuli\nCultivating a scholarly attitude\nResearch ethics\nHuman subjects; Institutional Review Board application advice\nDebriefing participants\nGroup Task #23: Running and debriefing participants\nGroup Task #24: Job description for research assistant\n\nMaking Your Own Way\n\nStarting up: facilities, equipment, and people\nStarting a subject pool\nReview Group Task #24 - Job description for assistant.\nThe value of collaboration; admitting your ignorance\nTalking to statisticians\nSubject pool survey\nGroup Task #25: Assess your strengths and weaknesses\n\nFinal advice\n\nSome research advice (video - 12 minutes)\nMore research advice\nFurthering your education\n\nWrap-up\n\nFinal review\nFeedback Day 4\nFinal workshop feedback"
  },
  {
    "objectID": "emp_methods_workshop/failure.html",
    "href": "emp_methods_workshop/failure.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Failure\n\nPart of doing research involves living with failure. (Remember, The best research invites failure.) Ideally, research projects fail for the right reasons: The best reason for a project to fail is that the hypothesis is wrong and you need to think differently about your problem. Unfortunately, there are many other reasons why a project can fail. It may be that your hypothesis is correct and the failure has a different origin.\nWhen you experience a research failure, it’s appropriate to spend some time thinking about possible reasons for the failure. Usually, it is impossible to determine why a project failed. However, in some cases the source of the failure is identifiable.\nFirst, why do hypotheses fail?\n\nBecause the hypothesis is wrong.\nBecause the population was incorrectly identified.\nBecause the sample was biased and so not representative of the population.\nBecause the sample size was too small.\nBecause the effect size is small.\nBecause the operationalized measures are poor approximations of the theoretical terms.\nBecause of a calculation or tabulation error.\nBecause the control group is biased.\nBecause an unidentified third variable is influencing the results.\nBecause the task was too easy (i.e., ceiling effect).\nBecause the task is too hard or the participants are unskilled (i.e., floor effect).\nBecause the participants didn’t try (also leading to a floor effect).\nBecause of some demand characteristic that is causing participants to undermine the hypothesis.\n\nWhy do research projects fail?\n\nBecause you can’t get a piece of equipment, or because of equipment failure.\nBecause you have difficulty recruiting appropriate participants.\nBecause you run out of time.\nBecause the cost of the project exceeds your budget.\nBecause your original goal was too ambitious.\nBecause a collaborator didn’t contribute as anticipated.\nBecause you lost your passion or enthusiasm."
  },
  {
    "objectID": "emp_methods_workshop/reading.html",
    "href": "emp_methods_workshop/reading.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Reading Empirical Research\n\nIn traditional arts and humanities scholarship, the principal publication is the monograph or book. Essays and scholarly articles are common, but not as important as books. In the sciences and social sciences, the principal publication is the journal article. Journals may contain letters to the editor, book reviews, and other items, but they mainly contain research articles.\nThree types of research articles can be distinguished: the research report, the meta-analysis study, and the review article. The research report is used to communicate the results of some original research. Perhaps 95% of all articles in the science and social sciences are research articles. The review article is a more theoretical work that summarizes many studies related to a particular topic and attempts to present a coherent interpretation about what all of the research implies. An example of a review article is a paper by Patrik Juslin and Petri Laukka (2004), reviewing research on music and emotion. Sometimes a review article does the reverse: it may draw attention to the incoherent and contradictory state of research pertaining to some topic, and may offer advice for future research that might help to resolve the mess. Most review articles are well-written and highly informative. They provide a good place to start when you begin working on a new project. Unfortunately, review articles are rarely labelled “I’m a review article.” Some journals, like Psychological Review specialize in publishing review articles. Because review articles are popular with readers, commercial publishers often aim to produce “Handbooks,” where each chapter reviews research on a specific topic written by a different author.\nA rarer type of article is the meta-analysis study. The meta-analysis is a “study of studies.” It is typically done when a large number of studies have been carried out related some problem. For example, many studies have been carried out related to whether television violence promotes violent behavior in viewers. Over a hundred studies have been carried out on this topic. Some of the studies seem to show a link, whereas other studies seem to show no link. In a meta-analysis, the researchers identify all of the pertinent studies. They then evaluate the quality of each study, including the quality of the samples used, the number of participants, the quality of the stimuli, the extensiveness of the controls, and other factors. Poor studies are simply discarded if they fail to achieve the minimum quality criteria established by the researchers. Then the researchers combine together all of the good studies, and do a statistical analysis on the aggregate data. The aim is to see if all of the studies ultimately tell a coherent story. In critically reading any meta-analysis, the main concern is how the author(s) set(s) the criteria for including or excluding a study in the final data analysis. Does the exclusionary criteria seem sensible? In general, meta-analyses are rare in music research. However, a fine example is found in the work of Friedrich Platz and Reinhard Kopiez (2012) who examined the influence of visual information in assessing the quality of a musical performance.\nIf it is a research report, determine what kind of study it is. Is it an exploratory study, descriptive study, correlational study, experimental study, modeling study, or some combination?\n\nAssessing Tests of Hypotheses\nFor correlational and experimental studies here are some useful guiding questions:\n\nWhat is the hypothesis (or hypotheses) being tested? Is the statement of hypothesis clear?\nIdentify the key terms in each hypothesis and ask how each term is operationally defined. How do the operational definitions deviate from the main sense of the corresponding concept?\nWhat is the population? It is actually rare for research reports to explicitly identify the population. Much research simply assumes that the population is all people.\nIs the sample reasonably representative of the presumed population?\nHow many different stimuli are used? (The fewer the number of stimuli used, the less the data independence: the conclusions may pertain only to those specific stimuli rather than to a general class of stimuli.)\nWhat varibles are controlled?\nWhat variables are left uncontrolled? (Remember that not all variables can be controlled: The main question is whether the research controlled those variables mostly likely to produce confounding effects.)\nWhat are the dependent variables?\nDo the dependent measures rely solely on self-report? (Self-report is more susceptible to demand characteristics.)\nAre the dependent measures implicit (less susceptible to demand characteristics and confabulation), or explicit (less good).\nGiven the design, what demand characteristics might be reasonably expected? Might these interfere with the experiment?\nAre the conclusions over-generalized?\nWas more than one experiment carried out? Is there converging evidence arising from different operationalizations, different stimuli, or different subjects?\n\nIf you are well-versed in statistics, then you can also address the quality of the statistical analyses. Are the results statistically significant? At what confidence level? Which variables are nominal, ordinal, interval or ratio? Are parametric or non-parametric tests used? Were tests of normalcy carried out? Are any statistical model assumptions violated? Did the author(s) perform multiple tests? If so, did they correct for multiple tests?\n\n\nEffect Size\nA common mistake by novices is to criticize an experiment for having too few participants. Experiments may have as few as five or six participants. The importance of the results depends largely on the effect size (also known as the magnitude of the effect). Recall our earlier discussion about testing the toxcity of cyanide. We don’t need many subjects if the effect size is large.\nEven in the case of music experiments, we will often see large effect sizes. The effect may be clearly evident in every one we test with only a few stimuli. For Western-enculturated listeners, does playing a slow passage in the minor mode sound “sadder” than playing the same passage in the major mode? Five or six participants listening to two or three stimuli may be all that is necessary in order to find statistically significant results consistent with the hypothesis.\nEffect sizes can be reported in percentages or using statistics such as d-primes, Cohen’s d, r2, R2, or phi (φ).\nIn most research, we are first interested in whether the results are statistically significant. If not, we know that the results can simply be attributed to chance. If the results are significant, then we can pay attention to the effect size. There are many results that are significant, but with small effect sizes. Small effect sizes suggest that the relationship is not strong, and that we should consider looking at other factors that may play a bigger role. On the other hand, an effect size might appear weak simply because there are some confounding elements that haven’t been properly controlled. It is possible that, with the right controls, a small effect will turn out to be a big effect that was diluted.\n\n\nTake-Home Messages\nWhen we finish reading a research article, our natural tendency is to make a mental note of the “conclusion.” We want to distill what we’ve read into some sort of “take-home message.” For example, after reading an article by Smith and Jones, we may try to remember that they showed (say) that lower pitch is heard as sounding sadder. Good researchers try to avoid such simple distillations. The sign of a careful researcher is that they remember a few important details about the study. Instead of simply recalling “low pitch is heard as sadder,” they will recall something like: “in a two-alternative forced-choice experiment, American non-musician listeners report that the melody with two or three pitches lowered by a semitone sounds sadder.”\nResearch is difficult enough without being burdened by ideas that are wrong. It’s not what you don’t know that is most problematic: it’s what you think is right (but is wrong) that causes the greatest havoc. You never know when it will be appropriate to reinterpret a past result. Instead of showing “low pitch sounds sadder,” you may later realize that a better interpretation is “lower-than-normal pitch sounds sadder,” or “higher-than-normal pitch sounds happier (less sad).” Research results may depend critical on the specific question. For example, a researcher may get different results if they ask participants to judge “unpleasantness” rather than “dissonance.” So it may be important to recall that experiment X measured “unpleasantness” whereas experiment Y measured “dissonance.” Conversely, many judgments are highly correlated. Researchers may find they get the same results whether they ask listeners to judge “sadness” or “sleepiness.” There are always other ways of interpreting the results of an experiment.\nFor important experiments, good researchers will recall the dependent and independent variables used and the way that the terms of the hypothesis were operationalized. They’ll also remember whether the effect size was large or small. Finally, they’ll pay attention to whether other researchers have replicated the result.\n\n\nFurther Information\nProfessional journals typically want authors to be brief and to-the-point. This is especially true in printed (rather than online) journals. Journal editors sometimes ask authors to shorten or summarize an otherwise verbose description. Consequently, journal articles may omit various details about the research. If you have a question about some published article, don’t be afraid to write to the author. Most researcher’s are happy to answer questions about their work.\n\n\nReferences\nFriedrich Platz and Reinhard Kopiez (2012). When the eye listens: A meta-analysis of how audio-visual presentation enhances the appreciation of music performance. Music Perception, Vol. 30, No. 1, pp.71-83."
  },
  {
    "objectID": "emp_methods_workshop/double_use.html",
    "href": "emp_methods_workshop/double_use.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Double-Use Data\n\nRecall that whenever we look at a set of data, we can immediately form a number of ideas that might account for any patterns evident in the data. However, these ideas are post hoc not a priori. After looking at the data, it would be wrong to formulate a theory and then to present the data as though it was an independent test of our theory. It is okay to use data to formulate a theory, but it is not okay to use a single set of data both to formulate the theory and to “independently” test the theory. We refer to this as double-use data.\nAn historical example of double-use data relates to the Theory of Continent Drift in the field of geology. Today, there is very good evidence that the continents behave as “plates” that slide very slowly across the earth’s surface. However, the theory began in an inauspicious manner. If you look at a map of the world, there seems to be a certain similarity between the eastern coastlines of north and south America, and the western coastlines of Europe and Africa. One could easily imagine all four continents pushed together like jigsaw puzzle pieces.\nOver the past century, excellent evidence has been assembled that is consistent with this theory. However, this evidence was not available when the theory was first proposed. If we ask “What inspired the theory?” the answer was “Look at how the continents seem to fit together like jigsaw puzzle pieces.” If we then ask “What evidence do we have that’s consistent with the theory?” the answer was “Look at how the continents seem to fit together like jigsaw puzzle pieces.” In other words, the same evidence was used both as the inspiration for the theory, and as evidence for the theory. Notice that this is circular reasoning. The Theory of Continental Drift took time to be accepted precisely because of the lack of independent evidence.\nNormally, we are not interested in where theories come from (the context of discovery). Instead, we are interested in testing theories (the context of legitimation). However, we are right to question a theory if the context of discovery and the context of legitimation are the same.\nWhen you look at some observations you have collected, any theory you form is now post hoc theory. You cannot then claim that your theory was a priori and then use the observations as evidence that tests your theory. Once you look at your data, you cannot pretend that you predicted the data. In short, post hoc theories are not “prophetic.” You cannot use the language of prediction that is the essence of hypothesis testing.\nIt is important to formulate your theories before you collect your data — and before you examine or analyze your data. Since we sometimes forget where ideas come from, be sure to keep a research diary or lab notebook. This will help you as a researcher to keep straight which ideas are a priori and which ideas are post hoc.\nSlogan: Beware of the post hoc theory.\nRemember that everything tends to be obvious in retrospect (hindsight is 20/20). We can make up a story for just about any set of observations. The true test is making up the story first (i.e., prediction)."
  },
  {
    "objectID": "emp_methods_workshop/types.html",
    "href": "emp_methods_workshop/types.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Identify whether the following studies are best regarded as reconnaissance, descriptive, measurement, correlational, or experimental. If correlational or experimental, further indicate whether it is exploratory correlational or exploratory experimental.\n\nA researcher publishes an article describing a newly discovered box containing composition sketches written by Bartók.\nWhile a musician takes a break, an ethnomusicologist clandestinely mistunes one of the strings on the instrument by 1/3 of a semitone. The ethnomusicologist suspects that the musician will notice the mistuning and so retune the instrument.\nA researcher at Billboard magazine assembles and publishes music sales by genre for 85 countries.\nA researcher analyses facial expressions in commercial videos of concert pianists. It is found that pianists tend to raise their eyebrows when the music is high in overall pitch.\nA researcher commissions ten composers to create “happy” and “sad” melodies. The researcher predicts that the happy melodies will exhibit larger melodic intervals than the sad melodies.\nThe manager of an orchestra is interested in identifying which repertoire attracts bigger audiences. She gathers audience statistics for a number of concerts from several orchestras. She classifies each program according to whether the music is predominantly Baroque, predominantly Classical, predominantly Romantic, predominantly Modern, or predominantly Pops. She finds that the largest audiences occur for Pops concerts.\nA researcher interviews twenty young mothers about singing to their newborn infants. He finds most mothers report singing to their babies, but that they are more likely to sing when no other adult is observing them. The researcher concludes that many young mothers are self-conscious about their own singing and prefer not to have an audience other than their baby.\nA researcher is interested in the phenomenon of globalization and collects data from different islands in the Kingdom of Tonga. For each island, she counts the number of Internet-connected computers, and also interviews people in order to determine their favorite music. She predicts that islands where people most prefer Western pop music will be the islands that are most Internet-connected.\nA school board is interested in the effect of introducing non-Western music into the middle school band program. Of eight middle schools in the district, four randomly selected middle schools include non-Western works in the band repertoire. At the end of the year, a sample of music students are interviewed from all eight schools in order collect feedback.\nA music theorist carries out analyses of most of the serial works by Schoenberg, Webern and Berg. After analyzing the works, the theorist publishes a general article comparing and contrasting the serial techniques of the three composers. In particular, the theorist observes that, compared with the other composers, Webern is more likely to compose works structured as extended palindromes — with the material in the second half of the work inverted."
  },
  {
    "objectID": "emp_methods_workshop/grounded.html",
    "href": "emp_methods_workshop/grounded.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Grounded Theory\n\nIn most empirical work, the emphasis is on testing a theory or hypothesis. One begins with a theory and then collects data intended to test the theory. A problem with this approach is that one must start with a theory. What if you don’t have a theory? Or worse yet, what if all of our theories are wrong, misguided, or irrelevant? We might spend our careers testing theory after theory — showing that none of them is any good.\nAn alternative approach might start with observation rather than theory. Grounded Theory gets its name from the research being “grounded” in observations. Start with the observations, and then develop the theory from what you observe.\nGrounded Theory was developed by sociologists Barney Glaser and Anselm Strauss.\nIn traditional scientific method, we begin with a question, develop a theory, and then test hypotheses that arise from that theory. In grounded theory, it is less valuable to begin with a question. If the researcher is guided by any question, it is: “What’s going on?” If the phenomenon being examined involves people, the motivating question might be: “What is the main problem these people are facing and how are they dealing with it?” Grounded Theory focuses on observation.\nGrounded Theory studies are not simply descriptive. Instead, they are explanatory: the aim is to create a theory that explains the observations. At the same time, efforts are made to avoid coming to the observations with prior preconceptions. That is, the aim is to develop theory “from the ground up.” rather than interpreting the observations in light of an already existing theory. Grounded Theory aims to let a phenomenon speak for itself, and to reduce researcher preconceptions from getting in the way.\n\n\nReferences\nKathy Charmaz (2000). Grounded theory: Objectivist and constructivist methods. In Denzin, N.K. and Lincoln, Y.S. (eds.), Handbook of Qualitative Research, 2nd edition. Thousand Oaks, CA: Sage. pp. 509–535.\nKathy Charmaz (2006). Constructing Grounded Theory. London: Sage.\nBarney Glaser & Anselm Strauss (1967). Discovery of Grounded Theory. Strategies for Qualitative Research. Sociology Press."
  },
  {
    "objectID": "emp_methods_workshop/creativity.html",
    "href": "emp_methods_workshop/creativity.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Task\nIn the next five minutes, list as many uses as you can for a large plastic soda bottle.\nYou will be graded on the number, variety, and originality of your suggestions.\n\n\nRationale\nGood research requires creative thinking. This task is intended to develop your powers of associative thinking."
  },
  {
    "objectID": "emp_methods_workshop/theory_hypothesis_answers.html",
    "href": "emp_methods_workshop/theory_hypothesis_answers.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "ANSWERS / DISCUSSION\nIdentify whether the following statements are questions, theories, conjectures, hypotheses or none.\n\nWhy do performers tend to slow down at the ends of phrases?\nThis is a question.\nBeethoven’s metronome markings are too fast because his metronome was broken.\nThis is a theory: it proposes a causal explanation. Note the use of the word “because.” (A common mistake is to think that “theories” make big or sweeping generalizations; theories can be small and focus on minor phenomena.)\nMuch of the popularity of World Music is due to commercial exploitation.\nThis is a theory: it proposes a causal explanation — although the mechanism of influence (“commercial exploitation”) may be a bit vague. Note the use of the phrase “due to”.\nHow is it that listening to music can sometimes cause shivers to run up-and-down your spine?\nThis is a question.\nThe language we use shapes the way we think.\nNote that the word “shapes” could be replaced by synonyms such as “affects” or “influences.” In effect, the claim is that language “causes” us to think differently. Consequently, it is appropriate to regard this as a theory.\nBrahms uses a lot of hemiolas in his music.\nThis is a hypothesis. It is easily tested.\nBrahms liked hemiolas.\nUnless Brahms wrote a letter or otherwise communicated that he “likes” (or dislikes) hemiolas, it would be difficult or impossible to determine what he “likes.” Consequently, it is better to call this a conjecture rather than a hypothesis.\nAfricans have a better sense of rhythm than Europeans.\nIf you think “sense of rhythm” is not possible to measure, then this is a conjecture. Otherwise it is a hypothesis.\nThe music of Carl Nielsen echoes the spirit of the Danish people.\nAs written, this statement admits several different interpretations. It is often helpful to try re-writing a statement in order to gain some clarity. For example, we might re-write this statement as follows: “The Danish spirit is echoed in the music of Carl Nielsen.” This suggests that being Danish influenced Nielsen’s music. Formulated this way, the statement would be regarded as a theory.\n“Music hath charms to soothe the savage breast.”\n(Quote from William Congreve, 1697). This is a conjecture or hypothesis.\nRaag Shree sounds sad to experienced Indian listeners.\nThis is a hypothesis. In principle, this claim could be easily tested.\nThe music of the Lakota has been influential primarily because of frequent portrayals of Plains Indians in Hollywood films.\nThis is a theory: it proposes a causal explanation for the widespread influence of Lakota music. Note the use of the word “because.”\nThe purpose of our research is to study the relationship between music and ritual.\nThis is not a question.\nIt is not a theory since no cause is proposed.\nIt could be construed as a conjecture or hypothesis if there were some doubt about the purpose of the research (say, if two collaborators were arguing about what they are doing). E.g.\nResearcher 1: The purpose of our research is … music and ritual. Researcher 2: No, no, the purpose of our research is … music and dance.\nBut this is a stretch.\nThe best answer is “none.”\nNotice that, as a research goal, it would be much better to say “The purpose of our research is to understand the relationship between music and ritual.” The “study” of something may be a reasonable goal for a student, but not for a researcher."
  },
  {
    "objectID": "emp_methods_workshop/strengths.html",
    "href": "emp_methods_workshop/strengths.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "The purpose of this exercise is to assess your research situation, to identify strengths and limitations, and to determine what courses of action are needed to make you a better researcher.\nIn groups, discuss each item in turn, then (privately) rate yourself according to the various criteria. Rate yourself on a scale from 1 to 5, where 1 means not prepared at all and 5 means well prepared.\nFinally, on the reverse of this page, make an ordered list of your weaknesses (especially items scored “1” or “2”). Identify any other criteria that you think are pertinent to pursuing your research interests. Then prioritize the items. They represent your priorities for improving your capacity for research.\nI have good ideas for research. 1 2 3 4 5\nI have specific theories/hypotheses to test. 1 2 3 4 5\nI regularly write in a research diary. 1 2 3 4 5\nI regularly talk with peers about research. 1 2 3 4 5\nI have good knowledge of the research literature pertinent to my interests. 1 2 3 4 5\nI have good knowledge of research procedures/methods/pitfalls. 1 2 3 4 5\nI have completed my institution’s IRB human subjects training. 1 2 3 4 5\nI am experienced running human subjects in an experiment. 1 2 3 4 5\nI am experienced interviewing informants. 1 2 3 4 5\nI have the appropriate computer skills. 1 2 3 4 5\nI can create/synthesize/perform and edit sounds for stimuli. 1 2 3 4 5\nI am experienced with recording gear. 1 2 3 4 5\nI have ready sources for recruiting suitable participants/informants. 1 2 3 4 5\nI have ready access to the materials (scores, cultural artifacts, etc.) I need. 1 2 3 4 5\nI have the appropriate knowledge of statistics. 1 2 3 4 5\nI know someone who can help with statistics. 1 2 3 4 5\nI am not math-phobic. 1 2 3 4 5\nI have good writing skills. 1 2 3 4 5\nI have good editorial skills for improving a document. 1 2 3 4 5\nI have appropriate foreign language skills for my work. 1 2 3 4 5\nI am a good public speaker. 1 2 3 4 5\nI am highly familiar with presentation software like Powerpoint or Keynote. 1 2 3 4 5\nI have good knowledge about library resources. 1 2 3 4 5\nI am fully versed in appropriate research tools available on the web. 1 2 3 4 5\nI have leadership skills for collaborative projects. 1 2 3 4 5\nI have the ability to work with others. 1 2 3 4 5\nI have time to do research. 1 2 3 4 5\nI am enthusiastic and passionate about research possibilities. 1 2 3 4 5"
  },
  {
    "objectID": "emp_methods_workshop/feedback3.html",
    "href": "emp_methods_workshop/feedback3.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Feedback - Day 3\n\n\nFirst, tell us about the speed or pacing of today’s workshop:\n\ntoo slow\na bit slow\nabout right\na bit fast\ntoo fast\n\nSecond, tell us about the level of difficulty. Overall, I found today’s workshop material:\n\ntoo easy\na bit easy\nabout right\na bit challenging\nsomewhat challenging\ntoo hard\n\nHow useful was each topic? (Circle 1, 2 or 3 stars — where three stars is best.)\n\n★ ★ ★ Descriptive statistics\n★ ★ ★ Null hypothesis\n★ ★ ★ Confidence level\n★ ★ ★ Statistical significance\n★ ★ ★ Chi-square test\n★ ★ ★ Multiple tests\n★ ★ ★ Daniel Shanahan’s lecture\n★ ★ ★ Advanced statistics (cluster analysis, factory analysis, MDS)\n★ ★ ★ Discussion of Perttu (2007) reading\n\nOverall, I found today’s group activities:\n\nof limited use\nokay\nuseful\nvery useful\n\nTell us the most important thing you learned today.\nOf all the material we covered today, the least useful topic was:\nGeneral suggestions (on reverse):"
  },
  {
    "objectID": "emp_methods_workshop/hindsight_task3.html#obvious-theories-hindsight-bias",
    "href": "emp_methods_workshop/hindsight_task3.html#obvious-theories-hindsight-bias",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "Obvious Theories: Hindsight Bias",
    "text": "Obvious Theories: Hindsight Bias\n\nAnything seems commonplace, once explained. - Dr. Watson to Sherlock Holmes\n\nDebriefing\nIn the two previous group tasks you formulated theories for various findings. Notice that people are able to construct plausible theories which-ever way the results go.\nWhen The American Soldier study was completed, the general public complained that it was a waste of tax-payer money because the results were all obvious.\nA large volume of research in psychology has shown that people are prone to think that events are much more obvious in retrospect (e.g., Slovic & Fischhoff, 1977; Baratz, 1983; Powell, 1988; Bolt & Brink, 1991). Asked to predict the outcome of an upcoming election, people will be unsure of their predictions. But after the election is over, when asked how well they would have predicted the outcome, people are very confident that they would have correctly anticipated the actual results. In hindsight, we think we “knew it all along” (Hawkins & Hastie, 1990). In general, psychological research has established that we grossly overestimate our predictive abilities (Watts, 2011).\nAt the University of Leicester (UK) Karl Teigen (1986) asked students to judge the truthfulness of both actual proverbs and their opposites. When given the genuine proverb “Fear is stronger than love,” most rated it as true. But the reverse form, “Love is stronger than fear,” was also rated by most students as true. Similarly, the actual proverb “He that is fallen cannot help him who is down” was rated by most students as true. But it’s opposite was also judged by most to be true — “He that is fallen can help him who is down.” Two other proverbs that were rated as likely to be true were: “Wise men make proverbs and fools repeat them” and its opposite: “Fools make proverbs and wise men repeat them.”\nAs the Danish philosopher-theologian Søren Kierkegaard put it, “Life is lived forwards, but understood backwards.” We over-estimate our ability to make sense of past events. It is common for people to think we “knew-it-all-along” — when, in fact, we didn’t.\nAs Paul Lazarsfeld (1949) noted, “Obviously, there is something wrong with the whole argument of”obviousness.”” It is essential for researchers to be aware of the ease with which one can generate plausible theories to account for any result. There is a huge difference between explaining something once you know the results, compared with formulating the theory first, before the results are known. We should be much more suspicious of post hoc theories than a priori theories.\nIntuition is important in life, but our intuitions about past events are clouded by what we know today.\nThe ease with which researchers can explain data after they look at it is referred to as hindsight bias. Everything is obvious in retrospect.\nSlogan: Hindsight is 20/20.\nHindsight bias contributes to the feeling that research doesn’t tell us anything we didn’t already know.\nThere is only one sure way to avoid hindsight bias — formulate your theory before you collect your data. (The best research invites failure.)\n\n\nReferences:\nHawkins, S. A., & Hastie, R. (1990). Hindsight: biased judgements of past events after the outcomes are known. Psychological Bulletin, Vol. 107, pp. 311-327.\nPaul Lazarsfeld (1949). The American Soldier: An expository review. Public Opinion Quarterly, Vol. 13, No. 3, pp. 377-404, 380.\nPaul Slovic and Baruch Fischhoff (1977). On the psychology of experimental surprises. Journal of Experimental Psychology, Vol. 3, No. 4, pp. 544-551.\nDaphna Baratz (1983). How Justified is the “Obvious” Reaction? PhD Dissertation, School of Eductation, Stanford University.\nKarl Teigen (1986). Psychological patterns in predicting disjunction and conjunction of clinical symptoms. Acta Psychologica, Vol. 61, pp. 183-195.\nHartmut Blank, Jochen Musch & Rüdiger F. Pohl (editors) (2007). Social Cognition, Vol 25, No. 1. Special issue on the Hindsight Bias.\nDuncan J. Watts (2011). Everything is Obvious: Once you Know the Answer. New York: Crown Business."
  },
  {
    "objectID": "emp_methods_workshop/operationalize_opinions.html",
    "href": "emp_methods_workshop/operationalize_opinions.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Some terms are easier to operationalize than others. It may be straightforward to count the number of people attending a concert, but usually some refinement of the counting procedure is necessary. Suppose for example that you are counting “the number of notes in a work” from a printed score. You might decide to exclude repeats signs but include a Da Capo return. You might reasonably exclude “tied-to noteheads,” but what about groupettos and grace notes? And what about trills and other ornaments? Some clarification will be necessary. How you settle on an operationalization will depend on the research goal and on the amount of labor required.\nWhen operationalizing terms, try to anticipate the kinds of problems that might arise, and define your terms accordingly. Even so, once the measuring begins, you are apt to encounter situations which will require further refinement to the operational definitions.\nIn some cases, it will seem impossible to define a “mechanical” procedure that will always produce unambiguous results. In these cases, it may be necessary to rely on opinion. For example, in several studies we’ve done, we have contrasted works in major and minor keys. Most of the time, musicians will have no difficulty identifying whether a score is in the major or minor mode. There are some difficulties, however. It is not uncommon for a minor-mode work to modulate to the relative major for a period of time. Minor-mode works may end with a Picardy Third (where the final tonic chord is major). Some works don’t end in the same key in which they began. There are atonal works, whole-tone works, and works written in medieval modes like Dorian or Phrygian. Despite these and other complications, for common-practice period music, 9 out of 10 musicians will agree on the key. It’s not outrageous for an experimenter to judge the key of most works. In many research projects, we’ve relied on the opinion of the researcher to determine key. Since some pieces can be ambiguous or questionable, we typically use three categories: (1) obviously major, (2) obviously minor, and (3) not obviously major or minor. In effect, we operationalize a work’s mode as “the opinion of the experimenter when examining the score, as to whether the work is obviously major or obviously minor.”\nIn other cases, it may be much less clear, and so opinion may be easily skewed by the beliefs of the experimenter. Suppose, for example, that an experimenter was testing the hypothesis that “composer A wrote shorter phrases than composer B.” Suppose further that neither composer took pains to write explicit phrase markings over every phrase. Opinions about “what’s a phrase” are much less unanimous than opinions about major or minor mode. Here, experimenter bias is more likely to be a problem. Nevertheless, an independent musician—not familiar with the aims of the project or the hypothesis being tested—may prove useful. Rather than trying to create a mechanical procedure for defining a phrase, we might simple rely on an independent opinion. In this case, one could operationalize phrases as “whatever an independent musician marks in the score as a phrase.”\nFor some tasks, opinions can diverge considerably. For example, although it is relatively easy to determine whether pieces in the 18th century exhibit a “sonata-allegro” form, this becomes much more contentious in music from the 19th century. If a researcher needs to know whether a given movement is in sonata-allegro form, it may be necessary to poll the opinions of several theorists. In thise case, one could operationalize “in sonata-allegro form” as “those movements that a minimum of 3 of 5 music theorists claim to conform to sonata-allegro form.”\nAll concepts ultimately prove to be difficult to “pin-down.” In operationally defining a term, the researcher should first aim to produce a mechanical procedure that anyone could follow, resulting in a clear and repeatable measurement. If that’s not possible, “opinions” can be used. In some cases, opinions are sufficiently uncontentious that the experimenter’s own opinion can be used. In order to avoid experimenter bias, independent opinions from people unfamiliar with the aims and hypothesis of the study might be used. It may be necessary to recruit “experts”—so that the opinions are more informed. If the term is especially fuzzy, the experimenter might employ several experts, and establish a minimum criterion of unanimity.\nThe important point is that opinions can provide useful operationalizations for especially intractable terms or concepts."
  },
  {
    "objectID": "emp_methods_workshop/conclusions.html",
    "href": "emp_methods_workshop/conclusions.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Writing a Conclusion\n\nLet’s return to consider the study by Goldin and Rouse (2000) about the effect of blind auditions on the hiring of female orchestral musicians. Recall that they found that the switch to blind auditions acounts for 30% of the increase in the proportion of female musicians hired.\nGoldin and Rouse are economists, not music researchers. We might expect that their motivation for conducting the study was to empirically chronicle general examples in inequality in the hiring, compensation, and promotion of women in the workplace. It is difficult to find overt evidence of prejudice. Blind and non-blind orchestral auditions provide one of relatively few situations in which it might be easy to identify the effects of prejudice.\nIn their paper, Goldin and Rouse never use the word “prejudice” or “misogyny.” Once again, their main conclusion is that the switch to blind auditions acounts for 30% of the increase in the proportion of female musicians hired.\nRecall that this is a correlational study. The researchers did manipulate anything. Many studies of prejudice are true experiments involving some type of manipulation. For example, studies have submitted identical resumes to employers, changing only the name of the applicant, manipulating the name so it sounds more black (Latitia Brown) or more white (Lindsay Wheatley). In other studies, the name remains the same, but an accompanying photograph suggests a black or white applicant. In especially controlled experiments the photograph is of a single person#&151;modified using photoshop to darken or lighten the skin. In such cases, you can infer that the manipulation is the cause of the different behavioral responses.\nHow do you report the conclusion?\n\n[just the facts?] The use of blind auditions is positively correlated with an increase in the hiring of female musicians for eight major U.S. orchestras between 1940 and 1995.\n[evidence of prejudice in a particular time and place?] The effect of blind auditions indicates the existence of a broad prejudice against women musicians in eight major U.S. orchestras between 1940 and 1995.\n[evidence of prejudice - broaden the time and place?] The effect of blind auditions indicates the existence of a broad prejudice against women musicians in the U.S.\nThe effect of blind auditions shows a broad pattern of prejudice against women.\nThe use of screens is consistent with the notion that broad pattern of prejudice against women.\n\nRecall that this is a correlational study. This means we can’t infer causality. In correlational studies, we can never dismiss the possibility of a third variable. The correlation may be spurious. Goldin and Rouse themselves identify other possible effects of screens apart from whether a player can be identified as male or female. For example, the absence of a screen can allow judges to identify particuar people. “That’s George Irving—he’s got a fine reputation.” Or the effect may be one of recognizing lineage. “That’s George Irving: he studied with Marc Garrett at Julliard (a fabulous teacher). Irving has an impecable lineage.” Alternatively, perhaps there is prejudice that favors older (more experienced) players. If the women candidates are generally younger than the male candidates, they may have suffered because of reverse agism.\nFinally, consider the unexplained variance. If 30% of the increase in the proportion of female musicians hired is attributed to the switch to blind auditions, what accounts for the remaining 70% increase in the proportion of female musicians? Most of the increase in female musicians is unrelated to the structure of the audition. Why else would more women be hired? There are several possibilities. First, there are proportional more female musicians who have undertaken advanced training compared with male musicians. This probably the main cause—although this would warrant it’s own study. Second, relative to men, women musicians may now be better musicians than women musicians were in the past. A third possibility is that women musicians have benefitted from affirmative action programs. These may be conscious or unconscious efforts to create a more gender-balanced ensemble. Notice that neither of these accounts is consistent with rampant prejudice against women.\nOne could imagine other (less plausible) scenarios that might be motivated by prejudice and that might account for the increased hiring of women. For example, an orchestra might have a policy to pay women less than men. Consequently, hiring more women would lower costs. Or hiring more women may make it easier for fund raisers to successfully approach affluent women benefactors.\nSlogan: Over-deliver and under-conclude.\nAim for modesty and competence: over-deliver on evidence while understating your conclusions. Offer more evidence than your readers think necessary, and express your conclusions in a more tentative or circumspect manner than your readers expect.\nWhat most displeases critics of empirical research is the smug, gloating, self-satisfied, triumphalism of science. Aim for modesty, even as you make your points. Remind critics that most of the unguarded and triumphalism language is the product of journalists, populists, and other non-researchers. The strength of empirical methodology is precisely that its practitioners choose methods that enforce intellectual humility.\n\nReferences\nClaudia Goldin & Cecilia Rouse (2000). Orchestrating impartiality: The impact of “blind” auditions on female musicians. American Economic Review, Vol. 90, pp. 715-741."
  },
  {
    "objectID": "emp_methods_workshop/empirical.html",
    "href": "emp_methods_workshop/empirical.html",
    "title": "Types of Knowledge",
    "section": "",
    "text": "Definition of empirical: knowledge gained through observation, experience, or experiment.\nAt least three sources of knowledge can be identified:\n\nintuition (intuitive knowledge)\ndeduction (deductive knowledge)\nobservation (empirical knowledge)\n\n\n\nIn everyday life, the most important source of knowledge is intuition.\n\nThe chicken salad in my refrigerator is not old, but it seems to smell a little odd. I decide to throw it out rather than eat it.\nA woman is shopping for a car. She feels a vague sense of unease about the salesman and so decides to go to another dealership.\nA man about to cross the street jumps back when he hears the sound of a car horn. The reaction is so fast, he is not even aware of having thought about anything.\nA piano teacher suspects that the reason why her student is not progressing faster is because he is not practicing enough. She decides to phone his parents.\n\nSome of our intuitions have been shaped by millions of years of evolution — and are designed to save our lives. We become suspicious for reasons which elude us.\nEspecially in matters related to music-making, the most important knowledge we have is probably intuitive knowledge. Without intuition, music-making would be impossible.\n\n\n\nDeductive knowledge is also called rationality. Deductive knowledge arises from logical thought.\n\nIf all people are mortal, and Socrates is a person, then Socrates is mortal.\nSeeing a guitarist move her finger one fret toward the tone hole, we conclude that the next pitch will be one semitone higher than the previous pitch.\nThe first 8 pages of a Beethoven manuscript are written using a dark blue ink. The remaining 11 pages in the manuscript use a more purple-colored ink. An accompaniment figuration for a phrase near the beginning of the work is written using the purple ink. If we assume that the purple ink was used after exhausting the dark blue ink, it follows that the accompaniment must have been added after the melody line was written.\n\n\n\n\nEmpirical knowledge is knowledge gained through observation. Observation may occur informally, or we may create a formal plan for observation. An example of informal observation is what we call “experience.” An example of formal observation is an “experiment.” So we can rephrase our definition as: Empirical knowledge is knowledge gained through observation, experience, or experiment.\n\nI know that many birds are capable of flight because I see them fly.\nI’ve observed that some musicians can always identify the names of pitches (“perfect pitch”) and others can’t.\nI have witnessed only a couple of occasions when people dance without music: People rarely dance without music.\nI know that some music can send shivers up my spine because I have experienced this.\nI know that works for xylophone are much more likely to be in the major mode than works for marimba because I’ve examined (and counted) a large number of works for xylophone and marimba.\n\nIntuitive, deductive and empirical forms of knowledge are all necessary for life. Each form of knowledge adds to the value of the others. That is, they are complementary forms of knowledge.\nIn addition, each form of knowledge also has limitations. There are times when our intuitions prove to be wrongheaded. For example, a child’s intuition may tell him not to let a nurse stick a needle into his arm–even though an inoculation will help prevent illness. Here, intuition fails us. There are many situations when rational deduction proves to be impossibly complicated or time-consuming. For example, when identifying the pros and cons of purchasing a particular house, we may find it impossible to “calculate” all of the pertinent factors. Here, rational deduction fails us. Regularly watching the local television news, an elderly woman may conclude that her neighborhood is full of crime, and that she should refrain from going outside for her daily walk. Here, observation fails us."
  },
  {
    "objectID": "emp_methods_workshop/orchestration.html",
    "href": "emp_methods_workshop/orchestration.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Cluster Analysis in Orchestration\n\nCluster analysis is a form of statistical data analysis in which subsets (called “clusters”) are formed according to some notion of similarity. There are many different variants of cluster analysis, but most are hierarchical — in which low-level clusters are successively joined together to make larger clusters, and so on, until everything is clustered into one large group. The result is a cluster tree or dendrogram.\nRandolph Johnson (2007) used cluster analysis to try to determine how instruments are grouped together in orchestral works. He examined a large number of scores from 19th century orchestral music. In order to circumvent possible experimenter bias, Johnson used David Dubal’s The Essential Canon of Classical Music (2001) as his source for sampling “orchestral music.” For each work in Dubal’s book, Johnson sampled just a single random vertical sonority — a single sounding moment. He coded which instruments were playing at that moment. For example, for a given moment in some score, the first and second violins might be active, along with the violas, ’cellos and a solo clarinet. Johnson ended up with several hundred coded sonorities — one sonority from each work. In this way, he ensured a high degree of data independence.\nJohnson then used cluster analysis to analyze the instrumental combinations. The computer first grouped together those instruments which most commonly appeared together. As can be seen in the dendogram, the first clusters include (1) violin + viola, (2) double bass + ’cello, (3) flute + clarinet, (4) bassoon + oboe.\nThe second group of clusters include (5) horn + trumpet, and (6) bass clarinet + English horn/cor anglais. In addition, two of the earlier groups group together: (7) the violin/viola + the bassoon/oboe. You can see all of the groupings and hierarchical groupings in the tree below.\n\nJohnson found that there were three over-arching instrumental groupings in 19th century orchestral music (identified in the dendrogram by the solid vertical line). One group consists of the violin, viola, ’cello, double bass, flute, oboe, clarinet and bassoon. The second group consists of the horn, trumpet, trombone, tuba, piccolo and timpani. The third group consists of the bass clarinet, English horn, harp, cornet, and contrabasson.\nCluster analysis does not tell you the “meaning” or origin of the groups. It simply tells you that there are statistical pertinent natural groupings that arise depending on the way you define similarity. In this case, Johnson proposed a musically plausible interpretation of his three over-arching groups: the first group he dubbed “Standard” instrumentation; the second group he dubbed “Power” instrumentation; and the third group he dubbed “Color” instrumentation. The results of his analysis are consistent with the idea that 19th century orchestral composers conceived or deployed instruments in terms of three broad categories: (1) core or standard instruments consisting of the strings and woodwinds, (2) loud, energetic or power instruments consisting of the brass, piccolo, and timpani, and (3) novelty or color instruments such as the English horn, cornet, and the harp. Notice that the color instruments further cluster into two groups: (i) quieter color instruments such as the English horn, bass clarinet and harp, and (ii) louder color instruments such as the cornet and contrabassoon. Johnson calls his resulting theory of orchestration, the SPC theory (for Standard, Power and Color).\n\nReferences\nJohnson, R. (2007). Towards a Theory of Orchestration: A Quantitative Study of Instrumental Combinations in Romantic-Era Symphonic Works. M.A. thesis. Ohio State University."
  },
  {
    "objectID": "emp_methods_workshop/pool_survey.html",
    "href": "emp_methods_workshop/pool_survey.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "For a number of years, music students at OSU have been participating in a variety of experiments sponsored by the School of Music. The purpose of this survey is for us to gain a better perspective on those students who participate in our experiments. Since OSU music students differ in several ways from other musicians as well as the general population, this information helps us better understand the limitations when generalizing research conclusions to other groups of people.\nThis questionaire is entirely anonymous. Please do not write your name or any other identifying marks on this paper. Note that data collected from this survey may be made available to other researchers, but that all data will be preserved or distributed in a completely anonymous format.\n\n\n\nWhat is your age? __________ years.\nPlease circle MALE or FEMALE.\nIs English your first language? (answer YES or NO): ________\nAre you fluent in any language other than English? If so, indicate the other language(s): ________________\nWere you born in the United States? (answer YES or NO): ________\nWere you raised predominantly in the United States? (answer YES or NO): ________\nWere your born in Ohio? (answer YES or NO): ________\nWere you raised predominantly in the state of Ohio? (answer YES or NO): ________\nOn a scale from 1 to 10, where 1 means highly rural and 10 means highly urban, circle the value which best characterizes your up-bringing. —————- – — – — – — – — – — – — – — – — – — – —- – —————- (highly rural) 1 2 3 4 5 6 7 8 9 10 (highly urban) —————- – — – — – — – — – — – — – — – — – — – —- – —————-\nHow would you characterize your ethnic background? ___________________________________________________\nHave you ever lived in a country other than the United States? (Y/N): __________\nIf, so please identify the country(s) in which you have lived and for how long. Use the space provided below.\nHave you ever travelled outside of the United States? (Y/N): ____________\nIf so, please identify all of the countries in which you have travelled. Use the space provided below.\nOn a scale from 1 to 10, where 1 means “working-class” and 10 means “upper-class”, circle the value which best characterizes your socio-economic background.\n\n\n\n(working-class)\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n(upper-class)\n\n\n\nDo you have a part-time job? (Y/N): ________\nIf so, approximately how many hours per week do you work? __________ hours/week\nIn your childhood home environment, how many people were normally living together? Include yourself, parents, siblings, relatives/others in your tally: ________\nIn your childhood home environment, how many people were involved in regular music-making activities? Include yourself, parents, siblings, relatives/others in your tally: ________\nIn your childhood home environment, how many people took formal music lessons? Include yourself, parents, siblings, relatives/others in your tally: ________\n\n\n\n\n\nWhat is your major? __________________________________________________\nHave you ever taken private singing lessons? (answer YES or NO): ________\nIf YES, at what age did you begin lessons? ________ years of age\nIf YES, how many years of lessons have you taken? ________ years in total\nHow would you characterize your voice? (soprano, alto, baritone …) __________\nAre you an instrumentalist who cannot/does not sing? (answer YES or NO): ________\nAre your currently a participant in a choir? (answer YES or NO): ________\nDo you frequently sing in the shower or when bathing? (answer YES or NO): ________\nDo you frequently sing along with recorded music? (answer YES or NO): ________\nThis question pertains to instrumental music only. In this question, we’d like you to identify any/all instruments that you play. For all instruments you may play, please tell us the age when you began playing the instrument, the age at which you began lessons on the instrument (if applicable), and the total number of years of instruction (if applicable). Begin with your most proficient instrument.\n\n\n\nInstrument\nAge you began playing\nAge you began lessons\nTotal years of lessons\n\n\n________________ ________________\n________________ _____________\n__\n\n\n\n________________ ________________\n________________ _____________\n__\n\n\n\n________________ ________________\n________________ _____________\n__\n\n\n\n________________ ________________\n________________ _____________\n__\n\n\n\n\nAt what age did you decide you wanted to pursue a music degree at university?\n__________ years old.\nIf you were not enrolled in a music program, what other area of study would you pursue?\n\nExcluding ensemble rehearsals, roughly how many hours per week to you practice? (include all instruments/voice): ________ hours\nRoughly how many hours per week do you rehearse with ensembles? ________ hours\nRoughly how many hours per week do you engage in music rehearsals/practising that are unrelated to your OSU music program? ________ hours\nIn the past two years, and excluding OSU music assignments, have you ever transcribed (notated from sound) a musical work? (answer YES or NO): ________\nDo you have perfect pitch? (answer YES or NO): ______________\nIf you have perfect pitch, is it restricted to a particular instrument?\n(answer YES or NO): ________; If YES, identify the instrument(s): ____________________\n\n\n\n\nThe following questions are intended to inform us about your listening habits and musical preferences.\n\nHelp us to identify the source of most of your listening. For each of the following sources, please indicate the number of hours per week of listening. Exclude practising and rehearsals.\n\n\n\nsource\nhours each week\n\n\nRadio\n__________ hours/week\n\n\nMTV/videos\n__________ hours/week\n\n\nCD/tape/stereo\n__________ hours/week\n\n\nLive music\n__________ hours/week\n\n\nInternet audio\n__________ hours/week\n\n\nLibrary/reserves\n__________ hours/week\n\n\nOther\n__________ hours/week\n\n\n\nExcluding practising, where do you do most of your listening? Please indicate the percentage of your listening the occurs in each of the following locations:\n\n\n\nlocation\npercentage of listening\n\n\nat home\n________ %\n\n\nautomobile\n________ %\n\n\nat a friend’s\n________ %\n\n\nat church/temple\n________ %\n\n\nschool\n________ %\n\n\npub/club/bar\n________ %\n\n\ngym\n________ %\n\n\nother\n________ % please specify location __________\n\n\n\nWhere do you do most of your practising?\n\n\n\nlocation\n\n\npercentage of practising\n\n\nat home\n\n\n________ %\n\n\nat school\n\n\n________ %\n\n\nother\n\n\n________ % please specify location __________\n\n\n\nWhat percentage of your total listening coincides with other (non-listening) activities? (Exclude practising and rehearsing.) Circle one.\n(listen without other tasks)\nHow many CDs/tapes/records do you think you own? __________ recordings\nOn average, how many music recordings do you acquire per year? __________ recordings\nDo you download music from the web? (answer YES or NO): ________\nExcluding rehearsals and practising, do you listen to MORE or LESS music now than you did before attending university? (answer MORE or LESS): ________\nAt what age do you think you listened to the most amount of music? ____________ years old\nHow similar would you say your musical tastes are to the musical tastes of the GENERAL POPULATION? Answer on a scale of 1 to 10 where 1 indicates “completely different” and 10 indicates “almost identical”: ————- – — – — – — – — – — – — – — – — – — – —- – ———– (different) 1 2 3 4 5 6 7 8 9 10 (similar) ————- – — – — – — – — – — – — – — – — – — – —- – ———–\nHow similar would you say your musical tastes are to the musical tastes of your biological MOTHER? Answer on a scale of 1 to 10 where 1 indicates “completely different” and 10 indicates “almost identical”:\ncheck box if question not applicable [ ]\n\n\n\n(different)\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n(similar)\n\n\n\nHow similar would you say your musical tastes are to the musical tastes of your biological FATHER? Answer on a scale of 1 to 10 where 1 indicates “completely different” and 10 indicates “almost identical”:\ncheck box if question not applicable [ ]\n\n\n\n(different)\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n(similar)\n\n\n\nHow similar are your musical tastes to the musical tastes of your closest MALE friend? Answer on a scale of 1 to 10 where 1 indicates “completely different” and 10 indicates “almost identical”: ————- – — – — – — – — – — – — – — – — – — – —- – ———– (different) 1 2 3 4 5 6 7 8 9 10 (similar) ————- – — – — – — – — – — – — – — – — – — – —- – ———–\nHow similar are your musical tastes to the musical tastes of your closest FEMALE friend? Answer on a scale of 1 to 10 where 1 indicates “completely different” and 10 indicates “almost identical”: ————- – — – — – — – — – — – — – — – — – — – —- – ———– (different) 1 2 3 4 5 6 7 8 9 10 (similar) ————- – — – — – — – — – — – — – — – — – — – —- – ———–\nPlease tell us about your favorite music.\nPlease tell us about any music you consider really bad."
  },
  {
    "objectID": "emp_methods_workshop/questionnaires.html",
    "href": "emp_methods_workshop/questionnaires.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Designing Questionnaires\n\nA common approach to survey research is the questionnaire. Questionnaires are more formal than interviews, but less formal than experiments. At the formal end, questionnaires can morph into experiments. At the informal end, questionnaires can morph into open interviews.\nSurveys may be conducted using printed documents, web-based questionnaires, or oral surveys conducted in person or via telephone. When the responses are hand-written or typed (on a computer), the survey is usually referred to as a questionnaire.\n\nQuestionnaire Advice\nThe most common problems with questionnaires: (1) lack of clarity about the purpose, (2) unrepresentative sampling, (3) leading questions, and (4) uncontrolled demand characteristics.\n\nEstablish the type of study. Before beginning a survey/questionnaire, decide what kind of study you are proposing. Is this an exploratory study, a descriptive study, or a correlational study? Though rare, it is possible to conduct a bona fide experiment using a survey/questionnaire method in which the experimenter manipulates some variable.[1] In general, surveys are exploratory or correlational.\nAre you testing an a priori hypothesis? For example, you might have the hypothesis that people who enjoy non-Western music exhibit greater “openness” as a personality trait. In this case you will need to include questions from a personality inventory that assess “openness” as well as questions that determine musical tastes and listening habits. Your questionnaire would therefore be a correlational study testing an a priori hypothesis.\nDo not begin designing the survey or questionnaire until you have a clear idea about the goals. Do not distribute a questionnaire before you have a clear plan for how you plan to analyze the data.\nLimit the number of questions. Don’t squander the patience of your respondents by asking too many questions.\nIf the questionnaire is intended to test one or more hypotheses then include only those questions necessary for testing. Avoid the temptation to add questions for vague or nebulous reasons.\nIn the case of exploratory studies there is more latitude to ask questions. Since the purpose of an exploratory study is to invite new ideas and unanticipated observations, it is appropriate to “go fishing”—asking questions simply to see what happens. Nevertheless, avoid squandering the patience of your respondents.\nUse lure questions to reduce demand characteristics. In some cases, the researcher may be concerned that respondents will infer the purpose of the questionnaire—introducing unwanted demand characteristics. It is sometimes useful to add “lure questions” whose role is to prevent respondents from guessing the true purpose of the study. A lure question is a question that is utterly irrelevant to the purpose of the study — a question that is intended to make respondents think that the purpose of the study is something else. For example, you might be interested in differences in listening behaviors between males and females. The structure of the questionnaire might make it easy for people to see that that is the purpose. Consequently, you might consider asking questions about pets, or travel experiences, or some other irrelevant topic in order to reduce the likelihood that participants will guess the true goal of the study. Lure questions are a form of mild deception.\nStart with easy questions. Respondents can become frustrated with a questionnaire or survey, and this may cause them to abandon the task before completion. A helpful strategy is to place easy questions at the beginning of the questionnaire. For example, asking a person’s age, sex, native language, country of birth, etc. As respondents answer more questions, they will tend to feel committed to the task, and so feel a greater sense of obligation to finish the questionnaire.\nGive feedback about the length. Respondents can become frustrated if a questionnaire goes on and on. It helps for respondents to feel a sense of progress. In the case of printed questionnaires, respondents can see how many pages/questions remain. With web-based surveys, it is helpful to include an indicator of progress. This might be expressed in words (e.g. “Page 3 of 6”) or as a horizontal bar-graph showing how much has been completed and how much remains. In the case of live or telephone surveys, this may be expressed verbally by the interviewer: E.g. “Just four more questions to go …”\nLeave difficult questions toward the end. Respondents are more likely to complete a survey if the difficult questions are posed at a time when they feel the questionnaire is nearly complete.\nAvoid leading questions. Formulate questions in a neutral way that doesn’t favor particular answers.\nPretest and pilot the questionnaire. First, pretest the questionnaire by soliciting feedback from friends and colleagues. Ask them what they think the purpose is? This will help alert you to any unwanted demand characteristics. Note that demand characteristics can cause problems, even for exploratory and descriptive studies that aren’t motivated by an a priori hypothesis. After soliciting this initial feedback, pilot the questionnaire using a small subset of the intended population. This will help you identify possible problems before distributing the questionnaire to the entire sample of interest.\nClosed and open questions. Closed questions provide a limited set of response options whereas open questions invite narrative comments. Closed questions are easier to code and analyze, but they reduce the possibilities for more nuanced answers. Open questions invite new and unanticipated information. Open questions are especially appropriate in reconnaissance, exploratory, and descriptive studies.\nRespondents tend to prefer closed questions since they are (typically) faster to respond to. However, if closed questions are poorly formulated, respondents are likely to be confused about how to answer (see below). When a participant is stumped by a closed question they are likely to wonder about the purpose of the questionnaire; some understanding of the purpose will help them resolve how to answer the question. Notice, however, that this sort of thinking will encourage demand characteristics. In short, when closed questions are confusing, they tend to increase demand characteristics.\nA useful way of reducing confusion about closed questions is to offer an “Other” response, with an opportunity for the respondent to provide further explanation. E.g.\n▢ Option #1 ▢ Option #2 ▢ Other (please explain:) ______________________________\nScrutinize each question. After assembling a questionnaire, get in the habit of “question scrutinizing.” For each question provide a written answer to the following questions: “1. Who would have trouble answering this question?” In the case of closed questions, also ask: “2. For whom would all of the choices be inappropriate?” For example, you may have a question about marital status with two choices: married or single. Ask yourself “Who would have trouble with this question?” Suppose a person was technically married, but had lived as a single person for the past decade. How should they answer this question? You might consider adding a third category: married, single, or divorced/separated Once again, ask yourself the question:Wwho would have trouble answering this question? For example, a divorced-and-remarried person might wonder whether to answer divorced/separated or married.\nAfter scrutinizing each question, you may nevertheless decide to keep the question as it is. For example, you may conclude that although the question has some ambiguity, a less ambiguous form would be too complicated. Nevertheless, question-scrutinizing is a useful exercise in alerting you to possible problems. Writing down your answers is important because this task will slow you down and help you think carefully about each question.\nUse ordinary language. Avoid fancy vocabulary and complex grammar. Aim for an everyday conversational style. Try to avoid questions that make especially refined distinctions: these may be confusing for many respondents.\nAvoid answer inertia. Respondents often exhibit “answer inertia” in which they tend to answer “yes” to every question or “no” to every question. For example, an impatient respondent may simply feel in a “no” mood. In order to avoid this, modify the questions so that most respondents are likely to answer with a mixture of affirmative and negative responses. Mixing questions in this way will also tend to reduce acquiescence bias.\nUse converging multiple questions for important measures. For important measures, aim to include several slightly different operationalizations that ultimately produce converging results.\nSuppose, for example, that a researcher wants a measure of the degree of musical interest for each respondent. One question might be:\nIdentify the phrase that best describes your level of musical enjoyment: ▢ experience relatively little enjoyment from music ▢ find music mildly enjoyable ▢ get quite a bit of enjoyment from music ▢ am a music lover ▢ am a passionate music nut\nA later question might address the level of musical interest differently:\nCompared with most people, how would you compare your musical interest? ▢ I am much less interested in music than most people. ▢ I am a little less interested in music than most people. ▢ I have about the same interest in music as most people. ▢ I am a little more interested in music than most people. ▢ I am much more interested in music than most people.\nYet another question might address the level of musical interest using a different approach:\nHow important is music? ▢ music is absolutely essential ▢ music is important, but not essential ▢ music is one of the many good things in life ▢ music is over-rated\nThe three questions address slightly different things. The first question relates to musical enjoyment; the second question relates to musical interest; and the third question relates to the importance of music. Notice that a person might cogently respond by claiming to find music intensely enjoyable (“a passionate music nut”), have only moderate interest in music (“I have about the same interest in music as most people”), and regard music as ultimately unimportant (“music is over-rated”). This is a theoretically tenable view. Nevertheless, we would predict that the answers to these three questions will tend to correlate: someone who regards him/herself as a music lover, will be more likely to believe he/she enjoy music more than most, and to believe that music is relatively important. Conversely, someone who regards him/herself as having little interest in music, is also apt to believe that he/she is less interested in music than most people and that music is over-rated in importance.\nAs with all operationalizations, the researcher has not captured the essence of the intended theoretical term. All are approximations of the true concept. In the analysis, we would check to see that the various operationalizations correlate positively. If so, we might then combine the results from the pertinent questions into a single measure that we hope captures well the theoretical concept of interest.\nAvoid conceptually mixed labels. Do not label response scale using different concepts for the endpoints. In general, avoid inexact antonyms. The following are bad uses:\n\n\n\nBitter\n◉\n◉\n◉\n◉\n◉\nSweet\n\n\nSad\n◉\n◉\n◉\n◉\n◉\nHappy\n\n\n\nSweet is not the opposite the bitter: we can have experiences that are simultaneously bitter and sweet. Similarly, happy is not the opposite of sad. A person may be unhappy (as, for example, when feeling angry), but that doesn’t mean the person feels sad. In music, it is not entirely clear that consonant is the opposite of dissonant. It is much safer to label bipolar scales with a single term, modified by negatives—such as un-, dis-, no, low or less or amplifier —more, greater, increasing. For example:\n\n\n\nHappy\n◉\n◉\n◉\n◉\n◉\nUnhappy\n\n\nLess Dissonant\n◉\n◉\n◉\n◉\n◉\nMore Dissonant\n\n\nNo Syncopation\n◉\n◉\n◉\n◉\n◉\nHighly Syncopated\n\n\n\nSome bipolar terms may be okay if they are especially clear, such as dark/light (rather than more-dark/less-dark), and warm/cool (rather than cool/less cool).\nLabelled and unlabelled scales. Suppose that our question asks “How often do you listen to music?” Compare the following response modes: ————– ————– ————– ————– ————– always frequently sometimes rarely never\nalways ◉ ◉ ◉ never ————– ————– ————– ————– ————–\nIn statistical terms, the first scale is clearly ordinal: we can’t say that the difference between “always” and “frequently” is the same as the difference between “sometimes” and “rarely.” However, the second scale may be arguably treated as an interval scale. In short, the second scale is likely to provide more statistical power.\nOdd and Even Response Categories. For most questions involving a rating scale, an odd number of response categories is preferred since this allows the respondent to select a middle or neutral response. This is the reason why most response scales involve 3, 5, or 7 positions:\n\n\n\n\n\n\n\n\n\n\nalways\n◉\n◉\n◉\nnever\n\n\n\nThere are times, however, when the researcher would prefer to force the respondent not to sit on the fence. A good example is found in political polling. A given voter may be genuinely conflicted about which candidate to vote for. However, if this person votes, then they will ultimately be forced to make a choice. A political pollster will be interested in forcing the respondent not to sit of the fence. Using an even number of response points will force the respondent to tip his/her hand, and show which direction they are leaning:\n\n\n\n\n\n\n\n\n\n\n\nPrefer Democratic candidate\n◉\n◉\n◉\n◉\nPrefer Republican candidate\n\n\n\nOrder effects. The order of questions can influence how people respond. Consider how changing the order of questions might lead respondents to think in different ways.\nPartition Dependence. In multiple-choice questions, the number of categories is known to influence how participants respond. Suppose, for example, that your survey asks musicians how long they practice each day. Consider two different versions, a 3-choice version:\n▢ Less than 30 minutes per day ▢ Between 30 and 60 minutes ▢ Over 1 hour\n… and a 5-choice version:\n▢ Less than 30 minutes per day ▢ Between 30 and 60 minutes ▢ Between 1 and 2 hours ▢ Between 3 and 4 hours ▢ Over 4 hours\nYou might think that how you frame the question won’t change the answers. Surely, just as many people will choose the first and second choices whether you use the 3-choice version or the 5-choice version. However, research has established that increasing the number of choices changes the behavior of respondents. More people will select the first and second choices in the 3-choice version than in the 5-choice version (See, Fox & Rottenstreich, 2006). That is, offering more choices will tend to spread out the responses, even when some of the categories are logically identical. This phenomenon is referred to as partition dependence. One way to reduce partition dependence is to ask respondents to provide their own number:\nHow long do you typically practice each day? ____________\nAvoid Anchoring Effects. People tend to be unduly influenced by any number or quantity they encounter. For example, participants in an experiment by Tversky and Kahneman observed a roulette wheel that stopped at either a low number (like 10) or a high number (like 65). They were then asked to guess the proportion of nations that are located in Africa. Those who had observed a high roulette number guessed almost twice the proportion of African nations compared with those who had observed a low roulette number. Many other experiments have demonstrated this anchoring effect. Mention a single number or range, and that will unduly influence participants responses.\nConsider once again a survey that asks musicians how long they practice each day. If we ask: How many hours per day do you practice? we are apt to get longer responses than if we ask: How many minutes per day do you practice? In short, the “range” words ‘minutes’ and ‘hours’ act as anchors that will influence the responses.\nConsider using more than one survey. Rather than putting everything in a single survey, consider using multiple surveys. For example, a between-subjects design may prove better than a within-subjects design, especially in reducing possible demand characteristics. This can also be done with surveys by distributing different surveys to two or more groups of respondents. As in the case of other areas of research, it is often helpful to pursue an explore-then-test approach. An initial survey can be used to alert the researcher to particular relationships. Conjectures and hypotheses inspired by the first survey can then be explicitly tested in a second follow-up survey.\nSolicit general comments. It is typically the case that completion of a survey is voluntary. It is important to ask yourself why someone would have taken the time to fill-out your questionnaire. Some people are simply more altruistic and cooperative; they may complete a questionnaire out of feelings of good citizenship. Other people may have an axe to grind. They have their own agenda in completing the survey. They may be unhappy about the high prices for concert tickets, or want to encourage more music-making in the schools. It is helpful to include a “general comments” section at the end of the survey and to make it clear at the beginning of the survey that general comments will be invited at the end of the survey.\nThe inclusion of an open-ended “general comments” section serves three purposes. First, knowing that there is a general comments section may encourage a respondent to complete the survey—even if they regard the closed questions ill-conceived or irrelevant. Second, the general comments can alert you to unanticipated demand characteristics. Third, the general comments can be a source of ideas for further research.\nMake sure you alert respondents at the beginning that general comments will be welcome at the end. For example, you might include the following statement in your instructions:\n“This questionnaire consists of 20 questions, and will take an estimated 8 minutes to complete. General comments are invited at the end of the questionnaire.”\nUse green for printed questionnaires. Oddly, research has shown that people are more likely to respond to a questionnaire if the paper is green in color.\nCraft an appropriate title. Give your questionnaire a title that helps in recruiting participants from the target population, while avoiding creating demand characteristics that confound the intended purpose.\n\n\n\nReferences:\nJean Converse and Stanley Presser (1986), Survey Questions: Handcrafting the Standardized Questionnaire. Sage Publishing: Beverly Hills, California.\nKelly See, Craig Fox, and Yuval Rottenstreich (2006). Between ignorance and truth: Partition dependence and learning in judgment under uncertainty. Journal of Experimental Psychology: Learning, Memory and Cognition, Vol. 32, pp. 1385-1402.\n\n\nFootnote:\n[1] For example, the questionnaire might begin with a printed story in which different groups of respondents receive different versions of the story. Subsequent questions can test a hypothesis about the influence of different ways of presenting the story."
  },
  {
    "objectID": "emp_methods_workshop/hindsight_task2.html",
    "href": "emp_methods_workshop/hindsight_task2.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Task\nIn the previous group task you were given a series of purported findings from The American Soldier study and asked to suggest theories that might account for the results. In fact, the “findings” in the previous task were precisely backward. The actual findings are given below. For each finding, suggest a theory that might account for the result.\n\nBetter educated soldiers suffered fewer problems in adjusting to military life than less educated soldiers.\n\n\n\nSouthern soldiers coped worse with the hot South Sea Island climate than northern soldiers.\n\n\n\nBlack privates were more eager to be promoted to noncommissioned officers than white privates.\n\n\n\nSouthern blacks preferred northern white officers over southern white officers..\n\n\n\nAs long as the fighting continued, soldiers were eager to return home. Once the war ended, soldiers felt more urgency to return home."
  },
  {
    "objectID": "emp_methods_workshop/theory2conjecture.html",
    "href": "emp_methods_workshop/theory2conjecture.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Introduction\nRecall that theories posit some sort of causality: such-and-such is the case because of …\nTheories are, in effect, stories of a certain sort. As researchers, we’re interested in assessing the persuasiveness of different theories. We test theories by determining how well observations conform (or are consistent with) the theory.\nTheories are not always clearly stated. Moreover, whole theories are often difficult to test. However, theories typically make a number of assumptions:\ngiven assumption X, and given assumption Y, and given assumption Z, the theory suggests that W is the case.\nRather than testing the entire theory, we might then focus on testing parts of the story—that is, testing one or more of the assumptions on which the theory relies. Consequently, in assessing a theory, an appropriate place to begin is by identifying as many of the underlying assumptions as possible.\n\n\nExample\nConsider the following theory:\nVery little sad music is played on commercial radio because advertisers don’t want their products associated with a sad mood.\nNotice that this theory implies several things.\n\nThe theory assumes that there is such a thing as “sad” music.\nThe theory also assumes that sad music can evoke sad mood in at least some listeners.\nThe theory assumes that feelings of sadness evoked by music can be indiscriminately associated with products that have no relationship to the music apart from a close juxtaposition.\nThe theory assumes that products associated with sad feelings are less likely to be appealing.\nThe theory assumes that advertisers are consciously concerned about the negative potential of sad music on product appeal.\nThe theory proposes that commercial radio stations play little sad music (presumaby compared with non-commercial radio stations).\n\nNotice that any one of the assumptions or presuppositions can be used to generate a conjecture that might ultimately be shaped into a testable hypothesis. For example, one might test whether: Playing of nominally “sad” music has a tendency to put listeners into a sad or negative mood state.\nAs we’ve noted, most theories are difficult to test as a whole. So instead, we typically aim to test various critical components of the theory.\nHaving identified the various assumptions, we can then evaluate them. Some assumptions will seem quite reasonable (grandmother research), so we might give the theory the benefit of the doubt. Other assumptions might be more contentious or questionable. Which assumptions are essential to the theory? If assumption X must be true in order for the theory to be true, and if assumption X is not entirely obvious, then we might focus on testing that assumption.\n\n\nTask\nFor each of the following theories, identify as many assumptions as you can that must be true in order for the theory to be true.\n\nMusicians who play the piano, violin or cello are more likely to win open concerto competitions partly because those instruments afford better opportunities for dramatic performer gestures than other instruments like the bassoon or the French horn.\nIn Western culture, women composers are at a disadvantage because women are less likely to conform to the image of the creative genius.\n\nHaving identified a set of assumptions, identify which assumptions are most critical. Then restate the assumption in the form of a conjecture. Once again, recall that conjectures do not need to be formulated in ways that are easily tested. Our hope is that, having stated a conjecture, we might then consider how it might ultimately lead to forming a (testable) hypothesis.\n\n\nRationale\nThe purpose of this exercise is to help develop skills in identifying potentially testable consequences implied in any theory."
  },
  {
    "objectID": "emp_methods_workshop/feedback6.html",
    "href": "emp_methods_workshop/feedback6.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Feedback - Presenter\n\nThe aim of this feedback is to evaluate the workshop presenter (rather than the workshop content).\n\nHow well did the instructor communicate the workshop objectives at the beginning?\n\nnot very well\nokay\nwell\nvery well\n\nHow successful was the instructor in achieving the workshop objectives?\n\nnot very successful\nokay\nsuccessful\nvery successful\n\nHow knowledgeable was the instructor of the subject matter?\n\nnot very knowledgeable\nokay\nknowledgeable\nvery knowledgeable\n\nHow organized was the instructor?\n\nnot very organized\nokay\norganized\nvery organized\n\nHow clear was the communication?\n\nnot very clear\nokay\nclear\nvery clear\n\nHow responsive was the instructor to participants’ questions and comments?\n\nnot very responsive\nokay\nresponsive\nvery responsive\n\nHow well did the instructor motivate or inspire you to learn?\n\nnot very motivating\nokay\nmotivating\nvery motivating\n\nSuggestions for improvement:"
  },
  {
    "objectID": "emp_methods_workshop/stats_conclude.html",
    "href": "emp_methods_workshop/stats_conclude.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Statistical Tests - General Remarks\n\nStatistics can be used for lots of purposes. As we have seen, a distinction can be made between descriptive statistics and inferential statistics. Examples of descriptive statistics include statements such as: the average German folksong is 23.4 notes in length; nearly a third of Western classical music is written in triple meter; or of all European countries, Italian music makes the least use of the minor mode. Descriptive statistics can be useful, but they have a certain “so what?” quality to them. They simply describe something about the world.\nThere is a second type of statistics, called inferential statistics. These statistics are used to test hypotheses or conjectures. They are the most important types of statistics in modern empirical research.\n\nConfidence Level and Statistical Significance\nRecall the concept of the confidence level. The confidence level is the line-in-the-sand drawn by the researcher that indicates when the research admits defeat. It is expressed as a percentage, e.g. 95% confidence level. Related to the confidence level is the significance level. The significance level is reciprocally related to the confidence level. It is expressed as a probability rather than a percentage. A 95% confidence level corresponds to a .05 significance level. A 99% confidence level corresponds to a .01 significance level. The significance level is symbolically represented by the value alpha (α). Formally, the significance level is the probability of rejecting a true null hypothesis.\n\n\nStatistical Tests\nOver the past century or so, statisticians have devised a number of statistical tests. Examples of statistical procedures include Student’s t-test, Wilcoxon signed rank test, the chi-square test, Kruskal-Wallis test, and the Kolmogorov-Smirnov test. There are also various statistical values that can be computed, such as Pearson’s r, Kendall’s tau, and Spearman’s rho.\nWhy, we might ask, are there so many statistical tests? The answer is that each test is specialized for a different data situation. Three conditions are especially important: (1) the type of measurement scale, (2) the distribution of the data, and (3) the data-gathering conditions.\nIn the first instance, recall that there are different measurement scales: nominal, ordinal, interval and ratio. For example, Pearson’s r is a good way to calculate correlations, but only for interval and ratio scales. If the data are ordinal (1st, 2nd, 3rd, etc.) then one should use Spearman’s rho instead.\nA second consideration is the distribution of the data. There are many different kinds of distributions. For example, many types of data are normally distributed. That is, the data exhibit a familiar bell-shaped curve where most of the values reside in near the center. Sometimes the data exhibit a uniform distribution, with each value equally likely (as in the case of numbers on a roulette wheel). Another kind of distribution is the bimodal distribution. Here, the data exhibit two peaks rather than one. The pitch distribution for a group of people exhibits a bimodal distribution with the lower peak corresponding to the pitch of the average male voice, and a higher peak corresponding to the pitch of the average female voice.\nData may conform to any of a large number of distributions, such as a logarithmic distribution, a chi distribution, a binominal distribution, or a normal distribution. When the distribution is known, there are mathematical tricks that allow statisticians to infer various properties about the data. Most of these tricks have been developed for the normal distribution, so when the data is normally distributed, it is possible to use more powerful statistical tools. However, in many research situations, we don’t have any idea how the data are distributed.\nStatistical tests differ in a number of other ways. For example, some are used to compare ratios, some are used to compare means, some are used to compare variances, etc. In addition, there are different statistical tests that are employed depending on the manner in which the data are collected. For example, different tests may be required depending on whether the experiment employs a within-subjects or between-subjects design. For each combination of different conditions, there is an appropriate statistical test.\nEach statistical test is said to employ a “model.” Each model makes various assumptions about the data. For example, all statistical tests assume that the data are independent. In order for the test to be legitimate, the data must conform to the assumptions (or “model”) for that test.\n\n\nParametric vs. Non-parametric Tests\nAs noted, when the distribution of the data is known, various mathematical tricks can be used and statistical procedures can be used that extract more information out of the data. Statistical tests based on known distributions are referred to as parametric tests.\nIt is common, however, for the researcher to have no knowledge about the distribution of the observed data. When the distribution is unknown, we employ so-called non-parametric tests. Non-parametric tests cannot squeeze as much information out of a data set as parametric tests. However, non-parametric tests are appreciated because they can be used for a much wider range of types of data. Non-parametric tests make no assumptions about how the data are distributed. Because non-parametric tests make fewer assumptions about the data, they are regarded as conservative tests. Examples of non-parametric tests include the chi-square test, the Siegel-Tukey test, Spearman’s rank correlation coefficient, Wald-Wolfowitz runs test, and the Wilcoxon signed-rank test.\nWhen using a non-parametric test, there is less chance of violating one of the assumptions of the statistical model behind the test. Consequently, the researcher is less likely to get into trouble when using a non-parametric test. If there is some uncertainty about which parametric test to use for a given set of data, the researcher may elect to use a non-parametric test instead. Especially if the results prove statistical significant, the non-parametric test is clearly adequate. However, using the appropriate parametric test is more likely to produce a significant result—compared with a non-parametric test.\n\n\nStatistical Significance\nA statistical test involves calculating the probability of observing our data, or data more extreme than our data, assuming that the null hypothesis is true. This probability is referred to simply as p. The calculation of p is the most important part of a statistical test.\nIn most research, the researcher hopes that the research hypothesis is true (i.e., the data will be consistent with the research hypothesis). This means that the researcher hopes that the null hypothesis can be rejected. Accordingly, we are typically looking for a small probability in favor of the null hypothesis—that is, we are looking for a small value for p.\nA result is said to be “statistically significant” when the value of p is less than the significance level, alpha. For example, if our significance level (α) is .05 (corresponding to a 95% confidence level), then a p less than .05 means the results are “statistically significant.” If our significance level (α) is .01, then p must be less than .01 in order for the results to be statistically significant.\n\n\nDegrees of Freedom\nIn research, we commonly use a sample to infer some property of a population. Recall that the law of large numbers states that as the sample size increases, the sample is more likely to provide a more accurate estimate of the true population value. The greater the number of observations, and the greater the number of ways values are free to vary, the greater the likelihood that our results are representative. In statistics, the number of ways that values are free to vary is referred to as the degrees of freedom (abbreviated df). Degrees of freedom are related to, but not the same as, the number of observations or the number of data categories. Different statistical tests interpret degrees of freedom differently. In all cases, the degrees of freedom plays a pivotal role in calculating p.\n\n\nReporting\nWhen reporting the results of a statistical test, two forms of reporting can be distinguished: formal and informal. A formal statement might look like this:\nIn comparing the tempo of minor-mode works with major-mode works, we carried out a matched-pairs t-test, whose results permit rejection of the null hypothesis at the prior established 95% confidence level (t=14.93; df=88; p=.0003). Consequently, the observations are consistent with the alternative hypothesis, that the tempo of minor-mode works tends to be slower than major-mode works.\nThis formal statement reports seven pieces of information: (1) It identifies the type of statistical test carried out (in this case, a matched-pairs t-test). (2) It reminds readers of the confidence level (95%)—which would have been identified earlier in the article, prior to any data collection. (3) It reports the value of the statistic, in this case t (t=14.93). (4) It identifies the degrees of freedom, abbreviated df (88). (5) It reports the calculated value of p (.0003). (6) It reminds the reader that the statistical test is a test of the null hypothesis (that there is no difference in tempo between major- and minor-mode works). (7) It states that, having rejected the null hypothesis, we accept the alternative (or research) hypothesis, namely, that the tempo of minor-mode works tends to be slower than major-mode works.\nThis sort of formal reporting is almost never used in contemporary empirical research publications. Instead, researchers prefer a shortened, less formal, way of reporting statistical tests. Here are a couple of examples:\nWe found a significant correlation between size and shape (r=0.83; df=80; p<0.0001).\nThere appears to be no relationship between stem length and notehead slant (Kendall’s T=0.98; df=142; p=.73).\nSometimes, the researchers will remind the reader of the presumed a priori confidence level.\nThe results are consistent with the hypothesis at the 95% confidence level (χ2=4.9; df=2; p=0.02).\nThis might seem gratuitous because it is rare that any research report will state a priori the selected confidence level before collecting their data. The 95% confidence level has become so commonplace in research that it is assumed to be THE minimum criterion for statistical significance. In other words, common research practice has tended to “fix” the line-in-the-sand. Statisticians remind us that there is nothing magical about the 95% confidence level. A researcher is free to choose whatever confidence level they deem appropriate (according to the moral repercussions of making a Type I or Type II error). However, the 95% confidence level has simply become a sort of “social norm” for research.\n\n\nMore About Reporting\nNearly all statistical tests make use of the following: (1) confidence level, (2) significance level (α), (3) calculation of an appropriate statistic, (4) calculation of p, (5) degrees of freedom, and (6) effect size.\nThe word “significant” should not be mistaken for “important.” Something can be statistically significant, but not especially important. In the past, only a minority of research papers reported the effect size.\nBy way of illustration, there is a small but statistically significant association between living under a high-voltage electrical transmission tower and death from leukemia. However, the effect is miniscule. In developed countries, a person is roughly 10,000 times more likely to die from an automobile accident.\nExamples of statistics related to effect size include: r2 (in correlation), R2 (in multiple regression), φ phi (in the chi-square test); and Cohen’s d.\nIn statistics related to Pearson’s correlation coefficient, we are interested in five interrelated values: r, df, α, p and r2. We interpret all five values with respect to each other:\n\nWhat is the correlation (r)? Is the correlation big or small, positive or negative? Recall that correlation values range between -1 and +1. We cannot take the correlation coefficient at face value: r values may be real or spurious. We need to interpret r with respect to other statistical values.\nWhat are the degrees of freedom (df)? Were there lots of observations, or just a few? Note that degrees of freedom are related to (though not exactly the same as) the number of observations. Degrees of freedom range between zero and infinity.\nWe use the correlation value (r) in conjunction with the degrees of freedom (df) to calculate the probability (p) that the relationship might arise by chance. p values range between 0 and 1.\nThe significance level (α - alpha) is where the researcher draws a line in the sand and says “If the probability of observing this relationship by chance is above this value, then I’m going to accept that the correlation could simply arise by chance, and therefore I will admit defeat—that my hypothesis is problematic.” Remember that the significance level must be set before you calculate p. (“The best research invites failure.”)\nNotice that we never test our experimental hypothesis directly. Instead, we test whether the results look like they could arise simply by chance. (“Aim not to be right, but to be not not right.”)\nIf the p value is bigger than the a priori significance level, then the correlation value (r) is considered spurious—we are simply looking at “noise” (i.e., natural variation). However, if the p value is smaller than the a priori significance level, then the correlation value has a high likelihood of being real.\nIf our correlation is considered statistically significant, we can now calculate the effect size (r2). The r-squared value tells us how much of the variability in one variable can be attributed to the other variable. This value ranges between zero (no effect) and one (all of the variability is accounted for).\n\n\n\nSeeking Help\nAcquiring an appropriate level of knowledge about statistics is one of the empirical researcher’s foremost challenges. Statistics is a rich and powerful field of knowledge. At times, statistics can seem magical—pulling useful information out of observations that otherwise appear to be a confusing mess. For the novice, the whole enterprise can seem intimidating. As you gain experience, you will become more comfortable with statistics. There are excellent software packages that will do all of the complicated calculations. But you will need advice about the most appropriate statistical test to use, given your project. The best advice for the novice researcher is to seek help:\nSlogan: Make friends with a statistician.\nEven experienced researchers appreciate the opportunity to chat with a statistician. Most important, talk with a statistician before you collect any data. Describe what you are planning to do and listen carefully to the advice. Statistical consultants are thrilled when people come and talk with them before the data are collected. Take advantage of their expertise."
  },
  {
    "objectID": "emp_methods_workshop/feedback4.html",
    "href": "emp_methods_workshop/feedback4.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Feedback - Day 4\n\n\nFirst, tell us about the speed or pacing of today’s workshop:\n\ntoo slow\na bit slow\nabout right\na bit fast\ntoo fast\n\nSecond, tell us about the level of difficulty. Overall, I found today’s workshop material:\n\ntoo easy\na bit easy\nabout right\na bit challenging\nsomewhat challenging\ntoo hard\n\nHow useful was each topic? (Circle 1, 2 or 3 stars — where three stars is best.)\n\n★ ★ ★ Tests, instruments & batteries\n★ ★ ★ Floor & ceiling effects\n★ ★ ★ Writing an empirical paper\n★ ★ ★ Economics of research\n★ ★ ★ Reserved data sets\n★ ★ ★ Methodological potholes\n★ ★ ★ Research creativity\n\nOverall, I found today’s group activities:\n\nof limited use\nokay\nuseful\nvery useful\n\nTell us the most important thing you learned today.\nOf all the material we covered today, the least useful topic was:\nGeneral suggestions (on reverse):"
  },
  {
    "objectID": "emp_methods_workshop/hearing_screening.html",
    "href": "emp_methods_workshop/hearing_screening.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Instructions\nThis questionnaire deals with a number of common situations. For each question you should select the response that describes you and your behaviors best. You can select from among the following response alternatives;\n\n\n\nNever (or almost never)\nSeldom\nOccasionally\nFrequently\nAlways (or almost always)\n\n\n\nSimply circle the letter that corresponds to the first letter of your choice. (If you normally use a hearing aid, answer as if you were not wearing it.)\n\nAre you ever bothered by feelings that your hearing is poor? N S O F A\nIs your reading or studying easily interrupted by noises in nearby rooms? N S O F A\nCan you hear the telephone ring when you are in the same room in which it is located? N S O F A\nCan you hear the telephone ring when you are in the room next door? N S O F A\nDo you find it difficult to make out the words in recordings of popular songs? N S O F A\nWhen several people are talking in a room, do you have difficulty hearing an individual conversation? N S O F A\nCan you hear the water boiling in a pot when you are in the kitchen? N S O F A\nCan you follow the conversation when you are at a large dinner table? N S O F A\n\n\n\nswer these\nquestions usin\ng\n\n\n\n\n\n\nGood\nAverage\nSlightly below average\nPoor\nVery poor\n\n\n\nOverall, I would judge my hearing in my RIGHT ear to be G A S P V\nOverall, I would judge my hearing in my LEFT ear to be G A S P V\nOverall, I would judge my ability to make out speech or conversation to be G A S P V\nOverall, I would judge my ability to judge the location of things by the sounds they are making alone to be G A S P V"
  },
  {
    "objectID": "emp_methods_workshop/revision_advice.html",
    "href": "emp_methods_workshop/revision_advice.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "How to Revise a Manuscript in Response to Reviewers’ Comments\n\n\nThe Review Process\nFor most journals, when a manuscript is received, the Editor examines the subject matter, and assigns the paper to one of the Associate Editors affiliated with the journal. At this point, the assigned Associate Editor becomes the Action Editor for that particular manuscript. In some cases, the Editor will assume the role of Action Editor for a given submission.\nThe Action Editor is typically involved in selecting and recruiting reviewers to review your manuscript. Potential reviewers usually receive just the title and abstract of the manuscript, and are asked to agree/disagree to review the submission on the basis of that information. In some cases, potential reviewers may receive the entire manuscript before having to make a decision whether or not to participate in the reviewing process. Once the reviewers have agreed to work on the manuscript, the Action Editor is responsible for encouraging the reviewers to complete the task.\nReviewing can be a time-consuming and thankless task. If a journal publishes 20 articles per year and has an 80% rejection rate, then typically 100 submissions are reviewed each year. Since most submissions are reviewed by two or three reviewers, that means that the 20 articles published in a given year represent the tip of an iceberg masking nearly 300 reviews. Reviewing is labor-intensive, and it is all done by volunteers.\nAfter receiving reports from all of the Reviewers, the Action Editor will then read your manuscript, consider the Reviewers’ comments, and form his or her own assessment. The Action Editor will write a letter. The action letter and the reviewers’ reports will then be sent to you. The reviewers’ reports will be labeled “Reviewer #1,” “Reviewer #2,” and so on.\n\n\nThe Reviews\nReading through the Reviewers’ comments, you’ll typically find a range of responses. It is not uncommon for one reviewer to recommend publication while another reviewer recommends rejection. Reviewers may like or dislike the paper for different reasons. When criticisms are offered, the reviewers frequently choose different things to criticize.\nWhen reading Reviewers’ comments, it is common for authors to experience a rollercoaster of emotion. You will celebrate (rare) moments of praise, lament points of (legitimate) criticism, feel embarrassed by (now obvious) lapses, and feel angered by points of (unfair) attack.\nAfter reading the Reviewers’ comments, it is important to return to the Action Editor’s letter. This letter is key.\nFirst, the Action Editor’s letter will communicate a formal decision regarding the manuscript. Most journals allow for four possible conclusions: either the manuscript will be rejected, accepted outright, accepted pending revisions, or the author(s) may be encouraged to revise and resubmit.\nEach of the Reviewers will also have rendered a judgment, but formally, the Reviewers’ comments are simply recommendations to the Action Editor. It is the Action Editor’s decision that counts. Some journals have very specific policies, such as rejecting the paper if any one of the reviewers rejects it. However, there is wide variation in editorial policy, and it is possible that an Action Editor may choose to accept a paper that is rejected by one or more reviewers, or may reject a paper that all of the reviewers praise.\nIf the paper has been rejected then the review process is over. Never attempt to argue with the editorial decision. Reviewing is an unpaid labor-intensive process, and the last thing an Action Editor wants to do is waste further time with a paper that has been deemed inadequate.\nIt is possible that the reviews and action letter suggest that the reviewers have grossly misread your work and have rejected your submission because of some misunderstanding. Even in this case, don’t contact the Action Editor or attempt to clarify the situation. The fact that the reviewers have formed a flawed understanding of your work simply means that your paper must be poorly written. Accept the decision and use the reviewer reports as a guide to how to revise your paper so as to avoid a similar misunderstanding should you submit the work to another journal.\nIn no case should you contact the Action Editor and say something like “Your assessment was based on an incorrect interpretation of my work. No matter: I’ll send my paper elsewhere.” In the long run, you will need to maintain a good working relationship with the people in your field. Drawing attention to their apparent stupidity will not win any friends, especially if their assessment was based on a misconception whose origin is your poor writing. Once again, never attempt to argue with an editorial decision.\nAcceptance outright is not common. Depending on the prestige of the journal, many or most submissions will simply be rejected. That brings the whole process to an end; at this point, you will need to consider alternative journals.\nFor good research (which is what we always aim to do), the most common assessment is either accepted pending revisions or revise and resubmit.\nAs noted, the Action Editor will form his/her own opinion about the quality of your work. In addition, the Action Editor will form an opinion about the value of each of the reviewers’ comments. Do not assume that the Action Editor agrees with each of the remarks made by the various reviewers. It is not uncommon for an Action Editor to think that some reviewer’s remark is off-target, inconsequential, or wrong.\nIn drafting the action letter, the Action Editor must navigate through a delicate social situation. Recall that Action Editors rely on the volunteer labor of reviewers, so they are usually eager to maintain good working relationships. This can be hard to do if the Action Editor sometimes disagrees with a reviewer. Reviewers become unhappy if their advice or assessments are routinely ignored, or considered inconsequential or inappropriate. Suppose, for example, that Reviewer #2 has voiced a “serious” objection concerning X. However, it may be the case that none of the other reviewers mention X, and the Action Editor doesn’t think X is a serious matter. The Action Editor must write a letter that communicates what is important — but at the same time doesn’t offend Reviewer #2.\nAttend carefully to the specific wording in the action letter. The Action Editor is likely to mention several major concerns raised by the reviewers. All of the concerns mentioned in the action letter must be addressed in any revised manuscript. However, if issue X is not mentioned, then it suggests that the Action Editor may disagree with Reviewer #2, and does not consider this an essential issue.\nYou will need to respond to every issue raised in the reviewers’ reports. But you will need to pay particular attention to those issues mentioned in the action letter.\n\n\nHow to Carry Out Revisions\nThe key to revising a manuscript lies not in your manuscript but in your letter of response to the Action Editor. When you resubmit your manuscript, you will need a covering letter. It is this letter that will get your revised paper accepted, not the revised manuscript itself.\nFirst, no one wants to read your paper again. Worse yet, no one want to have to read your revised paper while simultaneously referring to your original submission, while also looking at their original written critique.\nGetting your paper accepted depends on making life easy for the Action Editor and the reviewers. Your covering letter should take the editor and reviewers step-by-step through the revisions, chronicling the specific changes you have made. The aim is to have the editor and reviewers read your letter and conclude that your revisions are acceptable, and that there is no need for them to read the entire manuscript again. (Each time someone reads your manuscript, there is the possibility that they will discover further things to criticize.)\nOnce again, focus on the letter of response, not the manuscript. Your letter of response will typically be more than twice the length of the action letter plus the reviews. If the action letter plus reviews is 6 pages, then your letter of response will likely be at least 12 pages in length.\n\nBegin your revision by copying the action letter and reviews into your draft letter of response. Put all of this material in italics so you can distinguish your writing from the action letter and reviewer reports.\nAddress your letter to the Action Editor:\nDear ______\nThank you for your letter of July 28th concerning my manuscript entitled ________. Enclosed you will find a revised manuscript whose changes follow closely the comments in the action letter, as well as the individual detailed comments in the reviewers’ reports. The revisions are chronicled in detail below. For convenience, I have interspersed each comment with a description of the associated changes to the manuscript.\nRespond to each of the concerns raised by the reviewers in turn. For example, write along the following lines:\nFigure 2 (now Figure 3) has been fully revised in accordance with the comments of Reviewer #3. Specifically, the horizontal axis is now labeled “pitch distance (in semitones)” and the vertical axis includes a zero marking.\nAs suggested by Reviewer #1, I have provided more detail regarding the manuscript sources. Specifically, the following passage has been added on page 8, lines 38-40:\n“The Vivaldi opus 8 concertos survive in four sources: the Turin autograph, the Dresden manuscript, the Amsterdam print, and the Manchester part-books. Although most of the discrepancies are minor, there are some major deviations, including through-composed sections of up to 24 measures, and other differences that have analytic repercussions.”\nUse page and line numbers to identify each change:\nFormerly pg.7/lines 10-11/Now pg.7/lines 23-24 “Our results establish that …” now reads: “Our results are consistent with the view that …”\nThank the reviewers for good ideas.\nThe results from Experiment #2 have been re-analysed using a MANOVA procedure as suggested by Reviewer #1. I am pleased to report that the main effect remains statistically significant. However, the interaction between intraversion and preference ratings is no longer significant. The discussion has been revised accordingly (formerly page 13; now page 14).\nMy thanks to Reviewer #1 for recommending the MANOVA analysis.\nA good reviewer’s report will begin with a paragraph that summarizes your work. Sometimes a reviewer will misunderstand the purpose or nature of your work, and that will be evident in the summary or other points in the review. Since the reviewer is likely to be a respected scholar in the field, if the reviewer is confused, then it is very likely that many readers will find your work confusing. Take this as your cue to improve the paper. Say something like the following:\nIn light of the summary provided by Reviewer #2, it is apparent that the manuscript has led to confusion regarding the motivation for this study. Accordingly, I have revised the Introduction (pages 1-4) in order to better clarify the background and aims. Specifically, …\nYou don’t need to necessarily chronicle each of the minor changes recommended by a reviewer. In your letter of response, you may say something like:\nRegarding the “Minor Issues” section in the report by Reviewer #1, all of the recommended changes have been made.\nOtherwise, chronicle each of the changes you make to the manuscript in your letter of response.\n\nIn general, use your letter of response as your guide for making revisions. Write the letter of response first in order to clarify precisely the changes that need to be made in order to satisfy the Action Editor and reviewers. Don’t revise the paper and then later chronicle the changes in the letter. The key to successful revision is the letter of response.\n\n\nPick Your Fights\nInevitably, there will be criticisms made by one or more reviewers with which you disagree. In responding to the Action Editor, there is room to take issue with certain criticisms.\nFirst, avoid beginning your letter by disputing some issue. Use the bulk of your letter to show that you take the reviewers’ comments seriously, and have made a sincere effort to address all of the concerns. If you show that you have accepted and dealt with 90% of the criticisms, then the reviewers and Action Editor are more likely to give ground on the two or three items that you dispute.\nSince the reviewer’s reports often begin with “Major Concerns,” you might want to delay disputing one of these concerns. Say something like the following:\nRegarding Reviewer #2’s Major Concern #1, I will address this later in the letter (page 8).\nSecond, pick your fights. You might disagree with a dozen points made by the reviewers. However, you shouldn’t fight all of these at once. If you write several pages describing (in effect) how idiotic the reviewers’ comments are, it will make you look defensive and intransigent. Choose two or three major points of disagreement, and address those. Explain why you consider the criticisms misplaced.\nIt is not uncommon for a manuscript to generate two or three rounds of correspondence with the Action Editor and reviewers. Having resolved certain issues, you may have additional opportunities in subsequent correspondence to take issue with other critical points. However, each letter should demonstrate your sincere efforts to modify your manuscript in accordance with the reviewers’ concerns.\nThird, avoid intemperate language. Sometimes a reviewer will make a criticism using harsh language, and you may find the criticism hurtful or even anger-provoking. In your response, always tone-down the language. No matter how inflammatory the criticism, use calm and measured words in your response. Avoid any hint of sarcasm. You will win-over critics by always behaving in a professional manner (even when others fail to do so).\nFor empirically-oriented research, a common point of disagreement relates to the request for more work. It is not uncommon for a reviewer to ask for another follow-up experiment or corpus study. If you agree that an additional study is warranted, and if you have the resources and inclination, by all means, endeavor to fulfill the request. However, it is often the case that requests for additional research are impractical or excessive.\nRequests for addition research can usually be ignored if the request is not mentioned by the Action Editor in the action letter.\nYou may need to remind the reviewer(s) that no research is definitive. In analytic and empirical research there is no such thing as proof. Even if you do a follow-up study, there are surely other studies that could also follow. Your intention is not to bring “closure” to the subject, and you anticipate that other researchers will undoubtedly revisit the issue in the future.\nMoreover, you may have difficulty complying with the request for additional work. You may no longer have access to an essential manuscript or piece of equipment. A collaborator may have moved and it is no longer practical to continue with the collaboration.\nIt may be helpful to mention that you agree with the sentiment that more research is warranted and that the proposed additional study is a good idea. Nevertheless, practical considerations make it difficult to comply with the request. You would encourage the Action Editor to consider whether the work as it stands makes a contribution to knowledge, even without an additional study.\n\n\nThanks\nEnd your letter by thanking the Action Editor and Reviewers for their hard work. Say something like:\nI would like to thank the Reviewers for their many helpful comments and suggestions. I feel that the revised manuscript has greatly benefitted from the Reviewers’ input.\n\n\nSubsequent Rounds\nWhen a paper is revised and resubmitted, the revised paper is almost always reassigned to the original Action Editor. Moreover, the original reviewers will be invited to participate in the subsequent rounds of reviewing. At each stage, the reviewers will have access to each others’ comments as well as the Action Editor’s correspondence with you.\nIt is not uncommon for reviewers to change their minds when they read the assessments of other reviewers. For example, a single negative review might alert the other reviewers to problems they hadn’t thought about. Less commonly, an initially negative reviewer might soften his/her position in light of positive comments by the other reviewers. At the same time, a single negative reviewer is more likely to decline the invitiation to repeatedly review a paper.\nUltimately, take your cue from the Action Editor. If the Action Editor continues to write in a positive or encouraging way, then it may be worthwhile persevering with subsequent rounds of revisions even though one of the reviewers continues to resist approving the paper. Scholars have genuine disagreements about what constitutes quality work. There are famous historical examples of seminal research studies that were repeatedly rejected. Good Action Editors will defend work they think holds merit, even in the face of disagreement. On the other hand, if the Action Editor is not on your side, then it is time to move on.\n\n\nOpting Out\nIn light of an action letter, you might conclude that the requested modifications are simply too burdensome. Accordingly, you might decide to submit your manuscript to a different journal. Nevertheless, take advantage of the feedback to make revisions to your manuscript. Especially if an issue is raised by more than one reviewer, there is a strong likelihood that reviewers for another journal will raise similar concerns. There is no need to notify the first journal of your decision not to resubmit.\nThis scenario highlights the fact that there are actually two reasons for submitting a manuscript to a journal. Of course, the principal reason is that you hope that the journal will elect to publish your work. However, sometimes authors submit manuscripts to journals in the full knowledge that the journal will almost certainly reject the submission. The logic goes as follows:\nFor research that you know is controversial, it is helpful to be aware of the arguments opponents have against your work. You might consider sending a copy of your manuscript to someone you know is a severe critic of the sort of research you are doing. However, the likelihood is that the critic simply won’t read or respond to your unsolicited manuscript.\nWhen you submit your work to a journal, the journal makes a commitment to engaging reviewers who will be required to produce a written assessment and critique.\nSuppose you have decided that journal X is the most likely journal to publish your controversial work. Before submitting it to journal X, you might choose instead to submit to journal Y. You know that journal Y will almost certainly reject the work. However, the reviewers’ reports from journal Y may prove useful in revising your manuscript to address the concerns and arguments of your critics. By doing this, you will increase the likelihood that when you finally submit your work to journal X, you will have a better chance of acceptance.\n\n\nCalling It Quits\nConsistent negative reviews may be telling you something. Don’t be afraid to abandon a project. Economists speak of “opportunity cost.” When you choose to do something, there is a cost incurred by virtue of your inability to do something else instead. You will never invest so much in a project that it may not be worthwhile to move on to something else.\nThe most common mistake made by young scholars is to pursue excessively ambitious projects early in their careers. Grow your projects. Start with modest research ventures, and ramp up the ambition as you gain experience.\nEvery scholar writes more papers than get published. Who knows: with the passage of time, you may ultimately return to an abandoned early project, and know just how to polish the rough edges.\n\n\nSummary\nIn a nutshell, there are five essentials to bear in mind when revising a manuscript.\n\nPay attention to nuanced wording in the action letter.\nUse the action letter and reviewer reports as the structure for writing a detailed letter of response.\nThe letter of response provides the action-plan for revising the manuscript. Write the letter first, and then revise your manuscript in accordance with the letter.\nPick your fights.\nYour aim should be that your letter of response is sufficiently detailed that the Action Editor and Reviewers will be satisfied by your reported revisions, and will decide not to read the entire revised manuscript."
  },
  {
    "objectID": "emp_methods_workshop/measurement_scales.html",
    "href": "emp_methods_workshop/measurement_scales.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "For each case, identify the kind of measurement scale implied — either nominal, ordinal, interval, or ratio.\n\nWorks categorized as either ragtime, stride bass, big band, bebop, Dixieland, free jazz or other.\nThe age of audience members.\nYear of birth for audience members.\nThe participant’s sex, coded as male or female.\nInterval size in semitones.\nYear of composition.\nEither: medieval, renaissance, baroque, classical, romantic, or modern.\nDynamic level, according to the scale: ppp, pp, p, mp, mf, f, ff, or fff.\nThe skin temperature of listeners in Fahrenheit.\nWatts of power produced by an amplifier.\nA list of instruments.\nA list of instruments from brightest to darkest timbres.\nMIDI key numbers (e.g., middle C = 60).\nCoded S, A, T or B.\nNumber of hours of practice.\nIQ.\nNumber of instruments.\nReaction times in milliseconds.\nThree categories: (1) Obviously major, (2) obviously minor, or (3) not obviously major or minor.\nThe amount of money a busker makes per hour.\nLength of hairpin markings in centimeters.\nThe notated key of a passage.\nThe loudness of each instrument, measured in decibels.\nThe number of sharps (positive) or flats (negative) in a key signature. (E.g., F# & C# = +2; Bb = -1.)\nResponses: never, rarely, sometimes, frequently, always.\n\n\n\nsical prefe\nrence on a\n7-point sca\nle:\n\n\n\n\n\n\n\nlike\n◉\n◉\n◉\n◉\n◉\ndislike"
  },
  {
    "objectID": "emp_methods_workshop/latin.html",
    "href": "emp_methods_workshop/latin.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "a fortiori\nIn research, a phrase meaning “by stronger logic” or “all the more so”. For example: “The participant failed to respond to the stimulus, a fortiori since the participant had fallen asleep.”\n\n\na posteriori\nReasoning from fact or observation to theory. Theory based on experience. The opposite of a priori. Compare post hoc.\n\n\na priori\nBased on a (previously) assumed principle or fact.\n\n\nad hoc\n(1) improvised. (2) a hypothesis intended to compensate for some anomaly (3) in science and philosophy, a methodologically suspect effort to “patch” a theory in order to save it from falsification. E.g. “The test of my theory didn’t work because the data were collected while the planet Mercury was in retrograde.”\n\n\nad hominem\ncriticizing or attacking a person rather than disputing an idea\n\n\nconfer (cf.)\ncompare, see also\n\n\net alia (et al.)\nplus other authors\n\n\nexempli gratia (e.g.)\nfor example (compare with i.e.)\n\n\nid est (i.e.)\nthat is (compare with e.g.)\n\n\nincipit\nthe first few words/notes of some text/score\n\n\nin situ\nthe original place, the appropriate place, or natural setting\n\n\ninter alia\namong other things\n\n\nopere citato (op. cit.)\nin the book or article just cited\n\n\nipso facto\nby that very fact\n\n\nper se\nIn and of itself, E.g. “It is not the composer’s fault, per se.”\n\n\npost hoc\nFormulated after the fact, E.g. a hypothesis or theory proposed after observing the data.\n\n\nprima facie\nat fact value\n\n\nsic\nJust so; exactly that. Used in quotations to indicate that the error or phrasing is accurately quoted.\n\n\nsui generis\nin a class of its own\n\n\nde novo\nonce more, again\n\n\n\n\nSome Examples\n\nHaving discarded our stimuli as inadequate, Study #2 began de novo with a different strategy for stimulus generation.\nIn light of the failure of the test, Smith proposed the ad hoc hypothesis that the data were confounded by unanticipated demand characteristics.\nThe failure to correctly calibrate the equipment for Study #3 a fortiori argues these results should be ignored.\nIn light of these observations we might propose the following a posteriori theory …\nHaving collected data using the Ollen Musical Sophistication Index, we conducted a further post hoc test to determine whether musical sophistication is associated with listener performance.\nResearch consistent with this view can be found in Adams (2001), Baker and Cohen (2002), and Dodd et al. (2003) (although cf. Smith 2004).\nThe experiment was conducted in situ at Cleveland’s Severance Hall.\nJohn Cage’s 4’33” (silence) might be regarded as forming a sui generis category of music.\nHaving failed to dislodge the argument, the author has resorted to ad hominem criticisms.\nThe reviewer’s concerns with the manuscript relate to methodological considerations, inter alia.\nThe large number of statistical tests performed ipso facto cast doubt on the results.\nPrima facie, the performers are highly skilled."
  },
  {
    "objectID": "emp_methods_workshop/exploratory.html",
    "href": "emp_methods_workshop/exploratory.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Exploratory Studies\n\nThe emphasis in exploratory research is observation. At the same time, researchers anticipate that the exploratory observations will lead to new questions, and these questions will, in turn, lead to the development of explanatory theories or interpretations.\nWith regard to observation, the observer can often choose to be either visible or invisible. The invisible observer lurks in the background like the proverbial “fly-on-the-wall,” hoping not to influence or change what is going on. In most studies of animal behavior, the ethologist tries to observe clandestinely. For example, an ornithologist might build a blind in order to hide her/his presence. For some animal species, hiding may be entirely successful. However, for other species, it may prove impossible to disguise your presence. Bonobos and chimpanzees, for example, are far too attentive and curious to allow a researcher to observe them in the wild without their awareness.\nTechnology often allows effective clandestine observation. For example, in a study of audience behavior, Olaf Post made use of video recordings made from security cameras in the Amsterdam Concertgebouw concert hall. This allowed Post to observe members of the audience with minimal influence. Written documents can provide similar opportuities for “invisible” observation: when a theorist examines a published score, the observations made will have little or no impact on the behavior of the composer who created the score.\nAt the other extreme, the researcher may choose to observe “from the inside” rather than from the outside. In this case, the researcher aims to learn through active participation. The best-known example of this approach is the participant-observation method commonly used in anthropology and ethnomusicology. For example, a researcher might lean to play the rebab as a way to better understand Moroccan Berber music. Many important observations are introspective—recognizing what it feels like to dance, play an instrument, or be disciplined by a master teacher.\nAn unusual cross-species example of participant-observation involved musician Peter Gabriel working at the Great Ape Trust in Iowa. Gabriel improvized music—taking his lead from the music produced by Kanzi, a chimpanzee. The interactions were recorded on video.\nThere are advantages and disadvantages associated with each approach.\nIn some cases, artificial environments can be constructed to aid observation. In animal behavior, an ant colony might be created in a laboratory, with suitable use of glass and cameras to facilitate observation. In other cases, an artificial environment may be created in which humans interact closely with the observed individual. Close interaction between researcher and animal may occur. For example, the gorilla, Koko, was raised in a human family-like environment. It is often too dangerous for a researcher to attempt to fit-in with some animal group.\nOf course exploratory studies can also be done from the comfort of your own home. You might search the Internet for the phrases like “I love bluegrass” or “I hate music” as a way of exposing yourself to commentaries, opinions, or viewpoints representative of different communities or subcultures whose experiences differ from your own. In some cases, researchers have posed (online) as a member of a group, in order to provoke responses—such as asking questions.\nAs noted, in exploratory research, one hopes that the observations will inspire the researcher into posing new questions. Observation may lead to questions such as:\n\nWhy is reggae so popular among Pacific island cultures?\nWhy do people tap their feet while listening to music? Why don’t people shake their elbows instead?\nWhy are pianos and guitars more popular instruments than (say) bagpipes and ocarinas?\nWhy do young musicians in Pohnepei dress and speak like they are African-Americans?\nWhy is music so repetitive?\nWhy do people willingly listen to music that makes them sad?\nWhy are people nostalgic for the music they heard during their teens? Is this true around the world?\nWhy is American blues so popular in Russia?\nWhy is opera so polarizing? People either seem to love it or hate it.\nWhy has Chinese music remained unpopular outside of China, but Chinese food is popular all around the globe?\nWhy do tunes get stuck in your head?\n\nGood questions should, in turn, begin the process of theory formation. A good exploratory study may end by proposing a possible explanatory theory. However, any resulting theory is post hoc and so is regarded as empirically weak. The observations cannot be viewed as “supporting” or “testing” the theory, since the observations were the inspiration for the theory in the first place.\nExploratory research should not be considered “passive.” First, observation itself may involve extraordinary effort in order to be in a position to observe. A researcher may travel to a remote corner of the world (or even to another planet), simply to observe something. In addition, observation sometimes requires you to do things that are outside of your “comfort zone.” If you are Christian, you might consider visiting a mosque or a Hindu temple. If you are Moslem, you might attend a synagogue or a pow-wow.\n\nReference\nOlaf Post (2009). “The way these people can just listen!”: Inquiries about the Mahler tradition in the Concertgebouw. PhD dissertation, Columbia University."
  },
  {
    "objectID": "emp_methods_workshop/samples_answers.html",
    "href": "emp_methods_workshop/samples_answers.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "For each case, identify the kind of sampling employed.\n\nA researcher walks into a music library with a question: Are sharp keys more common than flat keys? Wandering through the stacks, she blindly grabs volumes off the shelves and allows each volume to open spontaneously to some page. She takes note of the key signature.\nSimple random sampling.\nA professional music marketer is interested in carrying out a detailed survey of musical tastes in Britain. The marketer decides to use the ACORN geodemographic profile. British households will be sampled in proportion to the second-level ACORN categories: wealthy executives (8.6 percent of the population), affluent greys (7.7%), flourishing families (8.8%), prosperous professionals (2.2%), educated urbanites (4.6%), aspiring singles (3.9%), starting out (2.5%), secured families (15.5%), settled surburbia (6.0%), prudent pensioners (2.6%), asian communities (1.6%), post-industrial families (4.8%), blue collar roots (8.0%), struggling families (14.1%), burdened singles (4.5%), high rise hardship (1.6%), and inner city adversity (2.1%).\nQuota sampling.\nA researcher is interested in assembling a random sample of “classical” keyboard music. She has determined that she needs roughly 20 pieces for her study. In order to maximize data independence, she wants each piece to be written by a different composer. Using Wikipedia, she finds an alphabetical list of “classical composers.” For each letter of the alphabet, she selects the first composer who she knows has written for piano: Isaac Albéniz, Carl Philipp Emanuel Bach, Alfredo Casella, Claude Debussy, Edward Elgar, Manuel de Falla, etc.\nSystematic sampling.\nIn piloting an experiment, a graduate student recruits her graduate student colleagues as experimental participants.\nConvenience sampling.\nA team of researchers is interested in emotional expression in Hindustani film music. Indian participants are asked to characterize the emotional tenor of various film scenes. Using the descriptions, the researchers then classify the scenes into 14 categories — such as romantic, humorous, physical conflict, emotional tension, etc. The researchers then select four scenes for each of the 14 categories and analyse the associated background music. Their goal is to identify musical features in Hindustani culture that signal romance, humor, etc.\nStratified sampling.\nA medievalist thinks that the Dorian mode was more likely to have been heard as comparatively “happy” whereas the Phrygian mode was more likely to have been heard as comparative “sad” for medieval listeners. In order to test this notion, the scholar examines all of the Glorias (nominally “happy” text) and Kyries (nominally “sad” text) in the Liber Usualis. The prediction is that Dorian will predominate for Glorias while Phrygian will be more likely to occur for Kyries.\nConvenience sample.\nA researcher is interested in changing harmonic patterns in the masses of Palestrina. The researcher makes us of the Humdrum database of the scores for the complete 103 masses assembled by musicologist John Miller.\nIf the database is considered complete, and if the study really is intended to focus on the masses of Palestrina, then no sampling is involved. The researcher is studying the population rather than a sample.\nPaul von Hippel and David Huron carried out a study to test the idea that melodies tend to change direction following a leap, and that this pattern is ubiquitous in musical melodies around the world. In order to test this idea, they made use of two musical samples. The first sample selected music spanning five centuries. The second sample selected music spanning five continents: Africa, Asia, Europe, North and South America.\nThis study involves two forms of stratefied samples: by geographical area and by historical period.\nUnsure of the contents of a box, an archivist reaches in and grabs a couple of documents, which he then examines.\nSimple random sample.\nA researcher wants to know whether there is anything Italian, French or German about augmented sixth chords. Using large computer databases, the researcher uses Humdrum to isolate 900 sonorities in which the lowered sixth and raised fourth appear concurrently (including enharmonic spellings): 300 each written by Italian, French and German composers. Each of the sonorities is then classified as either Italian, French, German or Other.\nStratified sampling. The researcher is attempting to create equal sampled sonorities for the three different nationalities.\n\n\nReferences:\nPaul von Hippel & David Huron (2000). Why do skips precede reversals? The effect of tessitura on melodic structure. Music Perception, Vol. 18, No. 1, pp. 59-85."
  },
  {
    "objectID": "emp_methods_workshop/hindsight_task1.html",
    "href": "emp_methods_workshop/hindsight_task1.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Task\nIn the aftermath of World War II, the Pentagon carried out a detailed study entitled The American Soldier. The study was based on research involving 600,000 men and women in the service. Below are five findings from this study. For each finding, suggest two theories that might account for the result.\n\nBetter educated soldiers suffered more problems in adjusting to military life than less educated soldiers.\n\n\n\n\nSouthern soldiers coped better with the hot South Sea Island climate than northern soldiers.\n\n\n\n\nWhite privates were more eager to be promoted to noncommissioned officers than black privates.\n\n\n\n\nSouthern blacks preferred southern to northern white officers.\n\n\n\n\nAs long as the fighting continued, soldiers were eager to return home. Once the war ended, soldiers felt less urgency to return home."
  },
  {
    "objectID": "emp_methods_workshop/prereviewing.html#replication-and-pre-reviewing",
    "href": "emp_methods_workshop/prereviewing.html#replication-and-pre-reviewing",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "Replication and Pre-Reviewing",
    "text": "Replication and Pre-Reviewing\n\nThe following general announcement was distributed May 9, 2014 by the Journal of Experimental Psychology: General. This is one of the flagship journals of the American Psychological Association. This call for papers embodies two efforts intended to counteract positive results bias: namely pre-reviewing and replication:\n``\n\nCall for Replication Studies Papers\nThe Journal of Experimental Psychology: General is inviting replication studies submissions. The Journal values replications and may publish them when the work clearly contributes to its mission. This includes contributions with interdisciplinary appeal that address theoretical debate and/or integration, beyond addressing the reliability of effects.\nThe Journal preference is for replication plans to be submitted to the Journal before data collection begins. A proposal should provide a strong motivation for the study in the context of the relevant literature, details of the design, sample size, and planned analyses. The editorial team may encourage a replication project, based on external guidance. Final manuscripts will be evaluated through further external review. Submit through the Manuscript Submission Portal (http://www.apa.org/pubs/journals/xge) and please note that the submission is a proposal for replication. Replication articles will be published online only and will be listed in the Table of Contents in the print journal.\nQuestions can be addressed to the Editor, Isabel Gauthier (gauthier.jepg@gmail.com) or to the Editorial Associate, Jennifer Richler, (richler.jepg@gmail.com).\nEditorial Office Journal of Experimental Psychology: General"
  },
  {
    "objectID": "emp_methods_workshop/feedback5.html",
    "href": "emp_methods_workshop/feedback5.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Please help us by assessing the relative value of the main topic units in the workshop: ———————————————– ————- —— ——– ————- ———– Empirical knowledge less useful okay useful very useful essential Inviting failure less useful okay useful very useful essential Line-in-the-sand less useful okay useful very useful essential Rhetoric of science less useful okay useful very useful essential Questions/Conjectures/Hypotheses/Theories less useful okay useful very useful essential Grandmother research less useful okay useful very useful essential American solider (hindsight bias) less useful okay useful very useful essential Reductionism less useful okay useful very useful essential Type I & II errors less useful okay useful very useful essential Hypothesislessness less useful okay useful very useful essential Operationalizing hypotheses less useful okay useful very useful essential Explore-then-test less useful okay useful very useful essential From Question to … to Protocol less useful okay useful very useful essential Causation & manipulation less useful okay useful very useful essential Sampling methods & problems less useful okay useful very useful essential Data Independence less useful okay useful very useful essential Correlation demo less useful okay useful very useful essential Regression-to-the-mean less useful okay useful very useful essential Demand characteristics less useful okay useful very useful essential Measurement scales less useful okay useful very useful essential Questionnaires less useful okay useful very useful essential Reading: Agatha Christie less useful okay useful very useful essential Reading: Dan Perttu less useful okay useful very useful essential Statistics less useful okay useful very useful essential Chi-square test less useful okay useful very useful essential Multiple tests less useful okay useful very useful essential File drawer effect less useful okay useful very useful essential Research instruments less useful okay useful very useful essential Interacting with subjects less useful okay useful very useful essential Writing empirical paper less useful okay useful very useful essential Writing the paper first less useful okay useful very useful essential Why experiments fail less useful okay useful very useful essential Final advice less useful okay useful very useful essential ———————————————– ————- —— ——– ————- ———–\nWhat topic(s) would you like to see added to the workshop curriculum?\n\n\n\nll us about\nthe four-day d\nuration of the workshop:\n\n\n\n\ntoo short\nabout right\ntoo long\n\n\n\nTell us about the scheduling: Were the start and end times okay? The number and duration of the breaks, the time allotted for lunch, etc.\n\n\n\nll us abou\nt the group ex\nercises:\n\n\n\n\ntoo few\nabout right\ntoo many\n\n\n\nWould you have liked more homework, or did you prefer the free evenings? ————— ————- ————— more homework about right less homework ————— ————- —————\nTell us about the guest presentation by Joshua Albrecht.\nWhat was the least-good thing about the workshop?\nWhat was the best thing about the workshop?\nHow did you learn about the workshop?\nWould you recommend the workshop to your colleagues?\nGeneral comments:"
  },
  {
    "objectID": "emp_methods_workshop/god_questions.html",
    "href": "emp_methods_workshop/god_questions.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Task\nYou have been granted an audience with God. Your group will be allowed to ask Her three questions related to music. What questions will you ask? Discuss the possibilities in your group and settle on three questions. Write the questions down. Assign a different group member to present each question to the larger workshop. Introduce each question with a short preamble that sets the stage for your question.\nN.B. You are not allowed to ask “meta-questions” (such as “What is a good question about music?”).\n\n\nRationale\nIdeally, research should focus on questions of the highest importance. The conscientious researcher always trys to answer the most important questions first.\nUnfortunately, the most important questions cannot always be answered given our limited resources and limited understanding. Instead, we tend to focus on answering questions for which we have the skills and resources to answer. By themselves, these questions are often not very interesting or compelling.\nThe best research endeavors to connect good “top-down” questions with good “bottom-up” research resources. Empirical research methods can provide helpful resources for answering questions about music, but these methods should not dictate our research agendas. In learning how to use do empirical research, it is equally important to learn to ask creative questions about music. The purpose of this task is to get you thinking about the larger issues. As a researcher, what do you really want to know?"
  },
  {
    "objectID": "emp_methods_workshop/subject_interaction2.html",
    "href": "emp_methods_workshop/subject_interaction2.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Interacting with human subjects involves a combination of legal, moral, experimental, and educational considerations. Four goals shape the experimenter’s interaction with a participant:\n\nensuring the physical and psychological well-being of the participants\ncollecting high quality experimental data with a minimum of confounds\nproviding a learning opportunity for participants\nproviding a learning opportunity for the experimenter about possible unanticipated confounds and insights pertinent to the research.\n\n\n\n\nWhether the participant is paid or unpaid, a student or non-student, it is important that the participant have a meaningful and informative experience. Not all experiments are inherently interesting, but it is possible for a participant to understand how participating in an otherwise boring task might produce research results of great interest and value.\nIf a participant has a negative experience, this indicates a failure of the experiment. Moreover, if a participant has a negative experience, it reflects poorly on our research program, and may make it difficult to recruit participants for future studies.\nOne of the most egregious assumptions experimenters make is to assume that other people are just like us (i.e., egocentric bias). Experimental research has established that not all people experience the world the same way. It is important not to assume that the participant is having the experience you think they are.\nConsider the following extreme case. The population of university undergraduate students corresponds demographically with the age group showing the greatest susceptibility to schitzophrenia. Roughly 1 in 200 students is likely to exhibit schitzophrenic symptoms. If you run enough subjects, you are likely to encounter a person who is schitzophrenic. YOU may think that the experiment is innocent and innocuous, but a schitzophrenic may experience the experiment as threatening and sinister.\nThroughout the experiment be vigilant for any signs that your participant is having a negative experience. The physical and mental welfare of the participant is your most important responsibility. If you deem that the participant is at mental or physical risk, or if the participant reports any negative feelings or concerns, stop the experiment (if it is safe to do so). If you inadvertently cause harm to your subject, do not attempt to cover up the problem or make light of it. Covering up a mistake will result in more onerous disciplinary actions than reporting an honest error. Should any untoward episode occur, contact Dr. Huron immediately in order to fill out an Adverse Event Form for the Institutional Review Board. Reporting such events is mandatory.\n\n\n\n\n\nPrepare. Before running an experiment be sure you are fully acquainted with the equipment, instructions, and procedures for the experiment. Unless the experiment is a minor variant of a previous experiment, you should practice running the experiment on a friend, colleague, or assistant. Check the technical set-up.\nBlind. In some experiments, different participants will receive different treatments (i.e. “between-subjects design”). If possible, organize the experiment so that you do not know which treatment the subject will receive. Remaining “blind” to the experimental treatment will help ensure that your interaction with the participant doesn’t bias the results. If more than one person is involved in running participant for a between-subjects design, avoid the possibility that one experimenter tends to run subjects in one condition, while a different experimenter tends to run subjects in another condition.\nIntroduce. Introduce yourself. Tell the person your name and indicate your position or status. For example, you might say “Hi, my name is _____; I’m a /student/graduate student/visiting scholar/post-doctoral fellow/ working here in the music department.”\nInteract. Endeavor to make the participant feel at ease. They are entering what they perceive as “your space”, and may feel that they are merely cogs in some impersonal research machine. Interact with your participant. Address them by name.\nAssess. Use your interaction to get a sense of whether the participant is nervous, or in a hurry, excited, depressed, or out-of-breath. Ask the participant how their day is going; ask whether they feel “up” for an experiment. If the participant has a cold or is otherwise ill, they are not likely to enjoy the experience; and they are not likely to provide good data.\nInform. Tell the participant about the experiment in accordance with the approved Human Subjects protocol. Describe the stimuli they will hear, the nature of the task, and remind them of the duration of the experiment. Ask them whether they have any questions. If the subject asks about the specific goal of the experiment, respond by (1) telling him or her about the broad goals of this field of research, (2) indicate that you prefer not to reveal the specific theory or hypothesis you are testing, but that (3) you will tell them exactly what you are doing following the end of the experiment.\nWritten Consent. When a consent form is required, hand the participant a clip-board with the consent form. Do not read the consent form aloud, since this will interfere with the subject’s own reading speed. Sit down. Use your body language to signal that you are not in a hurry, and that taking time to read the consent form is what is expected. Many people will not actually read the form; some will sign it immediately without reading. In either case, tell them that the form says they are free to terminate the experiment at any time without prejudice. Also tell them that you can provide them with a copy of the form if they wish. Nearly all Institutional Review Boards will require these two instructions. In addition, tell the participant that the resulting data from this experiment will be stored in an anonymous format, and that no one will know that the data came from them. Tell them that you may make the data available to other scholars for further study.\nN.B. Debriefings of past subjects indicates that in our music experiments, the most stressful on-site experience for participants is signing the consent form. Most people are naturally skeptical of signing a document. The fact that a signature is required tends to make them wary and wonder what is in store. Resist the temptation to tell them that there are no appreciable physical or psychological risks. If they seem concerned, it is better to end the experiment — provided you think it is safe to stop.\nVerbal Consent. If the IRB-approved experimental protocol does not require a written consent form, you must still obtain verbal consent from your participant before proceding with the experiment. When no consent form is involved, you are still required to convey two pieces of information: (1) Tell the participant your name, and that you are representing the Principal Investigator who is a lecturer/professor/member-of-staff employed at your instution. (You should have already done this when you introduced yourself; see above.) (2) Tell the participant that they are free to terminate the experiment at any time without prejudice or penalty. In addition, tell the participant that the resulting data from this experiment will be stored in an anonymous format, and that no one will know that the data came from them. Tell them that we may make the data available to other scholars for further study.\nCheck. Ensure that they have signed the consent form before proceeding with the experiment. Also ensure that they have printed their name on the form. Signatures are often unreadable, and we need to know the subject’s name in order to maintain a complete consent record. For student’s participating in the Music Research Program, a clear name is need in order to ensure that they receive credit. Add the correct date if the participant hasn’t already written this on the form. The form also requires a witness — your signature. It is often convenient to wait until after the experiment is underway before you add your signature to the form. Since you have observed the participant sign the form, there is nothing wrong with witnessing the signature some minutes later.\nSituate. Seat the participant in the appropriate place. Make sure they are comfortable. Especially tall or short people may require some adjustment of the apparatus, such as keyboard height. Ensure that there are no potentially dangerous impediments such as scissors, electrical cables, etc.\nInstruct. Read the formal instructions for the experiment. Often, these instructions will appear on a computer screen. Since participants often “skim” over any printed instructions, it is prudent to read the instructions aloud from the screen while the subject follows the written script. You might begin by saying “Let’s just go over the instructions here …” Aim to read the instructions in a consistent voice for all subjects. If you find it useful to embellish the instructions, use the same language for each participant and record in your lab book the exact words you use.\nPractice. For most experiments, there will be an initial set of practice trials. Don’t hover over the participant while they do the practice trials. Go away and let the subject continue on their own. With experience, you’ll have a good idea of how long to wait before you return. When the practice trials are completed, ask the subject whether they have any questions.\nQuery. Most participants won’t have any questions to ask at this point. If the subject asks a question, make a mental note of the question. Later, include the question in your experimental notes. Such questions can help identify ways to improve the experimental set-up. Subjects’ questions may also alert you to possible misinterpretations of the instructions. For example, you may be unaware that there is some ambiguity in the instructions, and that some subjects may be responding in a way you hadn’t anticipated. Finally, such questions may later prove helpful in identifying data that should be excluded from the final analysis.\nRespond. When answering subject’s questions, try to avoid giving them extra insights into the experimental hypothesis. Be careful in your choice of words. Say as much as needed in order for the subject to feel they understand.\nIf you re-explain the instructions, be wary if the subject says s/he now understands. After repeating an explanation, it is common for a confused subject to claim s/he understands in order not to be perceived as stupid.\nMake a mental note of the exact words you used to clarify the instructions. Include your response in your experimental notes. Be accurate. If you said something you shouldn’t have, be honest in reporting.\nPrivacy. Try to avoid “hovering” nearby during an experiment. Unless the experiment explicitly requires you to observe or interact with the subject, you should leave the room or leave the subject alone.\nRemind. Just before the main experiment starts, remind the subject about the approximate length of the experiment.\nStay. Don’t go too far away. Occasionally, a subject will stop the experiment before the end. Sometimes this is for a good reason — such as an equipment failure, or the decision of the subject to leave. In rare cases, a subject will stop to get a drink of water or ask you a question. If the subject wants to continue with the experiment, use your judgment in determining whether the experiment should resume.\nWrap-up. Following the end of the experiment, six tasks remain that involve interacting with the subject: (1) debriefing, (2) confessing, (3) educating, (4) paying, (5) thanking, and (6) soliciting.\nDebrief. Debriefing is an essential part of the experiment. The goal of debriefing is to gather information about the experiment from the subject’s point-of-view. “Did you have a strategy you were using for this experiment?” “Did you simply follow your intuitions, or did you follow a particular way of doing the task?” Let the subject talk about his/her experience. What did they think about the experiment? Were they able to concentrate on the task? Did they find the task easy or hard?\nIn one experiment, we were looking to see if phrases played in low transpositions are perceived as being “heavier” than mid-register or high transpositions. When debriefing one subject, he mentioned that pitch-height was an obvious factor influencing whether the phrase sounds heavy or light. Consequently, he said that he explicitly ignored the overall pitch height and made his judgments of “heaviness” on other factors! In this case, the debriefing provided critical information that allowed us to avoid using this data in our analysis.\nIn many experiments, it may be necessary to discard some collected data depending on certain criteria. For example, subjects who do not achieve a certain minimum performance level, or subjects who have certain skills. After a subject completes an experiment, do not then tell the subject that you are likely to throw away their data. Participants are understandably discouraged when they learn that their data is unusable.\nFinally, ask the subject if there was anything about their experience that was stressful, annoying, painful or uncomfortable. Immediately inform Dr. Huron if the subject reports any negative experience.\nConfess. Occasionally an experiment will involve deception. Deception cannot occur without explicit approval by the Institutional Review Board. When deception is used, an approved Debriefing Script is usually mandated. Read the script explaining the nature and purpose of the deception. Ask them if they have any questions about why the deception was necessary. If a subject is upset by the deception, do not attempt to make light of the situation. If the subject expresses any negative feelings about the deception, inform that subject that they are free to file a complaint with the Institutional Review Board.\nEducate. If your participants do not receive payment, or if research participation is a requirement for some class, then it is usually appropriate to reciprocate by doing something useful for the participant. In educational settings, we interpret this as a goal to educate music students (and other participants) regarding experimental approaches to music scholarship. Simply participating in an experiment will convey some idea of what’s involved. However, apart from this, it is valuable to explain more about an experiment. You can explain the purpose of the experiment and say something about what you expect to find. You might describe the different types of stimuli used or describe the method used in the experiment (e.g. method of adjustment, 2AFC, etc.). Identify the independent and dependent variables. Give the participant an opportunity to ask questions about the purpose and methods of the experiment. Some participants will be in a hurry to leave. Tell them that it is okay for them to leave, but that part of our goal is to educate students about experimental research in music. Indicate that you are prepared to converse.\nPay. If the subject is receiving payment, deliver the appropriate cash, and have them sign a receipt. If there is reimbursement for parking, this needs to be indicated on the receipt.\nThank. Thank your participant. Tell your subject how important s/he is in the research program, and that you are genuinely grateful for their participation.\nSolicit. Solicit the cooperation of the subject not to disclose any information about the experiment to their friends or class mates. Explain why this is important.\nClean-up. After the subject has left, six tasks remain: (1) recording, (2) crediting, (3) restoring, (4) securing, (5) backing up, and (6) reporting.\nRecord. When the participant has left, take time to record pertinent points made during the subject debriefing. Record any strategies, complications, or other subject comments. Also record any observations you may have made about the participant: Was the subject excited, sleepy, unattentive, etc.?\nCredit. For participants who are part of a subject pool, be sure to record their participation so they can receive proper credit. Remember that credit is earned whether or not the student completes the experiment.\nRestore. Return furniture, equipment and computer systems to their original state. Ensure that other people can use the lab and that other experiments can be run.\nSecure. Secure all data and personal information. File the consent form in the appropriate place. Make sure there is no personal information lying around (such as names, telephone numbers, e-mail addresses) that might be accessed by unauthorized individuals. Remove subject contact information from your schedule or smart-phone. Remove identifying information from data files according to the IRB-approved Protocol for the experiment. When throwing out pieces of paper consider whether the paper should be shredded. Log-out from the computer so that data can’t be accessed when you aren’t around.\nBackup. Make backup copies of important data in accordance with the IRB-approved Protocol for the experiment.\nReport. Report any failures to comply with the IRB-approved Protocol for the experiment.\n\nOne of the most difficult issues in interacting with experimental subjects is striking the right balance between formality and informality. From an experimental standpoint, the ideal would be to follow a written script for everything you say. This would ensure that each participant is treated in an identical way and reduces unconscious experimenter bias. However, it is not good for a participant to feel they are interacting with a machine rather than a human being. With practice, an experimenter can convey a sense of friendly informality while behaving in a nearly identical manner with each participant."
  },
  {
    "objectID": "emp_methods_workshop/types_answers.html",
    "href": "emp_methods_workshop/types_answers.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Identify whether the following studies are best regarded as reconnaissance, descriptive, measurement, correlational, or experimental. If correlational or experimental, further indicate whether it is exploratory correlational or exploratory experimental.\n\nA researcher publishes an article describing a newly discovered box containing composition sketches written by Bartók.\nThe description suggests that no explanatory theory is offered. No hypothesis is proposed. No manipulation is carried out. The description suggests that no measurements or counts are reported. So the best answer would be that it is an reconnaissance study.\nWhile a musician takes a break, an ethnomusicologist clandestinely mistunes one of the strings on the instrument by 1/3 of a semitone. The ethnomusicologist suspects that the musician will notice the mistuning and so retune the instrument.\nNotice that the final sentence is very close to “The ethnomusicologist predicts that …” Even if it is not stated, there is a clear test going on: if the musician recognizes the pitch change, then he/she is likely to retune the instrument. Notice that the ethnomusicologist has made a manipulation by mis-tuning the instrument. Consequently, the study is best regarded as an experiment.\nA researcher at Billboard magazine assembles and publishes music sales by genre for 85 countries.\nNo prediction or test is reported. However, numbers (counts) are reported. This suggests that the best characterization is as an measurement study.\nA researcher analyses facial expressions in commercial videos of concert pianists. It is found that pianists tend to raise their eyebrows when the music is high in overall pitch.\nIn this case the researcher is relating two measures — the eyebrow placement and the average musical pitch. This is some sort of correlational study. It appears that there was no a priori hypothesis related to this finding. Consequently, it is perhaps best characterized as an exploratory correlational study rather than a correlational study.\nA researcher commissions ten composers to create “happy” and “sad” melodies. The researcher predicts that the happy melodies will exhibit larger melodic intervals than the sad melodies.\nThe researcher has made a prediction. At the same time, the researcher has “manipulated” the world, by asking composers to create two melodies: a “happy” melody and a “sad” melody. The hypothesis is that happy melodies will involve larger melodic interval sizes. This makes the study a bone fide experiment.\nThe manager of an orchestra is interested in identifying which repertoire attracts bigger audiences. She gathers audience statistics for a number of concerts from several orchestras. She classifies each program according to whether the music is predominantly Baroque, predominantly Classical, predominantly Romantic, predominantly Modern, or predominantly Pops. She finds that the largest audiences occur for Pops concerts.\nThere is no hypothesis in this study. The researcher does not manipulate the world. She classifies the concerts and relates (correlates) these to measurements of audience size. The study is best described as exploratory correlational.\nA researcher interviews twenty young mothers about singing to their newborn infants. He finds most mothers report singing to their babies, but that they are more likely to sing when no other adult is observing them. The researcher concludes that many young mothers are self-conscious about their own singing and prefer not to have an audience other than their baby.\nNo hypothesis reported, and no measurements are made. An interpretation is made to account for the reluctance of mothers to sing in the presence of other adults. The study is best described as descriptive.\nA researcher is interested in the phenomenon of globalization and collects data from different islands in the Kingdom of Tonga. For each island, she counts the number of Internet-connected computers, and also interviews people in order to determine their favorite music. She predicts that islands where people most prefer Western pop music will be the islands that are most Internet-connected.\nThere is a hypothesis here, but no manipulation. So it is a correlational study.\nA school board is interested in the effect of introducing non-Western music into the middle school band program. Of eight middle schools in the district, four randomly selected middle schools include non-Western works in the band repertoire. At the end of the year, a sample of music students are interviewed from all eight schools in order collect feedback.\nThe research involves a manipulation that changes things. However, there is no explicit a priori hypothesis, and the interviews seem rather informal in structure. It is probably best to characterize this study as an exploratory experimental study.\nA music theorist carries out analyses of most of the serial works by Schoenberg, Webern and Berg. After analyzing the works, the theorist publishes a general article comparing and contrasting the serial techniques of the three composers. In particular, the theorist observes that, compared with the other composers, Webern is more likely to compose works structured as extended palindromes — with the material in the second half of the work inverted.\nThere is no a priori hypothesis. Although the analysis is formal, there are no explicit measurements made. This suggests that the study is not a measurement study. An interpretation of the music is offered, so it is probably best to classify the study as descriptive."
  },
  {
    "objectID": "emp_methods_workshop/pothole_quiz.html",
    "href": "emp_methods_workshop/pothole_quiz.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Circle and identify each of the methodological transgressions in the following description.\nN.B. Some events may be regarded as transgressing more than one methodological principle.\n\n\nFor a number of decades now, considerable controversy has surrounded a well-known theory of melody developed by Dr. Hoch of the Music Department at Poxford College. The theory relates people’s heights to music.\n[]{#double-use data}\nDr. Hoch formed the theory while doing research in the Congo. Hoch discovered that Pygmys sing melodies containing predominantly small intervals, whereas Bantu’s sing melodies containing much larger intervals. In Hoch’s first book, Pitch Reduction in Congolese Pygmy Melody, Hoch noted “This is evidence for my new theory that interval size is proportional to the heights of the people who make the music.” [1]\n[]{#discovery fallacy}\nHoch’s arch rival, Dr. Klein, has noted that Hoch’s theory is suspect because the theory originates in the Egyptian Book of the Dead, where an obscure verse is often translated: “Big people make big music.” [2]\n[]{#ad hominen argument}\nProfessor Ruse, one of Hoch’s defenders, has countered that Dr. Klein’s own views are suspect since Klein has admitted to having taken illegal drugs in the 1960s and holds a degree from Golly College (which is not even rated by the Pricetown Review). [3]\n[]{#research hoarding} []{#data neglect}\nKlein notes that, if only Hoch and Ruse could see Klein’s as-yet-unpublished data from Belize, they would see the error of their ways. [4] Dr. Wise points out that considerable pertinent data is available at the Smithsonian, if only Hoch and Klein would care to examine it. [5]\n[]{#naturalist fallacy}\nAs might be expected, Hoch’s theory has attracted considerable popular attention. An editorial writer for the Lower Arlington Snob has started a crusade:\n“Given that the national anthem contains so many large leaps, it is obvious that the [short] children in our kindergartens should not be singing it.” [6]\n[]{#third variable problem}\nOn the contrary, the OSU basketball Buckeyes coach has realized that the correlation between large melodic intervals and size is the perfect way to increase players’ heights. This year, Swiss yodeling has become a standard part of team practice. [7]\n[]{#cohort bias}\nIn order to resolve the question of whether yodeling causes increased height, Dr. Dee Velop decided to compare the melodies sung by people at different ages. Dr. Velop randomly sampled 900 people — 150 each at the ages of 2, 5, 8, 10, 16, and 25. Dr. Velop found that it is indeed the case that the older (taller) people sing melodies with wider intervals. However, many of the older participants in the study noted that yodeling was especially popular 10 years ago. [8]\n[]{#control failure}\nIn Hoch’s second book, Swedish Swing, Hoch reported on a replication study carried out in Sweden (where people are typically tall). Hoch found that the average interval size for Swedish melodies was 240 cents — which, he says, “only goes to prove my point.” [9]\n[]{#multiple tests} []{#post-hoc hypothesis}\nThe ever-suspicious Dr. Klein decided to study the Inuit of Greenland (who are somewhat short). Klein didn’t find that Inuit melodies had smaller intervals compared with Bantus. Nor did Dr. Klein find evidence in support of another 31 popular hypotheses about melodic organization. But Klein did find a statistically significant (p<0.05) result that Inuit melodies have slighly more notes per phrase than for Bantu melodies. [10] After spending time carefully examining the data, Klein also thought there was some evidence suggesting that Inuit singers take slightly longer to inhale while singing. A formal test of this hypothesis using Klein’s existing data showed that indeed, on average, Inuit singers took 15 milliseconds longer to inhale than Bantu singers and this was significant at p<0.05. [11] [12]\nMotivated by the controversy, Prof. Long carried out a 30-year experiment in rural Bolivia. Dr. Long recruited 1,255 children in 28 orphanages. Each orphanage was randomly assigned to either a “small interval” or “large interval” group. The children in the “small interval” orphanages were taught folk melodies with predominantly small intervals while children in the “large interval” orphanages were taught folk melodies with predominantly large intervals. Children had no access to radio or television. Keeping detailed records of the children’s heights, Dr. Long found that there was no systematic difference in the rates of growth and no difference in the absolute heights between the two groups.\n[]{#ad-hoc hypothesis}\nDr. Hoch was naturally displeased by the study, and has suggested that the reason why Dr. Long failed to find the effect was because of possible differences in diets between the various orphanages. Hoch pointed out that some of the orphanages in Long’s study are known to be located in mountainous areas where diets are less nutritious. [13]\n[]{#instrument decay}\nA more serious problem with Dr. Long’s study was that mid-way through the study, Bolivia changed to the metric system, so the numerical records are an indecipherable mixture of inches and centimeters. [14]\n[]{#anti-operationalizing problem}\nDr. Bliss has questioned the use of either inches or centimeters for measuring height:\n“We all know what height is, and it is only crudely related to such reductionistic measures as”yards” or “meters”.”\nGiven the origin of the theory in the Egyptian Book of the Dead, Bliss thinks cubits would be a more appropriate (though not infallible) measure of height. But Bliss thinks even this is impossible since no one can be sure how long a cubit is. [15]\n[]{#mortality problem}\nOne group that was pleased with Long’s study was the orphanage operators — since the researchers brought funds to support the children’s education. This income was a welcome supplement to selling the shorter orphans to the North Korean submarine program and the taller orphans to OSU basketball Buckeyes. [16]\n[]{#positive results bias} []{#bottom-drawer effect}\nThe Journal of Musicological Musings decided not to publish Dr. Long’s study because the results didn’t support any widely accepted theory of melody. [17] As it turns out, a number of other scholars have independently produced negative results. But each of these scholars has been reluctant to even submit the results of their research. [18]\n[]{#head-in-the-sand syndrome} []{#confirmation bias}\nDr. Hoch thinks there is enough research already, and there is no need for further testing of this well-established and obvious theory. [19] Hoch notes that Stravinsky once said of Ravel “Il est trés petit, non?” — and does anyone doubt that Stravinsky uses wider intervals than Ravel? [20]\n[]{#ipse dixit} []{#universalist phobia}\nDr. Hoch has also noted that no less a musicologist than Sir Percy Smart has stated that the matter is now resolved. [21] However, Prof. Bliss remains skeptical. Says Bliss, “It is pure folley to expect such a simple theory to apply to all of the world’s cultures.” [22]\n[]{#presumptive representation}\nBy contrast, Dr. Pomo has argued that this whole business is part of an upper-class conspiracy to denigrate short people. Yodeling has become an entrenched canon whereas chanting is now widely desparaged as “music for little people.” Paid programing on cable TV frequently shows vertically-challenged people engaged in IQ-demeaning activities while Gregorian chant plays in the background. Short people find that everyone expects them to like “The Monks of Saint Bede.” [23]\n[]{#relativist fallacy}\nDr. Pomo says that no music is any better or worse than any other music. “We all know that no theory can be conclusively shown to be better than any other theory. It simply doesn’t matter whether the height/melodic-interval theory is true or false.” [24]"
  },
  {
    "objectID": "emp_methods_workshop/demand_characteristics.html",
    "href": "emp_methods_workshop/demand_characteristics.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Demand Characteristics\n\n“It is to the highest degree probable that the subject[’s] … general attitude of mind is that of ready complacency and cheerful willingness to assist the investigator in every possible way by reporting to him those very things which he is most eager to find, and that the very questions of the experimenter … suggest the shade of reply expected” (Pierce, 1908).\nEarlier, we considered the general problem of reactivity where the presence of an observer changes the behavior of the person observed. When carrying out an experiment with human participants, participants will tend to form their own ideas about the purpose of the experiment. In some cases, the participant will correctly infer the aim, but in many cases, the participant’s idea will be wrong. Right or wrong, what a participant thinks about an experiment can often (though not always) shape their behaviors in ways that may confound the experiment. This type of reactivity is referred to in experimental research as demand characteristics.\nA demand characteristic is any aspect of the experiment (apart from the experimental manipulation) that causes a change in the participant’s behavior. Demand characteristics often add undesirable confounds to a study, so it is important to be able to identify and minimize possible demand characteristics.\nDemand characteristics do not merely arise because a participant thinks about the experiment. Demand characteristics can also be entirely unconscious. One of the most powerful examples of a demand characteristic is the placebo effect. The simple expectation that the placebo will improve one’s situation that leads to the placebo actually improving one’s situation.\n\nCooperation Bias\nIn many experiments, participants are often eager to please the experimenter. It is human nature to want to be cooperative. Even if you don’t tell the participant what you are hoping to observe, they may still form an opinion about what answers you might find pleasing.\nRelated to social desirability bias is cooperation bias. Here, the participant is eager to be perceived by the researcher as cooperative (Orne, 1962). In this case, the participant is apt to respond according to what they believe the experimenter wants to hear. Research suggests that cooperation bias differs between cultures. For example, research suggests that people from Asian cultures tend to behave in a more cooperative manner than people from American culture (Nisbett, 2003).\n\n\nContrarian Bias\nNot every participant is susceptible to cooperation bias. Participants can also become suspicious of the experimenter’s aim. The participant may form an opinion (either accurate or not) about the nature of the hypothesis being tested, and feel that the hypothesis is wrong or wrong-headed. Accordingly, participants may also behave in a manner contrary what they think you are doing.\n\n\nAcquiescence Bias\nApart from attempting to please the experimenter, people often respond in ways that favor positive rather than negative responses. This reactivity problem commonly occurs in interviews and surveys and is referred to as acquiescence bias. When asked whether they agree or disagree with various statements some respondents will favor the “yes” response over the “no” response — even if it is causes them to behave in a contradictory fashion. For example, at different points in a long questionnaire, a person may agree with both of the following statements: I prefer a cup of coffee to a glass of water. I prefer a glass of water to a cup of coffee. (Messick & Jackson, 1961).\nAcquiescence bias is frequently observed when people don’t understand what they are being asked. Travelers in a foreign culture often encounter this. A monolingual English speaker traveling in Egypt might ask a local person: “To get to the pyramids, do I turn left or right?” To which a local person might well answer “Yes.”\n\n\nSocial Desirability Bias\nAnother problem is social desirabiliy bias. Research shows that we commonly answer questions in a way that makes us appear to be better persons. We exaggerate charitable and altruistic traits, and minimize negative self-portrayals (Crowne & Marlowe, 1960).\nWe aren’t simply interested in making others view us favorably. We are also eager to view ourselves positively. Once again, research shows that we hide from ourselves our bad behaviors and attitudes, and perpetually interpret our thoughts and behaviors from the most positive perspective.\n\n\nA Story\nIn 1990 I carried out an experiment to test whether listeners are prejudice against women composers. I expected that people will tend to view women composers less favorably than male composers. I had student participants listen to excerpts of recordings of relatively obscure modernist works. For each work, I had reproduced old program notes that described aspects of the pieces. I had made slight changes to the program notes so that they would subtley indicate whether the composer was male, female, or of indeterminate gender. I was concerned about demand characteristics, so I made special efforts to ensure that the gender indicators were quite subtle. For example, consistent with typical concerts, most of the excerpts were assigned to male genders.\nThe experiment was a between-subjects design in which one group heard certain passages assigned to certain genders, whereas the other group was led to the impression that the composers of the same passages were of the opposite gender. Would listeners rate a musical passage as being of less quality because the composer was a woman? Were men more likely than women to exhibit such a bias?\nThe results were quite clear. The implied sex of the composer made no difference whatsoever on the assessed quality of the music. I was frankly surprised by the results. A couple of weeks after the end of the experiment, I happened to interact with one of my students in the hallway. Our discussion turned to the experiment. I asked him what he thought was the purpose of the experiment. Subsequent conversations with other student participants confirmed what the first student had told me: “It seemed pretty obvious that you were trying to determine whether we were prejudiced against women composers.” All of the students I talked to had correctly inferred what I was up to.\n\n\nMinimizing Demand Characteristics\nBroadly speaking, there are eight ways to minimizing or eliminate demand characteristics: (1) field observation, (2) blind procedure, (3) deception, (4) double-blind procedure, (5) confidence building, (6) between subjects design, (7) implicit measures, and (8) debriefing.\n\nField Observation. Anthropologists commonly rely on ethnographic fieldwork, however they typically interact or participate in social and interpersonal activities in which their presence as researchers is not hidden or disguised. By field observation, we mean clandestine observation in naturalistic situations where people are observed without their awareness.\nA classic example of such an approach is found in Simha Arom’s experiments with scales in the Central African Republic. Arom was interested to learn which tuning system was preferred by his African xylophone musicians. He constructed a xylophone-like instrument that was actually a MIDI synthesizer with several different tuning systems. Arom was eager to know what the musicians preferred, without them being influenced by his presence. So he set up a video camera near the instrument and left the musicians to play with the instruments without them being aware that they were being recorded. On the video, two musicians are recorded as saying “It’s all the same.” “There’s no difference [between the different tuning systems].”\nAnother example of field observation is Olaf Post’s (2011) study of audience behaviors at the Concertgebouw concert hall in Amsterdam. Using archived video recordings of concerts, Post was able to examine the figetting behaviors (scratching noses, cross-legs, etc.) of audience members. He then related the amount of figetting to the type of music being played — and to structural features of the music. In this case, the members of the audience were clearly unaware that they were being videoed In thise case, even if the members of the audience were aware of the presence of the video camera, they were surely unaware that their figetting behavior was being monitored.\nBlind procedure. An experiment is said to use a blind procedure either (1) when the participant is not told about the purpose of the experiment, or (2) when the participant is not aware whether they are in a control or experimental group. In drug research, for example, participants are commonly divided into two groups: the experimental group and the control group. The experimental group receives the actual drug being tested. The control group is given a placebo — an inert “sugar” pill. If participants are randomly assigned to the control and experimental conditions, then it is likely that both groups will show equivalent cooperation or contrarian bias — and so observed differences between the experimental and control conditions are less likely to be artifacts of demand characteristics.\nDeception. Since participants may sometimes infer the true purpose of the study (even without being told), it is sometimes useful to lead the participant to conclude that the experiment is about something else. The researcher may explicitly lie about the purpose, or implicitly lead the participant to infer that the experiment has a different purpose.\nDouble Blind Procedure. When both the experimenter and the participant are unaware of the purpose of the experiment, the experiment is said to be double blind. Double blind experiments are especially useful when the experimenter is required to make some subjective interpretation. For example, in music-education research, an experimenter may be required to assess whether or not a musician has improved. If the experimenter knows whether the musician is in the experimental-curriculum or control group, this is likely to lead to experimenter bias. It is better if both the musician-participant and the experimenter don’t know whether the participant is in the intervention condition or in the control condition.\nConfidence Building. Especially when participants are disposed to behave in a cooperative fashion, participants are less likely to be influenced by demand characteristics if you boost their self-confidence. If they are confident, then they are more likely to feel comfortable with their own opinions/judgments, rather than attempting to respond in a way that they think will please the experimenter.\nConfidence-building can be done in five basic ways.\n\nEmploying tasks that are relatively easy for the participant. (This might be purposely done toward the beginning of the experiment, where subsequent tasks become more difficult.) The simplicity of the task raises the self-confidence of the participant by making them aware of their own mastery.\nEncouraging self-confidence through verbal feedback: “You seem to be really good at this task.” “You were selected for this experiment because of your background and expertise.” Such feedback may be honest or deceptive. That is, even if a participant is not especially good at the task, such positive feedback will still boost their self-confidence and reduce the likelihood of them looking to echo what they think the experimenter wants.\nEmphasizing that there is no “right” or “wrong” answer, etc. and that we are genuinely interested in the participant’s unique view, opinion, or experience. Task instructions can be worded to reinforce this. For example, instead of asking “Which musical passage sounds better?” Ask “Which musical passage do you think sounds better?”\nBehaving in a deferential way toward the participant, or reducing the appearance of being an authority figure. A participant who feels they are socially equal to, or superior to the experimenter is more likely to respond honestly. For many participants, if the experimenter is an older male professor dressed in a white lab coat using formal language with a deep confident voice, then participants may be more likely to respond in a way that conforms to what the participant thinks the experimenter wants. Conversely, for many participants, if the experimenter is a younger female student dressed casually using colloquial language with a tentative insecure voice, then participants may be more likely to respond honestly. Along with dress, attitude, language, age and sex, physical setting also plays a role: people will feel more self-confident at home than in a laboratory environment.\nEmploying tasks that are not socially or politically charged. For example, asking “Which of these two tones has a longer duration?” is likely to incur fewer demand characteristics than asking “Which of these musical styles is better?”\n\nWithin- and Between-Subjects Designs. Recall dependent and independent variables. The independent variable (the one the experimenter manipulates) was whether the composer for a selection was deemed male or female. Within-Subjects Design. (what I used in my composer prejudice study). When the dependent variable is manipulated across its range for each each subject. Between-Subjects Design. when the dependent variable is fixed for each subject, but varies between subjects. E.g. Here are some excerpts of contemporary music, all written by women composers. Rate how good they are.\nImplicit Measures. Some kinds of dependent measures are less susceptible than others to demand characteristics. In particular, self-reports are often easiest to collect, but they are the easiest to be confounded by demand characteristics. There are several types of implicit measures, but one of the best is reaction-time (speed of response), since participants have no time to “think.”.\nAre you prejudiced against women or blacks? Virtually no one admits that they are prejudiced — even if their behaviors indicate that there are underlying predilections and biases. A useful technique for identifying a person’s preconceptions is the implicit association test (IAT).\nSuppose that you are interested whether people think that Native Americans are viewed as less “American” than white Americans. The implicit association test allows you to glimpse such prejudices. First, you begin with a series of photographs of people. Half of the photographs are of people who are obviously white and half are of people who are obviously native Americans. The task is simply to respond as quickly as possible to whether the faces are white or native-American. Next, we give you a task consisting of place names. Here the participant is asked to respond as quickly as possible to whether the place names are foreign or American. Again, the task is straightforward with places such as Seattle, France, Ohio, Russia, Miami, and Oslo. These tasks establish a person’s baseline reaction-times for identifying “white,” “native,” “foreign,” and “American.”\nNow same tasks joined together. Press one key if image is either a white person or an American placename; press a different key if the image is either a native-American person or a foreign placename. Finally, the reverse pairings are made: press one key if the image is either of a white person or a foreign placename; press a different key if the image is either a native-American or an American placename.\nIn this task, if a person tends to unconsciously think of Native Americans as less American than a white person, then they will tend to exhibit faster responses when Native+foreign are linked, and slower responses when Native+American are linked. Similarly, they will tend to exhibit faster responses when white+American are linked, and slower responses when white+foreign are linked.\nDo people tend to associate blacks with crime? Arabs with terrorism? Reggae music with drugs? The implicit association test has been widely used to investigate a wide range of associations and unconscious prejudices. You can take some of these tests yourself at https://implicit.harvard.edu/implicit/demo/\nDebriefing. The best method for becoming aware of demand characteristics is through the post-experiment interview. After an experiment, sit down with the participant and ask them to describe their experience. Begin with the most general questions: “How did it go?” “Did you find it easy or difficult?” “Did you have a strategy for doing this task?” The idea is simply to get the participant to talk about their experience. Try to avoid asking leading questions. Simply be vigilant for comments that suggests that the participant misunderstood the instructions, or was using a strategy you hadn’t imagined. If necessary, begin to ask them more direct questions: “What do you think of the experiment?” “What do you think the purpose of the experiment is?” These sorts of questions might alert you to some unanticipated demand characteristics.\nThe value of debriefing is evident in a study by Huron, Kinney & Precoda (2006). The study was intended to test the conjecture that low transpositions of melodies are hear as more aggressive than high transpositions of the same melodies. One participant was asked to judge how aggressive each melody sounded. In the debriefing, one participant remarked:\n“Some of the melodies were quite low, and I thought it was obvious that lower melodies would sound more aggressive, so I discounted that and tried to focus on the melody itself, and what would make it sound aggressive.”\nAlthough the participant hadn’t inferred the purpose of the experiment, she nevertheless regarded the main effect as obvious, and so answered in a way in which she thought controlled for the very effect we were anticipating. In this case, we used the results of the post-experiment interview to eliminate that person’s data from the analysis. Moreover, this was done before looking at any of the data.\nNotice that debriefing does not reduce demand characteristics. Instead, debriefing alerts you to the possible existence of demand characteristics. Debriefing is cheap and easy to do.\n\nSlogan: Always debrief.\n\n\nReferences:\nDouglas Crowne and David Marlowe, (1960). A new scale of social desirability independent of psychopathology. Journal of Consulting Psychology, Vol. 24, pp. 349-354.\nDavid Huron, Daryl Kinney, and Kristin Precoda (2006). Influence of pitch height on the perception of submissiveness and threat in musical passages. Empirical Musicology Review, Vol. 1, No. 3, pp. 170-177.\nSamuel Messick and Douglas Jackson (1961). Acquiescence and the factorial interpretation of the MMPI. Psychological Bulletin, Vol. 58, No. 4, pp. 299-304.\nRichard E. Nisbett (2003). The Geography of Thought: How Asians and Westerners Think Differently … and Why. New York: The Free Press.\nMartin Orne (1962). On the social psychology of the psychological experiment: With particular reference to demand characteristics and their implications. American Psychologist, Vol. 17, pp. 776-783.\nA.H. Pierce (1908). The subconscious again. Journal of Philosophy, Psychology, and Scientific Methods, Vol. 5, pp. 264-271.\nOlaf Post (2011). “The way these people can just listen!”: Inquiries about the Mahler tradition in the Concertgebouw. PhD Dissertation, Columbia University Department of Music.\nFrank Ragozzine (2011). Cross-modal affective priming with musical stimuli: Effect of major and minor triads on word-valence categorization Journal of ITC Sangeet Research Academy, Vol. 25, pp. 8-24."
  },
  {
    "objectID": "emp_methods_workshop/explore_then_test2.html",
    "href": "emp_methods_workshop/explore_then_test2.html",
    "title": "Explore then Test, part 2",
    "section": "",
    "text": "By Tara Parker-Pope New York Times September 10, 2012, 8:10 PM\n[continuation of the article]\nThe researchers then conducted an experiment pairing students with a friendly-faced robot, developed by Cynthia Breazeal, who directs M.I.T.’s personal robots group.\nThe setup was basically the same, except the students had a 10-minute conversation with the robot before they played the game. (The extra time was needed to help the student get over the “wow” factor of talking to a robot.) A woman acted as the robot’s voice, but she was unaware of its movements, which were controlled by two other people. Sometimes the robot used only typical gestures, like moving a hand or shrugging its shoulders, but sometimes it mimicked the four cues of distrust: clasping its hands, crossing its arms, touching its face or leaning away.\nSurprisingly, when students saw the robot make the hand and body gestures associated with distrust, they later made decisions in the token game that suggested they didn’t trust the robot.\nIn questionnaires afterward, students in both groups rated the robot equally likable. But those who had unknowingly witnessed the cues associated with distrust also rated the robot as less trustworthy, compared with students exposed to only the conversational gestures.\n“It makes no sense to ascribe intentions to a robot,” said an author of the study, Robert H. Frank, an economics professor at Cornell. “But it appears we have certain postures and gestures that we interpret in certain ways. When we see them, whether it’s a robot or a human, we’re affected by it, because of the pattern it evokes in our brain responses.” Dr. Frank said the study suggested that there might have been an evolutionary benefit to cooperation — and, more important, to the ability to determine who could be trusted.\n[N.B. A video of the robot making untrustworthy gestures can be found at: <http://well.blogs.nytimes.com/2012/09/10/whos-trustworthy-a-robot-can-help-teach-us/?ref=health]"
  },
  {
    "objectID": "emp_methods_workshop/stats_day_outline.html",
    "href": "emp_methods_workshop/stats_day_outline.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Outline for Our Day with Statistics\n\n\nToday, we’re going to spend the entire day talking about statistics. We’ll learn how to apply various statistical procedures to different musical problems.\nWe’ll start off by talking about measurement. Recall that there are two reasons for measuring: (1) To allow us to recognize when we are wrong (i.e., “We recognize failure by drawing a line in the sand.”) (2) To discover new phenomena that are otherwise invisible.\nWe’ll define what is meant by measurement. We’ll discuss some limitations of measurement and consider the main criticisms against measurement. We’ll see that there is broad value in measuring things that at first might seem impossible, or at least highly questionable.\nThe field of statistics is broadly divided into two: (1) descriptive statistics, and (2) inferential statistics. As you might imagine, descriptive statistics allow us to describe things. Inferential statistics is used to test hypotheses. Both descriptive and inferential statistics are used in exploratory research. But hypothesis testing relies exclusively on inferential statistics.\nInferential statistics is itself divided into two: (1) frequentist inference, and (2) Bayesian inference. Frequentist statistics are by far the most commonly used statistics in research—and that will be our exclusive focus in this workshop. However, Bayesian statistics have become increasingly important, and you should at least be aware that there are other approaches to statistics than what we will cover here.\nWe’ll cover the basics of descriptive statistics, including familiar concepts (such as an “average”), and related concepts like median and mode. Other descriptive concepts will include measures of variation such as range and standard deviation.\nYesterday, we saw that that there are four different kinds of measurement scales — nominal, ordinal, interval, and ratio scales. Today, we’ll see that, depending on the measurement scale, it may be necessarily to use different statistical methods.\nWe’ll describe in detail the logic behind a statistical test.\nWe’ll define the concept of statistical significance, and show you one way of calculating it.\nWe’ll teach you how to perform a chi-square test, and we’ll give you a good number of exercises so that you can feel confident doing this inferential test.\nWhen you read an empirical research paper, you often see a statement followed in parentheses by a string of numbers and funny Greek letters: today we’ll teach you the basics of how to interpret the meaning of these things.\nThis afternoon, we’ll hear a guest lecture by Professor Josh Albrecht from Mary Hardin Baylor University in Texas. Josh will lead us through some advanced statistical methods, notably multiple regression, cluster analysis, factor analysis, multi-dimensional scaling. and modeling. Josh will walk you through some sample research projects in music that make use of these methods. The purpose will be to show you some of the musical things you can do with statistics rather than teaching you how to do them.\nFinally, we’ll review the homework reading.\nOur aim today is two-fold: One is to get your feet wet and to teach you how to perform a specific statistical test. The test you’ll learn is one of the most robust tests — a test that can be performed in a wide variety of circumstances with lots of different data. Moreover, it is a test that you can calculate with pen-and-paper, without having to use a computer.\nOur second aim is to give you the lay-of-the-land: to introduce you to the broad sweep of what’s possible with statistics."
  },
  {
    "objectID": "emp_methods_workshop/context.html",
    "href": "emp_methods_workshop/context.html",
    "title": "Some Thoughts About Context",
    "section": "",
    "text": "One of the principal tenets of qualitative methodology has been the importance of understanding a phenomenon in context. From the perspective of qualitative research, quantitative research methods unduly ignore context. Quantitative research tends to treat context as an extraneous variable whose effects need to be controlled—either by holding the contextual effects constant, or by randomizing. By explicitly ignoring context, researchers may inadvertently exclude the most important factor pertaining to the phenomenon they are studying.\n A simple (and well-known) illustration of the role of context is found in Claude Debussy’s “Golliwog’s Cakewalk” from the Children’s Corner suite. The initial motive is a direct quote of the love-death leitmotif from Wagner’s Tristan und Isolde opera. Here, Debussy offers a parody of the “seriousness” of Wagner’s music. Music is not just sequences of sounds. Music reaches into the broader realm of social meaning, cultural reference, and political expression.\nScholars are right to draw attention to the importance of context. However, there are common misconceptions regarding context, the concept of holism, and the complexity of theories.\n\n\nTug on the smallest strand of existence, and soon you are pulling on an enormous web of connections. When a person behaves in a certain way, that behavior is shaped by innumerable factors—including social norms, cultural beliefs, economic conditions, media influences, peer and family pressures, and so on. Choices are also shaped by historical antecedents, personal autobiographical experience, education, gender identity and expectations, as well as personal and social power relations.\nBut that’s not all. Humans exist in an ecological context of flora and fauna. We exercise power relationships over plants and animals. We shape and are shaped by our environments, including rivers, mountains and oceans. Our behaviors are linked to climate and weather. Our beliefs and actions are shaped by viruses, bacteria, plagues, and accidents. Our physiology and behavior are shaped by the diurnal movements of the sun, and by seasons caused by the tilt of the earth. Our movements are influenced by the force of gravity, and by the size and structure of our bodies.\nBut that’s not all. Behaviors are also influenced by drugs, by physiology, neurochemistry, age, hormonal changes, immune responses, and yes, epigenetic and genetic factors. Humans are part of a grand ongoing experiment called evolution by natural selection: we are related as brothers and sisters to every living creature on the planet.\nBut that’s not all. Humans are made of atoms. With the exception of hydrogen, all of the atoms in our bodies were generated by the nuclear reactions of stars, and these atoms were dispersed into the universe because of the gargantuan explosions known as supernovas. Our bodies are literally constructed from stardust. We are intimately connected to the cosmos in ways that medieval astrologers would have found breathtaking. Once again, tug on the smallest strand of existence, and soon you are pulling on an enormous web of connections.\nThe problem with the use of the word “context” is that researchers hold specific preconceptions about what counts as an appropriate context. In practice, researchers who favor exclusively qualitative methods have tended to use the word “context” to mean “social, cultural or political context.” Among self-professed qualitative researchers, context rarely means ecological, nutritional, geographical, neurological, economic, or evolutionary context.\nPolitical and social scholarship encourages us to ask the question Whose context? Who decides what counts as legitimate contextual considerations for some phenomenon? Historically, both qualitative and quantitative researchers limit the context of their research. Researchers simply make different assumptions about which contexts might be germane. In short, the argument really isn’t about whether or not a researcher is sensitive to context. The argument pertains to which contexts are favored.\nIt is true that, in the past, many quantitative researchers have limited their research by controlling (i.e., discounting) the potential effects of social, cultural or political factors. But it is equally true that, in the past, most qualitative researchers have failed to consider other contexts that may play important roles in the phenomena they describe (including music). In the same way that we can’t assume all phenomena trace back to neuroanatomy, we can’t assume that all phenomena trace back to political class structure. The amount of context that’s pertinent to any given phenomenon is an empirical question. We simply have to do the research if we want to know whether (say) culturally-defined gender roles are more pertinent than (say) hormones levels for some specific behavioral phenomenon.\n\n\n\nThere are similar problems with the notion of a “whole.” When is our description unduly narrow, and when is our description sufficiently holistic? The point is well made by the anthropologist, Victor De Munck:\n“The point is that most wholes are also parts of larger wholes and are, in part, shaped by those larger wholes, so that any rigorous distinction between holism (usually privileged in anthropology) and reductionism (usually scorned) is absolutely false.”\n-Victor De Munck (2009).\nOnce again, in research, reductionism is a strategy for discovery, not a belief about how the world is. Researchers simplify problems, not because they believe problems to be simple, but because they believe problems to be complex. We cannot explain the whole world at once—and we should not even try. In research, we isolate portions of the complex web of existence, merely as a way of helping us to glimpse some of the relationships. Aiming for a holistic understanding is the ultimate goal; but holism is a goal not a method. Inevitably, as researchers, we must all narrow our fields of vision, simply because we have no better alternative for understanding the world.\n\n\n\nOften, discussions of reductionism versus holism are really about the complexity of theories. Simple theories have a limited range. Theories that are sensitive to many contexts will necessarily be more complex. Unfortunately, there is a price to pay for complex theories that involve many component parts.\nSelf-confident individuals can certainly offer sweeping interpretations about complex events in the world. But often these sweeping interpretive frameworks are unfalsifiable—they don’t invite the world to tell us that the interpretations are wrong. Preempting the possibility of failure is antithetical to the intellectual humility that is essential for good research.\n\n\n\nBy way of summary:\n\nContext is important, and researchers should always aim to contextualize their work. However, there is more than one context. For example, dancing is surely influenced by historical context; but dancing is also influenced by the biomechanics of bodily movement. The researcher who focuses on the historical context of dancing is not “more context sensitive” than the researcher who focuses on the context of human kinematics. We won’t know which context has a greater influence on dancing until both scholars do their research.\nReductionism is a method, not a goal. We simplify problems, not because the problems are simple, but because focusing on a part of the problem is a helpful strategy for discovery.\nHolism is a goal, not a method. We’d like to achieve an integrated account that embraces all of the pertinent contextual factors. But the methodology of reductionism is the best method we have for getting there.\nComplex theories are difficult (often impossible) to falsify. Complicated theories go contrary to the self-correcting dynamic of good research. It’s not helpful to start with a complicated theory—not because the theory may be wrong, but because it is difficult to show wrong complex theories to be wrong.\n\n\n\n\nVictor De Munck (2009). Research Design and Methods for Studying Cultures. Plymouth, UK: AltaMira Press."
  },
  {
    "objectID": "emp_methods_workshop/random.html",
    "href": "emp_methods_workshop/random.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "In the space below, pretend you are flipping a coin. Write-out a sequence of heads (H) and tails (T) that you might expect would be produced. (Generate 50 virtual “flips.”)\n\n\n\n\n\n\n\n\n\n\n\n\nTake out a coin, and flip it 50 times. Write-out the results (H/T) below."
  },
  {
    "objectID": "emp_methods_workshop/causality.html",
    "href": "emp_methods_workshop/causality.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Correlation and Causality\n\nIn 1994, executives of the seven largest US tobacco companies gave sworn testimony before a congressional inquiry. Despite decades of research, they testified (under oath) that there is no scientific evidence whatsoever that smoking causes lung cancer in humans. This event was widely regarded as a low point in business ethics—CEOs of major corporations willing to make bald-face lies under oath in order to preserve their businesses.\nIt may be that the tobacco executives behaved extremely badly. But what was missed was the opportunity for the public to better understand how empirical research works. Let’s go into some detail.\nThe Third Variable Problem.\nThere is a strong positive correlation between death by drowning and consumption of ice cream: whenever ice cream consumption increases, more people die by drowning and vice versa. It’s hard to imagine how one might cause the other. So what is responsible for this correlation?\nThe likely origin of this relationship is warm summer days. When the weather is nice, people tend to go to the beach. Lots of people go swimming, and regretably, some of those people are likely to drown. At the same time, warm summer days are also likely to encourage people to eat ice cream. So it is not that news of a drowning causes mourners to drown their sorrows by eating ice cream, or that eating large amounts of ice cream before swimming causes cramps and so leads to drowning. Instead, both ice cream consumption and swimming are caused by a third variable—warm summer days.\nNow consider the case of cigarette smoking and lung cancer. There is, in fact, a strong positive correlation between the two. The question is: Is this relationship causal, or just correlational?\nSuppose that there exists a gene: some people have this gene, and others don’t. The gene predisposes people who carry it to get lung cancer. At the same time, this gene also predisposes people to enjoy the experience of smoking. In short, there exists a third variable that is responsible both for smoking and for lung cancer. Notice that, in this scenario, smoking is not the cause of lung cancer, in the same way that consumption of ice cream is not the cause of drowning.\nWhenever we have a correlation between two phenomena, we cannot claim that one causes the other. There could always be some third variable (also called a hidden variable) that is responsible for both.\nThere is, of course, a method for testing whether a relationship is causal—namely, the true experiment. In the experiment, we manipulate one of the variables and observe the result. Let’s apply this to the case of tobacco. What kind of experiment would we need to perform in order to test the hypothesis that smoking causes lung cancer?\nLet us assemble a large sample of people for the experiment. We randomly divide our volunteers into two groups. One group we force to engage in smoking (whether they like it or not). The second group we force to never smoke (whether they like it or not). If there is a third variable (like a gene), this should be randomly distributed between the two groups, and so the group forced to smoke should be no more likely to get lung cancer.\nOf course, the problem with this experiment is that it would be impractical and immoral. We would never be able to enforce human behavior like this for a prolonged period of time. As you might expect, the proper controlled experiment needed to test the idea that smoking causes lung cancer in humans has never been carried out. The relationship is correlational, and so we cannot discount the possibility of causation through a third variable—unrelated to smoking. It is for this reason, that the tobacco company executive were able (under oath) to claim that “there is no evidence whatsoever that cigarette smoking causes lung cancer in humans.”\nNow here’s what they left out. Although the appropriate experiment has never been carried out with humans, these experiments have been carried out with mice, rats, and monkeys. Researchers have randomly divided animals into treatment and control groups, forcing some animals to regularly breath cigarette smoke. Sure enough, there is an association consistent with the hypothesis that smoking causes lung cancer. In addition, histological tests (tests with tissue samples) have shown that when tobacco chemicals are added to colonies of human lung tissue, the cells are more likely to become abnormal, and some cells become cancerous much beyond the normal rate of mutation. All of this was known at the time the tobacco company CEOs made their testimonies.\nThe third variable problem reminds us of the importance of true experiments in order to test causality.\nSlogan: No causation without manipulation."
  },
  {
    "objectID": "emp_methods_workshop/slogans16.html",
    "href": "emp_methods_workshop/slogans16.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Huron’s Research Slogans\n\n\nMotivated by truth, with no hope of Proof\nThere is no inductive proof. We are not in the business of proving something to be true. We would love to know the truth (if that exists), but we understand that we could never be sure of the truth, even if we had it. The best we can hope for is that what we observe is consistent with our theories.\nThe best research invites failure.\nGive the world an opportunity to tell you that you’re wrong. (This is the essence of good research.)\nWe invite failure by testing predictions.\nTest an idea by making a prediction, and then determine whether the observations are consistent with the prediction.\nWe recognize failure by drawing a line in the sand.\nIn order to make failure obvious, establish a criterion in advance that says, “If the evidence doesn’t cross this line, then I’ll admit failure.” In statistics, the line is referred to as the confidence level.\nAim not to be right, but to be not not right.\nInstead of establishing The Truth, our more modest aim is to be not obviously wrong. When our observations turn out to be consistent with our hypothesis, we don’t claim that we are right; instead the observations suggest that our hypothesis may not be wrong.\nTest hypotheses by operationalizing terms.\nTranslate all of the terms in a hypothesis into concrete things you can measure. We can’t directly measure concepts like “sadness.” We have no choice but to measure things using imperfect rulers.\nOperationalize, but don’t essentialize.\nAll concepts are inherently enigmatic and fuzzy. Terms like “melody,” “listen” or “note” can never be pinned-down. It is impossible to provide comprehensive definitions or grasp the essence of some concept. We are forced to approximate or estimate concepts through operational definitions — but don’t confuse the operational definition with the concept itself, and don’t assume concepts are “real.”\nCompare, compare, compare.\nContrast a “treatment” condition with one or more “control” conditions.\nThe rhetoric of science is the rhetoric of prophecy.\nPeople are most impressed when someone accurately foretells the future. Science is a form of rhetoric whose persuasive power resides in the testing of predictions. The rhetorical power of science comes not from scholars assembling evidence, but from scholars testing predictions.\nHindsight is 20/20.\nMost things seem obvious in retrospect (hindsight bias). When the results aren’t obvious, humans are enormously gifted at coming up with explanatory accounts. We can make up a story for just about any set of data. Post hoc theories don’t have the same plausibility as a priori theories. The true test is making up the story first (i.e., prediction)! Prefer theorizing first, then collect your data.\nReductionism is a method, not a belief.\nWe simplify problems, not because we believe problems to be simple, but because we believe problems to be complex. Restricting our gaze is a useful strategy for discovery.\nDon’t try to explain the whole world at once.\nManipulate one variable at a time. Seek simplicity, even as you distrust it.\nGeneralize, but don’t universalize.\nWhen presenting your results, frame them narrowly rather than broadly.\nAvoid chronic hypothesislessness.\nExploratory and descriptive studies are important, but you can’t invite failure without testing predictions.\nBeware of the post hoc theory.\nThe scholar who only offers theories after looking at the evidence is a scholar who is never wrong. Post hoc theorists don’t allow the world to tell them when their ideas are problematic.\nFrom Question to Theory to Conjecture to Hypothesis to Protocol.\nStart with a question, propose an explanatory theory, derive a conjecture, refine the conjecture into a hypothesis, then operationalize the terms of the hypothesis into a protocol. The protocol provides an action plan for how to carry out the research."
  },
  {
    "objectID": "emp_methods_workshop/confidence_level.html",
    "href": "emp_methods_workshop/confidence_level.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Confidence Level and Confidence Interval\n\nIn survey reports, it is common to see results expressed in the following way:\nA survey of likely voters in Ohio indicates that 52% favor the democrats with 48% favoring the republicans. The margin of error is plus or minus 2%, 95 percent of the time.\nFirst, recall that a sample is only an estimate of the true population parameter. In reality, the true value could be almost anything. For example, it could be that only 46% of likely Ohio voters favor the democrats. (Statistics is never having to say your sure.)\nThe report above can be interpreted as follows. We can be 95% “confident” that the true proportion of voters who support the democrats is between 50% (52 - 2) and 54% (52 + 2). In other words, there is a 95% chance that the margin of error contains the true population value.\nThe “plus or minus 2” phrase is referred to as the margin of error or the confidence interval. They are both the same thing. “Margin of error” is commonly used in the general population, however “confidence interval” (abbreviated CI) is the preferred term used by researchers. You could imagine studies in which the confidence interval is smaller than ±2. As you might expect, reducing the margin of error typically involves increasing the sample size.\nThe phrase “95 percent of the time” is referred to as the confidence level. Once again, you could imagine different confidence levels. For example, we might want to have a confidence level of 99 percent of the time.\nWhat does a confidence level of 95% mean? If we carried out 100 similar surveys (each with the same sample size), we would expect 95 of the error margins to contain the true value of the population parameter. We would anticipate that 5 out of 100 similar surveys would produce a value outside of the ±2 range. In the case of a 99% confidence level, this would mean that only 1 out of 100 similar surveys would produce a confidence interval that does not contain the true population mean.\nThere are statistical tools that can tell us the size of sample needed to produce a given confidence interval for a given confidence level. For example, see http://www.surveysystem.com/sscalc.htm for an online calculator.\nRecall our earlier slogan: We recognize failure by drawing a line in the sand. In empirical research, the confidence level is that line."
  },
  {
    "objectID": "emp_methods_workshop/reading0X.html",
    "href": "emp_methods_workshop/reading0X.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Reading #3: Turner & Huron (2008)\n\nRead: Ben Turner and David Huron (2008). A comparison of dynamics in major- and minor-key works. Empirical Musicology Review, Vol. 3, No. 2, pp.64-68. The article can be found at: <https://kb.osu.edu/dspace/bitstream/handle/1811/31941/EMR000047a_Turner_Huron.pdf?sequence=1\nWhile reading this article, keep the following comments and questions in the back of your mind:\n\nIn this first reading, you are not expected to understand any of the statistics, so don’t worry about the statistical descriptions.\nWhat is the hypothesis?\nDo the authors have more than one hypothesis?\nWhat are the main terms in the hypothesis that need to be operationalized?\nHow do they determine “major” and “minor”? Are you convinced by the method they used?\nHow do they measures the dynamic level?\nThey sampled only the first dynamic level encountered in each piece. Could you propose a better method?\nThey identified 192 pieces for possible sampling, but they only analysed 48? Why didn’t the authors examine all 192 pieces?\nWhat is the conclusion?\nIn the conclusion, do the authors use suitably circumspect language?"
  },
  {
    "objectID": "emp_methods_workshop/economics.html#the-economics-of-research",
    "href": "emp_methods_workshop/economics.html#the-economics-of-research",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "The Economics of Research",
    "text": "The Economics of Research\n\nAny research project is like an economic transaction: in this case, you are “purchasing” information. Like any transaction, there are at least four questions of interest:\n\nHow much will it cost?\nWhat is the quality?\nWhen can I take delivery?\nIs this really what I want or need?\n\nAt the supermarket, two of these four questions are answered directly: the price is marked on the product so there is no question about the cost. Also, if you grab the product and put it in your shopping cart, there is no question about when you can take delivery — you can take it home immediately.\nThe question of quality is more difficult. If it is something you’ve purchased before, then you’ll have your past experience that will help estimate quality. If you have no past experience, choosing a well-known brand might give you some assurance — although caveat emptor (buyer beware) still applies. You might consult with someone who has some experience with the product and can offer some advice.\nMost mysterious of all is the fourth question, importance: Is this really what I want or need? How many times have you purchased something you thought you should eat (for example), but it stayed in your kitchen cupboards for weeks because you never really wanted to eat it?\nIn contrast to supermarket shopping, purchasing information is much more risky. All four questions are up in the air.\n\nCost\nSome empirical studies can be carried out with little or no cost — such as where you borrow a stack of scores from the library. Here the cost is principally your own time. Other studies require multi-million-dollar machinery (such as fMRI scanners) and specially-trained staff.\nThere are a number of ways to handle the costs of research. These include: (1) Self-funding - Pay out of your own pocket. For small costs, this is often the simplest and quickest solution. (2) Borrow - Borrow equipment or facilities from others. (In the long run, this may make you unpopular.) (3) Collaborate - Join forces with someone who already has access to the skills, equipment or facilities you want. (4) Recruit volunteers - Find others willing to do the work for free. Traditionally, this has fallen on the shoulders of students. However, the web has provided new opportunities to seek the help of volunteers willing to lend a hand. Astronomers have very successfully recruited thousands of web volunteers to help with labor-intensive research tasks, such as classifying galaxies. There are millions of retired people looking for meaningful forms of volunteer work. You may need to pay for a research coordinator, however. (5) Seek financial support through grant applications.\nApplying for research grants can be time-consuming and unpredictable, so the motivation to seek funding to pay for high-cost research will depend on the importance of the anticipated results.\nBy definition, research always involves something you’ve never done before. (There’s no such thing as “routine research”.) So unlike most activities, research is inherently unpredictable. Even if you receive a large amount of financial support, there is no guarantee that your question will be answered. (It’s not research if you don’t invite failure.)\nQuality\nData vary widely in quality. EEG (brain wave) data are notoriously poor in quality. The important features are typically drowned-out by large amounts of uninterpretable noise. EEG researchers typically average hundreds of trials involving a dozen or more participants in order to glimpse any pattern in the brain waves. Reaction-time data are typically better quality.\nMusical scores have proved to provide remarkably high quality data for testing hypotheses. Unlike bored participants pushing buttons in an experiment, composers are often very discerning and deliberate when they chose one note over other possibilities. For example, Huron (1991) carried out a notation-based empirical study in which two factors were found to account for just under 90 percent of the variance in J.S. Bach’s choice of harmonic intervals. This is unheard of in typical psychological research because the collected data contain so much noise. Bach was very careful in his choice of harmonic intervals so it was much easier to decipher his principal motives.\nIn general, skilled participants (like musicians) provide better data than unskilled participants. Children can be difficult to work with. Infants and non-human animals often very difficult to work with.\nIn estimating costs, experience is the best guide. Many research projects are not attempted because knowledgeable researchers are aware that it would be difficult or impossible to collect data of sufficient quality.\nDelivery\nThe duration of a project is often the most unpredictable part of research. It is common to begin a project with the expectation that it will take a month or two, but in the end, it might take a year or more. Of cost, quality, delivery and importance, delivery is commonly the most badly estimated.\nImportance\nBehavioral economists have documented a phenomenon called “buyers remorse.” This is the feeling of disappointment a person sometimes experiences following a large purchase. Suppose you had $10,000. What would you do with it? You might consider investing it for your retirement. Or you might use it to purchase a new car. Perhaps a boat, or a round-the-world vacation with your partner, or maybe you’ll donate the money to a worthy charity. When sitting on a pile of money, people take pleasure in thinking about all the possible things they could do with it.\nOnce you spend the money, all of the possibilities “collapse” on just one outcome. If you purchased a new car, you might regret that you didn’t purchase the boat you have always dreamed of. Once you spend the money on a world vacation, you may regret that you weren’t more prudent by investing the money in a retirement account. As long as you don’t spend the money, you inhabit a world of possibilities — a world in which you can imagine having fun, being prudent, being virtuous, and so on. In the form of unspent cash, the money provides many opportunities to dream.\nSomething similar to buyer’s remorse happens with research. Once you select a research project and begin working on it, you lose some of the former freedom of thinking about all the things you could be doing with your time. Like spending your money, you need to be careful how you spend your time.\nIn order to minimize “researcher’s remorse” it is important not to pursue your first research idea. Think of several ideas, and then choose the one you think is the most important — but also has a good chance of quick delivery, good quality results, and doesn’t have a high cost."
  },
  {
    "objectID": "emp_methods_workshop/explore_then_test1.html#whos-trustworthy",
    "href": "emp_methods_workshop/explore_then_test1.html#whos-trustworthy",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "Who’s Trustworthy?",
    "text": "Who’s Trustworthy?\n\nBy Tara Parker-Pope New York Times September 10, 2012, 8:10 PM\nHow do we decide whether to trust somebody?\nWhile many people assume that behaviors like avoiding eye contact and fidgeting are signals that a person is being dishonest, scientists have found that no single gesture or expression consistently predicts trustworthiness.\nBut researchers from Northeastern University, the Massachusetts Institute of Technology and Cornell recently identified four distinct behaviors that, together, appear to warn our brains that a person can’t be trusted.\nThe findings, to be published this month in the journal Psychological Science, may help explain why we are sometimes quick to like or dislike a person we have just met. More important, the research could one day be used to develop computer programs that can rapidly assess behavior in airports or elsewhere to flag security risks.\nIn the experiment, 86 undergraduates from Northeastern were given five minutes to get to know a fellow student they hadn’t met before. Half the pairs met face to face; the other half interacted online by instant message.\nThen the students were asked to play a game in which all the players got four tokens and the chance to win money. A token was worth $1 if a player kept it for himself or $2 when he gave it to his partner. Players could win $4 each if both partners kept their tokens, but if they worked together and traded all four tokens, then each partner could win $8. But the biggest gain — $12 — came from cheating a partner out of his tokens and not giving any in return.\nOver all, only about 1 in 5 people (22 percent) were completely trustworthy and cooperative, giving away all their tokens so that each partner could win $8. Thirteen percent were untrustworthy, keeping all or most of their tokens. The remaining 65 percent were somewhat cooperative, giving away two or three tokens but also holding one or two back for security.\nBoth groups demonstrated the same level of cooperation. Whether the students met face to face or online didn’t change their decisions about how many tokens to give away or keep. But students who met in person were far better at predicting the trustworthiness of the partner; that suggested they were relying on visual cues.\n“Lack of face-to-face contact didn’t make people more selfish,” said the study’s lead author, David DeSteno, a professor of psychology at Northeastern. “But a person’s ability to predict what their partner was going to do was greater face to face than online. There is something the mind is picking up that gives you greater accuracy and makes you better able to identify people who are going to be trustworthy.”\nTo find out what cues the players were responding to, the researchers filmed the students’ five-minute conversations before the game started. They discovered that four specific gestures predicted when a person was less trustworthy: leaning away from someone; crossing arms in a blocking fashion; touching, rubbing or grasping hands together; and touching oneself on the face, abdomen or elsewhere. These cues were not predictive by themselves; they predicted untrustworthiness only in combination.\nAnd individuals intuitively picked up on the cues. “The more you saw someone do this, the more intuition you had that they would be less trustworthy,” Dr. DeSteno said.\n“Life is all about finding people you can trust in different situations.”"
  },
  {
    "objectID": "emp_methods_workshop/data_independence.html",
    "href": "emp_methods_workshop/data_independence.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Data Independence\n\nSuppose you were carrying out a telephone-based survey whose aim is to sample likely voting behavior for an up-coming election. Using an auto-dialing system, you are connected to successive random telephone numbers for land-lines in a particular geographical district. Once you make contact, you ask the survey participants: (1) whether they are eligible and likely to vote in the election, and (2) which candidate they are likely to vote for. You find that most telephone calls are unsuccessful: either no one answers, or the person is unwilling to participate. You are pleased whenever you encounter a cooperative respondent. As you complete your conversation with one person, you hear another voice in the background—it is the respondent’s husband. Eager to recruit another participant, you ask whether it would be possible to speak with the person’s spouse. They agree, and so you are able to collected data from two people rather than just one.\nSaavy researchers discourage this sort of behavior. People who live together share many things in common—including attitudes. For married couples, both spouses are likely to share similar political views. Collecting data from two people who live together is much less useful than collecting data from two unrelated people. When collecting data this way, the results will appear more uniform than would occur in a proper representative sample. The data collected from 50 couples is less representative than the data collected from 100 independent people.\nAll statistical methods used in empirical research assume “independent data.” That is, the analysis methods presume that each datum was collected in a way that avoids undue similarity.\nSuppose we asked someone to judge the degree of “expressiveness” in several recorded performances of the same work. If we collected all of our data from a single judge, then our measure of “expressiveness” would really be “Alice’s idea of expressiveness” or “Adam’s idea of expressiveness” rather than “expressiveness.” We may need to recruit many judges in order to increase the representativeness of “expressiveness” judgments. This is the reason why most experiments involve multiple participants rather than relying on a single person.\nNow consider the musical stimuli created for such an experiment. Suppose we recruited a single flute player (“Jean-Pierre”), and instructed him to play several renditions with differing degrees of musical expressiveness. Once again, the musical results might be limited. Instead of judging “expressiveness”, our participants would really be judging “expressiveness as produced by Jean-Pierre.”\nNow suppose that we recruited many performers, and many listeners and had the listeners judge the degree of expressiveness of different recorded performances of a passage by Hindemith. Once again, we may have a problem. The problem is that “expressiveness” may pertain to how performers interpret Hindemith, or interpret a particular musical passage by Hindemith. If our goal is to understand “expressiveness” in general, then we ought to broaden our stimuli to include different passages by different composers.\nIn each of these cases, the question is how representative of the population is our sample? How representative of composer’s expressiveness is Hindemith? How representative of performer’s expressiveness is Jean-Pierre? How representative of expressive judges are Alice and Adam?\nIn the ideal world, we would aim to have truly independent data by having each listener judge just one recording, and have each recording performed by a different performer using a different musical instrument playing a unique musical passage written by different composers from different periods and nations. Rather than having 1 person judge 100 stimuli, it would be better to have 100 people judge our stimuli. And rather than repeating the same 100 stimuli for each participant, it would be better for each participant to hear entirely novel stimuli.\nIn actual practice, researchers routinely violate the “data independence” ideal by having participants respond to more than one stimulus, having only a handful of judges evaluate something, or using the same stimuli for all participants. However, it is important to recognize that this is merely a matter of convenience. Having made the effort to participate in an experiment, most participants would think it silly to make just a single judgment.\nWhenever collecting data, ask yourself: What possible connections exist between this datum and the other data I’m collecting? In what ways do the data have similar origins that reduce the data independence? What practical steps might I take that would increase the data independence? Try to avoid data that clump together because of similar origins. If we think of clumped data as “sticky,” our slogan can be phrased as follows:\nSlogan: Avoid sticky data.\n\nAuditions and Music Adjudications\nIncidentally, the idea of data indepence has repercussions for doing music auditions and music adjudication. Most music schools conduct auditions and juries intended to evaluate the performance skills of different musicians. Rather than rely on a single judge, it is common to creat a two- or three-member panel. If music examiners are permitted to talk with each other before assigning their grades, then the independence of their judgments will be reduced. Suppose for example, that the three judges mentally judge a performer as roughly A-, B, and C+ respectively. In conversing among themselves, the person who speaks first will have the greatest impact on the final assessment. For example, if the first judge says “That was pretty good.” The second judge might already be thinking that their B could be revised to B+, and the third judge might be thinking that a B- or B might be appropriate. Conversely, if the third judge speaks first “I think there are some issues here …” the other two judges may mentally begin to down-grade their initial mental assessments.\nNearly every institution allows adjudicators to talk among themselves before assigning their grades. So instead of the three original grades: A-, B, and C+, the grades might be A-, B+ and B (if the first judge speaks first) or B, B- and C+ (if the third judge speaks first). Institutions often encourage conversation among the adjudicators because it makes the process look consistent. If a student were to receive a report giving A-, B, and C+, then the student is likely to think the assessments are invalid—and the judges are likely to feel some embarrassment. At many institutions, judges informally collude to avoid differences of more than a single point, so a performer might receive B, B, and B-. In fact, the process really is highly variable. But this variability is masked by collusion. Note that if the assessment process were highly reliable, then only one judge would be necessary.\nNotice that the simple act of talking makes the data “sticky.” A person’s judgment is no longer just “their judgment;” it is influenced by the other judges. As a consequence, it is a mistake to view the three grades as being produced by three independent judges. Later, we will learn that statisticians refer to data independence as degrees of freedom (abbreviated df). Colluding reduces the degrees of freedom, with the consequence of increasing the likelihood of making both Type I and Type II errors."
  },
  {
    "objectID": "emp_methods_workshop/quotes.html#citations-mentions-and-quotations",
    "href": "emp_methods_workshop/quotes.html#citations-mentions-and-quotations",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "Citations, Mentions, and Quotations",
    "text": "Citations, Mentions, and Quotations\n\n\nCitations\nA proper citation occurs when the cited publication supports the immediately preceding claim. An example would be:\nIn the 19th century, most minor-mode music is actually faster in tempo than most major-mode music (Post & Huron, 2009).\nThe abbreviation cf. refers to the Latin “confer” — which is best translated as “compare.” Since the abbreviation represents a single word, we write cf. rather than c.f. In scholarly citations, cf. is used to mean compare or contrast. It is used when the cited publication:\n\nmakes the same point, but using a somewhat different approach (not discussed here)\nmakes the same point, but only in passing; for example, the cited publication presents data that supports this point but the author(s) of the cited work don’t draw attention to this result (not discussed here)\nmakes the same point but provides a refinement (not discussed here), or\ntakes an opposing or alternative point of view (which will not be discussed here).\n\nAn example would be:\nMost music in the minor mode is slower in tempo than music in the major mode (cf. Post & Huron, 2009).\nWhen a cited work is contrary to a claim, it is common to add the phrase “although see:”\nThe amount of syncopation in American popular music increased steadily from 1850, peaking around 1970 (Smith, 1998; Smith & Jones, 1999; Jones & White, 2008; although see Black & White, 2006).\nIn complicated situations, an explanatory footnote or endnote is appropriate. Footnotes are ubiquitous in historical journals, however publishers generally dislike footnotes and usually ask authors to minimize or restrict their number.\n\n\nTopical Citations\nA topical citation refers to a publication that is cited because it deals with the same or related topic. The cited work may be old, deal with somewhat different concerns, or be superceded by later research. The topical reference doesn’t support a particular claim made in the text. Nevertheless, topical references help to situate the current work in a network of past and ongoing research and demonstrates the author’s awareness and engagement with the work of other scholars.\nConsiderable research has been carried out regarding human facial expressions (Duchenne, 1862; Darwin, 1872, Spitz & Wolf, 1946; Ekman, 1973, 1980, 2006; Fridlund, 1994; Freitas-Magalhães, 2007)\nAlthough much research is published today in English, there is important research (especially earlier research) published in many different languages. It is bad scholarship — and also deeply annoying (and offensive) for researchers in non-English countries — when important historical work is not cited simply because it is not in English. In addition to research in major European languages (like German, French and Italian), there is excellent research in other languages that is not widely known. For example, important scholarship in philosophy is found in Danish, important work in mathematics is published in Hungarian, essential work in history is published in Chinese, and superb work in chemisty is written in Russian. In the case of music psychology, one can find important work published in Swedish. It is appropriate to provide topical citations to works that are in languages you don’t understand. Topical references aren’t merely a “courtesy.”\nIn music analysis, it is common for different theorists to offer rather different analyses of the same musical work. Too often, one observes a regretable tendency for music scholars not to cite other analyses of the same work — simply because the author disagrees with another author’s analysis. This is simply bad scholarly practice. Always cite those who disagree with you, and with whom you disagree. Disagreements need to be kept professional rather than personal: failing to cite the work of people will simply make them angry and transform a civil disagreement into a personal dispute.\n\n\nDeceptive Citation\nThe deceptive citation is one that appears to support a claim, but doesn’t. An author might write to the effect that:\nX is bigger than Y (Smith, 2004).\nSmith (2004) might discuss the relative size of X and Y, but if Smith (2004) doesn’t present any evidence supporting the claim that X is bigger than Y, then the citation is deceptive. An especially egregious version of the deceptive citation is what might be called the “opinion-data swap.” It may be that Smith (2004) expresses the opinion or speculates that X is bigger than Y, but presents no evidence in support of this view. In this case, the above citation is wholly deceptive: it presents opinion as though it were supporting evidence.\n\n\nSecond-Hand Citations\nA second-hand citation is characterized by phrases like “as quoted in,” “as cited in,” or “as discussed in.” In general, one should simply look up the original work and read it for yourself — and so avoid a second-hand citation. However, on occasion, a second-hand citation is unavoidable. For example, an author (say Rossi, 2012) might cite or quote from Li (1965). You may be unable to find a copy of Li (1965) for yourself, or you may be unable to read the language in which Li (1965) is written. In these cases you may use a second-hand citation:\nAccording to Li “All observed billbirds are nosiceptors.” (Li, 1965, as quoted in Rossi, 2012).\nIt has been claimed that all observed billbirds are nosiceptors (Li, 1965, as cited in Rossi, 2012).\nIt is surprisingly common in published articles for references to contain a typo or mistake. Page numbers may be wrong, or the volume may be given as 23 instead of 32. Never simply copy a reference from another publication without checking the accuracy. Scholars can look very foolish when such errors are propagated through several publications. Arnold (1991) makes a typographical mistake in citing Novak (1985), and then Brown (1996), Cohen (2000), Donald (2005), Ellis (2012) all perpetuate the same typo. Evidently, none of the authors has checked the original publication!\nThere is one circumstance when you should always draw attention to a second-hand citation — even if you have looked up the original. Some scholars are especially conscientious and methodical in identifying pertinent research that other scholars have overlooked. If the reference is to a publication that has been generally neglected, you should also draw attention to the scholar who tracked-down the overlooked work. For example, Kim (2010) may have cited the work of Ferdinand (1937) — work that has otherwise not been recognized by other authors. In this case, it is appropriate to acknowledge the role of Kim in re-discovering this neglected research:\nFerdinand carried out early work on this issue (Ferdinand, 1937; as noted by Kim, 2010).\n\n\nMentions\nA “mention” is a referenced publication that is included out of courtesy. Sometimes authors include such references to flatter a particular person — such as a person who is likely to be recruited as a reviewer for the manuscript. Spurious mentions to one’s own previous publications would be considered a “mention” and should be avoided.\n\n\nQuotations\nUse quotations sparingly. There are two circumstances when quotation is warranted:\n\nThe Elegant quote. When the author has expressed an idea in an especially elegant or memorable way. The 18th century English poet, Alexander Pope, provides a nice characterization of this type of quotation: To quote Pope: “What oft was thought, but ne’er so well exprest.”\nThe Factual quote. When you want to demonstrate or establish a fact, such as that the author really did say something extreme, contentious, or questionable, or that the author was misquoted by another scholar.\n\nThe most common mistake is to resort to quotation because you don’t understand what the author is saying. Complicated quotations usually suggest that the author quoting the passage hasn’t been able to make sense of what was written. We might call this the Mystifying quote. In effect, the mystifying quote says: “I don’t really understand what this person is saying, so I’ll just simply pass along the quote and hope that you, dear reader, can make some sense of this …”\n\n\nGeneral Remarks\nFew experiences are more discouraging for a scholar than to read a publication that fails to cite one’s own research when the research is clearly pertinent. Scholarship is a community enterprise, and so you need to participate as a good citizen. Acknowledge the hard work of others, in the same way that you hope others will acknowledge your own hard work. Make the extra effort to identify all of the pertinent research literature and to cite this work in your writing. In particular, don’t neglect historical literature and literature published in languages you don’t understand.\nAs a general principle, aim to cite more research in your writing than is typical in your discipline. The corrolary of this principle is that you should read more publications than is common among your colleagues, and read beyond the core journals in a field.\nFinally, read (or at least skim) all of the literature you cite. Of course, this is essential when the purpose of the citation is to support a claim you are making.\n\n\nReference:\nPost, O., & Huron, D. (2009). Music in minor modes is slower (except in the Romantic Period). Empirical Musicology Review, Vol. 4, No. 1, pp. 1-9."
  },
  {
    "objectID": "emp_methods_workshop/further.html",
    "href": "emp_methods_workshop/further.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Furthering Your Education\n\n\nFurther Study\nRecommended studies:\n\nTake an introductory course in statistics. (Avoid courses offered by your Statistics Department; instead take statistics in an applied area such as psychology, education, or linguistics.)\nTake a course in experimental design.\nLearn a statistical package like SPSS, SAS, or R.\nCourses in qualitative research methods are taught in departments of anthropology, education, sociology, and also by ethnomusicologists.\nComputer skills are typically very useful in doing research. It is very helpful to know tools related to digital sound editing (e.g. Audacity) and sound generation (e.g., Max-MSP, various MIDI apps). These can help in preparing experimental stimuli. Knowledge of a scripting (programming) language, like Perl, Python, Awk, or Kornshell is often very helpful. These can help in running an experiment or data post-processing. Also, tools like Praat, Excel, Humdrum, and Matlab can help in sound and music analysis.\nAttend conferences that attract other scholars who do empirical research in music — such as the Society for Music Perception and Cognition, or the International Society for Music Information Retrieval.\n\n\n\nFurther Reading\nRecommended reading:\n\nPaul C. Cozby, Methods in Behavioral Research. Mountain View, CA: Mayfield Publishing Co. (any edition - purchase it second-hand).\nAlan F. Chalmers, What is This Thing Called Science? St. Lucia, Australia: University of Queensland Press. (any edition - purchase it second-hand).\nDavid Huron (1999), The new empiricism: Systematic musicology in a postmodern age. The 1999 Ernst Bloch Lectures. University of California, Berkeley (available free on the web).\nEdward Slingerland (2008). What Science Offers the Humanities: Integrating Body and Culture. Cambridge: Cambridge University Press. Slingerland is a scholar of ancient Chinese history who discovered the value of cognitive science in his work.\nFor help in assembling surveys, I recommend Jean Converse & Stanley Presser (1986), Survey questions: Handcrafting the Standardized Questionnaire. Sage Publishing: Beverly Hills, California.\nRead articles in Empirical Musicology Review (free on the web).\nOther suitable journals include: Empirical Studies in the Arts, Music Perception, Journal of New Music Research, Musicae Scientiae, and Psychomusicology.\nAlways read beyond your discipline. If you are interested in music history, consider reading works in historical linguistics, human geography, or archeology. If you are interested in performance research, consider reading general books on motor skills, or social psychology. If you are interested in music theory and analysis, consider reading works in statistics, machine learning, or artificial intelligence. If you are interested in music perception, consider reading books in animal behavior, speech and hearing sciences, or anthropology. Read general books in music, musicology, music theory, aesthetics and philosophy as a way to spur your creative thinking, and as a source of research ideas.\n\n\n\nFurther Actions\nThe very best way to learn how to do empirical research is to collaborate or hang around other people who do it. However, recognize that potential research collaborators may not be so eager to work with you. Collaboration is a lot like forming a musical ensemble: everyone wants to play with the best musicians and no one wants to play with musicians less good than themselves. The same thing happens with research collaborations. If you have only modest research skills, you will offer little attraction to other researchers. (Also, some researchers prefer to work alone.) Consider offering to volunteer. I recommend volunteering in a psychology, sociology, or speech & hearing laboratory as a way of gaining experience.\nThe next best way to learn how to do empirical research is to do it yourself. If you can’t find a collaborator, then take the plunge and just do it."
  },
  {
    "objectID": "emp_methods_workshop/interview_guide_task.html",
    "href": "emp_methods_workshop/interview_guide_task.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Task\nUnlike the formal interview, an Open Interview does not employ an exhaustive list of questions. Instead, open interviews are shaped by an interview guide. The interview guide is a written document that identifies the goals of the interviews, lists the topics of interest, and provides some suggested questions.\nChoose one of the following topics and then construct an appropriate interview guide for conducting open interviews.\n\nHow musicians practice.\nDJs who work at weddings.\nFilm-music composer.\nMusic therapists working in hospital settings.\nComposer for video games.\nConductors of community children’s choirs.\n\n\n\nSample\nHow might you recruit participants for your interview project?\n\n\nInstructions\nSpend no more than one or two minutes choosing a topic. Aim to complete your interview guide in about five minutes. At the end, have someone type-up the interview guide so your group truly produces a document. Spend no more than one or two minutes considering how you would recruit participants for the project.\n\n\nRationale\nThis task encourages thinking about how to maximize the insights gained using a common exploratory research method."
  },
  {
    "objectID": "emp_methods_workshop/generalize.html#generalizing-versus-universalizing",
    "href": "emp_methods_workshop/generalize.html#generalizing-versus-universalizing",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "Generalizing versus Universalizing",
    "text": "Generalizing versus Universalizing\n\nSuppose an acquaintance relayed the following incident to you. They were driving to work yesterday when they ran out of fuel. They knew the tank was nearly empty, but they were late for work and didn’t want to stop. However, running out of fuel turned out to be a huge nuisance. They walked for 20 minutes to the nearest gas station, where they discovered that they had to have a government approved container in order to purchase gas. They ended up having to take a taxi to a hardware store, purchase a gas container, take the cab back to the filling station, purchase the gas, and then take the taxi back to their car. The whole episode took more than an hour, turned out to be expensive, and they were even more late for work than they had planned.\nYou might feel sympathy for your friend. You might find the story entertaining or humorous. You might think that listening to your friend’s story was a waste of your time. Whatever your response, there is a strong likelihood that you might register a lesson: Note-to-self: Try to avoid running out of fuel. Even if you don’t have a car or drive, you might conclude that running out of fuel is a big inconvenience to be avoided.\nIn drawing a lesson from your friend’s story, notice that you are forming a generalization. You are taking a single incident, a unique event, and somehow conclude that what another person experienced might have some relevance for you as well.\nOne could well imagine that this story might be useful for some people. People who drive regularly (especially those who have a cavalier regard for the fuel gauge) might benefit from hearing this story. It is possible that hearing this story might actually save them from making a similar mistake in the future. One could also imagine that this story might have no utility for many other people. Most people in the world don’t own vehicles and may never experience the opportunity to ride in an autombile, let alone drive one. For these people, this story has little or no cautionary value (although they might find it entertaining).\nResearch involves chronicling a finite series of events or observations. In some research, we expect that the result will pertain to just a single event. For example, a historian might establish that a particular musical work was composed in 1680. No one expects that this research will generalize beyond this particular case.\nIn most research, however, the researcher anticipates that the result will produce some general lesson. At times, this assumption holds merit. Unfortunately, this presumption can lead to onerous mistakes. Fifty years ago, it was common to test new drugs using “convenience samples.” Drugs were often tested exclusively using white males. The presumption was that there were no significant physiological differences between men and women, or between whites and non-whites. However, drugs that were found to be effective for white men did not always prove effective for women or for non-whites. Today, the U.S. Food and Drug Administration (FDA) refuses to accept the results for drug trials that do not employ a more representative sample of participants.\nTo be fair, it is often difficult or impossible to recruit representative samples of people. Pharmaceutical companies can afford to recruit a wide range of people to participate in a drug trial. However, music researchers tend to have little research funding. Most psychological research relies on Western undergraduate students for the simple reason of cost. Problems arise when the researcher presumes that the results automatically apply to people of different ages, ethnicities, and backgrounds. Serious problems can arise when the researcher assumes that the results are “universal.”\nAware of the dangers of over-generalization, some researchers have come to believe that generalization is a bad thing. In the qualitiative research community in particular, one finds strong statements against generalization. Certain scholars have suggested that researchers should never imply that the findings generalize beyond the individual case. However, this view is highly problematic.\nBy way of illustration, consider a recent study, where a historian established that a harpist for the Amsterdam Concertgebouw orchestra in the late 1940s was discriminated against because she was a woman. If we hold the view that generalization is wrong, then the effect of this history is miniscule. Most people would conclude that this single historical event is consistent with a general pattern of discrimination against women musicians. If one makes the strong claim that generalization is forbidden, then this story has no broader meaning. If we accept the injunction against generalization, then all history becomes nothing more than a set of entirely disconnected “facts” that offer no general lessons. If generalization is wrong, then there is nothing to learn from history apart from a bunch of individual stories.\nNotice that the claim that generalization should be avoided in research is itself a generalization. Scholars who claim not to generalize are simply deceiving themselves.\nOnce again, we must navigate between two types of errors. A Type I error would falsely claim that a result is universal, when in fact, the phenomenon pertains only to some smaller subset. A Type II error would falsely claim that there is no general lesson to be drawn, when in fact, the phenomenon does indeed apply more broadly. It is important to understand that the degree to which a result applies broadly or narrowly is an empirical question. We might well expect that some phenomenon apply universally, others are entirely unique, while many other phenomenon belong somewhere in the middle.\nHistorically, music psychologists tend to assume universality, whereas ethnomusicologists tend to assume particularity. This means that music psychologists are susceptible to making lots of Type I errors while ethnomusicologists are susceptible to making lots of Type II errors. Both groups make generalizations. For example, an ethnomusicologist cannot make a claim regarding some culture without generalizing beyond the immediate observations made with a handful of informants. When Catherine Lutz famously concluded that the Ifaluk of Micronesia don’t experience anger the same way other people do, she was generalizing from her limited field experience with a small group of people (Lutz, 1988).\nWhen reporting the results for any study, use circumspect language when reporting your conclusions. Instead of saying “people hear X as Y,” say “At least in the case of Western-enculturated listeners, the results are consistent with undergraduate music students as hearing X as Y.” For any given study, we have no idea how broadly the results generalize. The degree of generalizability cannot be presumed unless the study has explicitly compared different populations.\nOur slogan reminds us that generalization is inevitable, but we have no idea of the breadth of applicability:\nSlogan: Generalize, but don’t universalize.\nWhen presenting your result, frame them narrowly rather than broadly. Let the reader form his/her own impression about the degree to which a result might generalize (Lincoln & Guba, 1985).\nReferences:\nDavid Huron (2009). Why some ethnomusicologists don’t like music cognition: Finding common ground in the study of musical minds. Keynote address, European Society for the Cognitive Sciences of Music (ESCOM) Conference. Jyväskylä, Finland.\nYvonna Lincoln & Egon Guba (1985). Naturalistic Inquiry. New York: SAGE Publications.\nCatherine Lutz (1988). Unnatural Emotions: Everyday Sentiments on a Micronesian Atoll and their Challenge to Western Theory. Chicago: University of Chicago Press."
  },
  {
    "objectID": "emp_methods_workshop/paradigms.html",
    "href": "emp_methods_workshop/paradigms.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Experimental Paradigms\n\nEarlier, we distinguished seven types of empirical studies: (1) reconnaissance study, (2) descriptive study, (3) measurement study, (4) correlational study, (5) experimental study, (6) meta-study, and (7) modeling study. Within each of these categories there are different ways of conducting the research. These methods are commonly referred to as research designs or research paradigms. There are dozens of research designs and new ones are constantly being invented. It can be useful to familiarize ourselves with some of the more common designs or paradigms.\nMany experimental paradigms are not mutually exclusive. For example, a within-subjects design is often contrasted with a between-subjects design—(see below). Although they seem to be mutually exclusive, there are studies that make use of both within- and between-subjects aspects. These are often referred to as “mixed” designs.\n1. Reconnaissance Study\nIn 2011, Swedish musicologist Alf Gabrielsson published a wonderful book entitled Strong Experiences with Music. The research for this book was carried out by placing advertisements in newspapers across Sweden (most of the work was done before the Internet was popular). Gabrielsson simply invited ordinary Swedes to write to him conveying their most memorable and moving experiences with music. The volume consists primarily of (translated) reports and anecdotes. Although the book is organized according to certain themes, the book is really a collection of reports with some (modest) interpretation. Gabrielsson’s book can be regarded as the endpoint for a Reconnaissance study. Notice that Gabrielsson didn’t travel anywhere: instead of doing “field work,” in effect, he asked people to send him “specimens” of strong musical experiences.\n2. Developmental Study\nA developmental study is any study that attempts to determine how something (or someone) changes over time. In music, developmental studies are used extensively to chronicle how people change with musical training. For example, a developmental study might examine the effect of some training regime (such as Suzuki method) on children’s musicality. There are two classic methods for studying development. One approach is the “cross-sectional design.” In a cross-sectional design, the research compares people at different points in their development. For example, a researcher might compare\nMost developmental studies focus on education or child development. Researchers might simply determine at what age a child is able to synchronize to a beat. Developmental studies are also used to compare different educational curricula. For example, we might choose to compare two commercial music theory textbooks. -pre-test/post-test whether one music theory textbook Developmental studies are\n[THIS IS NOT A DEVELOPMENTAL STUDY] Although most developmental studies focus on education, developmental studies can apply to any aspect of change over time, and the changes might relate to objects (like musical instruments) rather than people. For example, an ethnomusicologist may make use of historical documents to trace musical changes in some culture. In a study by Horn and Huron (2015) we examined simple musical changes between 1750 and 1900. For example, around 1750, only 17 percent of music was written in the minor mode. By 1900, the proportion of music in the minor mode had doubled to 34 percent.\n3. Cross-sectional Study\nA study that recruits participants (or data) for different ages (or periods) in order to determine how some behavior changes over time. In developmental research, a researcher might compare children at ages 3, 6, 9, 12, and 15 in order to trace how some skill develops. In historical research, a researcher might compare music in successive decades from 1950 to 2010 in order to chronicle how musical styles have changed.\nAn example of a cross-sectional study in developmental research in the work of Denis McAuley (2010). McAuley recruited people of different ages, from young children to elderly adults. He simply asked them to (spontaneously) tap their fingers. His research shows that the rate of spontaneous tapping decreases considerably with increasing age. Old people spontaneously tap at a much slower tempo than adolescents who tap slower than young children.\nCross-sectional studies can be confounded by so-called cohort bias. Suppose for example, a researcher wanted to study how musical tastes change with increasing age. Recruiting people of different ages, the results might appear as follows: People in their 80s prefer big band music. People in their 70s prefer smooth jazz and crooners like Frank Sinatra. People in their 60s prefer early rock and roll. People in their 50s prefer funk and disco. People in their 40s prefer heavy metal and punk. People in their 30s prefer rap. Etc. The researcher then concludes that as a person ages, their tastes evolve from rap, through punk, and disco, to early rock, and finally to big band music. Of course, the problem is that each generation grew up with different popular musics. It is not that people’s tastes change dramatically as they got older. This “generational effect” is known as cohort bias. If we see a difference between 6 year olds and 10 year olds, we can’t be sure that this is truly due to aging, or whether it is due to growing up at different times.\n4. Longitudinal Study\nThe longitudinal study is a type of developmental study. Unlike the cross-sectional study, this type of study follows individual changes over time. The most famous longitudinal study is the Framingham Heart Study named after the small town of Framingham, Massachusetts. The study began in 1948 with 5,209 adult participants. (The study continues to this day, and is now following the third generation of Framingham residents.) The study focuses on cardiovascular disease. Regular information is collected on lifestyle, environmental factors, and inheritance. Much has been learned from this study regarding factors that contribute to heart disease and related health problems.\nIn music, perhaps the best example of a longitudinal study in music is a famous 1999 study carried out by Eugenia Costa-Giomi. For years, research has shown that children who take music lessons perform better on a range on non-musical tasks. For example, children who study music exhibit better cognitive skills, exhibit better verbal and mathematical abilities, show better motor proficiency, higher self-esteem, and score higher in academic achievement compared with children who don’t take music lessons. These results are evident even when the children have been carefully matched for socio-economic status.\nThe problem with these studies is that they are all correlational. (Remember, “no causation without manipulation.”) Suppose we have two children: one who expresses interest in music lessons and another who doesn’t. Perhaps the first child is simply more outgoing, or has greater curiosity, or the second child suffers from lethargy, or whose parents are less enthusiastic. In short, taking music lessons per se may not be the cause of the improvement, but a symptom of better starting aptitudes and dispositions.\nA proper experiment would assemble a group of children, and randomly assign them to either music lessons, or no music lessons. The study would use a pre-test to establish intelligence, self-esteem, and other measures. After several years, we would then conduct a post-test to see if those children who had been randomly assigned to receive music lessons improved more than those children who had been randomly assigned not to receive music lessons.\nCosta-Giomi’s study did just that. Six-three fourth-grade children were randomly assigned to receive free weekly personal piano lessons (for three years!). Brand new pianos were delivered to each of their homes at no cost. Fifty-four children were randomly assigned not to receive lessons. At the beginning of the study, there were no differences between the two groups of children in terms of cognitive abilities, musical abilities, motor proficiency, self-esteem, academic achievement, or interest in studying music. Each year, these tests were repeated in order to observe possible changes in both groups. After three years of instruction, the students receiving piano lessons were no different than the control students in any of the measures of developmental, quantitative, or verbal cognitive abilities. Costa-Giomi’s study suggests that people who choose to take music lessons and who persevere at those lessons already have a disposition to high achievement. The research is not consistent with the hypothesis that the music lessons themselves contribute to higher achievement.\nNotice that cross-sectional studies are easier to perform than true longitudinal studies. But longitudinal studies are the only kinds of developmental studies that control for possible cohort bias.\nLest you find this story utterly depressing, it is appropriate to point out the Glenn Schellenberg did a follow-up study in 2004 with a very large sample and did see measurable cognitive improvements in the music treatment group compared with the control group, although the effect size was small.\n5. Preferential Looking Paradigm\nWhere do musical tastes come from? How do we learn how to repeat a heard melody? There are many questions about how we interact with music that have their origin in our earliest encounters with music. We can’t really answer these questions without some understanding the formation of skills or tendencies of infants. But, how does one learn how an infant perceives music? Certainly we can’t ask them questions.\nHowever, there are ways of finding out how infants perceive musical stimuli. Reliably, humans of any age will orient towards sounds that might indicate a threat. If I suddenly clap, it’ll likely startle most listeners, and the instinctual reaction will be to turn towards the sound. Even one full month before birth, fetuses exhibit both the startle reflex and the orienting response. However, if I keep clapping at regular intervals, eventually listeners will habituate to the sound and they’ll stop attending to it.\nThis orienting response is really helpful in studying infant perception. In addition to being startled by a sound, infants will reliably orient toward sounds that are new or sounds that they prefer. It’s common that babies habituate to sounds fairly quickly and look away, but if there’s a change to the sound, they tend to look back. The more significant the change, the more likely infants will re-orient themselves to the sound.\nJessica Philips-Silver and Laurel Trainor have used orienting responses to try to understand how we come to feel meter in a series of brilliant studies. Specifically, they were curious about why we tend to nod our heads to the music’s pulse. Is there something about the head moving to the beat that reinforces our sense of meter or that is rewarding in some way?\nThey concocted a rhythm that could easily be heard in either 3/4 (with three pulses per pattern) or 6/8 (with two pulses per pattern), and played it using a computer that avoided accenting any of the rhythms. The condition was whether each 7-month old baby was bounced by the mother twice or three times per pattern.\nIn the test phase, they situated babies between two speakers. When a baby looked at a speaker, it would play sound. One of the speakers played the rhythm with three accents per pattern (3/4), and the other played the rhythm with two accents per patter (6/8). They found that infants preferred listening to the speaker that matched the pulse they were bounced with, consistent with the notion that moving our heads up and down influences our perception of meter even at an early age before too much enculturation.\n6. Correlational Design\nTwo economists, Claudia Goldin and Cecilia Rouse (2000) conducted a study by gathering orchestral audition records and roster information from eight major U.S. orchestras, spanning the years 1940 to 1995. Their dataset included complete audition record (who auditioned, who was hired) and a complete roster of current ensemble performers (providing a base male-female ratio).\nIs there prejudice against female orchestra musicians? A major change in hiring practices has involved “blind” auditions in which the performer plays behind a screen. The Boston Symphony Orchestra adoped screens in auditions in 1952, and other orchestras made the switch to blind auditions in the 1960s, 70s, and 80s.\n\nIn general, whether using blind or non-blind auditions, the proportion of female musicians has increased, reflecting a general increase of female employment in various professional roles. The key question is how much of this increase can be attributed to the use of screens in auditions? The different dates of adoption by different orchestras allows us to assess the “before” and “after” effects of the screens. We need to know the proportion of females who auditioned for each opening, the proportion of females hired, and whether or not a screen was employed. In their study, Goldin and Rouse found that the switch to blind auditions acounts for 30% of the increase in the proportion of female musicians hired. Two conclusions follow: first, the use of screens is consistent with the notion that between 1940 and 1995 there existed a broad prejudice broad prejudice second, much or most of the increase in employment of female musicians was not related to the presence or absence of a screen. “The switch to”blind” auditions can explain, using the roster sample, between 30% and 55% of the increase in the proportion female among new hires and from 25% to 46% of the increase in the percentage of orhcestra musicians who are female.”\n7. Life-sample Studies\n(Formerly known as Pager Studies). How often do people have tunes going through there head? Is there a tune going through your head right now? Often, researchers want to know what is going on in people’s lives, and how frequently particular behaviors occur. Early studies made us of electronic pagers that could be activated at random moments during the day. Participants in the experiment would then record whatever information is being sought by the researcher. Usually that involves completing a survey. Pagers are still in use, but today it is more common to send text messages to mobile phones. For example, Adrian North, David Hargreaves and Jon Hargreaves (2004) recruited 346 volunteers spanning a range of ages: each person was sent one text message per day for 14 days. The texts were scheduled so that they had equal representation between 8 am to 11 pm. On receiving the text message, participants were required to complete a questionnaire about any music they could hear. Life-sample studies have been used to address such questions as how often people listen to music, and what time of day musicians tend to practice. In the North, et al. study, their participants heard music just under 40% of the time. The greatest amount of music listening occurred between 10-11 pm, and music was liked more when it was heard with no other people present.\n8. Discrimination Paradigm\nCan you tell the difference between an oboe and an English horn? When does a pitch become out-of-tune? Is there a difference between rock-n-roll and rock? Discrimination tasks are often aimed at trying to discover how it is that we identify things and what factors contribute to our categorizing.\nA striking example of a discrimination task is evident in in an extraordinary experiment carried out by Michael Hall and Richard Pastore. In the first part of their experiment, Hall and Pastore played pure-tone Es and E-flats and measured the intensity threshold below which each musician listener was unable to detect the presence of these tones. In the second part of the experiment, they repeatedly played a C-G dyad using a piano timbre. They added either a pure-tone E or E-flat, and asked their listeners to identify whether the resulting chord was major or minor. That is, the task was a discrimination task: major versus minor triads. During the experiment, Hall and Pastore manipulated the intensity of the E or E-flat tones making them progressively quieter. Hall and Pastore made a seemingly astounding discovery: the musicians continued to successfully classify the chords as major or minor even when the sound intensity for the E and E-flat pure tones was reduced below audibility. Some musicians were able to correctly report the chord quality even though the E and E-flat tones were as much as 20 decibels below their hearing thresholds. Said another way, the musicians were able to hear whether a chord was major or minor, even though they were completely unable to hear the crucial third of the chord.\nExplaining this result is rather complicated (see Huron, 2016; p.166). However, the important lesson from this experiment is that musicians can perceive chords as basic auditory objects without having to hear the constituent tones.\n9. Counterbalanced Design\nA counterbalanced design is any design in which the manipulations are conducted in a symmetrical way. Consider the following experiment by percussionist Michael Schutz. Among percussionists there has been a long-standing debate about the effects of different strokes. When playing a marimba or other mallet instrument, one can strike the key with a short-sharp gesture (let’s call that staccato), or one might strike the key with a smooth-graceful gesture (let’s call that legato). Does it matter? Acousticians argue that how you strike a key has no effect on the sounded duration of the note. That is, with a mallet instrument, a “staccato” stroke will not produce a sound that is shorter in duration than a “legato” stroke. Some percussionists consequently argue that performers should forget about differentiating staccato and legato strokes. They have no musical effect.\nSchutz wasn’t so sure—even though he thinks the acousticians are right. Schutz recorded the sounds produce by mallet instruments using different gestures. The recordings include both audio and video. He played the recordings to listeners who had to judge the duration of the tone produced by each stroke. Using a counterbalanced design, he also edited the videos so that one version included the sound produced by the staccato stroke but matched with the video image of the legato stroke. Similarly, he produed an edited version in which the sound produced by the legato stroke was matched with the video of the staccato stroke. So he produced four different stimuli: staccato(sound)+staccato(video), staccato(sound)+legato(video), legato(sound)+legato(video), and legato(sound)+staccato(video). (This makes the experiment a counterbalanced design.) Listeners judged the durations of sounds in both the audiovisual condition, and also judged the durations of the sounds in the audio-only condition.\nIn the audio only condition, the staccato and legato sounds were judged by listeners to have the same durations. That is, the results confirmed what acousticians claim. However, in the video condition, Schutz found a dramatic visual effect. Listeners judged the sounds matched with the staccato video to always sound shorter than the sounds matched with the legato video. The actual acoustical sound didn’t matter. The visual manner in which the percussionist played actually caused listeners to perceive the tone as longer or shorter. Schutz’s research suggests that percussionists can indeed make a musical passage sound more staccato or legato by the type of stroke they use. But the perceptual effect is due to sight, not sound (Schutz & Lipscomb, 2007; Schutz, 2008).\n10. Reaction-time Experiment\nBret Aarden’s work - perhaps emphasize how cadences and cadential approaches are more predictable (faster reaction times). e.g. reaction time experiments (do’s and don’ts) -importance of motivating participants (useful to set up as a game)\n11. Implicit Association Test (IAT)\nThe implicit association test (IAT) is a popular method for investigating pre-conscious associations between different kinds of stimuli. The method has been especially successful in establishing unconscious bias and uncovering stereotypes. For example, although a person may consciously think they hold no bias against blacks, the method can be used to show that that person is nevertheless suspicious of black people.\nThe method involves speeded judgments of different stimuli. For example, the participant might see photographs of The method makes use of reaction time—so it might be considered a type of reaction-time study.\nA common implicit association task asks people to identify as quickly as possible whether visually-presented words are positively or negatively valenced. Classifying the words is trivial since the valence is obvious — as in the words joy, fun, party, kiss, hatred, misery, victim, and disgust. Participants carry out the task as quickly as possible while their reaction time is measured. Immediately prior to presenting the word a sound is heard. For example, in one study by Frank Ragozzine (2011), participants heard either a major triad or a minor triad. The results for Ragozzine’s experiment are shown below:\n \nfrom Frank Ragozzine (2011)\nThe left graph displays the error rate. For example it shows that roughly 2 percent of judgments of negative words were incorrectly classified when the word was heard in conjunction with a major chord. The error rate is roughly half when the negative word is accompanied by a minor chord. Similarly, notice that positive words produce few errors when accompanied by a major chord, but many more errors when a positive word is linked with a minor chord.\nThe right graph displays the reaction times. Once again, the same pattern holds. Participants are slow to respond when a positive word is accompanied by a minor chord, or a negative word is accompanied by a major chord. Conversely, the fastest responses occur when positive words are linked to major, and negative words are linked to minor.\nNotice that all of the average reaction times are less than a second. This means that there isn’t enough time for a person to “think” about any relationship between what they hear and the word they see. That is, the judgment is not made consciously or explicitly. The relationship is more automatic or implicit. In short, at least for Western-enculturated listeners, there is a natural association between major+positive and minor+negative.\nA similar IAT experiment was conducted by Bernhard Sollberger, Rolf Reber, and Doris Eckstein (2003). Instead of major or minor chords, they played either consonant (D4, A4, D5) or dissonant (D4, A#4, D5) sonorities. Their results are shown below. Once again we see the same pattern of errors and reaction times suggesting that consonance is associated with something positive and dissonance is associated with something negative.\n \nOther IAT studies by Nikolaus Steinbeis and Stefan Koelsch (2011) have shown an association between major/happy and minor/sad.\n12. Probe-Tone Study\nThe probe-tone method involves playing some kind of musical context, followed by a single tone (or chord). The listener is asked to judge something about that probe tone or chord. For example, the listener might be asked to judge how well the probe tone fits with the preceding musical context. Probe tone studies are commonly used to explore listener expectations—whether the probe tone conforms to what they think might happen next.\nAn example of a probe tone study is an experiment conducted by theorist Mark Yeary. Yeary played a three-note chord followed by a single probe tone. Listeners simply indicated whether the probe tone matched the pitch of the middle tone in the target chord. The main manipulation was the context preceding the three-note chord. One context consisted of a four-note melody that would have been expected to stream with the highest note of the target chord. A contrasting context consisted of four three-note chords (none of which contained the final probe tone). Yeary found that listeners abilities to correctly discriminate the presence of the probe tone in the target chord were significantly reduced when the preceding context consisted of chords only—suggesting that a “chordal context” encourages listeners to hear the target chord holistically as a chord-object rather than as three individual tones.\n13. Two-Alternative Forced Choice (2AFC) experiment\nThe 2AFC experiment is one of the most commonly used designs in perceptual research. Here, the participant is presented with two stimuli and asked to make some sort of choice. Which of these two sounds is more dissonant? Which of these two sounds is louder? Which of these two sounds do you prefer? And so on. As you might expect, sometimes participants are given more than two choices. For example, if the participant is given three alternatives, the paradigm is (naturally) referred to as a three-alternative forced choice, abbreviated 3AFC.\nAn example of a forced-choice experiment is a study by Ollen and Huron (2004). We were interested in testing the effect of musical repetition on preferences for certain musical forms. Specifically, we were interested whether listeners prefer “early repetition”—where repeated material tends to happen early in the form rather than later in the form. For example, do listeners prefer a repetition pattern like A A B A over the reverse A B A A? For this experiment, we composed over 100 brief atonal piano textures ranging in duration from about half a second to 12 seconds. Using these passages, we constructed short “modernist-sounding” compositions by randomly assembling a pallet of two or three segments. Each composition made use of a repetition pattern and was paired with another composition using the reverse pattern. So, for example, a listener might hear a short composition structured as A A A B A B A, and it’s reverse A B A B A A A. Listeners heard these pairs and were asked to choose which “composition” they preferred. We found that listeners preferred the repetition patterns in which most of the repetition occurred early in the composition.\n14. Betting Paradigm\nTogether with Paul von Hippel and ethnomusicologist David Harnish, we endeavored to trace the musical expectations of Indonesian and American musicians by having them bet on successive notes of an unfamiliar Javanese melody played on a Peng Ugal instrument. The experiment works as follows. Bets are placed on the keys of a mock-up of an instrument Bets need not all be placed on a single outcome. Instead, participants are free to distribute the poker chips across several possible continuations—varying the number of chips wagered according to the degree of certainty or uncertainty. Bets placed on the correct pitch are rewarded tenfold. Bets placed on incorrect pitches are lost. Participants are instructed to try to maximize their winnings.\nWagers are made following each note in the melody. The experiment begins with the participant hearing the first note of the melody while the pitch is indicated on a computer monitor. The participant is then invited to bet on what she or he thinks will be the second note. Once bets are placed, the actual second note is revealed, the winnings tabulated, and a sound recording of the melody is played, stopping before the third note. The participant is then invited to bet on what she or he thinks will be the third note. This process is repeated until a complete melody has been revealed.\nIn our experiment both American and Balinese musicians were tested on a traditional Balinese melody (Huron, 2006). Throughout the experiment, participants could see the notation up to the current point in the melody, and could try out different continuations using a digital keyboard sampler that emulated the sound of the peng ugal. The betting context helped participants consider other possibilities apart from the first one that comes to mind.\nThe principal benefit of the betting paradigm is that it allows the experimenter to calculate the subjective probabilities for different continuations. Assuming that the participant is behaving rationally, bets should be placed in proportion to the subjective likelihood of subsequent events. For example, if a participant thinks that a certain pitch is twice as likely as another pitch, then the participant ought to place twice as large a bet on the more probable pitch.\nThere are also several problems with the betting paradigm. For one thing, the procedure is tedious. On average, we have found that it takes roughly three minutes for participants to complete their wagers for each note in the melody. So a thirty-five-note melody thus can take as long as two hours to complete. Fortunately, the majority of our participants report that the task is fun, and that the gambling aspect of the task was highly motivating.\n15. Physiological Study\nHajime Fukui (2001) carried out a study where he measured testosterone levels from saliva samples collected from undergraduate-aged participants while they listened to their favorite music. Compared with a control group that listened to no music, the testosterone levels dropped significantly. Moreover, Fukui found no sex-related differences: testosterone levels dropped by a similar proportion in both male and female listeners.\n16. Method of Adjustment\nIn many experiments, an experimenter plays various sounds to listeners and asks them make some kind of judgment. For example, a researcher might play several renditions of the same music in which the tempo has been changed in order to determine what tempo the listener thinks is most appropriate. Notice that there may be a simpler approach to this question. We might provide the participant with an adjustable slider or knob that controls the tempo, and simply ask them to “tune” the tempo to what they think is appropriate. When we give the participant the tools to manipulate the stimulus themselves, the design is said to employ the method of adjustment. There are several obvious advantages when using the method of adjustment. The experimenter often gets more precise data since the experimenter doesn’t need to reduce all of the possible values to a smaller number that can be tested. However, the most important benefit of the method of adjustment is that the experiment can be done much more quickly.\nAn example of a study that uses the method of adjustment is a study by Caitlyn Trevor (2016). Research has suggests that listeners are strongly influenced by the visual element of performance. Trevor wanted to determine whether the amount of performer movement plays a role: Do people prefer performers who gesticulate and move around a lot? She recorded four performers playing different instruments and simultaneously recorded their movements using motion capture technology. She then used the motion capture data to create stick-figure animations of the performer motion. This allowed her to eliminate the potential confounding effect of clothing, performer attractiveness, sex and other visual factors. She provided participants with a slider that allowed them to dynamically adjust the range of movement — from quite attenuated movement to highly exaggerated movement. Participants could hear the music and watch the stick-figure performers simultaneously. They were simply instructed to adjust the amount of performer movement so that it seemed most musically appropriate. Trevor found that all of the participants adjusted the amount of movement so that the performer motions were slightly exaggerated. That is, listeners prefer performers to move more than they actual move when playing.\n\n17. Factorial Design\nMany experiment manipulate two or more factors or variables. For example, a researcher might want to investigate the effect of different treatments on the perception of major and minor chords. For example, what happens when the chords are played loud versus soft, staccato versus legato, and using bright versus dark timbres. In short, there are four factors (chord quality, loudness, articulation, timbre). In this case, each factor has two levels (e.g., major/minor, loud/soft …). An experiment is said to employ a factorial design when all possible values or levels are used in all possible combinations. In this hypothetical experiment with chords, there would have to be a minimum of 16 stimuli: 2 chord qualities X 2 loudness levels X 2 articulations X 2 timbres. A full factorial design is also sometimes called a fully crossed design.\n18. Two-Experiment Ploy\n[used for music and depressive realism experiment]\n19. Telephone paradigm\nAlbrecht and Shanahan carried out an experiment that was inspired by John Spitzer’s (1994) JAMS article “Oh! Susanna: Oral transmission and tune transformation.” Spitzer traces variants of the song, asembles a stemma showing th efiliation of sources, and then offers four observations about oral transmission: transmission: (1) a tendency to alter rhythms in order to clarify the beat; (2) a tendency to pentatonicize the melody; (3) a tendency for a salient harmony to draw the melody to the chord root; and (4) a tendency to eliminate differences between parallel passages.\nShanahan and Albrecht tested #2 using a “telephone game”-like experiment. They selected various folksongs, some that end with ti-doh, and some that end with re-doh. They had sung recordings (without words) of each. They would play the recording three times to a given participant. That participant would then perform the song (making another recording). That recording was played three times to the next participant, who in turn made the next recording, and so on, for four participants. Each participant sang four different songs, in each ordinal role: 1st, 2nd, 3rd, and 4th. They then transcribed the target passages. As predicted, they found a statistically significant tendency for ti-doh to be “pentatonicized” to re-doh, but less likely for re-doh to switch to ti-doh.\n20. Catch-Recatch Method\nThis method is commonly used in biology, but it has value in other fields as well. Suppose a fisheries biologist may be interested in estimating the number of fish in some lake. How could you ever determine the population without draining the lake and counting them? What biologists do is to catch a given number of fish (say 100), tag them, and release them back into the lake. After some time (perhaps a month), they return to the lake and catch another 100 fish. This time, they are interested in the number of fish that are tagged. If two of the 100 fish are tagged, then the catch of 100 fish amounts to 2 percent of the total catchable fish. Consequently, we can estimate the number of (catchable) fish in the lake as 5,000.\nTo our knowledge, the catch-recatch method has not been used in music. However, it could certainly be used to try to estimate various population sizes. Suppose, for example, that you wanted to know how many Metal fans in Dayton (Ohio) regularly attend concerts at a particular venue. Audience members are not eager to have their names recorded. You might know that a concert is attended by 1,200 people and that the parking lot for the venue has 400 cars—suggesting that the average car transports three fans. The researcher might randomly wander through the parking lot during a concert and record the license numbers for 100 randomly-selected cars. At a subsequent concert, the researcher again wanders through the parking lot and identifies those cars whose licenses were recorded at the previous concert. If you have 20 repeat cars, then the catch-recatch ratio is 5:1. If the concert attendance is again 1,200 people, it suggests that the entire fan base for Metal fans in Dayton is roughly 6,000 people.\n21. Meta-Analysis Study\nVery few meta-analysis studies have been accaried out on musical topics. One notable study was conducted by Friedrich Platz and Reinhard Kopiez (2012). They examined 33 studies examining the influence of the visual component on music perception. To what extent does vision influence music appreciation. On might suppose, for example, that musical judgments might be influenced by such factors as the attractiveness of the performer, the manner of dress, how the performer moves, the pleasantness of the environment, and other visual aspects.\nThe purpose of a meta-analysis is to amalgamate all of the pertinent research and (1) attempt to resolve whether there is a real effect, and (2) if so, to estimate the size (or importance) of the effect. A meta-analysis begins by identifying all of the pertinent studies. Because of positive results bias, meta-analysis researchers endeavor to also uncover so-called “grey” literature—pertinent research reports that were never published. If one relies only on published journal articles, then the estimate of effect size is likely to be over-estimated.\nHaving identified both published and grey literature, the next stage is to specify eligibility criteria for inclusion. Not all studies are done well. Studies may fail to include appropriate control conditions. One may want to include only true experiments and exclude correlational studies, etc. In the case of the work by Platz and Kopiez (2012), just 15 studies passed their pre-specified eligibility criteria. These 15 studies can be treated as one big experiment (involving some 1,298 participants).\nAs you might expect, Platz and Kopiez found that the aggregate research points to a significant influence of visual aspects on musical experience. There are different ways of quantifying effect size, but a common measure is Cohen’s d. In the case of the visual aspect of performance, Platz and Kopiez calculated a Cohen’s d of 0.51. This is considered a “medium” effect size. Cohen would describe this effect size as something that would be apparent to a careful observer. The effect size is comparable to the perceived different in intelligence between two people whose IQs differ by 8 IQ points.\n\nReferences\nJoshua Albrecht & David Huron (2014). A statistical approach to tracing the historical development of major and minor pitch distributions, 1400-1750. Music Perception, Vol. 31, No. 3, pp. 223-243.\nYuri Broze & Daniel Shanahan (2013). Diachronic changes in jazz harmony: A cognitive perspective. Music Perception, Vol. 31, No. 1, pp. 32-45.\nLaetitia Bruckert, Patricia Bestelmeyer, Marianne Latinus, Julien Rouger, Ian Charest, Guillaume A. Rousselet, Hideki Kawahara, & Pascal Belin (2010). Vocal attractiveness increases by averaging. Current Biology, Vol. 20, pp. 116–120. doi 10.1016/j.cub.2009.11.034\nCohen, J. (1988). Statistical Power Analysis for the Behavioral Sciences. Hillsdale, NJ: Lawrence Erlbaum.\nEugenia Costa-Giomi (1999). The effects of three years of piano instruction on children’s cognitive development. Journal of Research in Music Education, Vol. 47, No. 3, pp. 198-212.\nDahl, S., Huron, D., Brod, G., & Altenmüller, E. (2014). Preferred dance tempo: Does sex or body morphology influence how we groove? Journal of New Music Research, pp. 1-10. DOI: 10.1080/09298215.2014.884144.\nHajime Fukui (2001). Music and testosterone: A new hypothesis for the origin and function of music. Annals of the New York Academy of Sciences, Volume 930, pp. 448-451.\nAlf Gabrielsson (2011). Strong Experiences with Music. Oxford: Oxford University Press.\nClaudia Goldin & Cecilia Rouse (2000). Orchestrating impartiality: The impact of “blind” auditions on female musicians. American Economic Review, Vol. 90, pp. 715-741.\nMichael Hall & Richard Pastore (1992). Musical duplex perception: Perception of figurally good chords with subliminal distinguishing tones. Journal of Experimental Psychology: Human Perception and Performance, Vol. 18, No.3, pp. 752–762.\nDavid Huron (2006). Sweet Anticipation: Music and the Psychology of Expectation. Cambridge, Massachusetts: MIT Press.\nDavid Huron (2016). Voice Leading: The Science behind a Musical Art. Cambridge, Massachusetts: MIT Press.\nDevin McAuley (2010). Chapter 6: Tempo and rhythm. Music Perception, Springer Handbook of Auditory Research, 36 DOI 10.1007/978-1-4419-6114-3_6 pp.165-199.\nAdrian North, David Hargreaves, & Jon Hargreaves (2004). Uses of music in everyday life. Music Perception, Vol. 22, No. 1, pp. 41-77.\nJoy Ollen & David Huron (2004). Listener preferences and early repetition in musical form. In Proceedings of the 8th International Conference on Music Perception and Cognition. S. Lipscomb, R. Ashley, R. Gjerdingen, & P. Webster (eds.), Evanston, IL. pp. 405–407.\nFriedrich Platz & Reinhard Kopiez (2012). When the eye listens: A meta-analysis of how audio-visual presentation enhances the appreciation of music performance. Music Perception, Vol. 30, No. 1, pp. 71-83.\nFrank Ragozzine (2011). Cross-modal affective priming with musical stimuli: Effect of major and minor triads on word-valence categorization. Ninad: Journal of ITC Sangeet Research Academy, Vol. 25, pp. 8-24.\nThomas Schäfer & Peter Sedlmeier (2009). From the functions of music to music preference. Psychology of Music, Vol. 37, No. 3, 279-300.\nMichael Schutz (2008). Seeing music? What musicians need to know about vision. Empirical Musicology Review, Vol. 3, No. 3, pp. 83-108.\nMichael Schutz, & Scott Lipscomb (2007). Hearing gestures, seeing music: Vision influences perceived tone duration. Perception, Vol. 36, No. 6, pp. 888-897.\nBernhard Sollberger, Rolf Reber, & Doris Eckstein (2003). Musical chords as affective priming context in a word-evaluation task. Music Perception, Vol. 20, No. 3, 263-282.\nJohn Spitzer (1994). “Oh! Susanna:” Oral transmission and tune transformation. Journal of the American Musicological Society, Vol. 47, No. 1, pp. 90-136.\nNikolaus Steinbeis & Stefan Koelsch (2011). Affective priming effects of musical sounds on the processing of word meaning. Journal of Cogntive Neuroscience, Vol. 23, No. 3, pp. 604-621.\nMark Yeary (2011). Perception, pitch, and musical chords. PhD dissertation. Chicago: University of Chicago. UMI No. 3472979."
  },
  {
    "objectID": "emp_methods_workshop/day2.html",
    "href": "emp_methods_workshop/day2.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Methods in Empirical Music Research A Workshop for Music Scholars\n\nDAY 1\n\n\nTitle page\n\n\nWelcome & Introduction\n\n+Introduction\n\nGenerals Aims & Preview\n\nDay 1 Program\nLearning objectives\n\nEmpirical Research\n\nPreamble: An Arts and Humanities Approach to Empirical Method +(Philosophical [continental phenomenological or formal logic approach to content; +Science [procedural orientation; norms of scientific method] +Humanities approach [I approach it from a rhetorical and ethical perspective] +Please read for homework.)\nTypes of knowledge (powerpoint presentation #1)\nSeven big ideas\n\nMotivated by truth, with no hope of proof.\nThe best research invites failure.\nWe invite failure by testing predictions.\n\nA line in the sand\n\nWe recognize failure by drawing a line in the sand.\n\nRefutation is easier than confirmation\n\nAim not to be right, but to be not not right.\n\nOperationalizing\n\nTest hypotheses by operationalizing terms.\nOperationalize, but don’t essentialize.\n\nComparison\n\nCompare, compare, compare. Randomized Control Studies - The Case of Microfinance Counterfactuals (powerpoint presentation #2)\n\nThe rhetoric of science (video - 8 minutes)\n\nThe rhetoric of science is the rhetoric of prophecy. “Science is a narrative activity, conducted by a community of scholars who hold each other to a methodological commitment to making and testing predictions.”\n\nReview the first 9 slogans: Quiz #1\nGroup Task #1: What’s worth knowing? An audience with God\nQuestions, conjectures, hypotheses and theories\nGroup Task #2: Question, theory or hypothesis? (Answers)\nGrandmother research\nThe quantitative/measurement obsession\nGroup Task #3a: Obvious theories - Part 1\nGroup Task #3b: Obvious theories - Part 2\n(Group Task #3 Debriefing) Obvious theories: Hindsight bias\n\nHindsight is 20/20.\n\nGrandmother research revisited\nTwo forms of reductionism\n\nIn research, reductionism is a method, not a belief.\nDon’t try to explain the whole world at once.\n\nEpistephobia\nTypes of failure (powerpoint presentation #3)\nReview (first 12) slogans\n\nTypes of Empirical Studies\n\nTypes of empirical studies (powerpoint presentation #5)\nGroup Task #4: Types of studies (Answers/Discussion)\nGeneralizing versus universalizing (video - 10 minutes)\n\nGeneralize, but don’t universalize.\n\nExploratory studies\nMeasurement studies\nHypothesislessness\n\nAvoid chronic hypothesislessness.\n\nGroup Task #5: Operationalize the following hypotheses\nSyncopation: From question to hypothesis (powerpoint presentation #8)\nOpinions as operationalizations\nDouble use data\n\nBeware of the post hoc theory.\n\nExploratory & Confirmatory: Contexts of discovery and legitimation (lecture)\nExplore-then-test Approach\nExplore-then-test example - Part 1 (Have a participant read the text aloud. What kind of study is this? What should happen next?)\nExplore-then-test example - Part 2 (Have another participant read the text aloud. Why make use of a robot rather than a human?)\nFrom question … to protocol.\n\nFrom question to theory to conjecture to hypothesis to protocol\n\nQuiz #2 (Answers)\nReview (first 16) slogans\nFeedback Day 1\n\nHomework\n\nHomework 1: Reading: Preamble: An Arts and Humanities Approach to Empirical Method Homework 2: Reading Guide #1: Lancashire & Hirst (2009)\nReading #1: Lancashire & Hirst (2009)\nHomework 3: Individual Task #6: From question to theory\n\n\n\nDAY 2\n\n\nReview and Elaborations\n\nWelcome; encourage mix-up seating/groups\nDay 2 Program\nReview slogans (first 16)\nGroup Task #7: From theory to conjecture\nMultiple operationalizations\nFormal observation: The taxi protocol\nThe Experiment\nThe Correlational Study\nCorrelation and causation\nThe third variable problem\n\nNo causation without manipulation.\n\n\nSampling\n\nSampling\nGroup Task #8: Sampling Approaches (Answers)\nWEIRD sampling (Western, Educated, Industrialized, Rich, Democratic)\nData Independence\n\nAvoid sticky data.\n\nBiased Sampling: Listening to Edith Boyd\nGroup Task #9: Sampling Issues (Discussion)\nGroup Task #10: What is Random?\nHomework review: Review questions and answers for Lancashire & Hirst (2009)\nSampling - A Practical Problem [omit?]\nSample Size: Law of Big Numbers\nSample Size: Law of Small Numbers\n\nThe law of large numbers does not apply to small numbers.\n\nEffect Size\nRegression-to-the-Mean\nRegression-to-the-Mean in Music\nReview (first 19) slogans\nGroup Task #11: Are dynamics asymmetrical? Data sheet\n\nBehavioral Data\n\nDependent and independent measures\nTypes of behaviors (summary) The full text (or video) are left as homework. Types of behaviors (full) (video - 32 minutes)\nImplicit vs. explicit responses\nReactivity\nDemand characteristics Clever Hans\n\nAlways debrief.\n\nMeasurement scales\nGroup Task #12: What kind of measurement scale? (Answers)\nDesigning questionnaires (powerpoint presentation #17)\nGroup Task #13: Questionnaire Design\nReview (first 20) slogans\nFeedback Day 2\n\nHomework\n\nReading Guide #2: Perttu (2007)\nReading #2: Perttu (2007)"
  },
  {
    "objectID": "emp_methods_workshop/sampling2.html",
    "href": "emp_methods_workshop/sampling2.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Sampling - A Practical Problem\n\n\nWhat Do People Listen To?\nOne of the most difficult sampling problems in music is the seemingly simple task of determining what everyone in some population listens to. How would you assemble a representative sample of the music listened to by people living in Ohio?\n\n\nMusic Sampling in Micronesia\nIn my study of music and globalization in Micronesia, my first aim was to determine how much Western music people listened in different Micronesian cultures. I visited several different cultures: Majuran, Palauan, Yapese, Ulithian, Kosraean, Pohnpeian Chamoru and Rotan. How do you suppose I estimated what people are listening to?\nFor this project, I used two different sampling methods, and then compared the results from both methods. Fortunately, I found that the results for the different sampling methods were very similar. That is, the different approaches produced converging evidence.\nThe first method involved radio sampling. Arriving on an island, I used a standard radio and scanned both the AM and FM frequency ranges. I made a note of every radio station that was accessible on the island. Micronesia is a remote place: one island had as many as three radio stations; most had just one accessible radio station; a couple had no access to AM or FM radio broadcasts at all. Once I had identified the accessible stations, I randomly sampled the music available on these stations at random times over a week or so. For each musical selection, I identified and coded the origin of the music. Some music is clearly local (sung in the local language). Other music is regional (from other cultures in Micronesia). Yet other music has a Pacific origin (such as music from Fiji, Tahiti, or Hawaii). Finally, other music originates in other places—notably American and British pop. From this information, I could characterize the proportion of music available via radio whose origin is local, regional, Pacific, or Western.\nThe second method involved asking locals where I might be able to purchase music on the island. For each island, I would track down the “three best places” where one can purchase CDs and tapes. On a few islands, there are specialty shops were you can buy music. On most islands, the only place to purchase music is at a general store of some description. In a few cases, the sum total of music for sale might be just a dozen cassette tapes sitting on a shelf or in a glass display. For each of the three “best” places to purchase music, I made a complete inventory of all the music available for purchase. From this information, once again I calculated the proportion of music whose origin is local, regional, Pacific, or Western.\nSome islands (like Majuro), music on the radio is dominated by American pop music. Interestingly, the music for sale on that island was also dominated by American pop. At the other extreme, on an island like Palau, music on the radio is dominated by local Palau music, and that is paralleled by the high proportion of Palau music available for sale in the shops. In short, I found a high positive correlation between the musical origin of broadcast music and the musical origin for retail music.\nWhat music are people listening to in Micronesia? It is impractical to follow local people during their day and chronicle what they are listening to. We don’t really know. However, if we operationalize what people listen to in two different ways (radio and sales), there is converging evidence that is consistent with the theory that radio programing and local music sales provide reasonable estimates of local listening habits."
  },
  {
    "objectID": "emp_methods_workshop/stats_infer.html",
    "href": "emp_methods_workshop/stats_infer.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Inferential Statistics\n\nWe’ve learned how to use descriptive statistics to characterize or summarize some set of measures. For example, we can count the number of measures in (say) each of Beethoven’s 32 piano sonatas, and express the average number of bars or measures.\nWhere statistics becomes really useful is when we can expand beyond describing properties of a known sets of measures, and infer some properties about an unknown set of measures. In particular, inferential statistics is process by which the we infer properties of a population given a sample.\nFor example, we might count the number of measures in only half (16) of Beethoven’s piano sonatas. On the basis of this sample, how well can we infer the true average number of measures in all 32 of the sonatas?\nSome useful terminology:\nExternal validity: Where the sample is truly representative of a population and so the sample measure correctly generalizes to the population.\nInternal validity: Where the measures of the sample are correct. That is, the results generalize to the sample.\nStatistic: Some value derived from a sample.\nParameter: A true population value.\nInferential statistics: The process by which population parameters are inferred from sample statistics.\nWe use inferential statistics to infer a population parameter based on a statistic which we calculate from sample measures."
  },
  {
    "objectID": "emp_methods_workshop/samples.html",
    "href": "emp_methods_workshop/samples.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "For each case, identify the kind of sampling employed.\n\nA researcher walks into a music library with a question: Are sharp keys more common than flat keys? Wandering through the stacks, she blindly grabs volumes off the shelves and allows each volume to open spontaneously to some page. She takes note of the key signature.\nA professional music marketer is interested in carrying out a detailed survey of musical tastes in Britain. The marketer decides to use the ACORN geodemographic profile. British households will be sampled in proportion to the second-level ACORN categories: wealthy executives (8.6 percent of the population), affluent greys (7.7%), flourishing families (8.8%), prosperous professionals (2.2%), educated urbanites (4.6%), aspiring singles (3.9%), starting out (2.5%), secured families (15.5%), settled surburbia (6.0%), prudent pensioners (2.6%), asian communities (1.6%), post-industrial families (4.8%), blue collar roots (8.0%), struggling families (14.1%), burdened singles (4.5%), high rise hardship (1.6%), and inner city adversity (2.1%).\nA researcher is interested in assembling a random sample of “classical” keyboard music. She has determined that she needs roughly 20 pieces for her study. In order to maximize data independence, she wants each piece to be written by a different composer. Using Wikipedia, she finds an alphabetical list of “classical composers.” For each letter of the alphabet, she selects the first composer who she knows has written for piano: Isaac Albéniz, Carl Philipp Emanuel Bach, Alfredo Casella, Claude Debussy, Edward Elgar, Manuel de Falla, etc.\nIn piloting an experiment, a graduate student recruits her graduate student colleagues as experimental participants.\nA team of researchers is interested in emotional expression in Hindustani film music. Indian participants are asked to characterize the emotional tenor of various film scenes. Using the descriptions, the researchers then classify the scenes into 14 categories — such as romantic, humorous, physical conflict, emotional tension, etc. The researchers then select four scenes for each of the 14 categories and analyse the associated background music. Their goal is to identify musical features in Hindustani culture that signal romance, humor, etc.\nA medievalist thinks that the Dorian mode was more likely to have been heard as comparatively “happy” whereas the Phrygian mode was more likely to have been heard as comparative “sad” for medieval listeners. In order to test this notion, the scholar examines all of the Glorias (nominally “happy” text) and Kyries (nominally “sad” text) in the Liber Usualis. The prediction is that Dorian will predominate for Glorias while Phrygian will be more likely to occur for Kyries.\nA researcher is interested in changing harmonic patterns in the masses of Palestrina. The researcher makes us of the Humdrum database of the scores for the complete 103 masses assembled by musicologist John Miller.\nPaul von Hippel and David Huron (2000) carried out a study to test the idea that melodies tend to change direction following a leap, and that this pattern is ubiquitous in musical melodies around the world. In order to test this idea, they made use of two musical samples. The first sample selected music spanning five centuries. The second sample selected music spanning five continents: Africa, Asia, Europe, North and South America.\nUnsure of the contents of a box, an archivist reaches in and grabs a couple of documents, which he then examines.\nA researcher wants to know whether there is anything Italian, French or German about augmented sixth chords. Using large computer databases, the researcher uses Humdrum to isolate 900 sonorities in which the lowered sixth and raised fourth appear concurrently (including enharmonic spellings): 300 each written by Italian, French and German composers. Each of the sonorities is then classified as either Italian, French, German or Other.\n\n\nReferences:\nPaul von Hippel & David Huron (2000). Why do skips precede reversals? The effect of tessitura on melodic structure. Music Perception, Vol. 18, No. 1, pp. 59-85."
  },
  {
    "objectID": "emp_methods_workshop/regression_to_mean.html#regression-to-the-mean",
    "href": "emp_methods_workshop/regression_to_mean.html#regression-to-the-mean",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "Regression to the Mean",
    "text": "Regression to the Mean\n\nSuppose we were walking down the street together and encountered an especially tall person. I immediately turn to you and make the following confident prediction: “I predict that the next person we encounter will be shorter in height.” Later in our walk we encounter an especially short person. Once again I turn to you and confidently predict: “I bet that the next person we encounter will be taller.” Are you impressed if my predictions turn out to be correct?\nMost people are of average height. If we had to predict the height of an unknown person, our best prediction would be that they are of average height. Compared to a tall person, a person of average height would necessarily be shorter; and similarly, compared to a short person, a person of average height will be taller.\nIn general, a tall person is likely to be followed by someone who is shorter. To the uninitiated, it might seem that the presence of the tall person somehow caused the next person to be shorter. In fact, the operative principle is quite simple: most people are near average height. It is not the case that the tall person caused a shorter person to appear. Instead, the next person is most likely to be of average height, and average height is shorter than a tall person.\nThe phenomenon we have just described is known as regression to the mean or regression toward the mean. Statisticians define regression to the mean as follows:\nAn extreme measurement is likely to be followed by a less extreme measurement.\nRegression-to-the-mean is not a phenomenon. Instead it is what logicians call a tautology — a necessary statistical truth. Unfortunately, it has proved to be one of the most difficult concepts for human minds to understand.\n\nThe Sports Illustrated Cover Jinx\nOne of the most well-known examples of regression-to-the-mean is the so-called Sports Illustrated Cover Jinx. Sports Illustrated is a popular magazine that covers all kinds of sports, from boxing to volleyball. Like any magazine, the cover picture is normal reserved for photos of an athlete who has recently achieved something extraordinary. The cover might show a celebratory photo of a cyclist winning the Tour de France, a hockey star celebrating a goal, or a gymnast holding up her olympic gold medal.\nFor decades, people have observed that the performance of athletes tends to decline immediately after they appear on the cover of Sports Illustrated. After having won a series of tournaments, a golfer who appears on the cover appears to have trouble placing among the top five finishers for the next several tournaments.\nStatisticians have shown that the Sports Illustrated cover jinx is real. It really is the case that athletes tend to do worse after appearing on the cover compared with their recent accomplishments. But statisticians have also established that the effect is entirely attributable to regression-to-the-mean.\nSuppose that a basketball player, on average, scores 16 points per game. We tend to think of all scored points as a result of skill alone. However, apart from skill, there are other factors that influence how many points a basketball player scores. For example, a player is apt to score more points when playing against a poorer team. If a star teammate is sidelined with an injury, a player may receive more passes from the other players, and that will increase the likelihood of scoring more points. On the other hand, a player is likely to score fewer points if the coach keeps him on the bench rather than playing in the game. You can think of the number of points scored per game as reflecting two broad factors: (1) the true skill level of the player, (2) other factors that have nothing to do with the player’s skill. Suppose that the player’s skill is truly stable and doesn’t change from game to game. Nevertheless, the number of points scored by that player will still vary because of the other factors. As a result, the number of points scored in successive games will vary around the player’s true skill level: 14 points, 18 points, 22 points, 11 points, 17 points, etc. That is, we expect random variation around the true population mean — the true average number of points scored.\nIf you flip a coin enough times, there will be times when the coin exhibits long identical strings by chance alone. For example, by chance, a coin may turn up heads 12 times in a row. There are literally hundreds of professional basketball players that play in the NBA. Just simply by chance, a particular player may have a string of games in which he scores lower than his average number of points-per-game. This is not necessarily because the player’s skill has declined. It simply occurs by chance.\nSimilarly, with enough players and enough games, some player will have a string of games in which he scores higher than his average number of points-per-game. Instead of scoring an average of 16 points per game, we might see a series of ten games in which the player scores an average of 22 points per game. The natural tendency is to believe that this increase is due to an improvement in the player’s skill. Indeed, it may be true that the player has improved. However, statistics teaches us to expect these things to occur by chance. In the case of the coin-flips, we understand that the coin is not “improving” merely because of a sequence of heads.\nScoring many points is likely to draw attention, and this is likely invite a journalist to write about the athlete’s “extraordinary improvement” in recent games. Sure enough, the athlete ends up with his photo on the cover of Sport Illustrated.\nIf a coin produces 12 heads in a row, on the next toss, the likelihood of turning up tails is 0.5. It is possible that the string of heads will continue, but it is highly unlikely that it will last for long. At some point, the coin will appear to return to its long-term average: 50% heads and 50% tails. (Remember, extreme values tend to be followed by a less extreme values.) Similarly, after a long string of “successes,” the athlete is likely to return to his long-term average.\nStatisticians have formally shown that the purported Sports Illustrated cover jinx is entirely a consequence of regression-to-the-mean. If we encounter a series of five tall people, the likelihood is that the next five people will be of average height. The people didn’t “get shorter.” It’s simply that most people are of average height. Similarly, the athlete’s skill didn’t decline. It’s simply that his skill for most games will be around his average skill level.\n\n\nRewards and Punishments\nIf you want someone to behave in a particular way, which is more effective: Punishing the person for poor behavior? Or rewarding the person for good behavior?\nPsychological research (with both humans and other animals) has established that rewards are more effective in shaping behavior than punishments. Unfortunately, people have difficulty believing the research. As you are about to discover, the reason why we have difficulty believing this is linked to regression to the mean.\nSuppose you are trying to teach someone how to do something. There are several possible feedback strategies. One approach is to scold them when they fail and praise them when they succeed. Another approach would be to praise them when they succeed and say nothing when they fail. A third approach would be to scold them when they fail and say nothing when they succeed. A fourth approach would be to say nothing at all. Each of these four strategies has different consequences.\nThe psychologist Daniel Kahneman (winner of the 2002 Nobel Prize in Economics) tells the following pertinent story:\n“I had the most satisfying Eureka experience of my career while attempting to teach flight instructors that praise is more effective than punishment for promoting skill-learning. When I had finished my enthusiastic speech, one of the most seasoned instructors in the audience raised his hand and made his own short speech, which began by conceding that positive reinforcement might be good for the birds, but went on to deny that it was optimal for flight cadets. He said,”On many occasions I have praised flight cadets for clean execution of some aerobatic maneuver, and in general when they try it again, they do worse. On the other hand, I have often screamed at cadets for bad execution, and in general they do better the next time. So please don’t tell us that reinforcement works and punishment does not, because the opposite is the case.”\nNotice what’s going on. When trying to learn a complex skill, we often require many attempts. Whether the attempt fails or succeeds depends on many factors. This includes our current skill level, and also depends to some degree on luck (chance). Sometimes we hit the target mainly by chance. Sometimes our technique is good, but chance intervenes so that we miss the target.\nOur actual performance on any trial will fluctuate randomly around the mean value of our true current skill. Sometimes we will do better, and sometimes worse. But most trials will be near our average skill level. Recall that an extreme measurement is likely to be followed by a less extreme measurement. When we do very poorly, the next trial is likely to be better—simply by chance. Conversely, when we do very well, the next trial is likely to be worse.\nNow consider what happens if someone is scolding or praising us. Because of regression to the mean, scolding someone for bad performance is likely to be followed by an improvement in performance (by chance). But the improvement is not due to the scolding; it is due to chance. Similarly, praising someone for especially good performance is likely to be followed by a decline in performance (by chance). The scolding or praise may have little effect. Nevertheless, what is the instructor likely to conclude about the value of scolding or praise?\nKahneman continues his story:\n“This was a joyous moment, in which I understood an important truth about the world: because we tend to reward others when they do well and punish them when they do badly, and because there is regression to the mean, it is part of the human condition that we are statistically punished for rewarding others and rewarded for punishing them. I immediately arranged a demonstration in which each participant tossed two coins at a target behind his back, without any feedback. We measured the distances from the target and could see that those who had done best the first time had mostly deteriorated on their second try, and vice versa. But I knew that this demonstration would not undo the effects of lifelong exposure to a perverse contingency.”\nWe live in a world in which we believe scolding is useful, largely because of regression to the mean. At the same time, we come to believe that praise is not very effective, principally because of regression to the mean. Kahneman is right to regard the situation as “perverse.”\n\n\nReferences:\nDaniel Kahneman (2012). Thinking, Fast and Slow. New York: Farrar, Straus and Giroux."
  },
  {
    "objectID": "emp_methods_workshop/day3.html",
    "href": "emp_methods_workshop/day3.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Methods in Empirical Music Research A Workshop for Music Scholars\n\nDAY 1\n\n\nTitle page\n\n\nWelcome & Introduction\n\n+Introduction\n\nGenerals Aims & Preview\n\nDay 1 Program\nLearning objectives\n\nEmpirical Research\n\nPreamble: An Arts and Humanities Approach to Empirical Method +(Philosophical [continental phenomenological or formal logic approach to content; +Science [procedural orientation; norms of scientific method] +Humanities approach [I approach it from a rhetorical and ethical perspective] +Please read for homework.)\nTypes of knowledge (powerpoint presentation #1)\nSeven big ideas\n\nMotivated by truth, with no hope of proof.\nThe best research invites failure.\nWe invite failure by testing predictions.\n\nA line in the sand\n\nWe recognize failure by drawing a line in the sand.\n\nRefutation is easier than confirmation\n\nAim not to be right, but to be not not right.\n\nOperationalizing\n\nTest hypotheses by operationalizing terms.\nOperationalize, but don’t essentialize.\n\nComparison\n\nCompare, compare, compare. Randomized Control Studies - The Case of Microfinance Counterfactuals (powerpoint presentation #2)\n\nThe rhetoric of science (video - 8 minutes)\n\nThe rhetoric of science is the rhetoric of prophecy. “Science is a narrative activity, conducted by a community of scholars who hold each other to a methodological commitment to making and testing predictions.”\n\nReview the first 9 slogans: Quiz #1\nGroup Task #1: What’s worth knowing? An audience with God\nQuestions, conjectures, hypotheses and theories\nGroup Task #2: Question, theory or hypothesis? (Answers)\nGrandmother research\nThe quantitative/measurement obsession\nGroup Task #3a: Obvious theories - Part 1\nGroup Task #3b: Obvious theories - Part 2\n(Group Task #3 Debriefing) Obvious theories: Hindsight bias\n\nHindsight is 20/20.\n\nGrandmother research revisited\nTwo forms of reductionism\n\nIn research, reductionism is a method, not a belief.\nDon’t try to explain the whole world at once.\n\nEpistephobia\nTypes of failure (powerpoint presentation #3)\nReview (first 12) slogans\n\nTypes of Empirical Studies\n\nTypes of empirical studies (powerpoint presentation #5)\nGroup Task #4: Types of studies (Answers/Discussion)\nGeneralizing versus universalizing (video - 10 minutes)\n\nGeneralize, but don’t universalize.\n\nExploratory studies\nMeasurement studies\nHypothesislessness\n\nAvoid chronic hypothesislessness.\n\nGroup Task #5: Operationalize the following hypotheses\nSyncopation: From question to hypothesis (powerpoint presentation #8)\nOpinions as operationalizations\nDouble use data\n\nBeware of the post hoc theory.\n\nExploratory & Confirmatory: Contexts of discovery and legitimation (lecture)\nExplore-then-test Approach\nExplore-then-test example - Part 1 (Have a participant read the text aloud. What kind of study is this? What should happen next?)\nExplore-then-test example - Part 2 (Have another participant read the text aloud. Why make use of a robot rather than a human?)\nFrom question … to protocol.\n\nFrom question to theory to conjecture to hypothesis to protocol\n\nQuiz #2 (Answers)\nReview (first 16) slogans\nFeedback Day 1\n\nHomework\n\nHomework 1: Reading: Preamble: An Arts and Humanities Approach to Empirical Method Homework 2: Reading Guide #1: Lancashire & Hirst (2009)\nReading #1: Lancashire & Hirst (2009)\nHomework 3: Individual Task #6: From question to theory\n\n\n\nDAY 2\n\n\nReview and Elaborations\n\nWelcome; encourage mix-up seating/groups\nDay 2 Program\nReview slogans (first 16)\nGroup Task #7: From theory to conjecture\nMultiple operationalizations\nFormal observation: The taxi protocol\nThe Experiment\nThe Correlational Study\nCorrelation and causation\nThe third variable problem\n\nNo causation without manipulation.\n\n\nSampling\n\nSampling\nGroup Task #8: Sampling Approaches (Answers)\nWEIRD sampling (Western, Educated, Industrialized, Rich, Democratic)\nData Independence\n\nAvoid sticky data.\n\nBiased Sampling: Listening to Edith Boyd\nGroup Task #9: Sampling Issues (Discussion)\nGroup Task #10: What is Random?\nHomework review: Review questions and answers for Lancashire & Hirst (2009)\nSampling - A Practical Problem [omit?]\nSample Size: Law of Big Numbers\nSample Size: Law of Small Numbers\n\nThe law of large numbers does not apply to small numbers.\n\nEffect Size\nRegression-to-the-Mean\nRegression-to-the-Mean in Music\nReview (first 19) slogans\nGroup Task #11: Are dynamics asymmetrical? Data sheet\n\nBehavioral Data\n\nDependent and independent measures\nTypes of behaviors (summary) The full text (or video) are left as homework. Types of behaviors (full) (video - 32 minutes)\nImplicit vs. explicit responses\nReactivity\nDemand characteristics Clever Hans\n\nAlways debrief.\n\nMeasurement scales\nGroup Task #12: What kind of measurement scale? (Answers)\nDesigning questionnaires (powerpoint presentation #17)\nGroup Task #13: Questionnaire Design\nReview (first 20) slogans\nFeedback Day 2\n\nHomework\n\nReading Guide #2: Perttu (2007)\nReading #2: Perttu (2007)\n\n\n\nDAY 3\n\n\nIntroduction to Statistics\n\nDay 3 Program\nOutline (lecture)\nWhy measure? How to measure anything Measurement (powerpoint presentation #18)\nFermi Questions: Are there a half a billion windows in New York City? Are there 500 piano tuners in Chicago? [You may be better than you think you are at estimating some values.]\nDescriptive Statistics\nMeasures of Central Tendency powerpoint presentation\nGroup Task #15: Descriptive Statistics\nInferential Statistics\nInferential Statistics (including Probability, The null hypothesis, Confidence and significance levels, Statistical tests)\nConfidence level and Confidence Interval\nCalculating confidence interval from confidence level (web tool)\nStatistical Significance\nChi-square test (powerpoint presentation #22)\nTable of critical values for chi-square (short) (long table - 5 pages)\nGroup Task #16: Chi-square tests; (Answers)\nGroup Task #17: Funeral marches in F minor: Hypothesis (Answer)\nCorrelation demonstration Pearson’s r Table;\nSpurioius correlations - see <http://www.tylervigen.com/spurious-correlations\nInterpreting p; “Highly” and “marginally” significant\nMultiple Tests and Related Topics: Bonferroni Correction; File Drawer Effect\n\nCorrect for multiple tests.\n\nPositive results bias. The case of Ego Depletion (video - 9 minutes)\nConcept of “research registry”\nPublishing practice\nStatistical tests - General remarks\n\nMake friends with a statistician.\n\n\nApplied Statistics\n\nHeroes and villains in opera (simplified analysis using chi-square test)\nPopulation density and tempo (demonstrating Pearson’s r)\nGerman vs. Chinese folksongs (regression analysis)\n\nSome Advanced Analytic Techniques\n\nAdvanced Analytic Techniques\nCluster analysis: Orchestration Cluster analysis: From Classical to Romantic\nMulti-dimensional scaling (MDS)\nModeling - B-flat trumpet\n\nIf you torture the data long enough, it will confess to anything.\n\nFoote Novelty (powerpoint presentation #26)\nHomework review: Review questions and answers for Perttu (2007)\nReview (first 23) slogans\nFeedback Day 3\n\nHomework\n\nReading Guide #3: Shanahan & Huron (2014)\nReading #3: Shanahan & Huron (2014)"
  },
  {
    "objectID": "emp_methods_workshop/funeral_marches.html",
    "href": "emp_methods_workshop/funeral_marches.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Task\nVolume 17 of the New Grove Dictionary of Music and Musicians identifies seven classic funeral marches. As it turns out 5 of those 7 marches are in the key of F minor. Test the hypothesis that funeral marches tend to be composed in the key of F minor.\nIn order to calculate chi-square, we need to determine the probability of any given musical work being written in F minor.\nThe following table shows a distribution of keys from a convenience sample of 3,121 works from the common practice Western art music tradition.\n\n\n\nTonic\nMajor\nMinor\n\n\nC\n358\n6\n\n\nC#\n2\n2\n\n\nD\n194\n96\n\n\nD#\n0\n2\n\n\nEb\n125\n0\n\n\nE\n24\n35\n\n\nF\n757\n10\n\n\nF#\n2\n4\n\n\nG\n978\n119\n\n\nG#\n0\n2\n\n\nAb\n11\n0\n\n\nA\n126\n61\n\n\nBb\n198\n2\n\n\nB\n2\n5\n\n\nTotals:\n2,777\n344\n\n\n\nIs the tendency to write funeral marches in F minor statistically significant at the 0.01 level?\n\n\nRationale\nThis exercise provides additional practice in calculating statistical significance using the chi-square test."
  },
  {
    "objectID": "emp_methods_workshop/reviewing_advice.html",
    "href": "emp_methods_workshop/reviewing_advice.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Some Advice About Reviewing\n\nIn general, reviewing is an important part of the scholarly enterprise. Unfortunately, it tends to be invisible. Consider a journal that publishes (say) 20 articles per year. It is common to commission three reviews for each article. Many journals have a rejection rate of about 80%, which means that 4 out of 5 submissions are rejected. So the 20 published articles were likely selected from 100 submissions, which means perhaps 300 reviews were written. That’s a lot of work, and it is almost entirely unseen.\nIf you’ve been submitting manuscripts for publication, it follows that you should do your part by contributing to the review process as well. That’s what peer review means.\nEach journal has its own editorial policy and different journals may have different procedures for reviewing submissions. In some cases, the journal may send “Instructions for Reviewers.” More commonly, the editor will simply provide a sentence or two describing what is expected.\n\nThe Review\nA review is typically divided into four parts: (1) recommendation, (2) summary, (3) major concerns, and (4) detailed comments. Reviews are typically between 2 and 5 pages in length.\n\n\nRecommendation\nThe first part is a one or two sentence recommendation. There are four common outcomes: (1) accept outright, (2) accept pending minor revisions, (3) revise and resubmit, and (4) reject.\nHere are some examples of recommendations:\n“This is an admirable paper and I’m delighted to recommend publication.”\n“I enjoyed reading this paper, although I have reservations about the presentation of the data — especially Figure 2. If the author(s) is/are able to address this concern (see below), I am happy to recommend publication.”\n“While the subject matter of this paper is interesting, I regret that there are a number of methodological issues that need to be addressed (see below). Accordingly I would recommend that the author(s) revise and resubmit.”\n“I recognize that the author(s) carried out a considerable amount of work in pursuing their study. However, I regret that there are several serious methodological problems that make it impossible for the author(s) to draw any reasonable conclusions. I recommend that the paper be rejected.”\nBe aware that your recommendation is just that — a recommendation. It is the Action Editor (the person in charge of the review process for this particular manuscript) who will normally make the final assessment. Very rarely, the Editor will intervene and override the activities of the Action Editor, but this is extremely rare.\nWhen the review process is complete, the Action Editor will send you copies of all of the reviewers’ reviews, as well as the action letter, conveying the Action Editor’s recommendation. So you’ll get to see what other reviewers wrote about the same submission.\n\n\nSummary\nAfter the assessment comes the summary. In about a paragraph, you should describe the paper in your own words. The purpose is to show that you understood the manuscript. This is not always straightforward. Unlike published articles, submitted manuscripts vary considerably in their quality. Sometimes the manuscript is disorganized and it is hard to decipher what the researcher(s) did. It is possible that the research is actually quite well done, but the writing quality is so bad that it is hard to realize this as a reviewer. (Remember that many researchers do not have English as a first language.)\nAs a reviewer, you might end up criticizing something that’s not germane. For example, the author(s) might have done the right thing, but they just didn’t describe it adequately. You can waste a lot of time because you misunderstand something.\nBy summarizing the work, you make it clearer to the author(s) that they have communicated (or failed to communicate) their work. Don’t be afraid to say you had trouble understanding the paper. For example, in your summary, you might want to say something like “If I understand the paper correctly, the author(s) …”\n\n\nMajor Concerns\nThis is the place to identify any major concerns you have. It is common to number each issue successively. It is useful to present the concerns in their order of importance. This ordering will help the author(s) understand the relative gravity of each point. The Action Editor will also appreciate your ordering. For example if all three reviewers identify a particular problem near the top of their list, this lends weight to the overall assessment.\nIn your recommendation, you may draw attention to particularly onerous problems.\n\n\nDetailed Comments\nThis is the place to identify simpler issues. Detailed comments might refer to simple problems such as spelling erorrs, confusing sentence structure, recommendations that the author(s) cite other existing research, etc. Typically, detailed comments begin with the page/line numbers, followed by the comment. Don’t be afraid to make specific recommendations. E.g.\npg.7/lines 10-11 “Our results establish that …” Please replace by “Our results are consistent with the view that …”\n\n\nGeneral Advice\nIt is common for new reviewers to feel unqualified. You may feel that you really don’t know very much about the specific area of research addressed in the manuscript. Unfortunately, there are simply not enough experts in the world. All scholars must leave their comfort zone, otherwise nothing would get done.\nThe most common mistake for new reviewers is to be overly critical. Avoid harsh language. People pour their hearts into their research. It is discouraging to receive rejection notices and disappointing to receive perpetual “revise and resubmit” letters. But if the assessments are written using acerbic language, this just deepens the wounds — for no good reason.\nBe helpful. Instead of saying “Why did you do this?” Offer specific advice. Tell the author what to say. Don’t be afraid to suggest replacement wording. Tell them what to do to bring the work up to sufficient quality.\nReviews are teachable moments. Take the opportunity to help your colleagues become better researchers. Explain the problems. Share your knowledge. Describe what they’ve done wrong. The vast majority of music scholars have never heard of “multiple tests.” If necessary, provide a reference to a book (include page numbers) that describes some methodological difficulty. Even if you recommend rejection, the entire exercise can still contribute to the discipline as a whole by helping other researchers to become better at their craft. Especially when I was younger, I learned a lot from the comments reviewers made about my work.\nBe circumspect about asking for additional work. There is no such thing as a definitive study, so there is no end to the number of follow-up studies that can be done. Especially when contrasted with books, journal articles are meant to be “contributions” — research that adds another piece to the puzzle. An excellent experiment may very well invite an obvious follow-up experiment to test one of the main assumptions. Don’t be quick to ask for the follow-up. Here’s why.\nI’ve had the following experience many times in my career: With student X we carried out a study. Student X then graduates, leaves the field, or gets a job in a small music department with little opportunity for research. We submit the study and the Action Editor asks for a follow-up study or experiment. I am busy with other projects and find that I’m unable to interest another student in continuing the work. Moreover, if a second student joins the project, they are apt to be the third author on the resulting paper and that often seems to them like insufficient recognition for their work. (The new student doesn’t realize how much work the first student did.) As a result the project gets abandoned. Some years later a colleague (one of the original reviewers) approaches me at a conference and says “I read this really interesting study you did some time ago; whatever happened to that?” :-)\nIn the end, ask yourself the following question: “Will publishing this article as it is (without any additional work) contribute to our knowledge?” If the answer is yes, don’t hold things up by asking for another experiment.\nDefend good work. If you think the work is good, say so. Many journals have a policy that if any one reviewer recommends rejection, then the Action Editor is expected to reject the submission. If you write enough reviews, you will have the experience where a perfectly good piece of research is rejected because one of the other reviewers was feeling spiteful or ornery. An Action Editor will have second thoughts if the other reviewers are enthusiastic about the work.\n\n\nSigned Reviews\nMost journals practice blind peer review in the sense that the reviewers remain anonymous. Most (though not all) journals also remove identifying information about authorship. You can often guess the identity of an author by the subject matter and by the cited references. However, as you gain experience, one tends to mistrust these guesses. Try to avoid the mental guessing-game about authorship. It’s just not helpful and is ultimately another form of research bias.\nIn some cases, reviewers will identify themselves in their reviews (so-called “signed reviews”). This is usuallly done when the reviewer thinks it would be beneficial for the author(s) to contact the reviewer for further clarification. For example, a reviewer might be quite enthusiastic about the research but has specific suggestions that could be better conveyed by conversation. It is rare for “rejections” to be signed.\n\n\nAnnual Report\nReviewing is something you can report in your annual report to your department chair. Being asked to review for journals testifies to your status in the scholarly community. When reporting, it’s important to maintain the anonymity of the author(s). Instead, simply report that you did a review for Journal X.\nOnce you have reviewed for several journals, this is something you may wish to include on your C.V. under SERVICE or PROFESSIONAL SERVICE. E.g. “Manuscript reviewing done for Music Perception, Music Theory Spectrum, and Music Theory Online.”"
  },
  {
    "objectID": "emp_methods_workshop/hypotheses.html#questions-theories-conjectures-and-hypotheses",
    "href": "emp_methods_workshop/hypotheses.html#questions-theories-conjectures-and-hypotheses",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "Questions, Theories, Conjectures, and Hypotheses",
    "text": "Questions, Theories, Conjectures, and Hypotheses\n\n\nQuestions\nA question is an expression of inquiry that invites or calls for a reply. Research is typically motivated by a question.\n“Why do Beethoven’s metronome markings seem so fast?”\n\n\nConjectures\nA conjecture is a supposition or speculative claim made about the world. Conjectures are not directly testable.\n“The first musical instruments were probably drums.”\n\n\nHypotheses\nA hypothesis is a testable conjecture. It is a claim or prediction that can, in principle be compared to existing or future observations.\n“The music people experience as adolescents will remain the most emotionally important music throughout their lives.”\n\n\nTheories\nA theory is an explanatory framework for understanding a phenomenon. Theories typically propose some cause — as in “X causes or influences Y.” Theories often involve words like “because,” “due to,” “affects,” “influences,” or “causes.” Theories themselves are rarely directly testable. Theories are used to generate conjectures and hypotheses.\n“The reason why music from adolescence is so memorable is because of the high levels of the hormone oxytocin experienced during adolescence.”"
  },
  {
    "objectID": "emp_methods_workshop/quantitative.html",
    "href": "emp_methods_workshop/quantitative.html",
    "title": "The Quantitative/Measurement “Obsession”",
    "section": "",
    "text": "Another common complaint is that empirical researchers are obsessed with numbers and measurement. To many, this quantitative preoccupation seems contrary to the humanistic spirit.\nQuantitative methods are important in empirical research for two reasons. The first reason is that counting can help us discover phenomena that might otherwise be invisible. The second (and more important) reasons is that counting helps make it clear when we are wrong. Counting and measuring are useful tools, like a magnifying glass or a stethoscope.\nFirst, consider how numbers can aid in discovery: consider the story of radar imaging of the earth.\n\n\n\nIn the early years of the space program, NASA launched a simple satellite that used a radar beam to image the planet’s surface. The satellite produced very accurate readings of the height (elevation) for each point as it orbitted the earth thousands of times. The purpose of the satellite was to help geographers produce more accurate topographical maps of the earth.\nAbout 3/4 of the earth’s surface is covered by water, so most of the data collected by the satellite was precise elevation measures of a given point on the ocean as the satellite passed overhead. Since the goal of project was to create accurate topographical maps of the landmass, the ocean data was considered useless. Moreover, each time the satellite passed over the same spot in the ocean, the water level was different (because of the effects of waves and tides).\nNevertheless, some bright scientist asked, “What would happen if, for a given point in the ocean, we averaged together all of the elevation measures?” Repeated measures would tend to erase the effects of the waves and tides. You might think that this would averaging procedure result in a smooth ocean surface. But when the researchers did this, they found that there were regions of the ocean where the average sea level was relatively high and other regions where the sea level was relatively low. This was how scientists discovered vertical currents.\nWe’re all familiar with horizontal currents — lateral movements of water, such as the “gulf stream” which moves water from the Gulf of Mexico to northern Europe. But there are also “vertical” currents: regions in the oceans where water is constantly welling-up from below, or being sucked-down from the surface. The radar data allowed researchers to see these vertical currents, and it was only possible to “see” these currents because of averaging large amounts of (highly variable) data. The averaging allowed the effects of waves and tides to be eliminated.\n\n\n\nIn music, the value of averaged measurements is apparent in the so-called melodic arch. Musicians have long observed an apparent tendency for melodic phrases to rise upward and then fall downward forming a sort of arch. Tunes like My Bonnie Lies Over the Ocean and Somewhere Over the Rainbow offer appropriate examples. In both songs, the phrases tend to ascend at the beginning and descend toward the cadence.\nOf course, one can think of all kinds of exceptions. For example, both Joy to the World and the American national anthem begin with high initial pitches — dropping downward and then rising upward over the course of the phrase. So is there any merit to the notion of a melodic arch?\nThe figures shown below are from Huron (1996). The graphs pertain to samples of 7-, 8- and 9-note phrases. Almost twenty thousand musical phrases were averaged together in order to produce these three figures. Specifically, the first data points in each graph represent the average pitch height (measured in semitones from middle C) for the first notes in each phrase. The second data points represent the average pitch for the second notes, and so on.\nAlthough we can identify plenty of individual exceptions, on average, the graphs are indeed consistent with the notion of a general melodic-arch tendency — at least in the case of some Western music.\n  \n\n\n\nIn my first year as an undergraduate student, I spent time reading some of the classic feminist writers — including works by Betty Friedan, Germaine Greer, Susan Brownmiller and Gloria Steinem. As a man, I frequently felt defensive when reading these works. From time-to-time I thought various claims were exaggerated or simply wrong. I recall reading one passage which discussed how men tend to dominate mixed conversations. My first response was skeptical. I’d grown up in a society where women were regularly portrayed as “chatterboxes” and “gossips.” I personally knew a couple of women whose monologues were perfectly capable of filling-in whatever air-time was available.\nMy skepticism led me to perform what may have been my first empirical study. One afternoon, I simply sat in a student lounge eavesdropping on different conversations around me. With pencil and notebook in hand, I watched the second-hand of a wall-clock, and recorded the amount of time men and women spoke in various groups. I also recorded the number of men and women participating in each conversation. After an hour or so, I tallied up the results. The numbers were sobering. In every mixed group that I monitored, men dominated the conversation by amounts that were disproportionate to their numbers. Despite the thousands of conversations I’d heard in my life to that point, somehow I’d failed to notice what was obvious to most women. The numbers in my notebook were not consistent with the “chatterbox” image of women. The numbers told me that I had been wrong. Ultimately, the numbers changed my attitude.\nResearch shows that it can be surprisingly hard to get people to change their views. Anecdotes can be easily dismissed as “one-off” exceptions to our beliefs. It takes constant battering with evidence, before people will finally acknowledge that their beliefs may be incorrect.\nAlthough the essence of good research is to invite failure, we may be slow to recognize failure when we see it. Numbers provide a useful tool for defining failure. In research, the principal benefit of quantitative methods is that they can provide compelling evidence that you’re wrong.\n\n\n\nEmpirical researchers typically don’t like being called “bean counters” for the same reason that anthropologist don’t like being called “travel writers.” It is true that anthropologists travel, and write about the cultures they encounter. It is also true that empirical musicologists count things. But in both cases the goal is understanding and discovery. Counting provides a useful way for discovering aspects about the world that might otherwise be invisible to intuition or informal observation.\nMore importantly, the humble act of counting is the clearest way to invite failure. Recall that we recognize failure by drawing a line in the sand. As noted earlier, drawing a line means that we must have some way of determining on which side of the line the observations lie. We count and measure things as a way of inviting failure. We count, not because of some quantitative obsession, but because counting helps us recognize when were are deceiving ourselves (Huron, 1999).\nCounting the number of malaria parasites in a blood sample is not a symptom of obsessive-compulsive disorder. Nor does it arises from some dark allegiance to the mechanical. It arises from an understanding that intuition can often warp perception rather than inform it, and that quantitative methods are valuable checks against our own intellectual enthusiasms. A person who genuinely cares about human welfare may ultimately find that his or her commitment leads to acts such as counting the number of malaria parasites in a blood sample. People who genuinely care about understanding music may similarly find that their commitment leads them to acts such as measuring the spectral centroid in a sung vowel.\nAt first, counting seems to represent the very antithesis of the humanistic spirit. But the humble act of counting offers one of the very best ways of addressing some of the most important questions in music and culture in a way that reduces self-deception.\n\n\n\nDavid Huron (1996). The melodic arch in Western folksongs. Computing in Musicology, Vol. 10, pp. 3-23.\nDavid Huron (1999). The 1999 Ernest Bloch Lectures. Lecture 3. Methodology: The New Empiricism: Systematic Musicology in a Postmodern Age."
  },
  {
    "objectID": "emp_methods_workshop/regression_in_music.html#regression-to-the-mean-in-music",
    "href": "emp_methods_workshop/regression_in_music.html#regression-to-the-mean-in-music",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "Regression to the Mean in Music",
    "text": "Regression to the Mean in Music\n\nRecall that encountering a tall person does not cause the next person to be shorter. It is simply that most people tend to be of average height.\nFor over 400 years, it has been frequently observed that large melodic intervals tend to be followed by a change in melodic direction. Statistical studies show that this is indeed the case. The following folksong melody provides a typical example. In this melody there are 13 melodic leaps—if a leap is defined as any interval larger than a diatonic step. Of these 13 melodic leaps, 11 are followed by a change of direction, 1 leap continues in the same direction, and 1 is followed by a repeat of the same pitch. This example is consistent with the notion that melodic leaps are followed by a change in direction. However, there are other possibilities.\n\nSuppose that a melody made use of just three pitches: A, B and C. Notice that there is only one possible leap: between A and C. Notice also that if we leap up to the C, the only possible continuations are a unison repetition of the C, or descending to B or A. Similarly, if we leap down to the A, the only possible continuations are a unison repetition or an ascending contour. Simply due to the constraints on range, melodic leaps will tend to be followed by a change of direction.\nIf we expand our melodic range from three pitches to four pitches, the effect is reduced, but there will be a tendency for leaps to be followed by a change of direction. If most of the pitches in a melody tend to be in the center of the tessitura, and if large leaps have a tendency to take the melody to the extremes of the range, then most leaps will tend to be followed by a change of direction.\nWe can now propose two competing theories regarding the melodic contours following a leap. The post-skip reversal theory proposes that melodic leaps tend to be followed by a change of direction. The regression-to-the-mean theory proposes that there is a tendency for melodies to stay in the central portion of the tessitura.\n\nNotice that the regression-to-the-mean theory implies that melodic behavior following a leap depends on whether the consequent pitch is high, medium, or low in tessitura. By contrast, the post-skip reveral theory makes the same prediction no matter where in the tessitura the leap occurs.\nLet’s distinguish four possible types of melodic leaps. A median-departing leap is a leap that begins above (or below) the median pitch, and moves even further away from the median. A median-crossing leap is a leap that begins above (or below) the median pitch, and crosses over the median in the other direction. A median-landing leap is a leap that land on the central or median pitch itself. Finally, a median-approaching leap is a leap that moves closer to the median, but both pitches are above (or below) the median. These different types of leaps are schematically illustrated below. The illustration pertains only to ascending leaps, but the concepts apply to descending leaps as well.\n\nNotice that post-skip reversal and regression-to-the-mean make different predictions. Post-skip reversal predicts a change in direction for intervals in all four conditions. However, regression-to-the-mean makes different predictions depending on the condition. In the case of median-departing and median-crossing leaps, regression-to-the-mean predicts a change of direction (back toward the median pitch). However, in the case of median-landing leaps, regression-to-the-mean predicts that the subsequent direction doesn’t matter: a melodic contour is just as likely to continue in the same direction or reverse direction. Finally, in the case of median-approaching leaps, regression-to-the-mean predicts that there will be a tendency for the melodic contour to continue in the same direction—exactly contrary to the post-skip reversal theory.\nSo what do actual melodies do? The following graph represents data from over 3,000 melodic leaps. As can be seen, for median-departing leaps there is a clear tendency for melodies to continue with a change of direction. Similarly, for median-crossing leaps, leaps tend to be followed by a change of direction. These results are consistent with both post-skip reversal and regression-to-the-mean. In the case of median-landing leaps, reversing direction or continuing in the same direction are equally likely—as predicted by regression-to-the-mean. Finally, in the case of median approaching leaps, it is more likely to melodies to continue in the same direction—as predicted by regression-to-the-mean. After a study by von Hippel and Huron (2000), further research published by von Hippel (2000a, 2000b) confirmed that melodic organization is consistent with regression-to-the-mean, and is not consistent with post-skip reversal.\n\n\nReferences:\nPaul von Hippel, & David Huron (2000). Why do skips precede reversals? The effect of tessitura on melodic structure. Music Perception, Vol. 18, No. 1, pp. 59-85.\nPaul von Hippel (2000). Redefining pitch proximity: Tessitura and mobility as constraints on melodic intervals. Music Perception, Vol. 17, No. 3, pp. 315-327.\nPaul von Hippel (2000). Questioning a melodic archetype: Do listeners use gap-fill to classify melodies? Music Perception, Vol. 18, No. 2, pp. 139-153."
  },
  {
    "objectID": "emp_methods_workshop/replication.html",
    "href": "emp_methods_workshop/replication.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "The Replication Crisis\n\nWe have discussed positive results bias (where journals are more likely to publish research whose data are statistically significant), and the file drawer effect (where researchers tend to ignore negative results). If these effects are real, then we might expect that a substantial proportion of published empirical articles are reporting spurious results. That is, the combination of these effects suggests that journals may be publishing lots of spurious positive results that arose simply due to chance. How common is this hypothetical scenario?\nThe classic way to ferret out spurious results is through replication. Simply repeating an experiment provides Nature with another opportunity to tell us we’re wrong. In science, replication is the gold standard: a discovery remains provisional or preliminary until it has been replicated many times under many different circumstances.\nIn medicine, replication studies are common. A single study might suggest that a particular drug is effective in treating some condition. However, researchers continue with follow-up studies—collecting data that allows them to frequently check whether the drug is having the effect they expect.\nUnfortunately, replication is less common in other disciplines. Part of the reason for this is yet another form of bias: discovery bias. Scholars receive awards for new discoveries. Few scholars get an award for providing additional evidence supporting a previous discovery. Moreover, few scholars get an award for showing that an existing idea doesn’t replicate. A researcher’s professional reputation tends to be linked to new claims. Professional incentives (like promotions, tenure, and increased salaries) are more likely to accrue to those researchers who discover something new. Replication studies just don’t seem as important as conducting new research. Consequently, scholars tend shun replication research. When a study replicates an existing study, it is hard for observers not to simply shrug their shoulders and say “We already knew that!” (In short, replication research looks like another form of grandmother research.) Journals are less likely to publish replication studies, employers are less likely to reward researchers who engage in replication, and the community of scholars is more likely to view replication research as lacking in creativity or innovation.\nIn 2015, a major study was published in the journal Science that aimed to estimate the magnitude of the problem. With a major grant from the Laura and John Arnold Foundation, Brian Nosek and the Open Science Collaboration initiated a project to conduct a large number of replication studies. They identified 100 experimental and correlational studies that had been published in three high-ranking psychology journals in the year 2008. Then 266 volunteer researchers from around the world went to work performing the appropriate replication studies. The goal of the project was to estimate the proportion of new results that are likely to be replicated, and to determine the effect sizes for those studies that replicated.\nAll of the replication studies closely followed the original studies. In fact, the volunteer researchers directly collaborated with the original authors to ensure that the designs, methods, and materials were the same or comparable.\nWhat were the results? Ninety-seven percent of the original published studies had reported significant results at the 95% confidence level (i.e., p<.05). However, only 36% of the replication studies produced similarly significant results. In addition, only 47% of the original effect sizes were in the 95% confidence interval of the replication effect size. The mean effect size for the original studies was r=.403. However, the mean effect size for the replication studies was half (r=.197). The relationship between the original studies and the replication studies is shown graphically below.\nEach circle plots the effect size for the original study (horizontal axis) versus the effect size for the replication study (vertical axis). Values that lie near the blue diagonal line represent studies that replicated very well—that is, the effect sizes for both the original and replication studies were very similar. Notice that many of the circles, however, lie considerably below the blue line. These represent replication studies that produced lower effect sizes. Green circles identify statistically significant results. The pink circles identify replication results that failed to achieve statistical significance. The dotted horizontal line represents no effect at all for the replication studies. Clearly, many of the original studies failed to replicate.\n\nNotice that the data for both an original study and a replication study can be combined and treated as a single experiment. When combining the data in this way, 68 percent of the studies produced statistically significant effects. This means that somewhere beween one-third and one-half of the original published studies are likely to be reporting spurious results.\nBy way of example, here are three “findings” in the original publications that failed to replicate:\n\nPeople are more likely to cheat after they read a passage telling them that their behaviors are biologically determined and that they don’t have free will.\nPeople make less severe moral judgements when they’ve just washed their hands.\nA partnered woman is more likely to be attracted to a single man when she is ovulating.\n\nIncidentally, the replication studies found no evidence of fraud—such as making up data. Instead, the high incidence of failures-to-replicate suggest the existence of positive results bias and the file-drawer effect.\n\nResearch in the Popular Press\nProfessional journals aren’t the only venues where discoveries get reported. Journalists eagerly browse through recent scholarly publications, looking for discoveries that the general public is apt to find interesting. This raises another question: Are there further biases that are introduced due to the selection criteria introduced by journalists? Said another way, how trustworthy are science reports in the popular press?\nIn 2017, Estelle Dumas-Mallet and her colleagues published a revealing study of how medical discoveries are reported in the popular press. They began with 306 medical “discoveries.” All 306 discoveries inspired a flurry of subsequent research, resulting in over 5,000 pertinent research articles. Moreover, all 306 discoveries had been the subject of formal meta-analyses—statistical review studies that endeavored to determine the reliability and effect size for each purported discovery.\nOf the 306 purported discoveries, 156 (about half) had been picked-up by the popular press, resulting in 1,561 newspaper articles—published soon after the first (inaugural) research report. All of the newspaper articles focused on the initial journal article (and research team) who had reported the original discovery. Dumas-Mallet et al. were interested in two main questions: (1) What proportion of the initial research reports proved ultimately to be spurious? (2) For those “discoveries” that proved to be wrong, did the popular press report later negative or null results?\nWhat they discovered was that only 49% of the 156 studies reported by newspapers were ultimately confirmed by the corresponding meta-analyses. In short, slightly over half of popular press reports of medical discoveries simply report spurious results that ultimately fail to be replicated.\nMost medical discoveries are actually replicated. So it is curious that most of the medical discoveries reported in the popular press fail to be replicated. This suggests that journalists themselves are attracted to ideas or studies that seem surprising, counter-intuitive, or unusual. Readers are more apt to find some ideas more interesting than others. Telling a reader that chocolate is bad for you (because of the high sugar content) is not nearly as compelling as telling readers that chocolate is good for you (because it releases endorphins).\nWith the advent of the Internet, it appears that things have become worse. Web pages are filled with so-called click bait—pictures with captions that are explicitly designed to grab our attention. It shouldn’t be a surprise that our attention is most attracted by the outlandish and surprising: pregnant man divorces wife, mouse attacks cat, car drives upside-down, … listening to music makes you smarter (Pietschnig, Voracek & Formann, 2010).\nAs you might expect, Dumas-Mallet et al. also found that newspapers never covered any initial studies reporting null findings, and rarely reported null findings of subsequent studies. That is, newspapers exhibit a significant positive results bias—a bias that is even greater than the positive results bias in professional journals.\nThe work of Dumas-Mallet and her colleagues suggests that there are at least two additional forms of reporting bias that plague the popular press. The first we might call the unusualness bias: publish ideas that are unusual (and so interesting) since they attract readers. Unfortunately, unusual “discoveries” are more apt to be wrong. The second bias we might call follow-up bias. When a previously published idea is shown to be wrong, journalist tend not to report the pertinent research reporting that it is wrong.\n\n\nLessons\nThese and other studies suggest that a lot of research published in scholarly journals are susceptible to Type I errors—making claims that are simply false. The results are consistent with the idea that both professional journals and the popular press exhibit positive results bias. Perhaps the most important lesson from these studies is that not enough replication studies are being conducted.\nFortunately, since 2010, the scientific world has become much more aware of these problems and efforts are being made to address the so-called replication crisis. In a recent New Yorker article, researcher Gary Marcus wrote:\n“there is something positive that has come out of the crisis of replicability—something vitally important for all experimental sciences. For years, it was extremely difficult to publish a direct replication, or a failure to replicate an experiment, in a good journal. Throughout my career, and long before it, journals emphasized that new papers have to publish original results; I completely failed to replicate a particular study a few years ago, but at the time didn’t bother to submit it to a journal because I knew few people would be interested. Now, happily, the scientific culture has changed.”\n\n\nReferences\nDumas-Mallet, E., Smith, A., Boraud, T., & Gonon, F. (2017). Poor replication validity of biomedical association studies reported by newspapers. PLoS ONE Vol. 12, No. 2, e0172650. https://doi.org/10.1371/journal.pone.0172650\nEverett, J.A.C., & Earp, Brian D. (2015). A tragedy of the (academic) commons: interpreting the replication crisis in psychology as a social dilemma for early-career researchers. Frontiers in Psychology, Vol. 6, article 1152. doi:10.3389/fpsyg.2015.01152.\nMarcus, G. (2013). The Crisis in Social Psychology That Isn’t. The New Yorker, May 1, 2013.\nOpen Science Collaboration (2015). Estimating the reproducibility of psychological science. Science, Vol. 349, Issue 6251, aac4716 DOI: 10.1126/science.aac4716\nPietschnig, J., Voracek, M., & Formann, A.K. (2010). Mozart effect — Shmozart effect: A meta-analysis. Intelligence, Vol. 38, No. 3, pp. 314-323. doi: 10.1016/j.intell.2010.03.001"
  },
  {
    "objectID": "emp_methods_workshop/operationalize_this.html",
    "href": "emp_methods_workshop/operationalize_this.html",
    "title": "Operationalize This",
    "section": "",
    "text": "Operationalize the following hypotheses:\n\nWomen like to dance more than men.\nThe progression from dominant to subdominant is more common in Reggae music than other musics."
  },
  {
    "objectID": "emp_methods_workshop/measurement.html",
    "href": "emp_methods_workshop/measurement.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "The Measurement Study\n\nThe measurement study is a type of exploratory study. It differs from the reconnaissance and descriptive studies insofar as some sort of measurement is involved. It differs from the exploratory correlational study insofar as the researcher doesn’t attempt to determine whether there are relationships between any of the measurements.\nThe centerpiece for a measurement study is the presentation of descriptive statistics. Typically, some graph or chart is produced. However, no a priori hypothesis is proposed, and no formal statistical test is performed. An example is provided in the table below—showing the number of musical instruments sold in the United States in 2012.\nAs with all exploratory studies, the hope is that the research will produce interesting observations, raise good questions, and ultimately inspire the formation of an explanatory theory. However, any resulting theory is post hoc. The observations cannot be viewed as “supporting” or “testing” the theory, since the observations were the inspiration for the theory in the first place.\n\n\n\nInstrument\nNumber Sold in the U.S. in 2012\n\n\n\n\nAcoustic guitars\n1,326,500\n\n\nElectric guitars\n1,162,890\n\n\nUkuleles\n1,087,000\n\n\nFlutes & Piccolos\n139,362\n\n\nTrumpets\n134,357\n\n\nElectronic pianos\n128,000\n\n\nClarinets\n111,160\n\n\nSaxophones\n86,188\n\n\nTrombones\n63,803\n\n\nAcoustic pianos\n37,200\n\n\nTubas\n35,144\n\n\nFrench Horns\n14,750\n\n\nOboes & Bassoons\n5,790\n\n\n\n(From Music Trades, April 2014)"
  },
  {
    "objectID": "emp_methods_workshop/desc_stats_task.html",
    "href": "emp_methods_workshop/desc_stats_task.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Task\n\nDraw a normal distribution. \nDraw a continuous distribution with positive skew. \nDraw a histogram distribution with negative skew. \nDraw a continuous distribution that is platykurtic. \nDraw a histogram distribution that is leptokurtic. \nDraw a continuous distribution in which the mean is lower than the mode. \nFor the following set of numbers, calculate the mean, mode, and median: 2, 4, 4, 6, 6, 10, 14, 14, 14, 18, 18, 20, 20, 30, 70.\nMean:\nMode:\nMedian:\nFor the above set of numbers, sketch a boxplot.\n\n\n\n\nRationale\nThis task is intended to encourage familiarity with concepts in descriptive statistics."
  },
  {
    "objectID": "emp_methods_workshop/day1.html",
    "href": "emp_methods_workshop/day1.html",
    "title": "Day 1",
    "section": "",
    "text": "Welcome & Introduction\nGenerals Aims & Preview\n\nDay 1 Program\nLearning objectives\n\nEmpirical Research\n\nPreamble: An Arts and Humanities Approach to Empirical Method\nTypes of knowledge\nSeven big ideas\n\nMotivated by truth, with no hope of proof.\nThe best research invites failure.\nWe invite failure by testing predictions.\n\nA line in the sand\n\nWe recognize failure by drawing a line in the sand.\n\nRefutation is easier than confirmation\n\nAim not to be right, but to be not not right.\n\nOperationalizing\n\nTest hypotheses by operationalizing terms.\nOperationalize, but don’t essentialize.\n\nComparison\n\nCompare, compare, compare. Randomized Control Studies - The Case of Microfinance\n\nThe rhetoric of science (video - 8 minutes)\n\nThe rhetoric of science is the rhetoric of prophecy. “Science is a narrative activity, conducted by a community of scholars who hold each other to a methodological commitment to making and testing predictions.”\n\nReview the first 9 slogans: Quiz #1\nGroup Task #1: What’s worth knowing? An audience with God\nQuestions, conjectures, hypotheses and theories\nGroup Task #2: Question, theory or hypothesis? (Answers)\nGrandmother research\nThe quantitative/measurement obsession\nGroup Task #3a: Obvious theories - Part 1\nGroup Task #3b: Obvious theories - Part 2\n(Group Task #3 Debriefing) Obvious theories: Hindsight bias\n\nHindsight is 20/20.\n\nGrandmother research revisited\nTwo forms of reductionism\n\nIn research, reductionism is a method, not a belief.\nDon’t try to explain the whole world at once.\n\nEpistephobia\nTypes of failure (powerpoint presentation #3)\nReview (first 12) slogans\n\nTypes of Empirical Studies\n\nTypes of empirical studies (powerpoint presentation #5)\nGroup Task #4: Types of studies (Answers/Discussion)\nGeneralizing versus universalizing (video - 10 minutes)\n\nGeneralize, but don’t universalize.\n\nExploratory studies\nMeasurement studies\nHypothesislessness\n\nAvoid chronic hypothesislessness.\n\nGroup Task #5: Operationalize the following hypotheses\nSyncopation: From question to hypothesis\nOpinions as operationalizations\nDouble use data\n\nBeware of the post-hoc theory.\n\nExploratory & Confirmatory: Contexts of discovery and legitimation (lecture)\nExplore-then-test Approach\nExplore-then-test example - Part 1 (Have a participant read the text aloud. What kind of study is this? What should happen next?)\nExplore-then-test example - Part 2 (Have another participant read the text aloud. Why make use of a robot rather than a human?)\nFrom question … to protocol.\n\nFrom question to theory to conjecture to hypothesis to protocol\n\nQuiz #2 (Answers)\nReview (first 16) slogans\nFeedback Day 1\n\nHomework\n\nHomework 1: Reading: Preamble: An Arts and Humanities Approach to Empirical Method Homework 2: Reading Guide #1: Lancashire & Hirst (2009)\nReading #1: Lancashire & Hirst (2009)\nHomework 3: Individual Task #6: From question to theory"
  },
  {
    "objectID": "emp_methods_workshop/write1st.html",
    "href": "emp_methods_workshop/write1st.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Write the Paper First\n\nPeople who are inexperienced with research imagine that research is about poking around through dusty archives, doing fieldwork in a remote culture, running experiments, or analyzing data. The sad truth is that research is mostly about writing. As a researcher, you will need to write research articles, dissertations, reviews, and perhaps even books or book chapters. Active researchers spend a lot of time writing grant proposals, writing human subjects (IRB) applications, writing to editors regarding papers, revising manuscripts, responding to reviewers’ comments, communicating with collaborators, e-mailing prospective participants, and many other writing tasks.\nSome people don’t like writing. If you don’t enjoy writing, then research may not be a good career choice. Writing can be tiresome. But like practicing a musical instrument, writing gets much easier with experience. In research, writing is essential.\nOver the decades, I have learned one truly important key to doing research well. Most researchers don’t follow this advice, but it is the single most important lesson I’ve learned. It dramatically improves the quality of one’s research, accelerates your productivity, enhances your enthusiasm, and reduces the total amount of work:\nSlogan: Write the paper first.\nBy “write the paper first,” I mean the whole paper — not simply an outline, or a literature review, or the “Methods” section. Before you do anything else, you should write the paper — all the way from title to conclusion. Write the paper as though you have already completed the study. The paper should include figures (that you make up) as well as the statistical analyses (again, entirely made up). Use the capital letter X to mark everything that you’ve make up, including claims about results. Here is a sample:\nA study was carried out in which 8X music teachers judged the intonation of 60X recorded amateur performances. The amateur musicians included 38X female and 22X male instrumentalists with an average age of 14.6X years. …\nSix reasons why you should write the paper first:\n\nDoing things right. In order for a research article to get published, it must be vetted by expert (peer) reviewers in the discipline. As any experienced researcher can tell you, journal reviewers can be quite picky. Reviewers will not approve a submission that does not meet a very high standard of quality. Reviewers read every submission with considerable skepticism. They will question whether your research question is appropriately framed. They will consider whether your literature review is adequate or whether you failed to cite other pertinent prior research. They will question whether your conjecture follows logically from your theory. They will ask whether the conjecture is also consistent with other theories. Most of all, they will critically examine your Method. Are the conceptual terms in the hypothesis appropriately operationalized? What alternative operationalizations would have been better? Should a second or third study have been carried out in which the concepts were operationalized in other (contrasting) ways. Was the sample biased in some way? Are there better sampling methods that should have been used? Should other participants have been recruited? Were the instructions to participants carefully phrased? Were there appropriate control conditions? What variables were left uncontrolled that should have been controlled? Should the design have been with-subjects or between-subjects? Were the stimuli appropriately generated and presented in random order for each participant? Did you use the correct statistical test? Are your statistics reported using the proper format? Did you correct for multiple tests? Have you carefully distinguished a priori from post hoc analyses? Were potential demand characteristics identified and efforts made to minimize them? Was the conclusion expressed using appropriately circumspect language?\nThe sad truth is that most submissions to professional journals are rejected because there is some flaw in the design. In many cases, the Reviewers ask the researcher to “Revise and Resubmit.” This can involve re-doing the research with an additional control, or adding another test of the hypothesis. Research articles can sometimes take years before a submission is finally approved by the journal’s reviewers.[1]\nFortunately, if the research is done carefully, reviewers will frequently accept the paper with only minor revisions, or sometimes accept the paper outright without any modifications. Experienced researchers learn that they will save themselves a great deal of frustration if they do the research carefully right from the beginning. If you do the research properly, you will only need to do it once.\nUnfortunately, problems can easily pass unrecognized. You’ve probably had the experience of thinking that you have a clear idea of what you are doing, and only later realized that your idea was not nearly as clear as you thought it was. A procedure is not clear until you write it down and think carefully about the logic.\nIn writing the paper first, you already imagine that you are addressing a skeptical audience of expert reviewers. Think of them as looking over your shoulder while you write. Imagine them asking the skeptical questions: Why are you doing it that way? Have you considered some alternative ways of addressing the same question? There is nothing like writing the Method out completely to make it clear what it is that you are planning to do. Above all, writing is a process of clarification. If you begin the research before you have a clear idea of how you will defend the method to skeptical experts, then you are tempting an ill fate.\nChanneling your enthusiasm. Successful researchers are passionate about what they are doing. If you are passionate, then you will find plenty of motivation to do your work. Passion, in turn, is fed by interest. If you are not interested in what you are doing, then the work will become drudgery. It is very common to be passionate about a project when the project first begins. But if the project drags on for a long time, you will inevitably lose some of your original passion. It is important that you don’t squander your passion.\nIn particular, it is important not to leave “hard” tasks to the end of a project. It is disheartening to have to face difficult tasks when your enthusiasm is waning. The hardest thing to do in research is writing-up the final report.\nIn general, aim to do the hardest tasks when your enthusiasm is greatest. In empirical research projects, there are two points in the project where the researcher is most excited. The first point is when you first dream-up your project. Take advantage of this burst of excitement: sit down and begin writing the paper as though the research is already finished. In particular, aim to write-out (and defend in prose) the complete Method section. Do your homework by doing a thorough literature review (if someone else has already done a pertinent study, you want to know that before you begin your data collection, not after you have already started your research). In addition, you must write the Results section. This may sound impossible and even unethical, but recall that the Results section consists principally of your statistical analysis. It is important for you to have a good plan of how you will analyze the data you collect.\nThe second point of high enthusiasm in research occurs when you have finally got your data. As you carry out your data analysis, return to the paper and revise all of the passages marked “X”. Instead of 38 female instrumentalists, you had 29: change 38 to 29 and remove the X. Instead of 60 recorded performances, you used 50: change 60 to 50 and remove the X. This second burst of enthusiasm will carry you through the final revision of your paper, which will then quickly find its way to submission for publication.\nIncreasing the likelihood of publishing negative results. Research often fails. You may have a nice theory, a good clear method, collect very clean data, do careful statistics, and discover that you have negative results: the collected data are not consistent with your hypothesis. You stuck your neck out, and it got chopped off. Negative results are often disappointing, but they are important nonetheless.\nNegative results are not just disappointing for the researcher: they are also often disappointing for the journal reviewers. If reviewers (like you) favor your hypothesis, they will be unhappy that the results turned out to be negative. As a result, they may become hyper-critical of your method. They may seize on one or another methodological detail and reject your article because it could have been done more carefully. Negative results should be published more frequently. If you have written the paper before doing the research, then you will have a stronger method, and reviewers will be more likely to accept your paper for publication, even though they are disappointed by the results. Remember, The best research invites failure. And failures are important ways in which we allow the world to tell us that our ideas are problematic.\nClarifying the distinction between a priori and post hoc tests. Recall that the rhetoric of science is the rhetoric of prophecy. That is, we present a hypothesis in the form of a prediction, and then collect data to test this prediction. Of course, when you already know what the data look like, it’s easy to formulate a hypothesis that appears to “predict” what the data will look like (hindsight is 20/20). When researchers don’t write the paper first, they may inadvertently come to believe that a post hoc hypothesis was an a priori hypothesis. Writing the paper first makes it clear what you were predicting before the data were collected. In short, writing the paper first helps keep researchers honest. Notice that this is contrary to our initial intuition that writing the paper first amounts to some form of “cheating.”\nAnticipating the statistical analysis. Many people collect data and then have trouble figuring out what sort of statistical analysis should be done. At this point, they commonly seek out a statistical consultant who might try to help them resolve the problem. It is possible to collect data that simply cannot be analyzed using common statistical procedures. Statistical consultants may wrestle for months, trying to help resolve a difficult situation. Statistical consultants are delighted when researchers approach them first and ask for advice. A statistician can help you determine the best way to collect your data so that the statistical analysis will be straightforward. Do not collect data without a clear idea of how you plan to analyze it. Writing the paper first will help you see the big picture and will ensure that you collect data that can be analyzed.\nRonald Fisher, one of the greatest statisticians in history, described the situation as follows:\n“To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of.”\nAvoiding repeated IRB applications. In carrying out research with human participants, it will be necessary to get Institutional Review Board (IRB) approval. The application process is commonly fairly tedious, and it can take several weeks or months to get approval. You will want to start the IRB application process as soon as possible in order to allow you to begin collecting your data.\nIn the IRB application, you will need to get approval for each aspect of your interaction with human participants, including recruitment, advertising, instructions to participants, data collection procedure, debriefing, storage of data, and so on. If you make any change to your procedure (for example, re-wording the instructions to participants), you will need to go back to the IRB in order for them to approve the modification. This is tedious and time-consuming. You don’t want to submit your IRB application until you are fully convinced that you have a fully refined method. By writing the paper first, you will have a detailed procedure which you have argued carefully: the written-out procedure can simply be imported to the IRB application, and you can feel confident that you won’t have to return with a later request to modify your application.\n\nFor most of us, we start many more things in life than we complete. (It is typically easier to start things than to complete things.) One of the best reasons for writing the paper first is that pushes most of the work to the beginning. Consequently, it becomes easier to finish projects than to start them. This dramatically improves the quality of one’s research, reduces the number of abandoned projects, accelerates your productivity, enhances your enthusiasm, and reduces the total amount of work. As I noted at the beginning, write the paper first is the single most important lesson I’ve learned as a researcher.\n\nFootnote\n[1] By way of example, my 2001 article “Tone and voice: A derivation of the rules of voice leading from perceptual principles” was under review for eight years. The manuscript was first submitted in 1993. There were several “revise and resubmits” involved before the paper was finally accepted. In fact, I carried out two additional experiments in order to convince a skeptical reviewer of my interpretation of one point. (The experiments were never published: they were simply done to convince a reviewer about something.) This experience is an outlier. It is rare that it takes so long for a paper to be accepted. But it can happen. See footnote #15 in the published article."
  },
  {
    "objectID": "emp_methods_workshop/grandmother2.html",
    "href": "emp_methods_workshop/grandmother2.html",
    "title": "Grandparent Research - Revisited",
    "section": "",
    "text": "Notice that hindsight bias further aggrevates the grandparent effect. It’s not simply the case that research often demonstrates the blindingly obvious. As we’ve seen, once people know the answer, they are convinced that they knew it all along. So the price paid for doing empirical research is three-fold:\nthe actual cost of doing the research (including time, effort, resources)\nthe indignity of confirming that 95% of human intuitions are correct, and\nthe ridicule from people—who having learned of your results—will wrongly believe that they knew it all along.\nIf you choose to engage in empirical research, be prepared that people will think you are naive and that you are involved in a colossal waste of effort.\nIn the arts especially, we place a premium on imagination and creativity. So among our arts colleagues, the majority of empirical research is likely to be viewed as trite, dull, and unimaginative –the very antithesis of what we expect in arts scholarship."
  },
  {
    "objectID": "emp_methods_workshop/freelisting.html#freelisting-and-pile-sorting",
    "href": "emp_methods_workshop/freelisting.html#freelisting-and-pile-sorting",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "Freelisting and Pile Sorting",
    "text": "Freelisting and Pile Sorting\n\n“The freelist is, I believe, the single most powerful and informative systematic data-collection technique available, because it is easy to use; you don’t need many informants; and it offers a powerful emic snapshot of the cultural domain that you are interested in studying.”\n\nVictor De Munck (2009, p.47)\n\nFreelisting and pile sorting are exploratory research methods intended to give a quick overview of some domain of human thought within a cultural context. They are usually used when a researcher first explores a cultural phenomenon. For example, a researcher might be interested in how people in some culture conceive of foods and food preparation, or a scholar might be interested in how people distinguish different forms of dance.\n\nFreelisting\nFreelisting typically involves brief interviews with a relatively small number of informants. The interview consists of posing a question of the form: “What’s sorts of X are there?” Or: “List all the Xs you know.” Sample freelist instructions might include the following:\nList all the colors you know. What are bad things a person might do? On what occasions do people sing and dance? List the kinds of activities people do while listening to music? List all of the musical instruments you know. What styles of music are there?\nFor each interview, the researcher records the listed items in the order in which the informant mentions them. For example, when asked to list all of the musical instruments you know, a person might begin: piano, guitar, violin, trumpet, etc.\nOnce the lists are acquired, an aggregate list is assembled in which the items are ordered from the most commonly mentioned to the least commonly mentioned. Usually, this aggregate omits those items that were mentioned only once. In general, people tend to begin their lists with the cognitively most salient items. So culturally important items are more likely to appear near the beginning of the list, and are most frequently mentioned. (For Western musicians, instruments like the contrabassoon and didjeridoo are likely to appear rarely and toward the end of the list.) A second version of the aggregate freelist will be ordered according to the mean rank-order. This can be calculated by numbering each item for each informant, and then averaging the rank numbers. For example, if one person lists the guitar first, and a second person lists the guitar second, then the average rank for this instrument is 1.5.\nA complication arises when there are items that might be regarded as synonyms. For example, when asked to identify occasions when music is played, responses might include “Easter,” “Good Friday,” “Easter vigil,” “Easter Sunday,” “Orthodox Easter,” and so on. The question is whether these should all be coded as a single item, or as two or more items. Similarly, one person might ennumerate soprano saxophone, alto saxophone, tenor saxophone, etc., whereas another person might simply identify “saxophone.” In order to minimize researcher bias, it may be useful to recruit one or more informants to resolve such situations.\n\n\nRecruitment\nAs with any sample, the best population estimates arise when there is high data independence. Interviewing the members of a single extended family, or a single social group of friends limits the representativeness of the results. Anthropologists recommend sampling individuals who are at least two social links apart. That is, the relationship between sampled informants should be no closer than “a friend of a friend.” (Werner & Schoepfle, 1989, 1992). In practical terms, the researcher can simply ask an informant (“Do you know [list of already interviewed informants]?”) before deciding whether to invite someone to participate in a freelisting interview.\nIf the domain of knowledge is commonly shared across a culture, representative results may be obtained with relatively few participants. For example, if you are interested in the “holidays” in some culture, you may find that three or four informants may be sufficient. On the other hand, if the domain of knowledge is more specialized (such as types of baked bread), then representative results may require a greater number of participants, and recruitment may require some eligibility criterion.\nResults are more trustworthy when there is high inter-informant agreement in the elicited terms. The proportion of shared terms across participants should be reported along with the frequency and rank-order lists. In general, an average inter-informant agreement of 2/3 or more may be considered good or adequate. That is, if the informants typically share two-thirds or more of the listed items, then one can have greater confidence that the lists are representative of a plausible cultural norm.\nFreelisting is relatively quick and easy to do. In addition, freelisting emphasizes culturally-grounded or “emic” conceptions and tends to reduce the influence of researcher bias. However, freelisting also has a number of disadvantages. De Munck (2009, p.66) notes that freelists elicit terms that are at “the top of people’s heads” or on the “tip of their tongues.” With more careful reflection, consultants may change their views, and wish to offer more nuanced distinctions. Once again, freelisting is typically an exploratory method.\n\n\nPile Sorting\nIt is common for freelisting to be followed by pile sorting. Pile sorting is a task in which respondents are asked to sort items into categories. Two types of pile-sorting tasks are usually distinguished: the constrained task in which the informant is told the number of categories, and the open-ended task in which the informant is free to determine the number of categories. Pile sorting can be done with all kinds of items. For example, pile sorting might be done with actual physical objects such as kitchen utensils, fruits and vegetables, bird feathers, or spices. Alternatively, printed cards may be used, such as photographs (e.g., different facial expressions), or written words. Photographs are often easier for a respondent to sort than the actual physical objects. In the case of sorting sounds, a computer display is helpful, where icons representing different sounds can be moved around the screen to form various groups.\nIt is worth noting that pile sorting is typically time consuming. It is important to recruit participants who have time, and potentially to provide a gift or offer compensation for their participation. Pile sorting can prove to be particularly onerous if there are rather few items, or an especially large number of items to be sorted. If the items used for pile sorting come from an earlier freelisting task, it may be useful to limit the number of items to those mentioned two or more times, or three or more times, etc.\nIn pile sorting tasks, the temptation is to tell the informant to sort items into “whatever categories you like.” This can leads to arbitrary criteria, such as sorting instruments according to their weight or color. In instructing the informant, it can be useful to explicitly reference culturally normative categories. For example, one might instruct an informant to sort the items “in a way that would make sense to a friend.”\nIn order to minimize order effects, the items are usually randomized for each participant. Participants are sometimes suspicious that there is some special trick or ulterior motive in the task, so when cards are used, it can be helpful to shuffle the cards in the presence of the participant.\nOnce the sorting task has been completed it is useful to have the participant describe each of the categories. Finally, encourage the participant to provide a single word or short phrase as a descriptive label for each category.\n\n\nHierarchies\nThe categories people use are often hierarchical organized. For example, a Westerner might divide foods into things you drink and things you eat. Things you eat might be subdivided into vegetable and animal sources. Animal sources might be further subdivided into meats and milk products, and so on. It is often useful to understand the sorts of mental taxonomies that might exist for certain phenomena.\nIn pile sorting tasks, one approach to better understanding mental taxonomies is to follow the pile sorting by a subjective stepwise clustering task. Suppose an informant has sorted the items into 12 categories. Having recorded the results, the researcher might then instruct the informant as follows. “You have 12 piles here. Suppose I told you that I want you to reduce the 12 to just 11. What two piles are the most similar? What two piles would you put together?” This reduction technique is then successively repeated until only two piles remain. Although this procedure is more time-consuming, it can better reveal latent hierarchies in the mental organization of the categories. In addition, it can reconcile the usual divergence between “lumpers” and “splitters” — people who tend to favor small numbers of groups and those who favor a larger number of groups. Note that it may be useful at each stage to ask the informant to re-label one or more categories.\nAn advantage of the stepwise clustering approach is that you may be able to better reconcile different pile-sorts by different informants. For example, one informant might produce 18 clusters, another informant might produce 13 clusters, and a third informant might produce 9 clusters. However, when asked to reduce the number of clusters, the researcher might find that at 8 clusters, all three informants are in very close agreement. The differences beyond 8 clusters might represent less culturally salient distinctions.\nIn the constrained pile sort task, the researcher tells the informant the number of categories for sorting. This approach can be used for hypothesis testing. For example, in previous work, the researcher may have observed a tendency for informants to regard the items as falling into the same five categories. As a test of this observation, the researcher may explicitly ask several informants to sort the items into five categories, without offering any additional instruction. This can be used to test whether the five categories are culturally robust.\nEspecially when working with people from an unfamiliar culture, it may be difficult for the researcher to infer why a particular item has been assigned to a particular category. In these cases, it can be insightful simply to ask a participant to explain why they have assigned a particular item to one group rather than another.\nPile sorting might be followed by more formal statistical methods. For example, pile sorting might be followed by multi-dimensional scaling (MDS) and/or formal hierarchical clustering (to be discussed later). Both MDS and hierarchical clustering invite the researcher to interpret the meaning of the resulting dimensions or the meaning of each cluster. In order to minimize cultural bias, these interpretations should not be done by the experimenter. It is more appropriate to recruit one or more informant to offer possible interpretations.\nDe Munck (1998) used freelisting, pile sorting, and multidimensional scaling to examine the concept of romantic love among Lithuanians. He found that Lithuanians tend to think of love along two dimensions — a negative-positive dimension and a real-fantasy dimension. He then tested this interpretation with a follow-up experiment in which Lithuanians were explicitly asked to rate a series of love-related terms along the two dimensions. He found that Lithuanians tend to view romantic love according to four prototypes: fantasy-good love, fantasy-bad love, real-good love, and real-bad love.\n\n\nReferences:\nVictor De Munck (1998). Romantic Love and Sexual Behavior: Perspectives from the Social Sciences. Westport, CT: Praeger.\nVictor De Munck (2009). Research Design and Methods for Studying Cultures. Plymouth, UK: AltaMira Press"
  },
  {
    "objectID": "emp_methods_workshop/null_hypothesis.html",
    "href": "emp_methods_workshop/null_hypothesis.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "The Null Hypothesis\n\nRecall that a hypothesis is is a testable conjecture. It is a claim or prediction that can, in principle be compared to existing or future observations. An example of a hypothesis might be:\nMost listeners prefer familiar music over unfamiliar music.\nIt is common in empirical research to refer to the research hypothesis using the upper-case letter H. Since empirical studies often test more than one hypothesis, it is common to number the hypotheses. Even if there is only a single hypothesis, authors commonly still refer to it as H1.\nH1. The use of rhyming words in musical lyrics increases the likelihood that listeners will understand the sung word.\nRecall that refutation is easier than confirmation. It is easier to show that the statement “All swans are white” is wrong (by observing a single non-white swan) than to show that the statement is right (by observing all possible swans). This asymmetry is captured in our slogan: Aim not to be right, but to be not not right.\nThere are an infinite number of incorrect hypotheses, and most of these hypotheses are not interesting. Usually, a researcher is hoping that their hypothesis is correct. However, it is impossible to show that a hypothesis is true. It is easier to show that a hypothesis is incorrect. As a result, empirical researchers restate their hypothesis in a reverse formulation. Here is an example:\nH0. The use of rhyming words in musical lyrics does not increase the likelihood that listeners will understand the sung word.\nThis “reverse formulation” of a hypothesis is referred to as the null hypothesis. The null hypothesis is abbreviated using the upper-case letter H followed by the subscript zero: H0. For convenience, it is often simplified to H0 without the subscript.\nIn empirical research, we rarely directly test the research hypothesis. Instead, we test the null hypothesis. Rather than testing the research hypothesis (and vainly hoping that it is “confirmed”), we test the null hypothesis (and hope that it is rejected). (Once again, we aim to be not not right, rather than aiming to be right.)\nIn analyzing our observations, we will use statistical methods to measure, not the probability that our research hypothesis is consistent with our data, but the probability that the null hypothesis is consistent with our data. If the probability is low, then we will conclude that the null hypothesis is inconsistent with our data. We will conclude that our data provides no reason to accept the null hypothesis. Having dismissed the null hypothesis, our research hypothesis will have survived the test. (“The best research invites failure.”)\nIn many books on statistics, the rejection of null hypothesis is considered grounds for “accepting the research hypothesis.” This is a old traditional formulation that should be avoided. A better formulation is the one given in the previous paragraph: having concluded that the data are not consistent with the null hypothesis, we will infer that the data are consistent with the research hypothesis. Said another way, we can claim that the research hypothesis has survived an empirical test.\nOnce again, in empirical research, we do not engage in “confirmation.” We never show things are “true.” Our data don’t “support” or “prove” or “demonstrate” anything. At best, we can claim that the data are consistent with the hypothesis.\nSo let’s review. Because refutation is easier than confirmation, we test hypotheses by attempting to refute them. The researcher reformulates the research hypothesis as the null hypothesis. Statistical methods are then used to calculate the probability that the data are consistent with the null hypothesis. If the probability is low, we conclude that the data are not consistent with the null hypothesis, and instead infer that the data are consistent with the reverse of the null hypothesis—namely, the research hypothesis. Traditionally, statisticians refer to this procedure as disproving the null hypothesis. However, good researchers avoid this language. Instead, we conclude simply that the data are not consistent with the null hypothesis—and conversely, that the data are consistent with the research hypothesis."
  },
  {
    "objectID": "emp_methods_workshop/chi_square_answers.html",
    "href": "emp_methods_workshop/chi_square_answers.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "GROUP TASK #16: Chi-Square Tests - ANSWERS\n\nIn a survey of orchestral works it was found that 70 percent include an oboe as part of the orchestration. A music scholar finds that composer A uses an oboe in only 10 of 22 orchestral works (just 45%). Is this a statistically significant difference?\n70 percent of 22 works is 15.4 works 30 percent of 22 works is 6.6 works   With one degree of freedom, at the 95% confidence level, the critical value of chi-square is 3.84. Since our calculated chi-squared value exceeds the critical value, we can declare that the results are not consistent with the null hypothesis. Instead, the results are consistent with the research hypothesis.\nOur statistical test allows us to state the following conclusion: The observations are consistent with the idea that, compared with other composers, composer A is less likely to include an oboe in the instrumentation of orchestral works.\nA music scholar finds that a sample of Italian baroque music shows 51 works in major keys and 9 works in minor keys. In a diverse sample of baroque music from many European countries (including Italy), 480 works were found to be in major keys and 210 works in minor keys. Compared with their European colleagues, did Italian baroque composers tend to favor works in major keys?\n210 (minor) + 480 (major) works = 690 works in total 210/690 = 30.4348 percent minor 480/690 = 69.5652 percent major For a sample of 60 Italian baroque works, we would expect: 30.4348 percent of 60 = 18.2609 in the minor mode 69.5652 percent of 60 = 41.7391 in the major mode   With one degree of freedom, at the 95% confidence level, the critical value of chi-square is 3.84. Since our calculated chi-square value exceeds the critical value, we can discard the null hypothesis, and accept that the results are consistent with the hypothesis. The results are statistical significant at p<0.05.\nOur statistical test allows us to state the following conclusion: The observations are consistent with the idea that, compared with other European baroque composers, Italian baroque composers tend to favor works in major keys.\nAn ethnomusicologist finds that just 130 of 220 Xhosa listeners say they prefer bumbuju rhythms to nobuju rhythms. In one Xhosa village however, 15 of 19 people say they prefer bumbuju. Can we claim that the members of this village differ significantly from other Xhosa in their rhythmic preferences?\n130/220 Xhosa prefer bumbuju = 59.0909 percent 220-130 = 90; 90/220 Xhosa prefer nobuju = 40.9091 percent In the village, we would expect: 59.0909 % of 19 = 11.2273 to prefer bumbuju 40.9091 % of 19 = 7.77273 to prefer nobuju   With one degree of freedom, at the 95% confidence level, the critical value of chi-square is 3.84. Since our calculated chi-square value fails to reach the critical value, we cannot discard the null hypothesis. The results are not consistent with the hypothesis."
  },
  {
    "objectID": "emp_methods_workshop/highly_significant.html",
    "href": "emp_methods_workshop/highly_significant.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "It is easy for researchers to misinterpret the meaning of p. Almost everyone gets confused about p, including some professional statisticians. Let’s approach it’s meaning through a series of steps. Ideally, we’d like p to be:\nThe probability that the null hypothesis is true. (The smaller the value, the better!)\nBut that’s not quite right. We calculate the value of p from the data we have collected, and data always contains some random variation due to the vagaries of sampling and measurement. So p is affected by both the truthfulness of the hypothesis and the random error which exists in the data. A perhaps more refined definition of p would be:\nThe probability that the null hypothesis is true—as estimated using the imperfect data we have.\nNow we can’t actually predict the probability of the truthfulness of either the research hypothesis or the null hypothesis. All we can predict is the likelihood of data. So instead of saying “What’s the probability of the null hypothesis being true, given the data we’ve collected?” We ask “What’s the probability of getting the data we got, assuming that the null hypothesis is true?” So we can turn around the above description in a way that statisticians would prefer: p is:\nThe probability that you would have made the observations you did—were the null hypothesis true.\nWe’re almost there.\nLet’s use a computer to pick a random number between zero and infinity. What do you suppose is the probability that you’d be able to successfully guess that number? It would be vanishingly small—basically zero. Now what is the probability that you’d collect the specific numerical data that you happened to collect in your study? Once again, that probability is going to be a very small number. In fact, any reasonably-sized set of numerical data is going to have a vanishingly small probability of occurrence. So we’re not actually interested in the probability of the specific data you collected. Instead, we’re interested in the probability that you would have collected data as extreme as the data you collected.\nWith this in mind, we can now provide a more accurate characterization of p:\np is the probability that you would make observations at least as extreme as the specific observations you made—were the null hypothesis true.\nWith this background we can now recognize some common incorrect interpretations of p: p is not the probability that the null hypothesis is true. Similarly, if you subtract p from 1, you don’t end up with the probability of the research hypothesis being true. Nor is p the probability of observing the data you observed, or the probability that the data occurred by chance. Finally, p is not the probability of making a Type 1 Error. Once again:\np is the probability that you would make observations at least as extreme as the specific observations you made—were the null hypothesis true.\n\nStatistical Significance\nLet’s return to consider the relationship between p and statistical significance. In the “Funeral March” problem we calculated a chi-square value of 113.05. With one degree of freedom, at the 99% confidence level, the critical value of chi-square is 6.635. In this case our calculated chi-square value is very much higher than needed in order to achieve statistical significance. In fact, this chi-square value is so large, that it would still have been statistically significant even if we had chosen a 99.999% confidence level.\nFirst, it bears reminding ourselves that in empirical research we never prove anything. At best, all we can do is show that the observations are consistent with a particular hypothesis. If we have statistically significant results, that doesn’t mean that the hypothesis is true. Similarly, if we fail to achieve statistically significant results, that doesn’t mean that the hypothesis is false.\nBecause the situation is necessarily “wishy-washy,” it is possible for a researcher to believe that his/her hypothesis is true—no matter what observations are made. Intellectually, we are disabled by a pervasive disposition to believe that we are right. It is for this reason that empirical researchers have established formal procedures for testing hypotheses.\nRecall the purpose of a confidence level: When we establish a confidence level, we are “drawing a line in the sand.” The purpose of this line is to make it clear when the world is telling us that our hypothesis is problematic. By drawing this line, we are sticking out necks out to invite failure. If our data lies on one side of the line, we conclude that the data are consistent with the research hypothesis; if our data lies on the other side of the line, we conclude that the data could well have arisen by chance.\nNotice that this is a binary proposition: our statistical test will give us a yes or no answer.\n\n\n“Highly” significant\nIt is a frequent occurrence that a statistical test will lead to a very small p value. That is, if the null hypothesis were true, there is a very small probability of seeing the data we collected. The temptation is for researchers to report this as “highly significant.” Good researchers never say this. Saying that a result is highly significant is like saying someone is highly pregnant: a person is either pregnant or not. The same applies to the concept of statistical significance. As researchers, we might be reassured that our data “is quite far inside the line we have drawn in the sand.” But the purpose of a p value is not to lend greater or lesser credence. It simply says “significant” or “not significant.”\n\n\n“Approaching” Significance\nSometimes a p value will be close to the significance level but not be significant. For example, with a confidence level of 95%, the signficance level will be p=0.05. What if your p value is p=0.06 or even p=0.051? It is common in empirical research to report such values as “marginally significant.” Once again, this is something of a subterfuge. The whole point of the statistical exercise is to produce a “thumbs-up” or “thumbs-down” judgment. In our eagerness to belive we are right, it is hard not to grasp onto a “nearly” significant result.\nActually, there is some merit to the phrase “marginally significant.” It is frequently (though by no means “mostly”) the case, that with further data collection, a “marginally significant” result will reach statistical significance. This happens frequently enough that there is merit to drawing attention to readers that a given statistical test is very close to being statistically significant. This is best reported as suggesting that further research may be warranted, or that the idea may continue to be of interest, even though the existing study failed to achieve significance.\nA bad habit to avoid is to describe a result as “approaching significance.” The implication is that if we collect more data, then the p value will necessarily get smaller and so will reach significance. In many cases, collecting more data will indeed result in a smaller value for p. But this is by no means guaranteed. In many studies, collecting more data has caused p to become bigger. So we should not describe p as “approaching” significance. Also, the word approaching implies a kind of agency, effort, or goal-directed behavior. Numbers don’t approach (or run away); they just are.\n\n\nSome Advice\nAvoid phrases like “highly significant,” and “approaching significance.” Instead, use the phrase “marginally significant” — even then, use this phrase sparingly. Point out that the data are not consistent with the hypothesis at the stated confidence level. Don’t imply that more data would surely have produced significant results. However, it is appropropriate to point out that further data collection might be warranted."
  },
  {
    "objectID": "emp_methods_workshop/chi_test.html",
    "href": "emp_methods_workshop/chi_test.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "The Chi-Square Test\n\n[]{#Brahms’s Hemiolas}\n\nBrahms’s Hemiolas\nA common statistical tests is the chi-square test. This test takes its name from the Greek letter X (pronounced “Kye” — rhymes with “pie”), which is used to compare ratios of counts. It facilitates the determination of whether the number of occurrences of some feature in one data sample is significantly greater than or less than what would otherwise be expected.\nBy way of example, let us test the hypothesis that Brahms use lots of hemiolas. Notice, first of all that “lots of hemiolas” is ambiguous. If we count three hemiolas in a short piano work, is that a small or big number? In order to answer that question we need a comparison or sense of what is normal or expected. An appropriate approach is to compare the number of hemiolas in Brahms with the number of hemiolas in the music of his contemporaries.\nSo, to set up the test we need first to count (a) the number of hemiolas in a sample of Brahms’s music, and (b) the number of hemiolas in the comparison population — the number of hemiolas in a similar sized sample of music by other composers who were active at the same time. Let us suppose that the raw data looks like this:\nCOMPOSER(S)\nMEASURES SEARCHED\nHEMIOLAS FOUND\nVarious\n5000\n23\nBrahms\n500\n9\nProportionally, hemiolas occurred in 1.8% of the data for Brahms (9 out of 500) but just 0.7% in the music by other composers (23 out of 5000). Is this difference within the range of what one might expect by chance? That is, is this difference statistically significant? A chi-square test will tell us.\n[]{#Calculating Chi-Square}\n\n\nCalculating Chi-Square\nThe test entails three parts. First, we draw our line in the sand. That is, we establish in advance of our test our confidence level. Let us use a conventional confidence level of 95 percent. This corresponds to a significance level of .05. Second, we calculate the value of chi-squared. This is done by subtracting the number of expected occurrences from the number of observed occurrences, squaring the result, and dividing it by the number of expected occurrences. Where O is the observed number of occurrences and E is the expected number, the formula is:\n\nThe summation symbol means that we must perform the calculation for each expected element. There are two elements to the expectation: how many measures we expect to contain a hemiola, and how many measures we would expect not to contain a hemiola. If we would normally expect 23 hemiolas in 5000 measures of music of the period, then we would expect about 2.3 hemiolas in the 500 measures of Brahms’s music, i.e., that E1 = 2.3. At the same time, in our sample of various composers we observed that 4977 measures of music (out of 5000) did not contain a hemiola. If Brahms was like other composers, we would expect 497.7 of the 500 measures not to contain a hemiola. Accordingly, E2 = 497.7. The actual number of hemiolas encountered was in his music was 9, and the actual number of non-hemiolas measures was 491. So the numeric substitutions would be as follows:\n\nOur value of chi-squared is 19.6.\n[]{#Determining Statistical Significance}\n\n\nDetermining Statistical Significance\nNow we need to determine whether our chi-squared value is statistically significant. Recall that our confidence level is 95 percent—and that this corresponds to a significance level of .05. We need to determine whether the chi-squared value is sufficient to achieve statistical significance at the .05 significance level. In order to answer this question, we will make use of a table of critical values for chi-square. Such tables have been pre-computed by statisticians, and are easily found in the appendices of any statistics book or on the web.\nIn order to determine the critical value for chi-square, we need to know the significance level (or confidence level) and the so-called degrees of freedom (abbreviated df). For this task, there is only a single degree of freedom. From the table below, we can see that for the 95% confidence level (=.05 significance level), with one degree of freedom, the critical value of chi-squared is 3.841. Our actual calculated value of chi-squared is 19.6. This means that our result is statistically significant. That is, the observations are not consistent with the null hypothesis; instead, they are consistent with our research hypothesis.\n\nThe value of X² is sensitive to the number of observations. The greater the number of observations, the greater the likelihood that the result will be considered significant. If a coin were flipped 6 times and came up heads 4 of them, this incidence would be considered to lie within the realm of chance. But if the coin came up heads on 400 of 600 tosses (preserving the same proportion), the value of p would be less than 0.001.\n\n\nTips\nThe chi-square test is about counts, not percentages. The values used in the calculation must be actual counts and expected counts. The percentages are useful, only as a way to identify the expected counts.\nThe test is called chi square (no ‘d’), but the calculated value is referred to as chi squared (with the ‘d’)."
  },
  {
    "objectID": "emp_methods_workshop/grounded2.html",
    "href": "emp_methods_workshop/grounded2.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Grounded Theory Revisited\n\nEarlier, we described the Grounded Theory method. Recall that Grounded Theory emphasizes observation, and avoids testing a priori hypotheses. The principal aim is to develop theory “from the ground up.”\nGrounded Theory has existed for more than half a century. GT has a following in the field of sociology, especially among researchers influenced by so-called “social constructivism.” Some postmodern scholars also favor Grounded Theory. However, GT has not found much support among mainstream empirical researchers? This raises the question - Why?\nAs we’ve learned, a critical distinction in empirical research is the difference between a priori and post-hoc testing. Recall our group task related to The American Soldier. We had the experience of devising theories for several different “findings” from this study. We then learned that the purported “findings” were exactly backwards. We went back to the drawing board, and formulated a new set of theories that could account for the opposite findings. We made the sobering discovering that it wasn’t especially difficult to devise several theories—theories that predict X and theories that predict the opposite-of-X.\nIf we formulate our theory after observing the data, the data will inevitably be consistent with our theory. In short, the theory will necessarily appear to be correct. For the researcher who only engages in post hoc theorizing, all of his/her theories will look plausible. It is only the researcher who posits a theory before looking at the observational data who can have the experience of seeing that the data are inconsistent with the theory. Said another way, a scholar who only engages in post-hoc theorizing, will never have the experience of the data contradicting their theory.\nAs we’ve noted, the essence of good scholarship is skepticism. Moreover, we’ve also learned that the most important form of skepticism is to become skeptical of ourselves. Good scholars are shaped by the humility that ensues from frequently being shown to be wrong. By contrast, the scholar who never experiences failure is likely to increase in confidence, and ultimately to develop an unwarranted high self-opinion: “All of my theories fit the data.”\nAmong mainstream empirical researchers, Grounded Theory is best viewed as an exploratory method. Scientists don’t doubt the importance of collecting data in the absence of a hypothesis. Nor do they doubt the value of attending closely to what the observations might suggest. That is, not all research should be a priori. The problems arise when researchers only engage in exploratory research and post hoc theorizing. Recall our second slogan: The best research invites failure. The key is to link exploratory research with hypothesis testing. Once a theory has been formulated based on observation, the conscientious researcher should then turn around and posit the theory in an a priori form, and collect independent data intended to test hypotheses arising from the theory. In short, explore then test.\nNotice that both Grounded Theorists and those who oppose Grounded Theory are aiming for humility. For Grounded Theorists, the humility lies in not approach a study with a preconceived theory. (“Let the data itself speak by suggesting a theory.”) For those who oppose Grounded Theory, the humility lies in inviting failure. (“Give the data an opportunity to tell you you’re wrong.”) Notice that a two-part explore-then-test approach permits the best of both worlds.\n\n\nReferences\nKathy Charmaz (2000). Grounded theory: Objectivist and constructivist methods. In Denzin, N.K. and Lincoln, Y.S. (eds.), Handbook of Qualitative Research, 2nd edition. Thousand Oaks, CA: Sage. pp. 509–535.\nKathy Charmaz (2006). Constructing Grounded Theory. London: Sage.\nBarney Glaser & Anselm Strauss (1967). Discovery of Grounded Theory. Strategies for Qualitative Research. Sociology Press."
  },
  {
    "objectID": "emp_methods_workshop/final_advice.html",
    "href": "emp_methods_workshop/final_advice.html",
    "title": "Empirical and Critical Methods in Musicology",
    "section": "",
    "text": "Final Advice\n\nYou’ve learned about all sorts of problems that can plague research. At this point, you may be so sensitive to the innumerable ways in which research can go astray that you might feel intimidated from doing any research. How is it possible to do good research when there are so many ways in which things can go awry?\nUltimately, we are saved by effect size. Consider, once again, the example we discussed earlier about testing the toxicity of cyanide. You give cyanide to a volunteer, and they drop dead. In light of this single observation, it doesn’t seem at all farfetched to conclude that cyanide is highly toxic. It doesn’t matter that the sample size is miniscule. It doesn’t matter that the sample is unrepresentative. It doesn’t matter that the treatment isn’t double blind. It doesn’t matter what demand characteristics are present. It doesn’t matter that the participant exhibited acquiescence bias, cooperation bias, or contrarian bias. It doesn’t matter that you can’t perform a post-experiment debriefing. Regression-to-the-mean doesn’t matter. It doesn’t matter that there is a ceiling effect. We might even be justified in concluding that a control condition isn’t needed!\nIn this case, the effect size is so great that it simply drowns-out any plausible confounds. The probability of any person dropping dead by chance is very small, so it’s hard not to want to attribute our volunteer’s death to the toxicity of cyanide.\nFortunately, it turns out that many of things we are interested in have large effect sizes. Of course, not all phenomena are as knock-you-over-the-head as the toxicity of cyanide. But most people find loud music to be more energizing than quiet music; few would claim that a minor second is more consonant than a major third; and dancing to a random beat sequence isn’t as easy as dancing to a regular beat. It is only as effect sizes grow smaller that we need to pay increasing attention to the various possible research confounds. If the effect size is small, then the chaos due to confounds becomes proportionally larger. Notice that the most important things to know in life are precisely those phenomena that have the biggest effect sizes. When some factor has a tiny effect, then by definition, it isn’t as important.\nIt is certainly important to know about the different ways in which research can go wrong. It is also important to minimize the potential confounds to the extent we can. Often it is simply impractical to carry out research the way we’d like to. But don’t let fear prevent you from going ahead and doing research. There’s no such thing as a perfect study. Once again, we’re not in the business of proving things: we’re in the business of telling plausible stories.\nThe history of research is full of false starts, missteps, and screw-ups (sometimes, some truly momentous screw-ups). However, the history of research is also an inspiring tale of many extraordinary successes. These successes occurred despite the fact that researchers were largely muddling through.\nThe study of research methods is ultimately a failure if it causes aspiring scholars to be intimidated or paralyzed. All we can do is do our best with the resources available to us. So go ahead and do it."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Empirical and Critical Methods",
    "section": "",
    "text": "Welcome to Empirical Methods in Music Research! This course website will serve as a central repository for examples, slides, and code. Readings will still be available on Canvas, and links to specific pages on this site can be found there, as well."
  },
  {
    "objectID": "slides/week1.html#hello",
    "href": "slides/week1.html#hello",
    "title": "Welcome!",
    "section": "Hello!",
    "text": "Hello!\nToday we will be covering…\n\nIntroductions\nThe plan for the quarter\nWhat is empirical musicology?\nWhat is science?\n\nHow does knowledge grow?"
  },
  {
    "objectID": "slides/week1.html#empiricism",
    "href": "slides/week1.html#empiricism",
    "title": "Welcome!",
    "section": "Empiricism",
    "text": "Empiricism\n\nEmpiricism isn’t a method, it’s just a smart thing to do.\n\n–Gjerdingen (2011, SMT)"
  },
  {
    "objectID": "slides/week1.html#what-is-empirical-musicology",
    "href": "slides/week1.html#what-is-empirical-musicology",
    "title": "Welcome!",
    "section": "What is empirical musicology?",
    "text": "What is empirical musicology?\n\nIn short, there is no useful distinction to be drawn between empirical and non-empirical musicology, because there can be no such thing as a truly non-empirical musicology; what is at issue is the extent to which musicological discourse is grounded on empirical observation, and conversely the extent to which observation is regulated by discourse.\n\n–Cook and Clarke (2004, p.3)"
  },
  {
    "objectID": "slides/week1.html#data-rich-vs.-data-poor",
    "href": "slides/week1.html#data-rich-vs.-data-poor",
    "title": "Welcome!",
    "section": "Data-Rich vs. Data-Poor",
    "text": "Data-Rich vs. Data-Poor\n\nMany aspects of musicology are “data-poor”\nWe have no real surviving original manuscripts in Josquin’s own hand. Only a few contemporaneous ones."
  },
  {
    "objectID": "slides/week1.html#data-rich-responsibilities",
    "href": "slides/week1.html#data-rich-responsibilities",
    "title": "Welcome!",
    "section": "Data-Rich Responsibilities",
    "text": "Data-Rich Responsibilities\n\n“…there would be grounds for legitimate criticism if musicologists working in data-rich fields did not take full advantage of the methds available under such conditions, instead restricting themselves to traditional”humanities” approaches developed for data-poor fields–and oe of the messages of this book is that musicology i or could be, in many instances, a significantly “data richer” field than we generally give it credit for.”\n\n–Cook and Clarke, (p.4)"
  },
  {
    "objectID": "slides/week1.html#one-more-quote",
    "href": "slides/week1.html#one-more-quote",
    "title": "Welcome!",
    "section": "One more quote…",
    "text": "One more quote…\n\nEmpirical musicology, to summarize, can be thought of as musicology that embodies a principled awareness of both the potential to engage with large bodies of relevant data, and the appropriate methods of achieving this […]\n– Cook and Clarke (2004, p.5)"
  },
  {
    "objectID": "slides/week1.html#types-of-knowledge",
    "href": "slides/week1.html#types-of-knowledge",
    "title": "Welcome!",
    "section": "Types of Knowledge",
    "text": "Types of Knowledge\n\nIntuition\nDeductive Knowledge\nEmpirical Knowledge\n\nSee Types of Knowledge"
  },
  {
    "objectID": "slides/week1.html#intuition",
    "href": "slides/week1.html#intuition",
    "title": "Welcome!",
    "section": "Intuition",
    "text": "Intuition\n\nA man about to cross the street jumps back when he hears the sound of a car horn. The reaction is so fast, he is not even aware of having thought about anything."
  },
  {
    "objectID": "slides/week1.html#deductive-knowledge",
    "href": "slides/week1.html#deductive-knowledge",
    "title": "Welcome!",
    "section": "Deductive knowledge",
    "text": "Deductive knowledge\n\nIf all people are mortal, and Socrates is a person, then Socrates is mortal.\nSeeing a guitarist move her finger one fret toward the tone hole, we conclude that the next pitch will be one semitone higher than the previous pitch."
  },
  {
    "objectID": "slides/week1.html#empirical-knowledge-knowledge-gained-through-observation",
    "href": "slides/week1.html#empirical-knowledge-knowledge-gained-through-observation",
    "title": "Welcome!",
    "section": "Empirical knowledge (knowledge gained through observation)",
    "text": "Empirical knowledge (knowledge gained through observation)\n\nI’ve observed that some musicians can always identify the names of pitches (“perfect pitch”) and others can’t."
  },
  {
    "objectID": "slides/week1.html#induction",
    "href": "slides/week1.html#induction",
    "title": "Welcome!",
    "section": "Induction",
    "text": "Induction\n\n“The problem of induction” (Hume)\n“Reasoning by drawing a conclusion not guaranteed by the premises” (Dienes, p. 164)"
  },
  {
    "objectID": "slides/week1.html#induction-1",
    "href": "slides/week1.html#induction-1",
    "title": "Welcome!",
    "section": "Induction",
    "text": "Induction\n\nSam the swan is white;\nGeorgina the swan is white;\nFred the swan is white;\n…\nEmma the swan is white.\nConclusion: All swans are white."
  },
  {
    "objectID": "slides/week1.html#proof",
    "href": "slides/week1.html#proof",
    "title": "Welcome!",
    "section": "Proof",
    "text": "Proof\n\nMotivated by truth, with no hope of proof.\nWe aim not to be right, but to be not not right."
  },
  {
    "objectID": "slides/week1.html#popper",
    "href": "slides/week1.html#popper",
    "title": "Welcome!",
    "section": "Popper",
    "text": "Popper\n\nThe goal of science is to test, criticize (and debunk) already existing theories.\nIt’s not a bad thing if your theory is wrong."
  },
  {
    "objectID": "slides/week1.html#demarcation",
    "href": "slides/week1.html#demarcation",
    "title": "Welcome!",
    "section": "Demarcation",
    "text": "Demarcation\n\nWhat makes something a science?\nFor Popper, something was a science if it contained falsifiable hypotheses."
  },
  {
    "objectID": "slides/week1.html#what-makes-something-a-hypothesis",
    "href": "slides/week1.html#what-makes-something-a-hypothesis",
    "title": "Welcome!",
    "section": "What makes something a hypothesis?",
    "text": "What makes something a hypothesis?\n\nConjectures\nHypotheses\nTheories"
  },
  {
    "objectID": "slides/week1.html#why-do-you-need-a-hypothesis",
    "href": "slides/week1.html#why-do-you-need-a-hypothesis",
    "title": "Welcome!",
    "section": "Why do you need a hypothesis?",
    "text": "Why do you need a hypothesis?\n\nHumans are very good at constructing narratives.\nHindsight bias is very real, and we should do everything we can to mitigate it."
  },
  {
    "objectID": "slides/week1.html#reductionism",
    "href": "slides/week1.html#reductionism",
    "title": "Welcome!",
    "section": "Reductionism",
    "text": "Reductionism\n\nMethodological reductionism is not philosophoical reductionism.\nPhilosophical Reductionism: A Belief about the World\nMethodological Reductionism: A Strategy for Discovery\nIn research, reductionism is a method, not a belief\nWe simplify problems, not because we believe problems to be simple, but because we believe problems to be complex. ## Operationalizing Terms\n\nListening to Mozart makes people smarter.\n              | |\n|:————–|:——————————-s————————————–| | Concept | Possible operationalization | | “smarter” | will score higher on the Stanford-Binet IQ test | | “Mozart” | all sound recordings that claim the music is composed by W.A. Mozart | | “listening” | conscious while being exposed to 20 minutes of sound recordings | | “people” | 2nd-year undergraduate music students from the Northwestern University |"
  },
  {
    "objectID": "slides/week1.html#types-of-studies",
    "href": "slides/week1.html#types-of-studies",
    "title": "Welcome!",
    "section": "Types of Studies",
    "text": "Types of Studies\nSee this page on types of studies"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "course-schedule.html",
    "href": "course-schedule.html",
    "title": "Course Schedule",
    "section": "",
    "text": "Week\nDate (Date)\nTopic\nImportant Dates\n\n\n\n\n1\nT (1/3)\nNo class; Monday schedule\n\n\n\n1\nTh (1/5)\nIntroductions; Types of knowledge and the rhetoric of science\n\n\n\n2\nT (1/10)\nTypes of empirical studies\n\n\n\n2\nTh (1/12)\nControls, demand characteristics and path diagrams.\nHW1 Due\n\n\n3\nT (1/17)\nSampling\n\n\n\n3\nTh (1/19)\nMeasurement scales and transforming data\nHW2 Due\n\n\n4\nT (1/24)\nDescriptive Statistics; measures of central tendency\n\n\n\n4\nTh (1/26)\nWhat does it mean to be significant?\nHW3 Due\n\n\n5\nT (1/31)\nCorrelational studies in music research\nMidterms Due\n\n\n5\nTh (2/2)\nCorrelation and regression in music research\nHW4 Due\n\n\n6\nT (2/7)\nMidterm Meetings\n\n\n\n6\nTh (2/9)\nClustering, PCAs, and MDS. (and the crazy world of attribution and authorship studies!)\nHW5 Due\n\n\n7\nT (2/14)\nModeling and simulations\n\n\n\n7\nTh (2/16)\nModeling and simulations (continued).\nHW6 Due\n\n\n8\nT (2/21)\nBayesian vs. Frequentist theories\nFirst Draft of Final Project Due\n\n\n8\nTh (2/23)\nBayesian vs. Frequentist theories\nHW7 Due\n\n\n9\nT (2/28)\nCritical Data Theory, Data Ethics\nPeer Reviews Due\n\n\n9\nTh (3/2)\nCritical Data Theory, Data Ethics\nHW8 Due\n\n\n10\nT (3/7)\nIndividual Meetings\n\n\n\n10\nTh (3/9)\nFinal Presentations\nFinal Presentations (Final Papers due on Friday, 3/10)"
  },
  {
    "objectID": "course-syllabus.html",
    "href": "course-syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Instructor: Dr. Daniel Shanahan\nContact: daniel.shanahan@northwestern.edu\nMeeting times:"
  },
  {
    "objectID": "course-syllabus.html#overview",
    "href": "course-syllabus.html#overview",
    "title": "Syllabus",
    "section": "Overview",
    "text": "Overview\nEmpirical methods in music research have been used to study aspects of musical style, performance practice, authorship, musical behaviors and auditory perception, among other topics. In this class, we will explore what it means to study music with the assistance of empirical methods, including philosophical debates that surround such approaches. Additionally, it will provide a foundation and overview of empirical methods used in musicological and music-theoretic research, equipping students with the tools needed to critically engage with the literature in the field, while designing their own empirically-based music research studies and engaging with the analysis of the data of musical behavior."
  },
  {
    "objectID": "course-syllabus.html#course-info",
    "href": "course-syllabus.html#course-info",
    "title": "Syllabus",
    "section": "Course info",
    "text": "Course info\n\n\n\n\n\n\n\n\n\n\nDay\nTime\nLocation\n\n\n\n\nMeeting Times:\nTue & Thu\n12:30pm - 1:50 pm\nRCMA 1-164\n\n\nOffice Hours\nMon & Weds\n10am–11am (and by appointment)\nRCMA 4-181"
  },
  {
    "objectID": "course-syllabus.html#learning-objectives",
    "href": "course-syllabus.html#learning-objectives",
    "title": "Syllabus",
    "section": "Learning objectives",
    "text": "Learning objectives\nBy the end of the quarter, you will be able to…\n\ndiscuss the basic fundamental assumptions of empirical musical research, including sampling methods, hypothesis testing, descriptive statistics, and introductory inferential statistics\ndiscuss some of the basic experimental paradigms of experimental music research\ndiscuss the basic differences between frequentist and Bayesian statistics\nanalyze data derived from corpus studies, surveys, and listening experiments\nengage critically with published empirical music research\nhave a working introductory knowledge of the R programming language"
  },
  {
    "objectID": "course-syllabus.html#course-policies",
    "href": "course-syllabus.html#course-policies",
    "title": "Syllabus",
    "section": "Course Policies",
    "text": "Course Policies\n\nAcademic Integrity\nStudents in this course are required to comply with the policies found in the booklet, “Academic Integrity at Northwestern University: A Basic Guide”. All papers submitted for credit in this course must be submitted electronically unless otherwise instructed by the professor. Your written work may be tested for plagiarized content. For details regarding academic integrity at Northwestern or to download the guide, visit this page.\n\n\nAccesibility\nNorthwestern University is committed to providing the most accessible learning environment as possible for students with disabilities. Should you anticipate or experience disability-related barriers in the academic setting, please contact AccessibleNU to move forward with the university’s established accommodation process (e: accessiblenu@northwestern.edu; p: 847-467-5530). If you already have established accommodations with AccessibleNU, please let me know as soon as possible, preferably within the first two weeks of the term, so we can work together to implement your disability accommodations. Disability information, including academic accommodations, is confidential under the Family Educational Rights and Privacy Act.\n\n\nCOVID-19 Classroom Expectations\nStudents, faculty and staff must comply with University expectations regarding appropriate classroom behavior, including those outlined below and in the COVID-19 Expectations for Students. With respect to classroom procedures, this includes:\nPolicies regarding masking, social distancing and other public health measures evolve as the situation changes. Students are responsible for understanding and complying with current University, state and city requirements. In some classes, masking and/or social distancing may be required as a result of an Americans with Disabilities Act (ADA) accommodation for the instructor or a student in the class even when not generally required on campus. In such cases, the instructor will notify the class.\nIf a student fails to comply with the COVID-19 Expectations for Students or other University expectations related to COVID-19, the instructor may ask the student to leave the class. The instructor is asked to report the incident to the Office of Community Standards for additional follow-up.\n\nIf you’re feeling sick…\nMaintaining the health of the community remains our priority. If you are experiencing any symptoms of COVID do not attend class. Follow the steps outlined on the NU sites for testing, isolation and reporting a positive case. Next, contact me as soon as possible to arrange to complete coursework.\nStudents who experience other personal emergencies should contact me as soon as possible to arrange to complete coursework.\nShould public health recommendations prevent in-person class from being held on a given day, I or the university will notify students.\n\n\n\nDiversity, Equity, and Inclusion\nThis course strives to be an inclusive learning community, respecting those of differing backgrounds and beliefs. As a community, we aim to be respectful to all students in this class, regardless of race, ethnicity, socio-economic status, religion, gender identity or sexual orientation."
  },
  {
    "objectID": "course-syllabus.html#textbooks",
    "href": "course-syllabus.html#textbooks",
    "title": "Syllabus",
    "section": "Textbooks",
    "text": "Textbooks\nThere is no textbook for this course, and most of the materials will be available on Canvas or the course website (or both).\nHaving said that, you should sign up for a free account for Posit Cloud (formerly RStudio Cloud), where many of the class notebooks will be held.I would also recommend downloading R and RStudio onto your personal machine, if possible.\nAlthough not required, I would highly recommend having a look at:\n\nR for Data Science by Garret Grolemund and Hadley Wickham"
  },
  {
    "objectID": "course-syllabus.html#support-for-wellness-and-mental-health",
    "href": "course-syllabus.html#support-for-wellness-and-mental-health",
    "title": "Syllabus",
    "section": "Support for Wellness and Mental Health",
    "text": "Support for Wellness and Mental Health\nNorthwestern University is committed to supporting the wellness of our students. Student Affairs has multiple resources to support student wellness and mental health. If you are feeling distressed or overwhelmed, please reach out for help. Students can access confidential resources through the Counseling and Psychological Services (CAPS), Religious and Spiritual Life (RSL) and the Center for Awareness, Response and Education (CARE). Additional information on all of the resources mentioned above can be found here:\nhttps://www.northwestern.edu/counseling/\nhttps://www.northwestern.edu/religious-life/\nhttps://www.northwestern.edu/care/\n\nHomework\nThere will be regular assignments in which you will be asked to respond to do one of the following:\n\nCritically engage with a study design\nAnalyze data (sometimes real, sometimes artificial)\nRespond to prompts about methodological dilemmas.\n\n\n\nMidterm Project\nThe midterm project will consist of a literature review and critical evaluation of a specific topic in the field of empirical musicology. You will discuss any methodological issues that might be present in the research questions at hand.\nAn example: We tend to exclude outliers when looking at data, but Debussy’s performances of his own pieces are statistical outliers when compared to other performances of his pieces. To what extent can we view performance data through such a lens, and how would you go about addressing the question?\n\n\nFinal Project\nThe final project will be focused on a research quetsion of your choosing, and will be broken up into several a peer-reviewed first draft, a presentation, and a final paper."
  },
  {
    "objectID": "course-syllabus.html#grading",
    "href": "course-syllabus.html#grading",
    "title": "Syllabus",
    "section": "Grading",
    "text": "Grading\nThe final course grade will be calculated as follows:\n\n\n\nCategory\nPercentage\n\n\n\n\nWeekly Assignments (x8)\n50%\n\n\nMidterm Topic Review Paper\n15%\n\n\nFinal Project (First Draft)\n10%\n\n\nFinal Project (Peer Review)\n5%\n\n\nPresentation\n5%\n\n\nFinal Project (Paper)\n15%\n\n\n\nThe final letter grade will be determined based on the following thresholds:\n\n\n\nLetter Grade\nFinal Course Grade\n\n\n\n\nA\n>= 93\n\n\nA-\n90 - 92.99\n\n\nB+\n87 - 89.99\n\n\nB\n83 - 86.99\n\n\nB-\n80 - 82.99\n\n\nC+\n77 - 79.99\n\n\nC\n73 - 76.99\n\n\nC-\n70 - 72.99\n\n\nD+\n67 - 69.99\n\n\nD\n63 - 66.99\n\n\nD-\n60 - 62.99\n\n\nF\n< 60"
  }
]