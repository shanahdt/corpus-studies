[
  {
    "objectID": "class_notes/week_4.html",
    "href": "class_notes/week_4.html",
    "title": "Week 4: Conceptual Debates: Key-Finding",
    "section": "",
    "text": "talk about key-finding\nplay with the different weightings\nwhat would it look like to devise your own key-finding algorithm?\n\n\n\n\n\n\nWhen we hear this ringtone, it sounds as though it’s in C, but why?\n\n\n\nNokia\n\n\n\nIt doesn’t begin with C, it begins with G.\nC isn’t the most common note–in fact, it only occurs once before the final bar, and it’s on the “and” of 2 in the third measure (a pretty weak position metrically).\nIs a key just whatever key the piece ends in? If we ended this on A, would it sound like it’s in A minor? It would be the same key signature, and we’d actually have a nice cadential ascent to the final A from the G in the third measure.\n\nSo what gives? Why do we hear this as being in C?\nPerhaps a follow-up question might simply be: what makes us hear something as being in a key?\n\n\n\n\n\nThis approach used what we might call an exclusionary approach, eliminating different key possibilities as pitch classes were introduced over the course of a musical passage.\nFor example, with the Nokia theme, the opening G would fit into seven major keys (G, C, D, F, B-flats, A-flat, E-flat;); six of those keys would include the opening two notes; and three of those six would still be possible when presented with the first three notes. By the end of the first measure, however, the only major key that would encompass all four melody notes would be C major. If more than one key was still available however, the algorithm would place more weight on the pitches present at the start of the piece. This worked quite well on pieces that were overtly tonal, but it was less effective for pieces that contained non-diatonic pitches (which is most pieces!)\n\n\n\nLonguet-Higgins and Steedman's 1971 Key-Finding Algorithm\n\n\n\n\n\nAs you might guess, the Longuet-Higgins and Steedman would miss a lot of musical instances. For example, pieces that have non-harmonic chords would struggle, as would pieces that had a lot of chromatic ornamentations. Ideally an algorithm would allow for these pitches to occur, but acknowledge that pitches in the key might be a better fit than those outside of the key, and that certain pitches in the key should be more heavily weighted than others.\nCarol Krumhansl and Mark Schmuckler (Krumhansl, 1990) would devise an algorithm that tallied up the pitch classes of an excerpt and compared the distribution of these pitch classes to ratings from an earlier probe-tone experiment. (Krumhansl and Kessler, 1982). The weightings can be seen below.\nWe might think of this as a correlational approach. We tally up all of the pitches in a corpus, and then run a correlation on this key-profile. We run this over all of the keys, and the one that best fits is then labeled as “the key”.\n\nks_major_key <-\n  c(6.35, \n   2.23, \n   3.48, \n   2.33, \n   4.38, \n   4.09, \n   2.52, \n   5.19, \n   2.39, \n   3.66, \n   2.29, \n   2.88)\n\nks_minor_key <-\n  c(6.33, \n  2.68, \n  3.52, \n  5.38, \n  2.60, \n  3.53, \n  2.54, \n  4.75, \n  3.98, \n  2.69, \n  3.34, \n  3.17)\n\nAn interesting distinction here is that of experiment-derived vs. corpus-derived weightings. Should a key-finding algorithm intend to match how we hear key in a controlled lab environment (with basic harmonic progression stimuli), or should they use real music as a starting point? If they use real music, which music?\n\n\n\nHector Bellman created a key-finding algorithm that used Helen Budge’s dissertation from the 1940s as a starting point. Budge tallied up note occurrences in composers from the classical music canon, looking at the tonal makeup of a large collection of pieces. Bellman then used these frequencies as the starting point for his own key-finding algorithm.\n\nmajor <- c(16.80, \n            0.86,\n            12.95,\n            1.41,\n            13.49,\n            11.93,\n            1.25,\n            20.28,\n            1.80,\n            8.04,\n            0.62,\n            10.57)\n\nminor <- c(18.16,\n            0.69,\n            12.99,\n            13.34,\n            1.07,\n            11.15,\n            1.38,\n            21.07,\n            7.49,\n            1.53,\n            0.92,\n            10.21)\n\n\n\n\nDavid Temperley (2001) also employed Western classical music as a starting point for his early key-finding work (not to be confused with his more dynamic Bayesian-informed later work). He used examples from a commonly used music theory textbook (Stefan Kostka and Dorothy Payne’s Tonal Harmony).\n\nmajor <- c(0.748, \n            0.060, \n            0.488,\n            0.082, \n            0.670, \n            0.460, \n            0.096, \n            0.715, \n            0.104, \n            0.366,\n            0.057, \n            0.400)\n\nminor <- c(0.712, \n            0.084, \n            0.474, \n            0.618, \n            0.049, \n            0.460, \n            0.105, \n            0.747, \n            0.404, \n            0.067, \n            0.133, \n            0.330)\n\n\n\n\nBret Aarden (2003) argued that folk music would be a better fit than those generated from classical music. He used the Essen Folksong collection (consisting of thousands of folksongs throughout Europe, although with an uneven balance toward German folksong), to come up with the weightings below.\n\nmajor <- c(17.7661, \n            0.145624, \n            14.9265, \n            0.160186, \n            19.8049, \n            11.3587, \n            0.291248, \n            22.062, \n            0.145624, \n            8.15494, \n            0.232998, \n            4.95122)\n            \nminor <- c(18.2648, \n            0.737619, \n            14.0499, \n            16.8599, \n            0.702494, \n            14.4362, \n            0.702494, \n            18.6161, \n            4.56621, \n            1.93186, \n            7.37619, \n            1.75623)\n\n\n\n\nCraig Sapp argued that we probably didn’t even need to get frequencies from corpora or experiments. If we just assume that the tonic and the dominant (scale degrees 1 and 5) are the most important, and the other pitches in the key are less important, but more important than those not in the key, then we have a pretty simple weighting system (that works quite well!).\n\nmajor <- c(2, 0, 1, 0, 1, 1, 0, 2, 0, 1, 0, 1)\n\nminor <- c(2, 0, 1, 1, 0, 1, 0, 2, 1, 0, 0.5, 0.5)\n\n\n\n\nJosh Albrecht and I tried our hands at this problem, and picked a set of classical works from the Humdrum corpus, looking at only the first and last eight measures of each. The numbers are below.\n\nmajor <- c(0.238, \n            0.006, \n            0.111, \n            0.006, \n            0.137, \n            0.094, \n            0.016, \n            0.214, \n            0.009, \n            0.080, \n            0.008, \n            0.081) \n\nminor <- c(0.220, \n            0.006, \n            0.104, \n            0.123, \n            0.019, \n            0.103, \n            0.012, \n            0.214, \n            0.062, \n            0.022, \n            0.061, \n            0.052)\n\nWe also tried a Euclidean distance approach, rather than a correlational approach.\nWe tried to explain it as follows:\n\nIn a two-dimensional space, if there were 70% of pitch X and 30% pitch Y, the Cartesian location of the point representing this pitch-class distribution would be at X 1⁄4 0.7 and Y 1⁄4 0.3. In this case, we are examining the distribution of 12 pitch classes, resulting in a 12-dimensional Cartesian space. The pitch-class distribution of each piece is represented by a point in that 12-dimensional space. The distance is then measured between this point and the 24 points representing the 12 major and 12 minor key pitch-class distributions, and the key separated by the shortest distance is taken to be the key of the work.\n\nBelow is a table comparing how well this did to the others.\n\n\n\nComparing the Albrecht and Shanahan to others\n\n\n\n\n\nThey each perform a bit differently on different types of tasks.\n\n\n\nComparing key-finding algorithms in major, minor, and overall (from Albrecht and Shanahan, 2013)\n\n\n\n\ncorrplot 0.92 loaded\n\n\n\ncircshift <- function(v, n) {\n  if (n == 0) v else c(tail(v, n), head(v, -n))\n}\n# \n# # ### uses the Krumhansl Schmuckler Profiles\nmajor_key <- c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)\nminor_key <- c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)\n\n##sapp's simple weightings\n# major_key <- c(2, 0, 1, 0, 1, 1, 0, 2, 0, 1, 0, 1)\n# \n# minor_key <- c(2, 0, 1, 1, 0, 1, 0, 2, 1, 0, 0.5, 0.5)\n\nkey_templates <-\n  tribble(\n    ~name, ~template,\n    \"Gb:maj\", circshift(major_key, 6),\n    \"Bb:min\", circshift(minor_key, 10),\n    \"Db:maj\", circshift(major_key, 1),\n    \"F:min\", circshift(minor_key, 5),\n    \"Ab:maj\", circshift(major_key, 8),\n    \"C:min\", circshift(minor_key, 0),\n    \"Eb:maj\", circshift(major_key, 3),\n    \"G:min\", circshift(minor_key, 7),\n    \"Bb:maj\", circshift(major_key, 10),\n    \"D:min\", circshift(minor_key, 2),\n    \"F:maj\", circshift(major_key, 5),\n    \"A:min\", circshift(minor_key, 9),\n    \"C:maj\", circshift(major_key, 0),\n    \"E:min\", circshift(minor_key, 4),\n    \"G:maj\", circshift(major_key, 7),\n    \"B:min\", circshift(minor_key, 11),\n    \"D:maj\", circshift(major_key, 2),\n    \"F#:min\", circshift(minor_key, 6),\n    \"A:maj\", circshift(major_key, 9),\n    \"C#:min\", circshift(minor_key, 1),\n    \"E:maj\", circshift(major_key, 4),\n    \"G#:min\", circshift(minor_key, 8),\n    \"B:maj\", circshift(major_key, 11),\n    \"D#:min\", circshift(minor_key, 3)\n  )\n\nLet’s look at Lucy Dacus’s “Night Shift”.\nThis grabs the track and does all the magic:\n\nnight_shift <-\n  get_tidy_audio_analysis(\"1yYlpGuBiRRf33e1gY61bN\") %>%\n  compmus_align(sections, segments) %>%\n  select(sections) %>%\n  unnest(sections) %>%\n  mutate(\n    pitches =\n      map(segments,\n          compmus_summarise, pitches,\n          method = \"mean\", norm = \"manhattan\"\n      )\n  )\n\nAnd this is just a plotting function:\n\nnight_shift %>% \n  compmus_match_pitch_template(\n    key_templates,         # Change to chord_templates if descired\n    method = \"euclidean\",  # Try different distance metrics\n    norm = \"manhattan\"     # Try different norms\n  ) %>%\n  ggplot(\n    aes(x = start + duration / 2, width = duration, y = name, fill = d)\n  ) +\n  geom_tile() +\n  scale_fill_viridis_c(guide = \"none\") +\n  theme_minimal() +\n  labs(x = \"Time (s)\", y = \"\")\n\n\n\n\n\nnight_shift <-\n  get_tidy_audio_analysis(\"1yYlpGuBiRRf33e1gY61bN\") %>%\n  compmus_align(sections, segments) %>%\n  select(sections) %>%\n  unnest(sections) %>%\n  mutate(\n    pitches =\n      map(segments,\n          compmus_summarise, pitches,\n          method = \"mean\", norm = \"manhattan\"\n      )\n  )\n\nLet’s do some exercises:\n\nVisualize a song with all of these weightings.\n\nHow do the algorithms differ?\n\nCan you write a function that would call each weighting as an argument? What would that look like?"
  },
  {
    "objectID": "class_notes/week_4.html#whats-the-key-profile-for-indie-pop",
    "href": "class_notes/week_4.html#whats-the-key-profile-for-indie-pop",
    "title": "Week 4: Conceptual Debates: Key-Finding",
    "section": "What’s the Key-Profile for “Indie-Pop”",
    "text": "What’s the Key-Profile for “Indie-Pop”\nThe basic code for getting a key-profile from a playlist is below. The process is as follows:\n\nGet the audio features from a playlist, and add the audio analysis onto the datafame.\nWe then create a “segments” column by using a map function from the tidyverse. Map functions basically apply a function over each element in a list. Here, we are saying “apply the compmus_c_transpose function to the key and segments lists from the add_audio_analysis function.”\n\nWhat does the compmus_c_transpose function do? It takes all of the chroma vectors and transposes them to the key of C, so that we can construct a single set of weightings from pieces in different keys.\n\nWe then only grab this transposed segments column and turn it into a more readable list with the unnest function.\n\nWe then grab the start, duration, and pitches info.\n\nWe then create a “pitches” column, and normalize these raw pitch counts. There are a few ways to do this, and there are different options for this.\nWe then used the compmus_gather_chroma function to take all of those chroma vectors and turn them into a list.\nWe then use the group_by and summarise functions from tidyverse, and get the mean count of each pitch class in the distribution.\n\n\n### grabs the key-profile of the indie-pop playlist.\nindie_pop_key_profile <- get_playlist_audio_features(\"\", \"37i9dQZF1DWWEcRhUVtL8n\") |>\n  \n  add_audio_analysis() |>\n  ## transpose all the chroma vectors to C.\n  mutate(segments = map2(segments, key, compmus_c_transpose)) |>\n  ## grab the segments data and unnest it, then only grabbing the start, duration, and pitches info.\n  select(segments) |>\n  unnest(segments) |> \n  select(start, duration, pitches) |> \n  mutate(pitches = map(pitches, compmus_normalise, \"euclidean\")) |>\n  compmus_gather_chroma() |>\n  group_by(pitch_class) |>\n  summarise(mean_value = mean(value)) \n\nindie_pop_key_profile\n\n# A tibble: 12 × 2\n   pitch_class mean_value\n   <fct>            <dbl>\n 1 C                0.312\n 2 C#|Db            0.169\n 3 D                0.240\n 4 D#|Eb            0.188\n 5 E                0.224\n 6 F                0.224\n 7 F#|Gb            0.175\n 8 G                0.279\n 9 G#|Ab            0.176\n10 A                0.194\n11 A#|Bb            0.171\n12 B                0.213\n\n\nIdeally, we’d be able to turn this into a more reusable function. Below we’ve just turned made the playlist URI an argument:\n\nget_key_profile_broad <- function(uri){\n   get_playlist_audio_features(\"\", uri) |>\n   add_audio_analysis() |>\n   ## transpose all the chroma vectors to C. (have I mentioned how great Burgoyne's library is??)\n   mutate(segments = map2(segments, key, compmus_c_transpose)) |>\n   ## grab the segments data and unnest it, then only grabbing the start, duration, and pitches info.\n   select(segments) |>\n   unnest(segments) |> \n   select(start, duration, pitches) |> \n   mutate(pitches = map(pitches, compmus_normalise, \"euclidean\")) |>\n   compmus_gather_chroma() |>\n   group_by(pitch_class) |>\n   summarise(mean_value = mean(value)) \n}\n\nAnd now we can just run the function like so:\n\nindie_pop <- get_key_profile_broad(\"37i9dQZF1DWWEcRhUVtL8n\")\nindie_pop\n\n# A tibble: 12 × 2\n   pitch_class mean_value\n   <fct>            <dbl>\n 1 C                0.312\n 2 C#|Db            0.169\n 3 D                0.240\n 4 D#|Eb            0.188\n 5 E                0.224\n 6 F                0.224\n 7 F#|Gb            0.175\n 8 G                0.279\n 9 G#|Ab            0.176\n10 A                0.194\n11 A#|Bb            0.171\n12 B                0.213\n\n\nand we can plot it in a pretty straightforward way:\n\nbarplot(indie_pop$mean_value)\n\n\n\n\nSo we can look at other genres pretty easily. Here is me looking at Spotify’s “EDM 2023” playlist:"
  },
  {
    "objectID": "class_notes/week_4.html#whats-the-key-profile-for-edm",
    "href": "class_notes/week_4.html#whats-the-key-profile-for-edm",
    "title": "Week 4: Conceptual Debates: Key-Finding",
    "section": "What’s the Key Profile for EDM?",
    "text": "What’s the Key Profile for EDM?\n\nedm <- get_key_profile_broad(\"37i9dQZF1DX1kCIzMYtzum\")\nedm\n\n# A tibble: 12 × 2\n   pitch_class mean_value\n   <fct>            <dbl>\n 1 C                0.311\n 2 C#|Db            0.182\n 3 D                0.217\n 4 D#|Eb            0.218\n 5 E                0.217\n 6 F                0.231\n 7 F#|Gb            0.194\n 8 G                0.264\n 9 G#|Ab            0.204\n10 A                0.197\n11 A#|Bb            0.202\n12 B                0.231\n\n\nand once again we can plot it:\n\nbarplot(edm$mean_value)\n\n\n\n\n\nSome points of interest\n\nFor both of these distributions, we see a strong showing for scale degrees 1 and 5 (they aren’t really labeled in these quickie plots, but it would be the first and seventh column, respectively).\nWith the “Indie Pop” plot, we see a strong showing of scale degrees 1 and 5, and are followed by the diatonic pitches, but with the “EDM” list, scale degrees 2, flat 3, and 3 occur with pretty much the same frequency. It might be worth splitting the major and minor pieces up a bit?"
  },
  {
    "objectID": "class_notes/week_4.html#getting-separate-major-and-minor-key-profiles",
    "href": "class_notes/week_4.html#getting-separate-major-and-minor-key-profiles",
    "title": "Week 4: Conceptual Debates: Key-Finding",
    "section": "Getting separate major and minor key-profiles",
    "text": "Getting separate major and minor key-profiles\nWe could break this into a few parts for our own comfort. Let’s start by just creating a function that grabs the data. As that’s the one that’s quite time intensive, and calls to the API, let’s try to run it only once.\n\ngrab_playlist_info <- function(uri){\n   get_playlist_audio_features(\"\", uri) |>\n   add_audio_analysis() \n}\n\nOnce we have that in place, we can create a variable, and then subset it from there. Here, I’m saving the full list, and then creating a major and a minor variable.\n\nplaylist <- grab_playlist_info(\"37i9dQZF1DX1kCIzMYtzum\")  \nminor <- playlist |> filter(mode == 0)\nmajor <- playlist |> filter(mode == 1)\n\n\nget_pitch_list <- function(input){\n   input |>     \n   ## transpose all the chroma vectors to C. (have I mentioned how great Burgoyne's library is??)\n   mutate(segments = map2(segments, key, compmus_c_transpose)) |>\n   ## grab the segments data and unnest it, then only grabbing the start, duration, and pitches info.\n   select(segments) |>\n   unnest(segments) |> \n   select(start, duration, pitches) |> \n   mutate(pitches = map(pitches, compmus_normalise, \"euclidean\")) |>\n   compmus_gather_chroma() |>\n   group_by(pitch_class) |>\n   summarise(mean_value = mean(value))\n}\n\nAnd now we can get separate pitch lists for major and minor:\n\nminor_key <- get_pitch_list(minor)\nmajor_key <- get_pitch_list(major)\n\nand then of course we can use these to inform our own key mapping.\nWe can start by putting this all into a super quick and inefficient function like this (hoping to improve it as we go along):\n\nkey_plotter <- function(uri, major, minor){\n   major_key <- major\n   minor_key <- minor\n   key_templates <-\n   tribble(\n      ~name, ~template,\n      \"Gb:maj\", circshift(major_key, 6),\n      \"Bb:min\", circshift(minor_key, 10),\n      \"Db:maj\", circshift(major_key, 1),\n      \"F:min\", circshift(minor_key, 5),\n      \"Ab:maj\", circshift(major_key, 8),\n      \"C:min\", circshift(minor_key, 0),\n      \"Eb:maj\", circshift(major_key, 3),\n      \"G:min\", circshift(minor_key, 7),\n      \"Bb:maj\", circshift(major_key, 10),\n      \"D:min\", circshift(minor_key, 2),\n      \"F:maj\", circshift(major_key, 5),\n      \"A:min\", circshift(minor_key, 9),\n      \"C:maj\", circshift(major_key, 0),\n      \"E:min\", circshift(minor_key, 4),\n      \"G:maj\", circshift(major_key, 7),\n      \"B:min\", circshift(minor_key, 11),\n      \"D:maj\", circshift(major_key, 2),\n      \"F#:min\", circshift(minor_key, 6),\n      \"A:maj\", circshift(major_key, 9),\n      \"C#:min\", circshift(minor_key, 1),\n      \"E:maj\", circshift(major_key, 4),\n      \"G#:min\", circshift(minor_key, 8),\n      \"B:maj\", circshift(major_key, 11),\n      \"D#:min\", circshift(minor_key, 3)\n  )\n\ntune <-\n  get_tidy_audio_analysis(uri) %>%\n  compmus_align(sections, segments) %>%\n  select(sections) %>%\n  unnest(sections) %>%\n  mutate(\n    pitches =\n      map(segments,\n          compmus_summarise, pitches,\n          method = \"mean\", norm = \"manhattan\"\n      )\n  ) \n\ntune |> compmus_match_pitch_template(\n    key_templates,         # Change to chord_templates if descired\n    method = \"euclidean\",  # Try different distance metrics\n    norm = \"manhattan\"     # Try different norms\n  ) %>%\n  ggplot(\n    aes(x = start + duration / 2, width = duration, y = name, fill = d)\n  ) +\n  geom_tile() +\n  scale_fill_viridis_c(guide = \"none\") +\n  theme_minimal() +\n  labs(x = \"Time (s)\", y = \"\")\n\n}"
  },
  {
    "objectID": "class_notes/week_4.html#one-piece-and-many-key-profiles",
    "href": "class_notes/week_4.html#one-piece-and-many-key-profiles",
    "title": "Week 4: Conceptual Debates: Key-Finding",
    "section": "One Piece and Many Key Profiles",
    "text": "One Piece and Many Key Profiles\nLooking at Lucy Dacus’s “Night Shift” with EDM Key Profiles:\n\nedm_major_key <- c(0.2949827,0.1842662, 0.2249348, 0.1796559, 0.2532545, 0.2391564, 0.2028676, 0.2607747, 0.1765553, 0.2105823, 0.1806760, 0.2562869)\n# \nedm_minor_key <- c(0.3247214, 0.1767437, 0.2066454, 0.2482824, 0.1811887, 0.2263670, 0.1830838, 0.2662832, 0.2340293, 0.1888321, 0.2203257, 0.2047107)\nkey_plotter(\"1yYlpGuBiRRf33e1gY61bN\", edm_major_key, edm_minor_key)\n\n\n\n\nAnd here is the piece with the more traditional Krumhansl-Schmuckler key profiles:\n\nkey_plotter(\"1yYlpGuBiRRf33e1gY61bN\", ks_major_key, ks_minor_key)\n\n\n\n\nWe can load our “indie pop” but now in major and minor:\n\nplaylist <- grab_playlist_info(\"37i9dQZF1DWWEcRhUVtL8n\")  \nindie_minor <- playlist |> filter(mode == 0)\nindie_major <- playlist |> filter(mode == 1)\nindie_minor <- get_pitch_list(indie_minor)\nindie_major <- get_pitch_list(indie_major)\n\n\n\n\nAnd then we put these weightings into the plotter:\n\nkey_plotter(\"1yYlpGuBiRRf33e1gY61bN\", ks_major_key, ks_minor_key)"
  },
  {
    "objectID": "class_notes/week_4.html#exercise",
    "href": "class_notes/week_4.html#exercise",
    "title": "Week 4: Conceptual Debates: Key-Finding",
    "section": "Exercise:",
    "text": "Exercise:\n\nPick one piece and construct a genre-specific key-profile that might be used to explain its tonal make-up.\n\nExplain this musically."
  },
  {
    "objectID": "class_notes/week_5.html#looking-at-tempo-over-time",
    "href": "class_notes/week_5.html#looking-at-tempo-over-time",
    "title": "Week 5: Testing Hypotheses with Spotify",
    "section": "Looking at tempo over time",
    "text": "Looking at tempo over time\nWe can start by eyeballing the data. Here is how we’d do it with base R (no ggplot/tidyverse):\n\nplot(tempo ~ album_release_year, data=jayz)\nabline(lm(tempo ~ album_release_year, data=jayz), col=\"red\")\n\n\n\n\nIf we’d like to use ggplot it can give us some confidence bars (the default here is a 95% confidence interval):\n\nggplot(jayz, aes(album_release_year, tempo) ) + geom_point() +\n  stat_smooth(method = lm) +\n  theme_bw()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nSo it looks promising. We can run a linear regression with a simple lm command. Here we can get a summary of the model pretty easily, as well.\n\nsummary(lm(tempo ~ album_release_year, data=jayz))\n\n\nCall:\nlm(formula = tempo ~ album_release_year, data = jayz)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-44.84 -21.17 -11.79  12.22  92.18 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(>|t|)    \n(Intercept)        -2158.9919   396.7564  -5.442 6.99e-08 ***\nalbum_release_year     1.1306     0.1979   5.714 1.55e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 30.94 on 815 degrees of freedom\nMultiple R-squared:  0.03852,   Adjusted R-squared:  0.03734 \nF-statistic: 32.65 on 1 and 815 DF,  p-value: 1.547e-08\n\n\nSo, as we can see from the results here, it’s significant (p < .001), but it really doesn’t account for much of the variance (an adjusted R-squared of .037).\n\nPost-Hoc Analyses\nPerhaps we can look at how other variables might be predictive of the year of the recording.\nLet’s look at how tempo, danceability, valence, speechiness, and energy might improve the model.\n\nsummary(lm(album_release_year ~ tempo + danceability + valence + speechiness + energy, data=jayz))\n\n\nCall:\nlm(formula = album_release_year ~ tempo + danceability + valence + \n    speechiness + energy, data = jayz)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.2530  -3.4825  -0.2837   3.5690  18.2393 \n\nCoefficients:\n               Estimate Std. Error  t value Pr(>|t|)    \n(Intercept)   2.016e+03  1.826e+00 1104.163  < 2e-16 ***\ntempo         2.861e-02  5.603e-03    5.105 4.12e-07 ***\ndanceability -5.986e+00  1.519e+00   -3.942 8.77e-05 ***\nvalence      -6.849e+00  9.630e-01   -7.112 2.51e-12 ***\nspeechiness  -7.808e+00  1.160e+00   -6.730 3.20e-11 ***\nenergy       -4.484e+00  1.348e+00   -3.326 0.000921 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.885 on 811 degrees of freedom\nMultiple R-squared:  0.2084,    Adjusted R-squared:  0.2035 \nF-statistic: 42.69 on 5 and 811 DF,  p-value: < 2.2e-16\n\n\nSo we have a more predictive model, with an adjusted R-squared of about .20.\nThere are some remaining questions, however. Firstly, is there covariance at play?\nWe can use the variance inflation factor (VIF) to look at this. Typically, each variable should be less than 5.\n\nlibrary(car)\n\nLoading required package: carData\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\njayz_model <- lm(album_release_year ~ tempo + danceability + valence + speechiness + energy, data=jayz)\nvif(jayz_model)\n\n       tempo danceability      valence  speechiness       energy \n    1.067352     1.350387     1.228451     1.093354     1.253967 \n\n\nA correlation plot can help us to visualize this a bit more.\n\nlibrary(corrplot)\n\ncorrplot 0.92 loaded\n\njz <- jayz %>% \n    select(c(\"acousticness\", \"liveness\", \"danceability\", \"loudness\", \"speechiness\", \"valence\"))\n  x <- as.matrix(cor(jz))\n  round(x, 2)\n\n             acousticness liveness danceability loudness speechiness valence\nacousticness         1.00     0.14         0.07    -0.12        0.33    0.07\nliveness             0.14     1.00        -0.20     0.01        0.03   -0.11\ndanceability         0.07    -0.20         1.00    -0.17       -0.01    0.28\nloudness            -0.12     0.01        -0.17     1.00       -0.20    0.03\nspeechiness          0.33     0.03        -0.01    -0.20        1.00    0.18\nvalence              0.07    -0.11         0.28     0.03        0.18    1.00\n\n  corrplot(x, method=\"pie\")"
  },
  {
    "objectID": "class_notes/week_5.html#sidenote-isare-the-data-normal",
    "href": "class_notes/week_5.html#sidenote-isare-the-data-normal",
    "title": "Week 5: Testing Hypotheses with Spotify",
    "section": "Sidenote: Is/Are the data normal?",
    "text": "Sidenote: Is/Are the data normal?\nWe can test to see if the tempo data is normally distributed:\n\nqqnorm(jayz$tempo)\n\n\n\nhist(jayz$tempo)\n\n\n\nshapiro.test(jayz$tempo)\n\n\n    Shapiro-Wilk normality test\n\ndata:  jayz$tempo\nW = 0.78869, p-value < 2.2e-16\n\nks.test(jayz$tempo, \"pnorm\")\n\nWarning in ks.test.default(jayz$tempo, \"pnorm\"): ties should not be present for\nthe Kolmogorov-Smirnov test\n\n\n\n    Asymptotic one-sample Kolmogorov-Smirnov test\n\ndata:  jayz$tempo\nD = 1, p-value < 2.2e-16\nalternative hypothesis: two-sided\n\n\nAt the moment, it doesn’t seem to be…"
  },
  {
    "objectID": "class_notes/week_5.html#stepwise-entry-regression",
    "href": "class_notes/week_5.html#stepwise-entry-regression",
    "title": "Week 5: Testing Hypotheses with Spotify",
    "section": "Stepwise Entry Regression",
    "text": "Stepwise Entry Regression\n\nsummary(step(lm(album_release_year ~ danceability + \n          tempo + acousticness + \n          speechiness + valence, data=jayz), \n     direction=\"backward\"))\n\nStart:  AIC=2573.56\nalbum_release_year ~ danceability + tempo + acousticness + speechiness + \n    valence\n\n               Df Sum of Sq   RSS    AIC\n<none>                      18788 2573.6\n- danceability  1    243.73 19032 2582.1\n- tempo         1    714.38 19502 2602.1\n- acousticness  1    826.79 19614 2606.7\n- speechiness   1   1422.54 20210 2631.2\n- valence       1   1687.89 20476 2641.8\n\n\n\nCall:\nlm(formula = album_release_year ~ danceability + tempo + acousticness + \n    speechiness + valence, data = jayz)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-12.9398  -3.6858  -0.2579   3.4408  16.3345 \n\nCoefficients:\n               Estimate Std. Error  t value Pr(>|t|)    \n(Intercept)   2.011e+03  1.257e+00 1599.612  < 2e-16 ***\ndanceability -4.496e+00  1.386e+00   -3.244  0.00123 ** \ntempo         3.075e-02  5.538e-03    5.553 3.80e-08 ***\nacousticness  6.752e+00  1.130e+00    5.974 3.46e-09 ***\nspeechiness  -9.233e+00  1.178e+00   -7.836 1.46e-14 ***\nvalence      -7.750e+00  9.080e-01   -8.536  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.813 on 811 degrees of freedom\nMultiple R-squared:  0.2314,    Adjusted R-squared:  0.2267 \nF-statistic: 48.83 on 5 and 811 DF,  p-value: < 2.2e-16\n\nsummary(step(lm(album_release_year ~ danceability + \n          tempo + acousticness + \n          speechiness + valence, data=jayz), \n     direction=\"forward\"))\n\nStart:  AIC=2573.56\nalbum_release_year ~ danceability + tempo + acousticness + speechiness + \n    valence\n\n\n\nCall:\nlm(formula = album_release_year ~ danceability + tempo + acousticness + \n    speechiness + valence, data = jayz)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-12.9398  -3.6858  -0.2579   3.4408  16.3345 \n\nCoefficients:\n               Estimate Std. Error  t value Pr(>|t|)    \n(Intercept)   2.011e+03  1.257e+00 1599.612  < 2e-16 ***\ndanceability -4.496e+00  1.386e+00   -3.244  0.00123 ** \ntempo         3.075e-02  5.538e-03    5.553 3.80e-08 ***\nacousticness  6.752e+00  1.130e+00    5.974 3.46e-09 ***\nspeechiness  -9.233e+00  1.178e+00   -7.836 1.46e-14 ***\nvalence      -7.750e+00  9.080e-01   -8.536  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.813 on 811 degrees of freedom\nMultiple R-squared:  0.2314,    Adjusted R-squared:  0.2267 \nF-statistic: 48.83 on 5 and 811 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "class_notes/week_5.html#comparing-fits",
    "href": "class_notes/week_5.html#comparing-fits",
    "title": "Week 5: Testing Hypotheses with Spotify",
    "section": "Comparing Fits:",
    "text": "Comparing Fits:\nWe could construct a few models But how can we tell which of these is more predictable? For this, we can look at Akaike’s ‘An Information Criterion’(or AIC). The lower number is better, and any difference of more than 2 is considered a statistically significant difference fit.\n\ndance_model <- lm(danceability ~ album_release_year, data=jayz)\nacoustic_model <- lm(acousticness ~ album_release_year, data=jayz)\nspeech_model <- lm(speechiness ~ album_release_year, data=jayz)\nvalence_model <- lm(valence ~ album_release_year, data=jayz)\ntempo_model <- lm(tempo ~ album_release_year, data=jayz)\ncombined_model <- lm(album_release_year ~ tempo + acousticness + speechiness + valence, data=jayz)\n\n\nAIC(dance_model, \n    acoustic_model, \n    tempo_model,\n    speech_model, \n    valence_model, \n    combined_model)\n\n               df        AIC\ndance_model     3 -1036.7120\nacoustic_model  3  -686.8298\ntempo_model     3  7930.2701\nspeech_model    3  -782.5959\nvalence_model   3  -438.7779\ncombined_model  6  4902.6326\n\n\nThe combined model doesn’t seem to do terribly well here, which seems to muddy the question up a bit."
  },
  {
    "objectID": "class_notes/week_5.html#is-a-linear-model-the-best-approach",
    "href": "class_notes/week_5.html#is-a-linear-model-the-best-approach",
    "title": "Week 5: Testing Hypotheses with Spotify",
    "section": "Is a linear model the best approach?",
    "text": "Is a linear model the best approach?\nWe can adjust how we are viewing this data (linear or polynomial fit) a bit with the code below. Here we can plot our data as a line:\n\nggplot(jayz, aes(album_release_year, tempo) ) + geom_point() +\n  stat_smooth(method = lm, formula = y ~ poly(x, 1, raw = TRUE)) +\n  theme_bw()\n\n\n\n\nAnd here we have it as a second order polynomial:\n\nggplot(jayz, aes(album_release_year, tempo) ) + geom_point() +\n  stat_smooth(method = lm, formula = y ~ poly(x, 2, raw = TRUE)) +\n  theme_bw()\n\n\n\n\nAnd we can compare fits here:\n\nlinear <- lm(album_release_year ~ tempo, data = jayz)\npoly_2 <- lm(album_release_year ~ tempo + I(album_release_year^2), data = jayz)\n\nAIC(linear, \n    poly_2)\n\n       df       AIC\nlinear  3  5069.029\npoly_2  4 -5640.652"
  },
  {
    "objectID": "class_notes/week_5.html#predicting-a-categorical-variable",
    "href": "class_notes/week_5.html#predicting-a-categorical-variable",
    "title": "Week 5: Testing Hypotheses with Spotify",
    "section": "Predicting a categorical variable",
    "text": "Predicting a categorical variable\nWhat does it look like to predict a categorical variable in R? We might explore this question by hypothesizing that musical features might be predictive of mode (labeled in Spotify as major or minor: 1 or 0).\nHere is what a binomial logistic regression would look like:\n\njayz.log <- glm(mode ~ tempo + danceability + valence +\n                     speechiness + acousticness, family = binomial, data = jayz)\n\nAnd it looks like “speechiness” is the most predictive of mode here.\n\nsummary(jayz.log)\n\n\nCall:\nglm(formula = mode ~ tempo + danceability + valence + speechiness + \n    acousticness, family = binomial, data = jayz)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.8836  -1.2367   0.9073   1.0739   1.3806  \n\nCoefficients:\n               Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  -1.468e-01  5.364e-01  -0.274   0.7843    \ntempo         3.210e-06  2.365e-03   0.001   0.9989    \ndanceability  1.062e-01  5.893e-01   0.180   0.8570    \nvalence      -5.733e-01  3.911e-01  -1.466   0.1427    \nspeechiness   2.561e+00  5.483e-01   4.671    3e-06 ***\nacousticness -9.680e-01  4.849e-01  -1.996   0.0459 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1120.6  on 816  degrees of freedom\nResidual deviance: 1095.8  on 811  degrees of freedom\nAIC: 1107.8\n\nNumber of Fisher Scoring iterations: 4\n\n\nWe can plot the log odds ratios as well:\n\nCI <- exp(confint(jayz.log))[-1,]\n\nWaiting for profiling to be done...\n\nsjPlot::plot_model(jayz.log,\n                   axis.lim = c(min(CI), max(CI)),\n                   auto.label = F,\n                   show.values = T) +\n                   theme_bw()"
  },
  {
    "objectID": "class_notes/week_5.html#clustering",
    "href": "class_notes/week_5.html#clustering",
    "title": "Week 5: Testing Hypotheses with Spotify",
    "section": "Clustering",
    "text": "Clustering\nCluster analysis is a form of statistical data analysis in which subsets (called “clusters”) are formed according to some notion of similarity. There are many different variants of cluster analysis, but most are hierarchical–in which low-level clusters are successively joined together to make larger clusters, and so on, until everything is clustered into one large group. The result is a cluster tree or dendrogram.\n\nHow does the R hclust function work?\nThe hclust function is part of the default package in R, and it clusters based on dissimilarities in the data. There are different algorithms it can use, but the default is Ward’s minimum variance. It requires some distance to be calculated first, so the dist function is used on the data. Again there are many options here, but the default is to simply calculate the Euclidean distance between the values.\nThe documentation states:\n\nThis function performs a hierarchical cluster analysis using a set of dissimilarities for the n objects being clustered. Initially, each object is assigned to its own cluster and then the algorithm proceeds iteratively, at each stage joining the two most similar clusters, continuing until there is just a single cluster. At each stage distances between clusters are recomputed by the Lance–Williams dissimilarity update formula according to the particular clustering method being used.\n\nThe default is Ward’s minimum variance method, which:\n\naims at finding compact, spherical clusters. The complete linkage method finds similar clusters.\n\nAnother method is the “single linkage method”.\n\nThe single linkage method (which is closely related to the minimal spanning tree) adopts a ‘friends of friends’ clustering strategy. The other methods can be regarded as aiming for clusters with characteristics somewhere between the single and complete link methods. Note however, that methods “median” and “centroid” are not leading to a monotone distance measure, or equivalently the resulting dendrograms can have so called inversions or reversals which are hard to interpret, but note the trichotomies in Legendre and Legendre (2012).\n\n\n# cluster demo modified from here: \n### https://www.r-bloggers.com/2021/04/cluster-analysis-in-r/\nlibrary(tidyverse)\nlibrary(cluster)\nbeyonce <- read.csv(\"beyonce.csv\")\ntaylor <- read.csv(\"taylor.csv\")\n\n\ndf <- beyonce %>% \n  filter(album_name == \"4\") %>%\n  select(c(\"track_name\", \"acousticness\", \"liveness\", \"danceability\", \"loudness\", \"speechiness\", \"valence\"))\n\n\ndf <- df %>% distinct(track_name, .keep_all = TRUE)\n\n## cleaning up the data.\nz <- df[,-c(1,1)]\n\n### getting means of each category.\nmeans <- apply(z,2,mean)\n### getting standard deviation of each category.\nsds <- apply(z,2,sd)\n\n### scales the data in the matrix.\nscaled_data <- scale(z,center=means,scale=sds)\ndistance <- dist(scaled_data)\n\nAnd we can plot the data like this:\n\n### helps with the size of the image.\npar(mar = c(5, 4, 4, 1))\n\n### creates the cluster\ndf.hclust <- hclust(distance)\n\n### plots the data but with row numbers.\nplot(df.hclust)\n\n\n\n\nAnd we can add the track name like so:\n\nplot(df.hclust,labels=df$track_name,main='Default from hclust')\n\n\n\n\nWe can clean up the plot the be along a single x-axis with the hang argument.\n\nnodePar <- list(lab.cex = 0.6, pch = c(NA, 19), \n                cex = 0.7, col = \"blue\")\nplot(df.hclust,hang=-1, labels=df$track_name,main='Default from hclust')"
  },
  {
    "objectID": "class_notes/week_5.html#which-track-belongs-to-which-cluster",
    "href": "class_notes/week_5.html#which-track-belongs-to-which-cluster",
    "title": "Week 5: Testing Hypotheses with Spotify",
    "section": "Which track belongs to which cluster?",
    "text": "Which track belongs to which cluster?\nIt might be helpful with this analysis to look at how each of the songs fits on the tree. We can use the cutree function, which “cuts a tree” from the cluster based on how many groups we ask it for.\nThe following code can tell us how many fall into each broader tree, assuming we think that the tree should be cut into three. Notice that the third branch is the most populous, with the second being the most sparsely populated.\n\nmember <- cutree(df.hclust,3)\ntable(member)\n\nmember\n 1  2  3 \n 4  3 11 \n\n\nBut how is each category being weighted? The code below shows that acousticness and danceability do a fair bit of work in separating groups 1 and 3, and valence separates 1 and 2 from one another.\n\n##but how are these clusters calculated?\naggregate(scaled_data,list(member),mean)\n\n  Group.1 acousticness   liveness danceability   loudness speechiness\n1       1    1.2293770 -0.3037203   -1.5341655 -0.4289046   0.3971934\n2       2    0.6165508 -0.5097641    0.5152708 -1.0002027   1.0862774\n3       3   -0.6151964  0.2494703    0.4173500  0.4287478  -0.4406914\n     valence\n1 -1.2506018\n2  1.0310423\n3  0.1735709\n\n\nA slightly more even split occurs if we break it into four groups rather than three.\n\nmember <- cutree(df.hclust,4)\ntable(member)\n\nmember\n1 2 3 4 \n4 3 9 2 \n\n\nAnd that how they’re split into four is a bit different from how we might split them into three, but danceability and acousticness still playing a strong role.\n\naggregate(scaled_data,list(member),mean)\n\n  Group.1 acousticness   liveness danceability   loudness speechiness\n1       1    1.2293770 -0.3037203   -1.5341655 -0.4289046   0.3971934\n2       2    0.6165508 -0.5097641    0.5152708 -1.0002027   1.0862774\n3       3   -0.6115250 -0.1547202    0.4044253  0.3613888  -0.5628163\n4       4   -0.6317177  2.0683279    0.4755110  0.7318636   0.1088705\n      valence\n1 -1.25060184\n2  1.03104230\n3  0.05196371\n4  0.72080353\n\n\n\nK-Means Clustering\nWe can also run a simple k-means clustering on the data. With this, we are clustering the data into k groups. R’s documentation explains it like so:\n\naims to partition the points into k groups such that the sum of squares from points to the assigned cluster centres is minimized. At the minimum, all cluster centres are at the mean of their Voronoi sets (the set of data points which are nearest to the cluster centre).\n\nThere are a few algorithms to pick from. R uses the Hartigan and Wong (1979) algorithm by default.\n\n###split it into three groups\nset.seed(123)\nkc <- kmeans(scaled_data,3)\n\n### add labels.\nrow.names(scaled_data) <- df$track_name\n\n###get the shortest distance.\ndatadistshortset<-dist(scaled_data,method = \"euclidean\")\n\nThe code below will cluster it based on this k-means clustering distance, and plot them into the amount of groups listed (here 3).\n\nhc1 <- hclust(datadistshortset, method = \"complete\" )\npamvshortset <- pam(datadistshortset,3, diss = FALSE)\n\nclusplot(pamvshortset, shade = FALSE,labels=2,col.clus=\"blue\",col.p=\"red\",span=FALSE,main=\"Cluster Mapping\",cex=1.2)"
  },
  {
    "objectID": "class_notes/week_5.html#example-1-looking-at-nirvana-albumss",
    "href": "class_notes/week_5.html#example-1-looking-at-nirvana-albumss",
    "title": "Week 5: Testing Hypotheses with Spotify",
    "section": "Example 1: Looking at Nirvana Albumss",
    "text": "Example 1: Looking at Nirvana Albumss\nI’m going to get the global features from Nirvana, and specifically I’m just going to look at the Unplugged in New York album. I do this in two, rather inefficient, steps: I get all of the Nirvana data and put those in a dataframe, and then I create a variable that has filtered out only the specific album I’m looking for.\n\nnirvana <- get_artist_audio_features('nirvana')\nunplugged <- filter(nirvana, album_name == \"MTV Unplugged In New York\")\nboth <- filter(nirvana, album_name == \"MTV Unplugged In New York\"  | album_name == \"Nevermind\")\n\nThis gets lots of data, and I’m just interested in their global measures (tempo, danceability, liveness, etc.). Here, I’ve gone with column number rather than name, but the other version might be a bit easier/cleaner. Nevertheless, this is another way of doing it:\n\n###i've just picked out the columns I want.\nselected <- c(9,10,12,14,15,16,17,18,19,30)\n### This subsets the data based on only the columns I want.\nunplugged <- unplugged[,selected]\n\n### I assign the track name column (30) with the rownames, to have a labeled cluster.\nrownames(unplugged) <- unplugged$track_name\n\n\nhc <- hclust(dist(unplugged), method = \"complete\", members = NULL)\n\nWarning in dist(unplugged): NAs introduced by coercion\n\n\n\nPlotting the cluster\nTechnically, you could just use the plot function at this point, but there are some long title names, so I added these extra plot options to make the text smaller and increase the marins.\n\npar(cex=0.5, mar=c(5, 8, 4, 1))\nplot(hc, xlab=\"\", ylab=\"\", main=\"\", sub=\"\", axes=FALSE)\npar(cex=1)\ntitle(xlab=\"tunes\", ylab=\"height\", main=\"Nirvana unplugged\")\naxis(2)\n\n\n\n\nThis plot is a bit strange now, as we have a pretty big negative number on the y-axis. Nevertheless, we see some cool things. The songs written by the Meat Puppets cluster together, for example.\n\n\nk-means clustering\nOur next type of clustering analysis is be a k-means cluster. We will start off by using a scree plot to see how many clusters we should use. There are a number of ways of analyzing where an “elbow” on this plot might be, but many people actually just eyeball it.\n\nunplugged <- filter(nirvana, album_name == \"MTV Unplugged In New York\" )\ntitles <- unplugged$track_name\nunplugged <- unplugged[,selected]\nunplugged <- scale(unplugged[,-10]) # standardize variables\nunplugged <- as.data.frame(unplugged) # standardize variables\n\n# Determine number of clusters\nwss <- (nrow(unplugged)-1)*sum(apply(unplugged,2,var))\nfor (i in 2:9) wss[i] <- sum(kmeans(unplugged, \n                                    centers=i)$withinss)\n  plot(1:9, wss, type=\"b\", xlab=\"Number of Clusters\",\n    ylab=\"Within groups sum of squares\")\n\n\n\n\nAnd now we can look at the k-means clustering based on however many clusters we think are necessary.\n\n# K-Means Cluster Analysis\nfit <- kmeans(unplugged, 3) # 3 cluster solution\n# get cluster means \naggregate(unplugged,by=list(fit$cluster),FUN=mean)\n\n  Group.1 danceability     energy   loudness speechiness acousticness\n1       1     1.026773  0.1850689  0.4603061 -0.67242123    0.5744139\n2       2    -0.466715  0.7286156  0.3748797  0.92850767   -1.1595582\n3       3    -0.653401 -0.7679614 -0.7602098 -0.07038491    0.3532327\n  instrumentalness    liveness    valence      tempo\n1       -0.1528028  0.59048330  0.5986320  0.1965621\n2       -0.3318123  0.09159538  0.4107587  0.7767115\n3        0.4182526 -0.66375960 -0.9272390 -0.8179312\n\n# append cluster assignment\nunplugged_appended <- data.frame(unplugged, fit$cluster)\n\n\nrownames(unplugged_appended) = titles\nclusplot(unplugged_appended, fit$cluster, color=TRUE, shade=TRUE, \n   labels=3, lines=0)"
  },
  {
    "objectID": "class_notes/week_5.html#conditional-inference-tree-with-party",
    "href": "class_notes/week_5.html#conditional-inference-tree-with-party",
    "title": "Week 5: Testing Hypotheses with Spotify",
    "section": "Conditional Inference Tree with Party",
    "text": "Conditional Inference Tree with Party\nA conditional inference tree is basically a regression tree, and it tells you exactly how it picks apart the data in a pretty clear way.\nI’ve always thought that Weezer was a bit derivative, so we might look at how we can separate them other (much better) bands, like Pavement…\n\npavement <- get_artist_audio_features('pavement')\nweezer <- get_artist_audio_features('weezer')\npavement_weezer <-rbind(pavement, weezer)\n\nHere’s a regression tree that tries to account for the variance between deciding whether a piece is from Pavement or Weezer.\n\nlibrary(rpart.plot)\n\nLoading required package: rpart\n\n# grow tree \nfit <- rpart(as.factor(artist_name) ~ danceability + valence + tempo + liveness,  data=pavement_weezer)\n\nprintcp(fit) # display the results \n\n\nClassification tree:\nrpart(formula = as.factor(artist_name) ~ danceability + valence + \n    tempo + liveness, data = pavement_weezer)\n\nVariables actually used in tree construction:\n[1] danceability liveness     valence     \n\nRoot node error: 349/979 = 0.35649\n\nn= 979 \n\n        CP nsplit rel error  xerror     xstd\n1 0.047755      0   1.00000 1.00000 0.042940\n2 0.022923      4   0.80516 0.89398 0.041776\n3 0.020057      5   0.78223 0.85960 0.041331\n4 0.010000      6   0.76218 0.85673 0.041293\n\nplotcp(fit) # visualize cross-validation results \n\n\n\nsummary(fit) # detailed summary of splits\n\nCall:\nrpart(formula = as.factor(artist_name) ~ danceability + valence + \n    tempo + liveness, data = pavement_weezer)\n  n= 979 \n\n          CP nsplit rel error    xerror       xstd\n1 0.04775549      0 1.0000000 1.0000000 0.04294041\n2 0.02292264      4 0.8051576 0.8939828 0.04177567\n3 0.02005731      5 0.7822350 0.8595989 0.04133127\n4 0.01000000      6 0.7621777 0.8567335 0.04129270\n\nVariable importance\ndanceability      valence     liveness        tempo \n          61           28           10            1 \n\nNode number 1: 979 observations,    complexity param=0.04775549\n  predicted class=Weezer    expected loss=0.3564862  P(node) =1\n    class counts:   349   630\n   probabilities: 0.356 0.644 \n  left son=2 (618 obs) right son=3 (361 obs)\n  Primary splits:\n      danceability < 0.522    to the left,  improve=43.858290, (0 missing)\n      valence      < 0.2975   to the left,  improve=13.962690, (0 missing)\n      tempo        < 142.843  to the right, improve= 7.641565, (0 missing)\n      liveness     < 0.05625  to the right, improve= 6.520634, (0 missing)\n  Surrogate splits:\n      valence  < 0.5905   to the left,  agree=0.697, adj=0.177, (0 split)\n      liveness < 0.05625  to the right, agree=0.647, adj=0.042, (0 split)\n      tempo    < 76.103   to the right, agree=0.635, adj=0.011, (0 split)\n\nNode number 2: 618 observations,    complexity param=0.04775549\n  predicted class=Weezer    expected loss=0.4708738  P(node) =0.6312564\n    class counts:   291   327\n   probabilities: 0.471 0.529 \n  left son=4 (53 obs) right son=5 (565 obs)\n  Primary splits:\n      liveness     < 0.5765   to the right, improve=7.022553, (0 missing)\n      valence      < 0.1925   to the left,  improve=6.959549, (0 missing)\n      danceability < 0.404    to the left,  improve=5.352002, (0 missing)\n      tempo        < 125.6825 to the left,  improve=4.458552, (0 missing)\n\nNode number 3: 361 observations\n  predicted class=Weezer    expected loss=0.1606648  P(node) =0.3687436\n    class counts:    58   303\n   probabilities: 0.161 0.839 \n\nNode number 4: 53 observations\n  predicted class=Pavement  expected loss=0.2830189  P(node) =0.05413687\n    class counts:    38    15\n   probabilities: 0.717 0.283 \n\nNode number 5: 565 observations,    complexity param=0.04775549\n  predicted class=Weezer    expected loss=0.4477876  P(node) =0.5771195\n    class counts:   253   312\n   probabilities: 0.448 0.552 \n  left son=10 (61 obs) right son=11 (504 obs)\n  Primary splits:\n      valence      < 0.6655   to the right, improve=10.232180, (0 missing)\n      danceability < 0.5165   to the right, improve= 4.949092, (0 missing)\n      tempo        < 77.8915  to the left,  improve= 3.823105, (0 missing)\n      liveness     < 0.09715  to the left,  improve= 3.710828, (0 missing)\n\nNode number 10: 61 observations\n  predicted class=Pavement  expected loss=0.2786885  P(node) =0.06230848\n    class counts:    44    17\n   probabilities: 0.721 0.279 \n\nNode number 11: 504 observations,    complexity param=0.04775549\n  predicted class=Weezer    expected loss=0.4146825  P(node) =0.514811\n    class counts:   209   295\n   probabilities: 0.415 0.585 \n  left son=22 (98 obs) right son=23 (406 obs)\n  Primary splits:\n      valence      < 0.192    to the left,  improve=7.635957, (0 missing)\n      danceability < 0.5165   to the right, improve=4.863906, (0 missing)\n      tempo        < 77.8915  to the left,  improve=3.964739, (0 missing)\n      liveness     < 0.3565   to the left,  improve=3.188013, (0 missing)\n  Surrogate splits:\n      tempo        < 202.119  to the right, agree=0.813, adj=0.041, (0 split)\n      danceability < 0.196    to the left,  agree=0.812, adj=0.031, (0 split)\n\nNode number 22: 98 observations,    complexity param=0.02292264\n  predicted class=Pavement  expected loss=0.4081633  P(node) =0.1001021\n    class counts:    58    40\n   probabilities: 0.592 0.408 \n  left son=44 (90 obs) right son=45 (8 obs)\n  Primary splits:\n      danceability < 0.497    to the left,  improve=6.102494, (0 missing)\n      liveness     < 0.09655  to the left,  improve=4.191436, (0 missing)\n      tempo        < 130.712  to the right, improve=3.409439, (0 missing)\n      valence      < 0.113    to the right, improve=1.896793, (0 missing)\n\nNode number 23: 406 observations,    complexity param=0.02005731\n  predicted class=Weezer    expected loss=0.3719212  P(node) =0.4147089\n    class counts:   151   255\n   probabilities: 0.372 0.628 \n  left son=46 (7 obs) right son=47 (399 obs)\n  Primary splits:\n      danceability < 0.5165   to the right, improve=5.619653, (0 missing)\n      valence      < 0.22     to the right, improve=3.489861, (0 missing)\n      liveness     < 0.363    to the left,  improve=2.817898, (0 missing)\n      tempo        < 124.612  to the left,  improve=2.773645, (0 missing)\n\nNode number 44: 90 observations\n  predicted class=Pavement  expected loss=0.3555556  P(node) =0.09193054\n    class counts:    58    32\n   probabilities: 0.644 0.356 \n\nNode number 45: 8 observations\n  predicted class=Weezer    expected loss=0  P(node) =0.008171604\n    class counts:     0     8\n   probabilities: 0.000 1.000 \n\nNode number 46: 7 observations\n  predicted class=Pavement  expected loss=0  P(node) =0.007150153\n    class counts:     7     0\n   probabilities: 1.000 0.000 \n\nNode number 47: 399 observations\n  predicted class=Weezer    expected loss=0.3609023  P(node) =0.4075587\n    class counts:   144   255\n   probabilities: 0.361 0.639 \n\n# plot tree \nrpart.plot(fit, uniform=TRUE, \n   main=\"Regression Tree for Pavement/Weezer\")"
  },
  {
    "objectID": "class_notes/other_files.qmd/sampling_exercises.html",
    "href": "class_notes/other_files.qmd/sampling_exercises.html",
    "title": "Sampling Exercise",
    "section": "",
    "text": "For each case, identify the kind of sampling employed.\n\nA researcher walks into a music library with a question: Are sharp keys more common than flat keys? Wandering through the stacks, she blindly grabs volumes off the shelves and allows each volume to open spontaneously to some page. She takes note of the key signature.\nA professional music marketer is interested in carrying out a detailed survey of musical tastes in Britain. The marketer decides to use the ACORN geodemographic profile. British households will be sampled in proportion to the second-level ACORN categories: wealthy executives (8.6 percent of the population), affluent greys (7.7%), flourishing families (8.8%), prosperous professionals (2.2%), educated urbanites (4.6%), aspiring singles (3.9%), starting out (2.5%), secured families (15.5%), settled surburbia (6.0%), prudent pensioners (2.6%), asian communities (1.6%), post-industrial families (4.8%), blue collar roots (8.0%), struggling families (14.1%), burdened singles (4.5%), high rise hardship (1.6%), and inner city adversity (2.1%).\nA researcher is interested in assembling a random sample of “classical” keyboard music. She has determined that she needs roughly 20 pieces for her study. In order to maximize data independence, she wants each piece to be written by a different composer. Using Wikipedia, she finds an alphabetical list of “classical composers.” For each letter of the alphabet, she selects the first composer who she knows has written for piano: Isaac Albéniz, Carl Philipp Emanuel Bach, Alfredo Casella, Claude Debussy, Edward Elgar, Manuel de Falla, etc.\nIn piloting an experiment, a graduate student recruits her graduate student colleagues as experimental participants.\nA team of researchers is interested in emotional expression in Hindustani film music. Indian participants are asked to characterize the emotional tenor of various film scenes. Using the descriptions, the researchers then classify the scenes into 14 categories — such as romantic, humorous, physical conflict, emotional tension, etc. The researchers then select four scenes for each of the 14 categories and analyse the associated background music. Their goal is to identify musical features in Hindustani culture that signal romance, humor, etc.\nA medievalist thinks that the Dorian mode was more likely to have been heard as comparatively “happy” whereas the Phrygian mode was more likely to have been heard as comparative “sad” for medieval listeners. In order to test this notion, the scholar examines all of the Glorias (nominally “happy” text) and Kyries (nominally “sad” text) in the Liber Usualis. The prediction is that Dorian will predominate for Glorias while Phrygian will be more likely to occur for Kyries.\nA researcher is interested in changing harmonic patterns in the masses of Palestrina. The researcher makes us of the Humdrum database of the scores for the complete 103 masses assembled by musicologist John Miller.\nPaul von Hippel and David Huron (2000) carried out a study to test the idea that melodies tend to change direction following a leap, and that this pattern is ubiquitous in musical melodies around the world. In order to test this idea, they made use of two musical samples. The first sample selected music spanning five centuries. The second sample selected music spanning five continents: Africa, Asia, Europe, North and South America.\nUnsure of the contents of a box, an archivist reaches in and grabs a couple of documents, which he then examines.\nA researcher wants to know whether there is anything Italian, French or German about augmented sixth chords. Using large computer databases, the researcher uses Humdrum to isolate 900 sonorities in which the lowered sixth and raised fourth appear concurrently (including enharmonic spellings): 300 each written by Italian, French and German composers. Each of the sonorities is then classified as either Italian, French, German or Other.\n\n\n\nPaul von Hippel & David Huron (2000). Why do skips precede reversals? The effect of tessitura on melodic structure. Music Perception, Vol. 18, No. 1, pp. 59-85."
  },
  {
    "objectID": "class_notes/week_2.html#chords",
    "href": "class_notes/week_2.html#chords",
    "title": "Week 2: Pitch",
    "section": "Chords",
    "text": "Chords\nBurgoyne’s chordogram functions allow us to look at the likely chordal spaces for specific piecses. The code below does a few things:\n\nFirst we define what a major, minor, and seventh chord looks like in terms of pitch space.\nWe then use the key-profiles from the Krumhansl-Kessler article on the probe tone experiments and store them into major_key and minor_key variables.\nThe circshift function rotates these key profiles through the chord variables and provides the best fit for that moment. This is done through the key_templates variable (Notice the compmus_match_pitch_template below).\n\n\n#      C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B\nmajor_chord <-\n  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)\nminor_chord <-\n  c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)\nseventh_chord <-\n  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)\n\nmajor_key <-\n  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)\nminor_key <-\n  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)\n\ncircshift <- function(v, n) {\n  if (n == 0) v else c(tail(v, n), head(v, -n))\n}\nchord_templates <-\n  tribble(\n    ~name, ~template,\n    \"Gb:7\", circshift(seventh_chord, 6),\n    \"Gb:maj\", circshift(major_chord, 6),\n    \"Bb:min\", circshift(minor_chord, 10),\n    \"Db:maj\", circshift(major_chord, 1),\n    \"F:min\", circshift(minor_chord, 5),\n    \"Ab:7\", circshift(seventh_chord, 8),\n    \"Ab:maj\", circshift(major_chord, 8),\n    \"C:min\", circshift(minor_chord, 0),\n    \"Eb:7\", circshift(seventh_chord, 3),\n    \"Eb:maj\", circshift(major_chord, 3),\n    \"G:min\", circshift(minor_chord, 7),\n    \"Bb:7\", circshift(seventh_chord, 10),\n    \"Bb:maj\", circshift(major_chord, 10),\n    \"D:min\", circshift(minor_chord, 2),\n    \"F:7\", circshift(seventh_chord, 5),\n    \"F:maj\", circshift(major_chord, 5),\n    \"A:min\", circshift(minor_chord, 9),\n    \"C:7\", circshift(seventh_chord, 0),\n    \"C:maj\", circshift(major_chord, 0),\n    \"E:min\", circshift(minor_chord, 4),\n    \"G:7\", circshift(seventh_chord, 7),\n    \"G:maj\", circshift(major_chord, 7),\n    \"B:min\", circshift(minor_chord, 11),\n    \"D:7\", circshift(seventh_chord, 2),\n    \"D:maj\", circshift(major_chord, 2),\n    \"F#:min\", circshift(minor_chord, 6),\n    \"A:7\", circshift(seventh_chord, 9),\n    \"A:maj\", circshift(major_chord, 9),\n    \"C#:min\", circshift(minor_chord, 1),\n    \"E:7\", circshift(seventh_chord, 4),\n    \"E:maj\", circshift(major_chord, 4),\n    \"G#:min\", circshift(minor_chord, 8),\n    \"B:7\", circshift(seventh_chord, 11),\n    \"B:maj\", circshift(major_chord, 11),\n    \"D#:min\", circshift(minor_chord, 3)\n  )\n\nkey_templates <-\n  tribble(\n    ~name, ~template,\n    \"Gb:maj\", circshift(major_key, 6),\n    \"Bb:min\", circshift(minor_key, 10),\n    \"Db:maj\", circshift(major_key, 1),\n    \"F:min\", circshift(minor_key, 5),\n    \"Ab:maj\", circshift(major_key, 8),\n    \"C:min\", circshift(minor_key, 0),\n    \"Eb:maj\", circshift(major_key, 3),\n    \"G:min\", circshift(minor_key, 7),\n    \"Bb:maj\", circshift(major_key, 10),\n    \"D:min\", circshift(minor_key, 2),\n    \"F:maj\", circshift(major_key, 5),\n    \"A:min\", circshift(minor_key, 9),\n    \"C:maj\", circshift(major_key, 0),\n    \"E:min\", circshift(minor_key, 4),\n    \"G:maj\", circshift(major_key, 7),\n    \"B:min\", circshift(minor_key, 11),\n    \"D:maj\", circshift(major_key, 2),\n    \"F#:min\", circshift(minor_key, 6),\n    \"A:maj\", circshift(major_key, 9),\n    \"C#:min\", circshift(minor_key, 1),\n    \"E:maj\", circshift(major_key, 4),\n    \"G#:min\", circshift(minor_key, 8),\n    \"B:maj\", circshift(major_key, 11),\n    \"D#:min\", circshift(minor_key, 3)\n  )\n\nHere we have a piece of code that grabs a single audio file (“Those magic changes”). In class we listened to it while going through the chordogram. Can you spot the modulation? Why do we get that yellowish color at the end of the graph?\n\nthose_magic_changes <-\n  get_tidy_audio_analysis(\"1WHauHX7U6FqOWh46lK4IV\") %>%\n  compmus_align(sections, segments) %>%\n  select(sections) %>%\n  unnest(sections) %>%\n  mutate(\n    pitches =\n      map(segments,\n          compmus_summarise, pitches,\n          method = \"mean\", norm = \"manhattan\"\n      )\n  )\n\nthose_magic_changes %>% \n  compmus_match_pitch_template(\n    key_templates,         # Change to chord_templates if descired\n    method = \"euclidean\",  # Try different distance metrics\n    norm = \"manhattan\"     # Try different norms\n  ) %>%\n  ggplot(\n    aes(x = start + duration / 2, width = duration, y = name, fill = d)\n  ) +\n  geom_tile() +\n  scale_fill_viridis_c(guide = \"none\") +\n  theme_minimal() +\n  labs(x = \"Time (s)\", y = \"\")\n\n\n\n\nSome activities:\n\nGo through some songs that you know. How close is the chordogram of providing some sort of brief explanatory analysis?\nWhat does this look like on music that might be considered less adherent to notions of western tonal music?"
  },
  {
    "objectID": "class_notes/week_3.html#midterms",
    "href": "class_notes/week_3.html#midterms",
    "title": "Week 3: Time",
    "section": "Midterms",
    "text": "Midterms\n\nDue April 28th\n1-3 pages long.\nconstruct a literature review of the topic you’re interested in writing about. This could culminate in a hypothesis, and a discussion of how your work fills the current gaps in the literature.\nBe sure to include an overview of previous research related to the topic. This should include both empirical and non-empirical work. So if you’re focusing on the memory for jazz licks, for example, research on improvisational styles should be included alongside work on memory for musical ideas. It need not be all-encompassing, but it should try to cover as much ground as possible.\nAddress a gap in the current literature, or that between theory and research. End with a discussion of your study, and how it hopes to fill in these gaps.\nFeel free to meet with me if you have any questions."
  },
  {
    "objectID": "class_notes/week_3.html#populations",
    "href": "class_notes/week_3.html#populations",
    "title": "Week 3: Time",
    "section": "Populations",
    "text": "Populations\n\nA population is everything or everyone that you’re interested in.\ne.g. all the world’s people\nall the world’s people including living and deceased\nall Western-enculturated people\nall people who enjoy listening to music\nall clarinet players\n\nA “population” does not refer only to people: Other examples:\n\nall of the music written by Vivaldi\nall solo flute music (both with and without accompaniment)\nall music in the minor mode\nall of the jazz scores available in the New York Public Library\nall performances of Rachmaninov’s 2nd piano concerto"
  },
  {
    "objectID": "class_notes/week_3.html#sample",
    "href": "class_notes/week_3.html#sample",
    "title": "Week 3: Time",
    "section": "Sample",
    "text": "Sample\n\nSample: a subset of the population that you hope closely resembles the population as a whole.\nA sample is said to be representative when the property of interest is identical in both the sample and the population."
  },
  {
    "objectID": "class_notes/week_3.html#bias",
    "href": "class_notes/week_3.html#bias",
    "title": "Week 3: Time",
    "section": "Bias",
    "text": "Bias\n\nA sample is said to be biased when the property of interest differs between the sample and the population."
  },
  {
    "objectID": "class_notes/week_3.html#weird",
    "href": "class_notes/week_3.html#weird",
    "title": "Week 3: Time",
    "section": "WEIRD",
    "text": "WEIRD\n\nWestern\nEducated\nIndustrialized\nRich\nDemocratic\n\nsee Henrich’s Work on this"
  },
  {
    "objectID": "class_notes/week_3.html#defining-your-population",
    "href": "class_notes/week_3.html#defining-your-population",
    "title": "Week 3: Time",
    "section": "Defining Your Population",
    "text": "Defining Your Population\n\nYou can’t sample a population unless you have a clear idea of what constitutes the population of interest.\n\nSuppose, for example, that you are a political pollster. Your aim is to predict the likely election results for a national election in Denmark. What, precisely, is the population you are interested in?"
  },
  {
    "objectID": "class_notes/week_3.html#defining-your-population-continued",
    "href": "class_notes/week_3.html#defining-your-population-continued",
    "title": "Week 3: Time",
    "section": "Defining Your Population (continued)",
    "text": "Defining Your Population (continued)\n\nAll Danish citizens?\nAll people living in Denmark?\nAll people living in Denmark eligible to vote?\nAll people eligible to vote in Danish elections?\nAll people likely to vote in Danish elections?"
  },
  {
    "objectID": "class_notes/week_3.html#sampling-method",
    "href": "class_notes/week_3.html#sampling-method",
    "title": "Week 3: Time",
    "section": "Sampling Method",
    "text": "Sampling Method\n\nSampling method: the way you recruit or assemble your sample. When your population consists of people, sampling methods might include soliciting information by telephone (telephone sampling), street sampling, mail sampling, web sampling, classroom sampling, concert sampling, etc."
  },
  {
    "objectID": "class_notes/week_3.html#sampling-bias",
    "href": "class_notes/week_3.html#sampling-bias",
    "title": "Week 3: Time",
    "section": "Sampling Bias",
    "text": "Sampling Bias\n\nSampling bias: when the sampling method introduces differences that cause the sample not to be representative. We try to avoid or minimize sampling bias.\nWhen conducting a telephone survey, a pollster may be tempted to ask to speak to a respondent’s spouse. However, spouses are likely to share many things in common (such as political views) so the sampling method will introduce a bias."
  },
  {
    "objectID": "class_notes/week_3.html#simple-random-sampling",
    "href": "class_notes/week_3.html#simple-random-sampling",
    "title": "Week 3: Time",
    "section": "Simple Random Sampling",
    "text": "Simple Random Sampling\n\nSimple Random Sampling. Suppose we want to know about musical instrument sales in the City of Nashville. We could use the phone book to identify all of the shops within the city boundaries that sell musical instruments. Perhaps we discover that there are 131 retailers. From this list, we might randomly select 25 retailers in order to carry out our survey."
  },
  {
    "objectID": "class_notes/week_3.html#systematic-sampling",
    "href": "class_notes/week_3.html#systematic-sampling",
    "title": "Week 3: Time",
    "section": "Systematic Sampling",
    "text": "Systematic Sampling\nSuppose that we have a questionnaire we want to distribute to people who attended a concert. There might be 500 audience members, but we have only 50 surveys to distribute. One approach would be to distribute the questionnaires to the first 50 people leaving the concert hall."
  },
  {
    "objectID": "class_notes/week_3.html#matched-random-sampling",
    "href": "class_notes/week_3.html#matched-random-sampling",
    "title": "Week 3: Time",
    "section": "Matched Random Sampling",
    "text": "Matched Random Sampling\n\nA way of linking members from two or more samples. For example, a study might involve matching each professional musician with an amateur musician who plays the same instrument."
  },
  {
    "objectID": "class_notes/week_3.html#convenience-sampling",
    "href": "class_notes/week_3.html#convenience-sampling",
    "title": "Week 3: Time",
    "section": "Convenience Sampling",
    "text": "Convenience Sampling\n\nConvenience Sampling. A convenience sample simply takes advantage of whatever might be available. For example, a sample of organ music by Gabriel Fauré might simply consist of all of the scores available in a music library. Similarly, we might stand on a street corner and ask whoever passes by to answer questions on a survey."
  },
  {
    "objectID": "class_notes/week_3.html#stratified-sampling",
    "href": "class_notes/week_3.html#stratified-sampling",
    "title": "Week 3: Time",
    "section": "Stratified Sampling",
    "text": "Stratified Sampling\n\nWhen we have reason to suspect that differences in sub-populations might influence the results, it is common to sample in such a way to ensure that each of the main sub-populations is represented.\nPost and Huron (2009) were interested in common-practice era tonal classical music. So we decided to use a stratified sample consisting of music from three periods: Baroque, Classical and Romantic. Our overall sample consisted of equivalent numbers of works from each of these historical eras."
  },
  {
    "objectID": "class_notes/week_3.html#quota-sampling",
    "href": "class_notes/week_3.html#quota-sampling",
    "title": "Week 3: Time",
    "section": "Quota Sampling",
    "text": "Quota Sampling\n\nA type of stratified sampling in which sub-samples are weighted according to their prevalence in the population.\nSuppose that we find that 52% of instrumentalists are most accomplished on guitar, 33% are most accomplished on keyboards, 12% on flute, 9% on trumpet, 8% on violin, etc. In quota sampling, we would aim to sample the same proportions for each instrument."
  },
  {
    "objectID": "class_notes/week_3.html#exercise",
    "href": "class_notes/week_3.html#exercise",
    "title": "Week 3: Time",
    "section": "Exercise",
    "text": "Exercise\nGroup Exercise"
  },
  {
    "objectID": "class_notes/week_3.html#tempo-average",
    "href": "class_notes/week_3.html#tempo-average",
    "title": "Week 3: Time",
    "section": "Tempo Average",
    "text": "Tempo Average\nThe general default at the track level is an averaging of an entire piece. This can be useful at times, but it should be noted that it is a broad average for a parameter in which the variability is often quite meaningful.\nWhat if we wanted to see how the songs of Daft Punk changed in tempo changed over time?\nFirst we would make sure that Spotify had your user access token, and then get the artist audio features:\n\naccess_token <- get_spotify_access_token()\ndaft_punk <- get_artist_audio_features('daft punk')\n\nThen we would be able to simply plot the album’s mean tempo with the variance (and outliers) with ggplot, as below:\n\nggplot(daft_punk, aes(x=album_release_year, y=tempo, group = album_name)) + geom_boxplot() +\ntheme_bw()\n\n\n\n\nWe could also explore the variance of tempo within an album by looking at the standard deviation.\n\nalbum_sd <- daft_punk %>% \n    group_by(album_name, album_release_year) %>%\n    summarise(sd_tempo = sd(tempo))\n\n`summarise()` has grouped output by 'album_name'. You can override using the\n`.groups` argument.\n\n\nAnd then we can similarly plot this information:\n\nggplot(album_sd, aes(x=album_release_year, y=sd_tempo, group = album_name)) + geom_point() +\n  geom_label(\n    aes(label=album_name)) +\ntheme_bw()\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\n\n\nWarning: Removed 1 rows containing missing values (`geom_label()`)."
  },
  {
    "objectID": "class_notes/week_3.html#other-aspects-of-tempo",
    "href": "class_notes/week_3.html#other-aspects-of-tempo",
    "title": "Week 3: Time",
    "section": "Other aspects of tempo:",
    "text": "Other aspects of tempo:\nWe can also look at other elements of tempo, such as variability within sections. Here we have a question about the differences in tempo between punk in the 1980s and later punk (1990s and 2000s). I’m interested not just in the tempo, but also the variation of tempo.\nThere are a couple of points to notice in this code:\n\nNote how we are able to get data from a playlist. A playlist can be a good way for you to construct a sample.\nNote the add_audio_analysis function from the compmus library. This adds track level analysis information to the broader list of global information. It’s great.\n\nSit for a minute with this data. You’ll see the columns at the end that provided the specific audio analysis for each piece.\n\neighties_punk <-\n  get_playlist_audio_features(\n    \"kristian\",\n    \"5sxuwIQlaByb6Sx2OEwWTx\"\n  ) %>%\n  slice(1:30) %>%\n  add_audio_analysis()\n\nnineties_and_aughts_punk <-\n  get_playlist_audio_features(\n    \"CW\",\n    \"39sVxPTg7BKwrf2MfgrtcD\"\n  ) %>%\n  slice(1:30) %>%\n  add_audio_analysis()\n\nHere we bind both playlists together as a single data frame:\n\npunk <- \n  eighties_punk %>%\n  mutate(genre = \"eighties\") %>%\n  bind_rows(nineties_and_aughts_punk %>% mutate(genre = \"newer\"))\n\nThe spotify analysis gives us section markers as well, and we can use the code below to summarise the tempo, loudness, and duration for each section. Note the use of the map function, which takes the input and applies a function to that input (here the summarise_at function, and the summarise_at itself, which provides a summary of each of these columns.\nHere we are storing it in a variable called summarised_punk.\n\nsummarised_punk <- punk %>%\n  mutate(\n    sections =\n      map(\n        sections,                                    # sections or segments\n        summarise_at,\n        vars(tempo, loudness, duration),             # features of interest\n        list(section_mean = mean, section_sd = sd)   # aggregation functions\n      )\n  )\n\nNow we take this variable and plot it using ggplot.\nThe process below is as follows:\n\ntake the table above with summarized section information and unnest it (this takes the sections list of information and turns it into rows and columns).\nPipe that into ggplot, with the aesthetics function plotting the tempo on the x-axis, the standard deviation on the y-axis, the color being which genre we used (eighties or not). The color saturation is set to the loudness variable.\nWe then tell ggplot that we want this to be a scatterplot with the geom_point function, and that the size of each point should be the duration of the piece (divided by 60 as Spotify just gives it in seconds).\nWe then add a rug plot which gives the ticks on both axes to show the distribution of events.\nWe then add a black and white theme because nobody likes default graphics.\nWe then add the size of the graph and the axis labels.\n\n\n  summarised_punk %>%\n  unnest(sections) %>%\n  ggplot(\n    aes(\n      x = tempo,\n      y = tempo_section_sd,\n      colour = genre,\n      alpha = loudness\n    )\n  ) +\n  geom_point(aes(size = duration / 60)) +\n  geom_rug() +\n  theme_bw() +\n  ylim(0, 5) +\n  labs(\n    x = \"Mean Tempo (bpm)\",\n    y = \"SD Tempo\",\n    colour = \"Genre\",\n    size = \"Duration (min)\",\n    alpha = \"Volume (dBFS)\"\n  )  \n\nWarning: Removed 7 rows containing missing values (`geom_point()`).\n\n\n\n\n\nIn-class exercises:\n\nHow is tempo treated differently across the albums of the Beatles?"
  },
  {
    "objectID": "class_notes/week_3.html#plan-for-the-day",
    "href": "class_notes/week_3.html#plan-for-the-day",
    "title": "Week 3: Time",
    "section": "Plan for the day:",
    "text": "Plan for the day:\n\nTalk about your homework a bit.\nLook at some tap data, and what we might actually be able to do with it.\nTalk about how Spotify (might) calculate tempo.\n\nHow might we look at tempo across pieces?\nOne way to get this is to just tap the tempo, and then align it to the onsets. But how do we find onsets?\nThis is where a novelty function comes in. (see Müller on Fourier Tempograms). Put (extremely) succinctly, a novelty function detects changes in the energy or the spectrum of the signal. So looking for energy peaks might be a good marker for “peaks in energy”.\n\n\n\nMüller’s Onset Detection Example (p.311)\n\n\nAfter finding these onsets, it then examines a correlation between various sinusoids and picks the most likely one. There are many different ways of approaching this.\nOne issue is the presence of so-called “tempo octaves”. That is, it finds tempos at twice the beat, half the beat, etc..\nHere’s a graph of AJR’s “World’s Smallest Violin”:\n\nget_tidy_audio_analysis(\"3jHBgKdLCf46aP3HRI0WYv\") %>%\n  tempogram(window_size = 8, hop_size = 2) %>%\n   ggplot(aes(x = time, y = bpm, fill = power)) + \n    geom_raster() + \n    scale_fill_viridis_c(guide = 'none') +\n    labs(x = 'Time (s)', y = 'Tempo (BPM)') +\n    theme_classic()\n\n\n\n\nThis seems quite strange, though. It’s not really a great indicator of tempo…\nMüller points out that the Fourier-based method tends to struggle with these tempo-octaves, and a cyclic model, which look at “subharmonics” rather than harmonics, and are a bit better for mid-level tempo finding. The example below seems to work a bit better. Notice how the cyclic option has been switched to TRUE.\n\nget_tidy_audio_analysis(\"3jHBgKdLCf46aP3HRI0WYv\") %>%\n  tempogram(window_size = 8, hop_size = 2, cyclic=TRUE) %>%\n   ggplot(aes(x = time, y = bpm, fill = power)) + \n    geom_raster() + \n    scale_fill_viridis_c(guide = 'none') +\n    labs(x = 'Time (s)', y = 'Tempo (BPM)') +\n    theme_classic()"
  },
  {
    "objectID": "class_notes/week_1.html",
    "href": "class_notes/week_1.html",
    "title": "Week 1: Representing Musical Data",
    "section": "",
    "text": "In the first week, we worked through basic introductions for the class, and went through the syllabus and the course structure.\n\n\nHere, we install the necessary library. As you can see, you will need to install devtools, which will allow you to install packages that aren’t on CRAN from github.\nThen, we install the package (you can uncomment these installation lines as necessary for you).\n\n### installing everything as needed\n# library(devtools)\n# devtools::install_github(\"Computational-Cognitive-Musicology-Lab/humdrumR\", build_vignettes = TRUE)\nlibrary(humdrumR)\n\nIn the code below, you can see how we load all of the Chopin files into a preludes variable with the readHumdrum function.\nThen we subset it by spines. We are interested in various ways of calculating pitch, so we looked at pc (pitch class), as well as solfa and deg, which gave us solfege syllables and scale degrees, respectively.\nWe then plot this data in a barplot. Note the |> or “pipe” that we are using. The older tidyverse-style pipe (%>%) will also work here.\n\n### Load in Chopin preludes, grab the left hand and see all the scale degrees.\npreludes <- readHumdrum(\"~/gitcloud/corpora/humdrum_scores/Chopin/Preludes/*.krn\")\nleft_hand <- subset(preludes, Spine == 1)\n###solfa, deg, pc\ntable_data <- with(left_hand, pc(Token,simple=TRUE)) |> table() \nbarplot(table_data)\n\n\n\n\nYou can use a similar with syntax to get rhythm variables, as seen below:\n\n## rhythminterval\nrhythms <- with(preludes[2], duration(Token))\n\nWarning in is.na(x) || is.na(y): 'length(x) = 93 > 1' in coercion to\n'logical(1)'\n\nWarning in is.na(x) || is.na(y): 'length(x) = 93 > 1' in coercion to\n'logical(1)'\n\nWarning in is.na(x) || is.na(y): 'length(x) = 93 > 1' in coercion to\n'logical(1)'\n\nWarning in is.na(x) || is.na(y): 'length(x) = 93 > 1' in coercion to\n'logical(1)'\n\n\nWarning in is.na(x) || is.na(y): 'length(x) = 84 > 1' in coercion to\n'logical(1)'\n\nWarning in is.na(x) || is.na(y): 'length(x) = 84 > 1' in coercion to\n'logical(1)'\n\n\nWarning in is.na(x) || is.na(y): 'length(x) = 93 > 1' in coercion to\n'logical(1)'\n\nWarning in is.na(x) || is.na(y): 'length(x) = 93 > 1' in coercion to\n'logical(1)'\n\n\nWarning in is.na(x) || is.na(y): 'length(x) = 11 > 1' in coercion to\n'logical(1)'\n\nWarning in is.na(x) || is.na(y): 'length(x) = 11 > 1' in coercion to\n'logical(1)'\n\n\nWarning in is.na(x) || is.na(y): 'length(x) = 2 > 1' in coercion to\n'logical(1)'\n\nWarning in is.na(x) || is.na(y): 'length(x) = 2 > 1' in coercion to\n'logical(1)'\n\n\nWarning in is.na(x) || is.na(y): 'length(x) = 93 > 1' in coercion to\n'logical(1)'\n\nWarning in is.na(x) || is.na(y): 'length(x) = 93 > 1' in coercion to\n'logical(1)'\n\n\nWarning in is.na(x) || is.na(y): 'length(x) = 84 > 1' in coercion to\n'logical(1)'\n\nWarning in is.na(x) || is.na(y): 'length(x) = 84 > 1' in coercion to\n'logical(1)'\n\n\nWarning in is.na(x) || is.na(y): 'length(x) = 11 > 1' in coercion to\n'logical(1)'\n\nWarning in is.na(x) || is.na(y): 'length(x) = 11 > 1' in coercion to\n'logical(1)'\n\n\nWarning in is.na(x) || is.na(y): 'length(x) = 5 > 1' in coercion to\n'logical(1)'\n\nWarning in is.na(x) || is.na(y): 'length(x) = 5 > 1' in coercion to\n'logical(1)'\n\n#### group exercise:\n#### using a repertoire in the Humdrum scores collection, \n#### print a table of most common musical events.\n\n\n\n\nWe can start by loading our spotifyr library, and tidyverse for good measure:\n\nlibrary(spotifyr)\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0     ✔ purrr   1.0.1\n✔ tibble  3.2.1     ✔ dplyr   1.1.1\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.4     ✔ forcats 1.0.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ purrr::%@%()         masks rlang::%@%()\n✖ dplyr::changes()     masks humdrumR::changes()\n✖ dplyr::count()       masks humdrumR::count()\n✖ tidyr::expand()      masks humdrumR::expand()\n✖ dplyr::filter()      masks stats::filter()\n✖ purrr::flatten()     masks rlang::flatten()\n✖ purrr::flatten_chr() masks rlang::flatten_chr()\n✖ purrr::flatten_dbl() masks rlang::flatten_dbl()\n✖ purrr::flatten_int() masks rlang::flatten_int()\n✖ purrr::flatten_lgl() masks rlang::flatten_lgl()\n✖ purrr::flatten_raw() masks rlang::flatten_raw()\n✖ humdrumR::int()      masks rlang::int()\n✖ purrr::invoke()      masks rlang::invoke()\n✖ dplyr::lag()         masks humdrumR::lag(), stats::lag()\n✖ dplyr::lead()        masks humdrumR::lead()\n✖ purrr::splice()      masks rlang::splice()\n✖ dplyr::symdiff()     masks bit::symdiff()\n✖ purrr::transpose()   masks humdrumR::transpose()\n\n\nYou will need your own spotify client ID and client secret. You can get them by filling out the brief online form here.\n\n### setting up spotify\nSys.setenv(SPOTIFY_CLIENT_ID = YOUR SPOTIFY CLIENT ID)\nSys.setenv(SPOTIFY_CLIENT_SECRET = YOUR SPOTIFY CLIENT SECRET)\naccess_token <- get_spotify_access_token()\n\n\n\n\nFor the most part, in this class we will be looking at global features data (the “danceability” of a song), and track-level analysis features, such as chroma vectors.\nHere we see how you might grab artist features for Ryan Adams and Taylor Swift, comparing the performances of each of their 1989 albums.\n\n###getting artist level data\nryan_adams <- get_artist_audio_features('ryan adams')\ntaylor_swift <- get_artist_audio_features('taylor swift')\n\n### cleaning up the data\nadams_swift <- rbind(ryan_adams, taylor_swift)\nadams_swift_1989 <- adams_swift %>% filter(album_name == \"1989\") \nadams_swift_1989$track_name <- tolower(adams_swift_1989$track_name)\n\n## comparing energy\nggplot(adams_swift_1989, aes(x=track_name, y=energy, group=artist_name)) +\n  geom_line(aes(linetype=artist_name, color=artist_name))+\n  geom_point(aes(color=artist_name))+\n  theme(legend.position=\"top\", axis.text.x = element_text(angle = 90, hjust = 1))"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Corpus Studies and Music",
    "section": "",
    "text": "Welcome!\nWelcome to the Corpus Studies and Music class."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is the class site for Corpus Studies in Music."
  },
  {
    "objectID": "course-schedule.html",
    "href": "course-schedule.html",
    "title": "Course Schedule",
    "section": "",
    "text": "Unit\nWeek\nDate (Date)\nTopic\nImportant Dates\n\n\n\n\nRepresenting, Searching, and Counting with Symbolic Data\n1\nT (3/28; Northwestern Monday)\nIntroductions; a history of corpus-based approaches to music.\n\n\n\n\n1\nW (3/29)\nRepresenting musical data (kern, MIDI, XML, MEI)\nAssignment #1 Due\n\n\n\n2\nM (4/3)\nPitches, melodic intervals and scale degrees\nResponse #1 Due\n\n\n\n2\nW (4/5)\nRepresenting Harmony\nAssignment #2 Due\n\n\n\n3\nM (4/10)\nRepresenting Rhythm and Meter\nResponse #2 Due\n\n\n\n3\nW (4/12)\nCharacteristic Features and TF-IDF\nAssignment #3 Due\n\n\n\n4\nM (4/17)\nn-grams and Transition Probabilities\nResponse #3 Due\n\n\n\n4\nW (4/19)\nFinding patterns\nAssignment #4 Due\n\n\nConceptual Debates\n5\nM (4/24)\nMeasurements of Similarity\nMidterm Literature Review Due\n\n\n\n5\nW (4/26)\nEntropy and Variability\nAssignment #5 Due\n\n\n\n6\nM (5/1)\nKey-Finding Algorithms\nResponse #4 Due\n\n\n\n6\nW (5/3)\nKey-Finding Algorithms (continued)\nAssignment #6 Due\n\n\n\n7\nM (5/8)\nFast Fourier Transforms\nResponse #5 Due\n\n\n\n7\nW (5/10)\nClustering and Authorship\nAssignment #7 Due\n\n\nAudio Data\n8\nM (5/15)\nPerformance Data/the Mazurka Project\nFirst Draft of Final Project Due\n\n\n\n8\nW (5/17)\nTempo flexibility, and other performance data\nAssignment #8 Due\n\n\n\n9\nM (5/22)\nThe Spotify API\nResponse #6 Due; Peer Reviews Due\n\n\n\n9\nW (5/24)\nThe Spotify API (continued)\nAssignment #9 Due\n\n\n\n10\nM (5/29)\nNo Classes (Memorial Day)\nFinal Presentations\n\n\n\n10\nW (5/31)\nFinal Presentations\nFinal Papers due on Friday, 6/2"
  },
  {
    "objectID": "course-syllabus.html",
    "href": "course-syllabus.html",
    "title": "Corpus Studies (Music 348/448) Syllabus",
    "section": "",
    "text": "Instructor: Dr. Daniel Shanahan\nContact: daniel.shanahan@northwestern.edu"
  },
  {
    "objectID": "course-syllabus.html#overview",
    "href": "course-syllabus.html#overview",
    "title": "Corpus Studies (Music 348/448) Syllabus",
    "section": "Overview",
    "text": "Overview\nCorpus studies, or distant readings of multiple musical works, are often employed as a way of better understanding issues such as the relationships between pieces, authorship, trends over time, or differences and similarities between genres. In this class, we will explore the techniques, history, and philosophy of such approaches, and will construct and analyze our own corpora. For the most part, this class will deal with notated scores, and students will be encouraged to ask their own research questions of the music that they are most interested in."
  },
  {
    "objectID": "course-syllabus.html#course-info",
    "href": "course-syllabus.html#course-info",
    "title": "Corpus Studies (Music 348/448) Syllabus",
    "section": "Course info",
    "text": "Course info\n\n\n\n\n\n\n\n\n\n\nDay\nTime\nLocation\n\n\n\n\nMeeting Times:\nMon & Weds\n12:30pm - 1:50 pm\nRCMA 1-164\n\n\nOffice Hours\nTBD (and by appointment)\nTBD (and by appointment)\nRCMA 4-181"
  },
  {
    "objectID": "course-syllabus.html#learning-objectives",
    "href": "course-syllabus.html#learning-objectives",
    "title": "Corpus Studies (Music 348/448) Syllabus",
    "section": "Learning objectives",
    "text": "Learning objectives\nBy the end of the quarter, you will…\n\nhave an understanding of how music has been examined through distant readings of scores and recordings\nbe able to explore how the concepts of concordances, schemata, key-finding, clustering, and introductory machine learning approaches can be applied to music analysis\nhave a working introductory knowledge of the R programming language and the HumDrumR package."
  },
  {
    "objectID": "course-syllabus.html#course-policies",
    "href": "course-syllabus.html#course-policies",
    "title": "Corpus Studies (Music 348/448) Syllabus",
    "section": "Course Policies",
    "text": "Course Policies\n\nAcademic Integrity\nStudents in this course are required to comply with the policies found in the booklet, “Academic Integrity at Northwestern University: A Basic Guide”. All papers submitted for credit in this course must be submitted electronically unless otherwise instructed by the professor. Your written work may be tested for plagiarized content. For details regarding academic integrity at Northwestern or to download the guide, visit this page.\n\n\nAccesibility\nNorthwestern University is committed to providing the most accessible learning environment as possible for students with disabilities. Should you anticipate or experience disability-related barriers in the academic setting, please contact AccessibleNU to move forward with the university’s established accommodation process (email: accessiblenu@northwestern.edu; p: 847-467-5530). If you already have established accommodations with AccessibleNU, please let me know as soon as possible, preferably within the first two weeks of the term, so we can work together to implement your disability accommodations. Disability information, including academic accommodations, is confidential under the Family Educational Rights and Privacy Act.\n\n\nCOVID-19 Classroom Expectations\nStudents, faculty and staff must comply with University expectations regarding appropriate classroom behavior, including those outlined below and in the COVID-19 Expectations for Students. With respect to classroom procedures, this includes:\nPolicies regarding masking, social distancing and other public health measures evolve as the situation changes. Students are responsible for understanding and complying with current University, state and city requirements. In some classes, masking and/or social distancing may be required as a result of an Americans with Disabilities Act (ADA) accommodation for the instructor or a student in the class even when not generally required on campus. In such cases, the instructor will notify the class.\nIf a student fails to comply with the COVID-19 Expectations for Students or other University expectations related to COVID-19, the instructor may ask the student to leave the class. The instructor is asked to report the incident to the Office of Community Standards for additional follow-up.\n\nIf you’re feeling sick…\nMaintaining the health of the community remains our priority. If you are experiencing any symptoms of COVID do not attend class. Follow the steps outlined on the NU sites for testing, isolation and reporting a positive case. Next, contact me as soon as possible to arrange to complete coursework.\nShould public health recommendations prevent in-person class from being held on a given day, I or the university will notify students.\n\n\n\nDiversity, Equity, and Inclusion\nThis course strives to be an inclusive learning community, respecting those of differing backgrounds and beliefs. As a community, we aim to be respectful to all students in this class, regardless of race, ethnicity, socio-economic status, religion, gender identity or sexual orientation."
  },
  {
    "objectID": "course-syllabus.html#textbooks",
    "href": "course-syllabus.html#textbooks",
    "title": "Corpus Studies (Music 348/448) Syllabus",
    "section": "Textbooks",
    "text": "Textbooks\nThere is no textbook for this course, and most of the materials will be available on Canvas or the course website (or both). Many of the readings will be taken from the forthcoming Oxford Handbook of Music and Corpus Studies, edited by Daniel Shanahan, Ashley Burgoyne, and Ian Quinn.\nHaving said that, you should sign up for a free account for Posit Cloud (formerly RStudio Cloud), where many of the class notebooks will be held.I would also recommend downloading R and RStudio onto your personal machine, if possible.\nAlthough not required, I would highly recommend having a look at:\n\nR for Data Science by Garret Grolemund and Hadley Wickham\nThe Humdrum User Guide\nThe music21 documentation\nThe Oxford Handbook of Music and Corpus Studies"
  },
  {
    "objectID": "course-syllabus.html#support-for-wellness-and-mental-health",
    "href": "course-syllabus.html#support-for-wellness-and-mental-health",
    "title": "Corpus Studies (Music 348/448) Syllabus",
    "section": "Support for Wellness and Mental Health",
    "text": "Support for Wellness and Mental Health\nNorthwestern University is committed to supporting the wellness of our students. Student Affairs has multiple resources to support student wellness and mental health. If you are feeling distressed or overwhelmed, please reach out for help. Students can access confidential resources through the Counseling and Psychological Services (CAPS), Religious and Spiritual Life (RSL) and the Center for Awareness, Response and Education (CARE). Additional information on all of the resources mentioned above can be found here:\nhttps://www.northwestern.edu/counseling/\nhttps://www.northwestern.edu/religious-life/\nhttps://www.northwestern.edu/care/\n\nHomework\nThere will be regular assignments in which you will be asked to respond to do one of the following:\n\nCritically reflect upon a reading about the history, methods, and dilemmas commonly found in corpus studies.\nWrite code that addresses a musical question (e.g. what’s the most common pitch transition in this group of pieces?)\nAnalyze a given collection of musical data.\n\nTypically, we will have reading reflections due on Mondays, and code-related questions relevant to those readings due on Wednesdays.\n\n\nMidterm Project\nThe goal of this class is for you to both understand corpus studies as a method with a long history, and for you to be able to incorporate these methods in your own research. There will be a midterm project that is primarily used a stepping stone into your final project, and it will consist of presenting a literature review in which you situate your own research question within the existing literature and propose a study that examines this question. You may use existing data, but you might find it more relevant to you if you use your own dataset. Therefore, this would be a good time to have a bulk of your data encoded, so that you are aware of the time needed to construct your corpus.\n\n\nFinal Project\nThe final project will be focused on a research question of your choosing, and will be broken up into several a peer-reviewed first draft, a presentation, and a final paper."
  },
  {
    "objectID": "course-syllabus.html#grading",
    "href": "course-syllabus.html#grading",
    "title": "Corpus Studies (Music 348/448) Syllabus",
    "section": "Grading",
    "text": "Grading\nThe final course grade will be calculated as follows:\n\n\n\nCategory\nPercentage\n\n\n\n\nReading Reflection Questions\n20%\n\n\nCode-focused assignments\n30%\n\n\nMidterm Literature Review\n15%\n\n\nFinal Project (First Draft)\n10%\n\n\nPresentation\n10%\n\n\nFinal Project (Final draft)\n15%\n\n\n\nThe final letter grade will be determined based on the following thresholds:\n\n\n\nLetter Grade\nFinal Course Grade\n\n\n\n\nA\n>= 93\n\n\nA-\n90 - 92.99\n\n\nB+\n87 - 89.99\n\n\nB\n83 - 86.99\n\n\nB-\n80 - 82.99\n\n\nC+\n77 - 79.99\n\n\nC\n73 - 76.99\n\n\nC-\n70 - 72.99\n\n\nD+\n67 - 69.99\n\n\nD\n63 - 66.99\n\n\nD-\n60 - 62.99\n\n\nF\n< 60"
  },
  {
    "objectID": "course-syllabus.html#schedule",
    "href": "course-syllabus.html#schedule",
    "title": "Corpus Studies (Music 348/448) Syllabus",
    "section": "Schedule",
    "text": "Schedule\n\n\n\n\n\n\n\n\n\nUnit\nWeek\nDate (Date)\nTopic\n\n\n\n\nRepresenting, Searching, and Counting with Symbolic Data\n1\nT (3/28; Northwestern Monday)\nIntroductions; a history of corpus-based approaches to music.\n\n\n\n1\nW (3/29)\nRepresenting musical data (kern, MIDI, XML, MEI)\n\n\n\n2\nM (4/3)\nPitches, melodic intervals and scale degrees\n\n\n\n2\nW (4/5)\nRepresenting Harmony\n\n\n\n3\nM (4/10)\nRepresenting Rhythm and Meter\n\n\n\n3\nW (4/12)\nCharacteristic Features and TF-IDF\n\n\n\n4\nM (4/17)\nn-grams and Transition Probabilities\n\n\n\n4\nW (4/19)\nFinding patterns\n\n\nConceptual Debates\n5\nM (4/24)\nMeasurements of Similarity\n\n\n\n5\nW (4/26)\nEntropy and Variability\n\n\n\n6\nM (5/1)\nKey-Finding Algorithms\n\n\n\n6\nW (5/3)\nKey-Finding Algorithms (continued)\n\n\n\n7\nM (5/8)\nFast Fourier Transforms\n\n\n\n7\nW (5/10)\nClustering and Authorship\n\n\nAudio Data\n8\nM (5/15)\nPerformance Data/the Mazurka Project\n\n\n\n8\nW (5/17)\nTempo flexibility, and other performance data\n\n\n\n9\nM (5/22)\nThe Spotify API\n\n\n\n9\nW (5/24)\nThe Spotify API (continued)\n\n\n\n10\nM (5/29)\nNo Classes (Memorial Day)\n\n\n\n10\nW (5/31)\nFinal Presentations"
  },
  {
    "objectID": "class_notes/week_5.html#christmas-or-not",
    "href": "class_notes/week_5.html#christmas-or-not",
    "title": "Week 5: Testing Hypotheses with Spotify",
    "section": "Christmas or Not?",
    "text": "Christmas or Not?\n\nchristmas <- get_playlist_audio_features(\"\", \"5OP7itTh52BMfZS1DJrdlv\")\nchristmas$christmas <- \"yes\"\n\nnot <- get_playlist_audio_features(\"\", \"6i2Qd6OpeRBAzxfscNXeWp\")\nnot$christmas <- \"no\"\nchristmas_not <-rbind(christmas, not)\n\nfit <- rpart(as.factor(christmas) ~ danceability + valence + tempo + liveness + tempo + mode, data=christmas_not)\n\n# plot tree \nrpart.plot(fit, uniform=TRUE, \n   main=\"Regression Tree for Christmas/Not\")\n\n\n\n\n\ntable(not$mode)\n\n\n 0  1 \n33 81"
  },
  {
    "objectID": "class_notes/week_6.html",
    "href": "class_notes/week_6.html",
    "title": "Week 6: Classifying",
    "section": "",
    "text": "Look at running a principal components analysis for authorship\nWork on some models for classifying data\nDiscuss how we might evaluate our models\n\n\n\nWe will be using a of libraries today:\n\nlibrary(spotifyr)\nlibrary(compmus)\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.2     ✔ purrr   1.0.1\n✔ tibble  3.2.1     ✔ dplyr   1.1.1\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.4     ✔ forcats 1.0.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(caret)\n\nLoading required package: lattice\n\nAttaching package: 'caret'\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\nlibrary(devtools)\n\nLoading required package: usethis\n\n#install_github(\"vqv/ggbiplot\")\nrequire(ggplot2)\nlibrary(ggbiplot)\n\nLoading required package: plyr\n------------------------------------------------------------------------------\nYou have loaded plyr after dplyr - this is likely to cause problems.\nIf you need functions from both plyr and dplyr, please load plyr first, then dplyr:\nlibrary(plyr); library(dplyr)\n------------------------------------------------------------------------------\n\nAttaching package: 'plyr'\n\nThe following objects are masked from 'package:dplyr':\n\n    arrange, count, desc, failwith, id, mutate, rename, summarise,\n    summarize\n\nThe following object is masked from 'package:purrr':\n\n    compact\n\nLoading required package: scales\n\nAttaching package: 'scales'\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\nLoading required package: grid\n\nlibrary(class)\n\n\n\n\n\n\n\nPCAs are often used for reducing dimensions when we have lots of variables but a model might be better suited from combining those variables. PCAs have also been used a fair bit to explore questions of authorship. Here we have a question of authorship using symbolic data taken from scores. We are trying to explore the music of Josquin.\nHere we load the data in:\n\ncomplete_data <- read.csv(\"attribution_data_new.csv\", na.strings=c(\"\",\"NA\"), header=T)\ncomplete_data <- complete_data[,-62]\n\nJesse Rodin’s Josquin Research Project has given levels of security for attribution, including pieces that we know are Josquin’s, those we think might be, and those which are more questionable.\n\n# Josquin attribution level 1 and palestrina\n\njosquin <- complete_data[complete_data$Composer == 'Josquin des Prez',-12]\n\njosquin_secure <- josquin[josquin$Attribution.Level <= 2 ,]\njosquin_secure$Composer <- as.character(josquin_secure$Composer)\njosquin_less_secure <- josquin[ josquin$Attribution.Level >= 3,]\n\n\n####Other composers\nbach <- complete_data[complete_data$Composer == \"Bach_Johann Sebastian\",-12]\nlarue <- complete_data[complete_data$Composer == \"la Rue_Pierre de\",-12]\npalestrina <- complete_data[complete_data$Composer == \"Palestrina_Giovanni Perluigi da\",-12]\nockeghem <- complete_data[complete_data$Composer == \"Johannes Ockeghem\",-12]\norto <- complete_data[complete_data$Composer == \"de Orto_Marbrianus\",-12]\ndufay <- complete_data[complete_data$Composer == \"Du Fay_Guillaume\",-12]\n\njosquin_bach <- rbind(josquin_secure, bach)\njosquin_palestrina <- rbind(josquin_secure, palestrina)\njosquin_larue <- rbind(josquin_secure, larue)\n\ncomparison <- rbind(josquin_secure, bach)\n\n\ncolumns_wanted <- c(5:11)  \nMatrix <- comparison[,columns_wanted]\nMatrix <- as.matrix(Matrix)\nMatrix[is.na(Matrix)] <- 0\n# log.pieces <- log(Matrix)\nlog.pieces <- log(Matrix)\n\nWarning in log(Matrix): NaNs produced\n\ncomposer <- comparison[,1]\n\nThis code runs the actual principal components analysis.\nIt also provides a scree plot, allowing us to see which components are the most heavily weighted. This can allow us to reduce the dimensions as we see fit.\n\n####principle component analysis.\n\npieces.pca <- prcomp(Matrix,\n                 center = TRUE,\n                 scale. = TRUE) \nplot(pieces.pca, type = \"l\", main=\"Principal Components Analysis\")\n\n\n\n\nIt’s worth taking some time to explore what each of these components actually means and how they’re weighted. PCA is weighting instances of parallel motion and similar motion pretty heavily, but negatively weighting pitch entropy and oblique motion. PC2 seems to be looking at nPVI and 9-8 suspensions.\n\nprint(pieces.pca)\n\nStandard deviations (1, .., p=7):\n[1] 1.8907847 0.9923828 0.8705046 0.8298104 0.7104739 0.5567648 0.4230672\n\nRotation (n x k) = (7 x 7):\n                         PC1         PC2        PC3         PC4         PC5\nnPVI_Entire       -0.2826479  0.52894566 -0.2336756  0.74429280 -0.17804201\nNine_Eight        -0.2553594  0.61806193  0.5670902 -0.41414816 -0.23123893\npitch_correlation -0.3244143  0.05847133 -0.7184471 -0.47933237 -0.36850440\npitch_entropy     -0.4038052  0.19724082 -0.1329848 -0.14687309  0.77362200\nparallel_motion    0.4444947  0.24809410 -0.1263420 -0.08782873 -0.20277222\nsimilar_motion     0.4682238  0.29107268 -0.1026235 -0.05294267  0.04450771\noblique_motion    -0.4120550 -0.38680631  0.2519115  0.11252328 -0.37073657\n                           PC6          PC7\nnPVI_Entire        0.006914729  0.001955825\nNine_Eight         0.076657435 -0.018255504\npitch_correlation  0.076213006  0.061699313\npitch_entropy     -0.387728754  0.099780551\nparallel_motion   -0.750907432 -0.334991080\nsimilar_motion     0.016773922  0.824891734\noblique_motion    -0.523249921  0.439584522\n\n\nAs we can see, about 65% of the variance is accounted for with the first two principal components:\n\nsummary(pieces.pca)\n\nImportance of components:\n                          PC1    PC2    PC3     PC4     PC5     PC6     PC7\nStandard deviation     1.8908 0.9924 0.8705 0.82981 0.71047 0.55676 0.42307\nProportion of Variance 0.5107 0.1407 0.1082 0.09837 0.07211 0.04428 0.02557\nCumulative Proportion  0.5107 0.6514 0.7597 0.85804 0.93015 0.97443 1.00000\n\n\nPlotting our two composers with the first two principal components.\n\ng <- ggbiplot(pieces.pca, obs.scale = 1, var.scale = 1, \n              groups = composer, ellipse = TRUE, \n              circle = TRUE)\ng <- g + scale_color_discrete(name = '')\ng <- g + theme(legend.direction = 'horizontal', \n               legend.position = 'top') +\n               theme_bw()\nprint(g)\n\n\n\n# we can change the number of components\n# seven_component_model <- data.frame(pieces.pca$x[,1:8])\n\nWe can also look at how much each of these features is being weighted within the first two components.\n\ntheta <- seq(0,2*pi,length.out = 100)\ncircle <- data.frame(x = cos(theta), y = sin(theta))\np <- ggplot(circle,aes(x,y)) + geom_path()\n\nloadings <- data.frame(pieces.pca$rotation, \n                       .names = row.names(pieces.pca$rotation))\np + geom_text(data=loadings, \n              mapping=aes(x = PC1, y = PC2, label = .names, colour = .names)) +\n  coord_fixed(ratio=1) +\n  labs(x = \"PC1\", y = \"PC2\") +\n  theme_bw()\n\n\n\n\n\n\n\nA classifier is a model that assigns a label to data based on the input. There are many types of classifiers, and we will be evaluating various models throughout the week.\nOur goal will be to train a model on the features generally associated with a category, and then test the accuracy of that model. For now, a good starting point might be our Christmas Song question from last week.\n\n\n\nFirst, let’s get the data and add a column that tells us whether it’s a Christmas song or not\n\n### get the data and add yes/no column.\nchristmas <- get_playlist_audio_features(\"\", \"5OP7itTh52BMfZS1DJrdlv\")\nchristmas$christmas <- \"yes\"\n\nnot <- get_playlist_audio_features(\"\", \"6i2Qd6OpeRBAzxfscNXeWp\")\nnot$christmas <- \"no\"\n\n## combine the two datasets and get the columns we want to use.\nchristmas_subset <-rbind(christmas, not)\nchristmas_subset <- christmas_subset %>% \n    select(c(\"christmas\", \"acousticness\", \"liveness\", \"danceability\", \"loudness\", \"speechiness\", \"valence\"))\n\nNow we can use the createDataPartition function from the caret library to create a testing and a training dataset. Here, I’ve chosen a 70/30 partition of training and testing, but you can adjust as you see fit.\n\nTrain <- createDataPartition(christmas_subset$christmas, p=0.7, list=FALSE)\ntraining <- christmas_subset[ Train, ]\ntesting <- christmas_subset[ -Train, ]\n\nWe can pretty easily implement something like a neural network, using our training dataset to train it:\n\n\n# weights:  9\ninitial  value 109.013980 \niter  10 value 99.822187\niter  20 value 92.753186\niter  30 value 89.757107\niter  40 value 87.983746\niter  50 value 87.505147\niter  60 value 87.225130\niter  70 value 86.931696\niter  80 value 86.821810\niter  90 value 86.787236\niter 100 value 86.721401\nfinal  value 86.721401 \nstopped after 100 iterations\n# weights:  25\ninitial  value 116.804506 \niter  10 value 98.426532\niter  20 value 78.689701\niter  30 value 68.969735\niter  40 value 52.270500\niter  50 value 41.784200\niter  60 value 41.263983\niter  70 value 41.263097\nfinal  value 41.263095 \nconverged\n# weights:  41\ninitial  value 124.302082 \niter  10 value 94.246262\niter  20 value 76.142688\niter  30 value 71.062331\niter  40 value 65.810110\niter  50 value 63.889012\niter  60 value 63.315223\niter  70 value 63.142900\niter  80 value 63.043260\niter  90 value 62.989277\niter 100 value 62.960232\nfinal  value 62.960232 \nstopped after 100 iterations\n# weights:  9\ninitial  value 130.794600 \niter  10 value 102.524799\niter  20 value 100.219927\nfinal  value 100.219607 \nconverged\n# weights:  25\ninitial  value 106.028971 \niter  10 value 96.392507\niter  20 value 89.032463\niter  30 value 88.414408\niter  40 value 88.099150\niter  50 value 88.061611\niter  60 value 88.050140\nfinal  value 88.049973 \nconverged\n# weights:  41\ninitial  value 108.495195 \niter  10 value 94.356809\niter  20 value 88.381768\niter  30 value 88.151901\niter  40 value 88.135345\niter  50 value 88.108483\niter  60 value 88.038325\niter  70 value 88.019494\niter  80 value 88.004154\niter  90 value 88.001696\niter 100 value 88.001011\nfinal  value 88.001011 \nstopped after 100 iterations\n# weights:  9\ninitial  value 104.705923 \niter  10 value 94.455823\niter  20 value 91.648447\niter  30 value 90.098847\niter  40 value 89.479893\niter  50 value 89.062165\niter  60 value 88.734022\niter  70 value 88.586593\niter  80 value 88.508505\niter  90 value 88.498008\niter 100 value 88.480059\nfinal  value 88.480059 \nstopped after 100 iterations\n# weights:  25\ninitial  value 106.466357 \niter  10 value 91.663647\niter  20 value 73.129050\niter  30 value 60.370042\niter  40 value 56.270839\niter  50 value 55.204016\niter  60 value 55.018822\niter  70 value 54.827348\niter  80 value 54.456546\niter  90 value 54.274992\niter 100 value 53.920170\nfinal  value 53.920170 \nstopped after 100 iterations\n# weights:  41\ninitial  value 105.323601 \niter  10 value 96.757382\niter  20 value 80.830910\niter  30 value 64.245543\niter  40 value 54.157355\niter  50 value 47.566475\niter  60 value 44.798486\niter  70 value 43.828741\niter  80 value 43.477462\niter  90 value 43.349957\niter 100 value 43.049377\nfinal  value 43.049377 \nstopped after 100 iterations\n# weights:  9\ninitial  value 103.594601 \niter  10 value 102.335954\niter  20 value 94.790740\nfinal  value 93.804604 \nconverged\n# weights:  25\ninitial  value 102.920508 \niter  10 value 100.370674\niter  20 value 96.371249\niter  30 value 81.528530\niter  40 value 78.944145\niter  50 value 78.938865\niter  50 value 78.938864\niter  50 value 78.938864\nfinal  value 78.938864 \nconverged\n# weights:  41\ninitial  value 103.566296 \niter  10 value 97.463666\niter  20 value 92.762123\niter  30 value 85.359448\niter  40 value 83.112275\nfinal  value 83.110983 \nconverged\n# weights:  9\ninitial  value 111.757313 \niter  10 value 102.628734\niter  20 value 100.776944\niter  30 value 98.915634\nfinal  value 98.907923 \nconverged\n# weights:  25\ninitial  value 103.047938 \niter  10 value 102.504355\niter  20 value 100.037399\niter  30 value 98.818231\niter  40 value 98.685930\niter  50 value 98.679572\niter  60 value 98.672820\nfinal  value 98.672445 \nconverged\n# weights:  41\ninitial  value 103.892077 \niter  10 value 100.591855\niter  20 value 98.721614\niter  30 value 98.633211\niter  40 value 98.631831\niter  50 value 98.631251\niter  60 value 98.630811\niter  70 value 98.625955\niter  80 value 98.624066\nfinal  value 98.624055 \nconverged\n# weights:  9\ninitial  value 105.075548 \niter  10 value 102.161890\niter  20 value 98.248055\niter  30 value 92.457010\niter  40 value 91.561177\niter  50 value 90.738094\niter  60 value 90.454864\niter  70 value 90.110087\niter  80 value 89.846953\niter  90 value 89.815886\niter 100 value 89.796340\nfinal  value 89.796340 \nstopped after 100 iterations\n# weights:  25\ninitial  value 102.679027 \niter  10 value 99.947429\niter  20 value 88.337602\niter  30 value 85.631860\niter  40 value 85.331310\niter  50 value 85.052107\niter  60 value 84.967670\niter  70 value 84.885741\niter  80 value 84.782308\niter  90 value 84.669732\niter 100 value 84.632841\nfinal  value 84.632841 \nstopped after 100 iterations\n# weights:  41\ninitial  value 123.283160 \niter  10 value 98.225005\niter  20 value 89.760174\niter  30 value 83.014861\niter  40 value 71.617247\niter  50 value 68.133899\niter  60 value 66.499083\niter  70 value 65.602236\niter  80 value 64.577496\niter  90 value 63.622224\niter 100 value 63.506177\nfinal  value 63.506177 \nstopped after 100 iterations\n# weights:  9\ninitial  value 103.752763 \niter  10 value 100.830304\niter  20 value 81.798335\niter  30 value 77.396862\niter  40 value 74.151094\niter  50 value 73.754359\niter  60 value 73.621172\niter  70 value 73.340448\niter  80 value 72.949038\niter  90 value 72.767388\niter 100 value 72.490337\nfinal  value 72.490337 \nstopped after 100 iterations\n# weights:  25\ninitial  value 104.665826 \niter  10 value 100.082123\niter  20 value 82.644577\niter  30 value 81.074171\niter  40 value 80.878698\niter  50 value 80.812496\niter  60 value 80.781925\niter  70 value 80.735657\niter  80 value 80.585675\niter  90 value 80.489808\niter 100 value 80.360239\nfinal  value 80.360239 \nstopped after 100 iterations\n# weights:  41\ninitial  value 108.379558 \niter  10 value 93.794587\niter  20 value 76.463650\niter  30 value 75.185859\niter  40 value 70.248788\niter  50 value 60.759398\niter  60 value 59.214064\niter  70 value 59.195903\nfinal  value 59.195885 \nconverged\n# weights:  9\ninitial  value 103.717846 \niter  10 value 91.603370\niter  20 value 86.155131\nfinal  value 86.150397 \nconverged\n# weights:  25\ninitial  value 112.366900 \niter  10 value 89.887357\niter  20 value 83.771236\niter  30 value 83.464015\niter  40 value 83.191907\niter  50 value 83.187712\nfinal  value 83.187697 \nconverged\n# weights:  41\ninitial  value 116.893640 \niter  10 value 97.899262\niter  20 value 86.319464\niter  30 value 83.678138\niter  40 value 83.439057\niter  50 value 83.397978\niter  60 value 83.208313\niter  70 value 83.126615\niter  80 value 83.116770\nfinal  value 83.116569 \nconverged\n# weights:  9\ninitial  value 106.887489 \niter  10 value 97.565929\niter  20 value 79.254260\niter  30 value 75.969731\niter  40 value 74.863519\niter  50 value 74.304143\niter  60 value 73.976551\niter  70 value 73.875094\niter  80 value 73.811131\niter  90 value 73.717397\niter 100 value 73.688047\nfinal  value 73.688047 \nstopped after 100 iterations\n# weights:  25\ninitial  value 114.401670 \niter  10 value 86.631393\niter  20 value 76.255200\niter  30 value 74.730509\niter  40 value 71.972483\niter  50 value 71.285973\niter  60 value 71.167137\niter  70 value 71.132536\niter  80 value 71.068725\niter  90 value 71.033029\niter 100 value 70.765802\nfinal  value 70.765802 \nstopped after 100 iterations\n# weights:  41\ninitial  value 107.982016 \niter  10 value 96.533685\niter  20 value 75.835126\niter  30 value 75.150173\niter  40 value 74.717942\niter  50 value 74.074523\niter  60 value 73.989534\niter  70 value 73.953575\niter  80 value 73.888720\niter  90 value 73.521200\niter 100 value 72.092285\nfinal  value 72.092285 \nstopped after 100 iterations\n# weights:  9\ninitial  value 105.052004 \niter  10 value 101.592838\niter  20 value 88.998387\niter  30 value 88.240560\niter  40 value 88.081480\niter  50 value 87.940713\niter  60 value 87.906657\niter  70 value 87.138710\niter  80 value 86.861182\niter  90 value 83.504583\niter 100 value 82.708773\nfinal  value 82.708773 \nstopped after 100 iterations\n# weights:  25\ninitial  value 116.623322 \niter  10 value 86.366274\niter  20 value 80.590448\niter  30 value 75.635315\niter  40 value 71.784642\niter  50 value 67.729586\niter  60 value 67.468587\niter  70 value 67.381280\niter  80 value 67.338770\niter  90 value 67.337911\niter 100 value 67.323797\nfinal  value 67.323797 \nstopped after 100 iterations\n# weights:  41\ninitial  value 103.276928 \niter  10 value 94.118926\niter  20 value 90.406459\niter  30 value 88.509407\niter  40 value 87.855484\niter  50 value 87.708877\niter  60 value 87.673590\niter  70 value 87.611448\niter  80 value 87.605925\niter  90 value 87.577945\nfinal  value 87.577725 \nconverged\n# weights:  9\ninitial  value 116.689830 \niter  10 value 100.567732\niter  20 value 92.609541\niter  30 value 92.548576\nfinal  value 92.548571 \nconverged\n# weights:  25\ninitial  value 103.602407 \niter  10 value 100.953375\niter  20 value 92.004457\niter  30 value 90.219688\niter  40 value 89.900308\niter  50 value 89.895509\nfinal  value 89.895376 \nconverged\n# weights:  41\ninitial  value 112.835606 \niter  10 value 97.536868\niter  20 value 90.833726\niter  30 value 89.927426\niter  40 value 89.690691\niter  50 value 89.528614\niter  60 value 89.101177\niter  70 value 89.059680\niter  80 value 89.056365\nfinal  value 89.056283 \nconverged\n# weights:  9\ninitial  value 103.881765 \niter  10 value 101.510470\niter  20 value 96.855779\niter  30 value 96.852552\niter  40 value 96.833202\niter  50 value 96.827901\niter  60 value 96.821242\nfinal  value 96.820040 \nconverged\n# weights:  25\ninitial  value 103.577588 \niter  10 value 95.593733\niter  20 value 77.318431\niter  30 value 74.478249\niter  40 value 73.041631\niter  50 value 72.788449\niter  60 value 72.707704\niter  70 value 72.320461\niter  80 value 71.985988\niter  90 value 71.595372\niter 100 value 71.413155\nfinal  value 71.413155 \nstopped after 100 iterations\n# weights:  41\ninitial  value 103.398167 \niter  10 value 95.594721\niter  20 value 83.741977\niter  30 value 79.030808\niter  40 value 73.328041\niter  50 value 71.423314\niter  60 value 69.313689\niter  70 value 69.067856\niter  80 value 68.887348\niter  90 value 68.747859\niter 100 value 68.664376\nfinal  value 68.664376 \nstopped after 100 iterations\n# weights:  9\ninitial  value 105.614599 \niter  10 value 92.376240\niter  20 value 89.534060\niter  30 value 89.028966\niter  40 value 86.163496\niter  50 value 85.639521\niter  60 value 85.448167\niter  70 value 85.437844\niter  80 value 85.426655\niter  90 value 85.424036\niter 100 value 85.403438\nfinal  value 85.403438 \nstopped after 100 iterations\n# weights:  25\ninitial  value 105.797100 \niter  10 value 100.780459\niter  20 value 86.376043\niter  30 value 79.696231\niter  40 value 75.534673\niter  50 value 75.052181\niter  60 value 74.852231\niter  70 value 74.486727\niter  80 value 72.552381\niter  90 value 71.457579\niter 100 value 71.010444\nfinal  value 71.010444 \nstopped after 100 iterations\n# weights:  41\ninitial  value 112.005184 \niter  10 value 91.126796\niter  20 value 86.197615\niter  30 value 82.125889\niter  40 value 79.851189\niter  50 value 79.840121\nfinal  value 79.838798 \nconverged\n# weights:  9\ninitial  value 122.220074 \niter  10 value 102.662210\niter  20 value 92.905437\nfinal  value 92.867511 \nconverged\n# weights:  25\ninitial  value 108.869076 \niter  10 value 93.872402\niter  20 value 92.406659\niter  30 value 92.399280\nfinal  value 92.399262 \nconverged\n# weights:  41\ninitial  value 128.978282 \niter  10 value 100.441483\niter  20 value 92.198250\niter  30 value 91.605565\niter  40 value 91.575421\niter  50 value 91.574261\niter  60 value 91.567446\niter  70 value 91.566757\nfinal  value 91.566668 \nconverged\n# weights:  9\ninitial  value 104.443202 \niter  10 value 103.186055\niter  20 value 87.714108\niter  30 value 86.399097\niter  40 value 83.298802\niter  50 value 81.979076\niter  60 value 80.728889\niter  70 value 80.650932\niter  80 value 80.543376\niter  90 value 80.447409\niter 100 value 80.401023\nfinal  value 80.401023 \nstopped after 100 iterations\n# weights:  25\ninitial  value 104.577753 \niter  10 value 90.632851\niter  20 value 86.094967\niter  30 value 82.518450\niter  40 value 81.778823\niter  50 value 80.396461\niter  60 value 78.245430\niter  70 value 77.132196\niter  80 value 76.464236\niter  90 value 76.289489\niter 100 value 76.226295\nfinal  value 76.226295 \nstopped after 100 iterations\n# weights:  41\ninitial  value 104.457775 \niter  10 value 90.778443\niter  20 value 84.387606\niter  30 value 71.913391\niter  40 value 70.737872\niter  50 value 69.655745\niter  60 value 67.308590\niter  70 value 66.456094\niter  80 value 63.526908\niter  90 value 62.103054\niter 100 value 61.366516\nfinal  value 61.366516 \nstopped after 100 iterations\n# weights:  9\ninitial  value 110.329275 \niter  10 value 102.343518\niter  20 value 101.880450\niter  30 value 100.336327\niter  40 value 98.806027\niter  50 value 97.965424\niter  60 value 97.434736\niter  70 value 97.171932\niter  80 value 97.164499\niter  90 value 97.150263\niter 100 value 97.147165\nfinal  value 97.147165 \nstopped after 100 iterations\n# weights:  25\ninitial  value 113.859070 \niter  10 value 99.815735\niter  20 value 88.400534\niter  30 value 78.462551\niter  40 value 67.643821\niter  50 value 64.021538\niter  60 value 60.138317\niter  70 value 58.621970\niter  80 value 57.732972\niter  90 value 57.034988\niter 100 value 56.249701\nfinal  value 56.249701 \nstopped after 100 iterations\n# weights:  41\ninitial  value 120.444441 \niter  10 value 94.619408\niter  20 value 86.586346\niter  30 value 80.041371\niter  40 value 76.449434\niter  50 value 62.186741\niter  60 value 53.588018\niter  70 value 49.989708\niter  80 value 47.939106\niter  90 value 47.293952\niter 100 value 46.726758\nfinal  value 46.726758 \nstopped after 100 iterations\n# weights:  9\ninitial  value 115.455510 \niter  10 value 96.579140\niter  20 value 92.947593\nfinal  value 92.943462 \nconverged\n# weights:  25\ninitial  value 113.873629 \niter  10 value 94.998327\niter  20 value 92.825089\niter  30 value 91.465016\niter  40 value 91.391685\niter  50 value 91.275005\niter  60 value 91.047354\niter  70 value 90.993605\nfinal  value 90.992640 \nconverged\n# weights:  41\ninitial  value 106.661302 \niter  10 value 94.628873\niter  20 value 91.564628\niter  30 value 91.151863\niter  40 value 90.886017\niter  50 value 90.826942\niter  60 value 90.817355\niter  70 value 90.810773\nfinal  value 90.810753 \nconverged\n# weights:  9\ninitial  value 108.359276 \niter  10 value 102.261095\niter  20 value 99.709934\niter  30 value 97.171362\niter  40 value 97.084859\niter  50 value 97.062258\niter  60 value 97.056343\niter  70 value 97.046074\niter  80 value 97.044452\niter  90 value 97.042156\niter 100 value 97.041640\nfinal  value 97.041640 \nstopped after 100 iterations\n# weights:  25\ninitial  value 113.058526 \niter  10 value 102.345909\niter  20 value 102.180318\niter  30 value 101.059424\niter  40 value 98.682287\niter  50 value 98.342419\niter  60 value 98.332209\niter  70 value 98.185851\niter  80 value 96.274726\niter  90 value 95.763823\niter 100 value 95.652430\nfinal  value 95.652430 \nstopped after 100 iterations\n# weights:  41\ninitial  value 103.378356 \niter  10 value 92.759295\niter  20 value 85.795870\niter  30 value 74.966894\niter  40 value 66.214359\niter  50 value 61.973229\niter  60 value 59.821027\niter  70 value 57.465059\niter  80 value 54.959703\niter  90 value 54.293532\niter 100 value 53.651192\nfinal  value 53.651192 \nstopped after 100 iterations\n# weights:  9\ninitial  value 106.354010 \niter  10 value 99.220414\niter  20 value 83.070856\niter  30 value 80.722983\niter  40 value 79.731308\niter  50 value 75.574779\niter  60 value 74.690990\niter  70 value 74.621335\niter  80 value 73.742056\niter  90 value 73.576264\niter 100 value 73.494806\nfinal  value 73.494806 \nstopped after 100 iterations\n# weights:  25\ninitial  value 104.439677 \niter  10 value 89.240564\niter  20 value 81.103816\niter  30 value 78.753065\niter  40 value 75.604549\niter  50 value 71.534875\niter  60 value 65.413707\niter  70 value 61.634458\niter  80 value 58.669926\niter  90 value 57.755268\niter 100 value 57.290206\nfinal  value 57.290206 \nstopped after 100 iterations\n# weights:  41\ninitial  value 109.760800 \niter  10 value 98.958035\niter  20 value 83.290287\niter  30 value 72.434239\niter  40 value 71.105180\niter  50 value 71.096922\niter  60 value 71.096653\niter  60 value 71.096653\niter  60 value 71.096653\nfinal  value 71.096653 \nconverged\n# weights:  9\ninitial  value 107.395671 \niter  10 value 98.399478\niter  20 value 90.977240\nfinal  value 90.976954 \nconverged\n# weights:  25\ninitial  value 104.604744 \niter  10 value 94.288055\niter  20 value 90.346526\niter  30 value 89.511317\niter  40 value 88.713083\niter  50 value 88.686120\niter  60 value 88.679471\niter  70 value 88.678451\niter  70 value 88.678450\niter  70 value 88.678450\nfinal  value 88.678450 \nconverged\n# weights:  41\ninitial  value 106.748026 \niter  10 value 94.273376\niter  20 value 89.836457\niter  30 value 89.063090\niter  40 value 88.949706\niter  50 value 88.788540\niter  60 value 88.757094\niter  70 value 88.736496\niter  80 value 88.675337\niter  90 value 88.663125\niter 100 value 88.659383\nfinal  value 88.659383 \nstopped after 100 iterations\n# weights:  9\ninitial  value 104.561469 \niter  10 value 95.587572\niter  20 value 81.922614\niter  30 value 80.067170\niter  40 value 79.882501\niter  50 value 79.185963\niter  60 value 79.022212\niter  70 value 78.975065\niter  80 value 78.949383\niter  90 value 78.915572\niter 100 value 78.896163\nfinal  value 78.896163 \nstopped after 100 iterations\n# weights:  25\ninitial  value 104.109636 \niter  10 value 90.290680\niter  20 value 81.586519\niter  30 value 75.416563\niter  40 value 67.623498\niter  50 value 66.665474\niter  60 value 65.859850\niter  70 value 65.535453\niter  80 value 65.389474\niter  90 value 65.145846\niter 100 value 64.231931\nfinal  value 64.231931 \nstopped after 100 iterations\n# weights:  41\ninitial  value 196.371816 \niter  10 value 95.883558\niter  20 value 81.852246\niter  30 value 76.654336\niter  40 value 67.479175\niter  50 value 58.677653\niter  60 value 57.121708\niter  70 value 56.624656\niter  80 value 55.843847\niter  90 value 55.419221\niter 100 value 54.592537\nfinal  value 54.592537 \nstopped after 100 iterations\n# weights:  9\ninitial  value 111.585738 \niter  10 value 102.887994\niter  20 value 102.749744\niter  30 value 102.064224\nfinal  value 102.064217 \nconverged\n# weights:  25\ninitial  value 103.119832 \niter  10 value 93.617840\niter  20 value 91.130633\niter  30 value 88.672027\niter  40 value 75.978576\niter  50 value 72.941815\niter  60 value 71.464567\niter  70 value 70.814876\niter  80 value 70.339205\niter  90 value 69.611143\niter 100 value 69.402245\nfinal  value 69.402245 \nstopped after 100 iterations\n# weights:  41\ninitial  value 115.551771 \niter  10 value 100.801841\niter  20 value 87.813872\niter  30 value 77.004659\niter  40 value 72.169854\niter  50 value 68.179956\niter  60 value 67.348337\niter  70 value 65.299809\niter  80 value 65.277575\nfinal  value 65.277534 \nconverged\n# weights:  9\ninitial  value 103.436061 \niter  10 value 95.038920\niter  20 value 94.956339\nfinal  value 94.956336 \nconverged\n# weights:  25\ninitial  value 111.053491 \niter  10 value 100.847142\niter  20 value 94.547817\niter  30 value 94.372986\niter  40 value 94.307062\niter  50 value 93.751235\niter  60 value 92.706062\niter  70 value 92.006859\niter  80 value 91.928631\niter  90 value 91.926301\nfinal  value 91.926244 \nconverged\n# weights:  41\ninitial  value 104.745842 \niter  10 value 101.768834\niter  20 value 93.297854\niter  30 value 92.054234\niter  40 value 91.994862\niter  50 value 91.889673\niter  60 value 91.802779\niter  70 value 91.786211\niter  80 value 91.703788\niter  90 value 91.556517\niter 100 value 91.282861\nfinal  value 91.282861 \nstopped after 100 iterations\n# weights:  9\ninitial  value 102.871819 \niter  10 value 100.434233\niter  20 value 92.183781\niter  30 value 91.241525\niter  40 value 90.927977\niter  50 value 89.761845\niter  60 value 89.218262\niter  70 value 89.166494\niter  80 value 89.106129\niter  90 value 89.001609\niter 100 value 88.957816\nfinal  value 88.957816 \nstopped after 100 iterations\n# weights:  25\ninitial  value 133.828442 \niter  10 value 100.992399\niter  20 value 79.241786\niter  30 value 74.105772\niter  40 value 72.369300\niter  50 value 71.613872\niter  60 value 71.297295\niter  70 value 69.714687\niter  80 value 69.046687\niter  90 value 68.894273\niter 100 value 68.596898\nfinal  value 68.596898 \nstopped after 100 iterations\n# weights:  41\ninitial  value 103.162591 \niter  10 value 91.258893\niter  20 value 77.452148\niter  30 value 75.203754\niter  40 value 74.826587\niter  50 value 74.568674\niter  60 value 74.221888\niter  70 value 73.913715\niter  80 value 73.818359\niter  90 value 72.961724\niter 100 value 71.112663\nfinal  value 71.112663 \nstopped after 100 iterations\n# weights:  9\ninitial  value 111.294153 \niter  10 value 86.978560\niter  20 value 85.403923\niter  30 value 85.095627\niter  40 value 85.021882\niter  50 value 85.002743\niter  60 value 84.980672\niter  70 value 84.949016\niter  80 value 84.942078\niter  90 value 84.941395\niter 100 value 84.928187\nfinal  value 84.928187 \nstopped after 100 iterations\n# weights:  25\ninitial  value 108.479080 \niter  10 value 87.772988\niter  20 value 79.728363\niter  30 value 69.773048\niter  40 value 61.956432\niter  50 value 61.858308\niter  60 value 61.857236\nfinal  value 61.857227 \nconverged\n# weights:  41\ninitial  value 117.173307 \niter  10 value 98.384744\niter  20 value 81.859344\niter  30 value 72.356768\niter  40 value 65.036475\niter  50 value 59.705507\niter  60 value 48.422378\niter  70 value 46.514338\niter  80 value 45.631837\niter  90 value 44.914176\niter 100 value 44.865667\nfinal  value 44.865667 \nstopped after 100 iterations\n# weights:  9\ninitial  value 110.919337 \niter  10 value 103.568451\niter  20 value 100.158326\niter  30 value 90.428631\nfinal  value 90.417318 \nconverged\n# weights:  25\ninitial  value 104.792544 \niter  10 value 91.034469\niter  20 value 88.570942\niter  30 value 88.313829\nfinal  value 88.313722 \nconverged\n# weights:  41\ninitial  value 127.828617 \niter  10 value 96.154587\niter  20 value 90.223907\niter  30 value 88.948294\niter  40 value 88.664364\niter  50 value 88.590443\niter  60 value 88.576479\nfinal  value 88.570969 \nconverged\n# weights:  9\ninitial  value 115.083579 \niter  10 value 103.636790\niter  20 value 103.536160\niter  30 value 101.344039\niter  40 value 99.953238\niter  50 value 99.854527\niter  60 value 99.246075\niter  70 value 99.182592\niter  80 value 99.173452\niter  90 value 99.171863\nfinal  value 99.171861 \nconverged\n# weights:  25\ninitial  value 104.636297 \niter  10 value 87.789671\niter  20 value 82.148091\niter  30 value 73.708709\niter  40 value 70.736290\niter  50 value 69.920101\niter  60 value 69.859624\niter  70 value 69.848173\niter  80 value 69.820429\niter  90 value 69.796990\niter 100 value 69.785025\nfinal  value 69.785025 \nstopped after 100 iterations\n# weights:  41\ninitial  value 108.181162 \niter  10 value 101.761938\niter  20 value 83.879278\niter  30 value 80.779784\niter  40 value 73.032230\niter  50 value 69.417892\niter  60 value 68.713188\niter  70 value 67.915103\niter  80 value 66.568397\niter  90 value 64.169700\niter 100 value 61.900671\nfinal  value 61.900671 \nstopped after 100 iterations\n# weights:  9\ninitial  value 116.527842 \niter  10 value 103.963142\niter  20 value 103.143998\niter  30 value 102.607121\nfinal  value 102.572609 \nconverged\n# weights:  25\ninitial  value 109.303717 \niter  10 value 89.904344\niter  20 value 86.860841\niter  30 value 81.516044\niter  40 value 71.152556\niter  50 value 63.830157\niter  60 value 62.725757\niter  70 value 62.585832\niter  80 value 62.556810\niter  90 value 62.495313\niter 100 value 62.203064\nfinal  value 62.203064 \nstopped after 100 iterations\n# weights:  41\ninitial  value 134.392680 \niter  10 value 100.156097\niter  20 value 86.747708\niter  30 value 76.167844\niter  40 value 70.647767\niter  50 value 66.946098\niter  60 value 62.728448\niter  70 value 53.758219\niter  80 value 48.073620\niter  90 value 43.906001\niter 100 value 43.831804\nfinal  value 43.831804 \nstopped after 100 iterations\n# weights:  9\ninitial  value 120.294973 \niter  10 value 103.934472\niter  20 value 92.561957\niter  30 value 91.476016\nfinal  value 91.475956 \nconverged\n# weights:  25\ninitial  value 107.597756 \niter  10 value 95.559274\niter  20 value 91.132503\niter  30 value 91.082296\niter  40 value 91.059054\niter  50 value 91.051857\nfinal  value 91.047376 \nconverged\n# weights:  41\ninitial  value 106.547154 \niter  10 value 97.470766\niter  20 value 91.693052\niter  30 value 91.344168\niter  40 value 91.081313\niter  50 value 90.968739\niter  60 value 90.683808\niter  70 value 90.666493\niter  80 value 90.664297\niter  90 value 90.663573\niter 100 value 90.662259\nfinal  value 90.662259 \nstopped after 100 iterations\n# weights:  9\ninitial  value 104.130679 \niter  10 value 89.959401\niter  20 value 86.731981\niter  30 value 86.710942\niter  40 value 86.602153\niter  50 value 86.136012\niter  60 value 85.928770\niter  70 value 85.604047\niter  80 value 84.696363\niter  90 value 84.091976\niter 100 value 84.038288\nfinal  value 84.038288 \nstopped after 100 iterations\n# weights:  25\ninitial  value 110.570115 \niter  10 value 97.138957\niter  20 value 86.189782\niter  30 value 79.189952\niter  40 value 71.263509\niter  50 value 66.624275\niter  60 value 64.060231\niter  70 value 62.457609\niter  80 value 61.551871\niter  90 value 60.957694\niter 100 value 60.669482\nfinal  value 60.669482 \nstopped after 100 iterations\n# weights:  41\ninitial  value 104.454128 \niter  10 value 100.275649\niter  20 value 85.608411\niter  30 value 72.423786\niter  40 value 58.436694\niter  50 value 53.019069\niter  60 value 51.502022\niter  70 value 51.051894\niter  80 value 50.333362\niter  90 value 49.067850\niter 100 value 48.140197\nfinal  value 48.140197 \nstopped after 100 iterations\n# weights:  9\ninitial  value 113.017993 \niter  10 value 103.450169\niter  20 value 97.113497\niter  30 value 96.938599\niter  40 value 96.923040\niter  50 value 93.070628\niter  60 value 89.340019\niter  70 value 88.571412\niter  80 value 87.827079\niter  90 value 87.148625\niter 100 value 86.252550\nfinal  value 86.252550 \nstopped after 100 iterations\n# weights:  25\ninitial  value 103.882623 \niter  10 value 98.791791\niter  20 value 79.750824\niter  30 value 70.195890\niter  40 value 67.275517\niter  50 value 60.720195\niter  60 value 59.437540\niter  70 value 59.432040\nfinal  value 59.432013 \nconverged\n# weights:  41\ninitial  value 119.645732 \niter  10 value 97.036061\niter  20 value 85.799609\niter  30 value 84.717027\niter  40 value 74.472587\niter  50 value 71.244210\niter  60 value 66.803782\niter  70 value 63.162569\niter  80 value 58.561183\niter  90 value 54.589387\niter 100 value 51.834868\nfinal  value 51.834868 \nstopped after 100 iterations\n# weights:  9\ninitial  value 104.357642 \niter  10 value 101.471132\niter  20 value 91.647769\niter  30 value 90.744539\nfinal  value 90.744516 \nconverged\n# weights:  25\ninitial  value 104.807360 \niter  10 value 94.930296\niter  20 value 90.997543\niter  30 value 90.893710\niter  40 value 89.398738\niter  50 value 88.455387\niter  60 value 88.299603\niter  70 value 87.879097\niter  80 value 87.867806\niter  90 value 87.867541\niter 100 value 87.852766\nfinal  value 87.852766 \nstopped after 100 iterations\n# weights:  41\ninitial  value 117.089874 \niter  10 value 100.742639\niter  20 value 88.249681\niter  30 value 87.847529\niter  40 value 87.843032\niter  50 value 87.842912\niter  60 value 87.832964\niter  70 value 87.705742\niter  80 value 87.643393\niter  90 value 87.623372\niter 100 value 87.610605\nfinal  value 87.610605 \nstopped after 100 iterations\n# weights:  9\ninitial  value 106.702756 \niter  10 value 102.429858\niter  20 value 88.275261\niter  30 value 79.671760\niter  40 value 78.039637\niter  50 value 74.773495\niter  60 value 74.314102\niter  70 value 74.089882\niter  80 value 73.887543\niter  90 value 73.843184\niter 100 value 73.767819\nfinal  value 73.767819 \nstopped after 100 iterations\n# weights:  25\ninitial  value 106.643911 \niter  10 value 102.309499\niter  20 value 82.028870\niter  30 value 75.489785\niter  40 value 73.090450\niter  50 value 73.032449\niter  60 value 72.638153\niter  70 value 70.698626\niter  80 value 70.418538\niter  90 value 70.305860\niter 100 value 69.781813\nfinal  value 69.781813 \nstopped after 100 iterations\n# weights:  41\ninitial  value 105.169947 \niter  10 value 94.224132\niter  20 value 80.002561\niter  30 value 75.726729\niter  40 value 67.287239\niter  50 value 63.482363\niter  60 value 61.342112\niter  70 value 58.424949\niter  80 value 56.576939\niter  90 value 52.209856\niter 100 value 50.157657\nfinal  value 50.157657 \nstopped after 100 iterations\n# weights:  9\ninitial  value 107.669777 \niter  10 value 98.035189\niter  20 value 90.659319\niter  30 value 85.072644\niter  40 value 77.930935\niter  50 value 75.330301\niter  60 value 74.371610\niter  70 value 74.307225\niter  80 value 74.175431\niter  90 value 73.904069\niter 100 value 73.814388\nfinal  value 73.814388 \nstopped after 100 iterations\n# weights:  25\ninitial  value 107.409415 \niter  10 value 89.499567\niter  20 value 70.586979\niter  30 value 62.699240\niter  40 value 61.943217\niter  50 value 61.880942\niter  60 value 61.880399\nfinal  value 61.880393 \nconverged\n# weights:  41\ninitial  value 103.632475 \niter  10 value 81.548062\niter  20 value 67.385366\niter  30 value 59.282400\niter  40 value 51.967661\niter  50 value 50.321230\nfinal  value 50.312586 \nconverged\n# weights:  9\ninitial  value 109.541828 \niter  10 value 100.992532\niter  20 value 87.573837\niter  30 value 87.045959\niter  30 value 87.045959\niter  30 value 87.045959\nfinal  value 87.045959 \nconverged\n# weights:  25\ninitial  value 120.328098 \niter  10 value 99.976843\niter  20 value 86.313134\niter  30 value 83.711999\niter  40 value 82.687928\niter  50 value 82.077705\niter  60 value 82.066558\nfinal  value 82.065117 \nconverged\n# weights:  41\ninitial  value 110.877501 \niter  10 value 100.142854\niter  20 value 85.984679\niter  30 value 83.036542\niter  40 value 82.861240\niter  50 value 82.859871\niter  60 value 82.859238\niter  70 value 82.856835\niter  80 value 82.856216\niter  90 value 82.846204\niter 100 value 82.557114\nfinal  value 82.557114 \nstopped after 100 iterations\n# weights:  9\ninitial  value 103.730756 \niter  10 value 83.716197\niter  20 value 78.176599\niter  30 value 77.577930\niter  40 value 75.181527\niter  50 value 75.120057\niter  60 value 75.110464\niter  70 value 75.105159\niter  80 value 75.102795\niter  90 value 75.101597\niter 100 value 75.100883\nfinal  value 75.100883 \nstopped after 100 iterations\n# weights:  25\ninitial  value 105.954161 \niter  10 value 93.530861\niter  20 value 80.303409\niter  30 value 71.979876\niter  40 value 64.654228\niter  50 value 63.462771\niter  60 value 63.329721\niter  70 value 63.301159\niter  80 value 63.289221\niter  90 value 63.266487\niter 100 value 63.244264\nfinal  value 63.244264 \nstopped after 100 iterations\n# weights:  41\ninitial  value 121.305412 \niter  10 value 84.701181\niter  20 value 68.121512\niter  30 value 65.227317\niter  40 value 56.450124\niter  50 value 55.119977\niter  60 value 54.419485\niter  70 value 54.173216\niter  80 value 54.000054\niter  90 value 53.874768\niter 100 value 53.768862\nfinal  value 53.768862 \nstopped after 100 iterations\n# weights:  9\ninitial  value 115.950565 \niter  10 value 103.314130\niter  20 value 103.155902\niter  30 value 102.108821\nfinal  value 102.097833 \nconverged\n# weights:  25\ninitial  value 106.338269 \niter  10 value 92.363600\niter  20 value 80.534066\niter  30 value 67.144512\niter  40 value 63.775865\niter  50 value 63.575765\nfinal  value 63.573818 \nconverged\n# weights:  41\ninitial  value 109.554988 \niter  10 value 93.971502\niter  20 value 87.962198\niter  30 value 82.563943\niter  40 value 73.158762\niter  50 value 65.991229\niter  60 value 56.051920\niter  70 value 52.793643\niter  80 value 52.009243\niter  90 value 51.737743\niter 100 value 51.492657\nfinal  value 51.492657 \nstopped after 100 iterations\n# weights:  9\ninitial  value 118.348242 \niter  10 value 103.436674\niter  20 value 95.598256\niter  30 value 93.176183\nfinal  value 93.175912 \nconverged\n# weights:  25\ninitial  value 112.844142 \niter  10 value 100.678909\niter  20 value 92.392686\niter  30 value 91.922223\niter  40 value 91.606565\niter  50 value 91.594303\niter  60 value 91.589520\niter  70 value 91.533340\niter  80 value 90.022636\niter  90 value 90.002844\niter  90 value 90.002843\niter  90 value 90.002843\nfinal  value 90.002843 \nconverged\n# weights:  41\ninitial  value 108.344564 \niter  10 value 94.569510\niter  20 value 92.364142\niter  30 value 91.966556\niter  40 value 91.773890\niter  50 value 90.870360\niter  60 value 90.289901\niter  70 value 90.149420\niter  80 value 89.835994\niter  90 value 89.583235\niter 100 value 89.490529\nfinal  value 89.490529 \nstopped after 100 iterations\n# weights:  9\ninitial  value 108.411382 \niter  10 value 89.881061\niter  20 value 87.844405\niter  30 value 87.429014\niter  40 value 87.237779\niter  50 value 87.177746\niter  60 value 86.882973\niter  70 value 86.743405\niter  80 value 86.510020\niter  90 value 86.298102\niter 100 value 86.288223\nfinal  value 86.288223 \nstopped after 100 iterations\n# weights:  25\ninitial  value 109.701775 \niter  10 value 103.060605\niter  20 value 89.771322\niter  30 value 87.693877\niter  40 value 87.396035\niter  50 value 87.349068\niter  60 value 86.313715\niter  70 value 85.766343\niter  80 value 81.422822\niter  90 value 74.723583\niter 100 value 73.425325\nfinal  value 73.425325 \nstopped after 100 iterations\n# weights:  41\ninitial  value 122.981170 \niter  10 value 93.445483\niter  20 value 80.851201\niter  30 value 70.137636\niter  40 value 68.989917\niter  50 value 68.505158\niter  60 value 68.301484\niter  70 value 68.228934\niter  80 value 68.089367\niter  90 value 67.653243\niter 100 value 67.004894\nfinal  value 67.004894 \nstopped after 100 iterations\n# weights:  9\ninitial  value 110.935545 \niter  10 value 103.849465\niter  20 value 103.195022\niter  20 value 103.195022\nfinal  value 103.195022 \nconverged\n# weights:  25\ninitial  value 113.997625 \niter  10 value 89.182001\niter  20 value 85.441384\niter  30 value 83.319750\niter  40 value 80.223957\niter  50 value 80.211199\nfinal  value 80.211186 \nconverged\n# weights:  41\ninitial  value 105.615794 \niter  10 value 92.558731\niter  20 value 78.583648\niter  30 value 70.012288\niter  40 value 66.956103\niter  50 value 65.862864\niter  60 value 65.571986\niter  70 value 64.928832\niter  80 value 64.543092\niter  90 value 64.293669\niter 100 value 64.151334\nfinal  value 64.151334 \nstopped after 100 iterations\n# weights:  9\ninitial  value 104.273118 \niter  10 value 99.863023\niter  20 value 92.053209\nfinal  value 92.052805 \nconverged\n# weights:  25\ninitial  value 108.033271 \niter  10 value 96.861618\niter  20 value 91.597070\niter  30 value 90.473512\niter  40 value 90.285408\niter  50 value 90.260991\niter  60 value 90.221142\niter  70 value 90.167164\niter  80 value 90.164994\nfinal  value 90.164923 \nconverged\n# weights:  41\ninitial  value 110.484038 \niter  10 value 101.360535\niter  20 value 91.871129\niter  30 value 90.420174\niter  40 value 90.267625\niter  50 value 90.229010\niter  60 value 90.224264\niter  70 value 90.219803\niter  80 value 90.214393\niter  90 value 90.206355\niter 100 value 90.194519\nfinal  value 90.194519 \nstopped after 100 iterations\n# weights:  9\ninitial  value 110.157282 \niter  10 value 103.679534\niter  20 value 92.501832\niter  30 value 86.599346\niter  40 value 85.470079\niter  50 value 84.856987\niter  60 value 83.683625\niter  70 value 83.368930\niter  80 value 83.184159\niter  90 value 83.175277\niter 100 value 83.173941\nfinal  value 83.173941 \nstopped after 100 iterations\n# weights:  25\ninitial  value 109.793370 \niter  10 value 90.123577\niter  20 value 83.530753\niter  30 value 82.109957\niter  40 value 82.090477\niter  50 value 82.034698\niter  60 value 82.029443\niter  70 value 81.987148\niter  80 value 81.857116\niter  90 value 81.817381\niter 100 value 77.477982\nfinal  value 77.477982 \nstopped after 100 iterations\n# weights:  41\ninitial  value 111.031796 \niter  10 value 92.512714\niter  20 value 84.616432\niter  30 value 74.999835\niter  40 value 64.781664\niter  50 value 61.598454\niter  60 value 59.418654\niter  70 value 58.809918\niter  80 value 57.876038\niter  90 value 56.820475\niter 100 value 56.522906\nfinal  value 56.522906 \nstopped after 100 iterations\n# weights:  9\ninitial  value 102.178809 \niter  10 value 89.470102\niter  20 value 88.666007\niter  30 value 87.996700\niter  40 value 87.860388\niter  50 value 87.765989\niter  60 value 87.743759\niter  70 value 87.724572\niter  80 value 87.713687\niter  90 value 87.708683\niter 100 value 87.699586\nfinal  value 87.699586 \nstopped after 100 iterations\n# weights:  25\ninitial  value 102.656880 \niter  10 value 92.988620\niter  20 value 81.569242\niter  30 value 71.230646\niter  40 value 67.783222\niter  50 value 66.809279\niter  60 value 66.610966\niter  70 value 66.375314\niter  80 value 65.843442\niter  90 value 65.474006\niter 100 value 65.399809\nfinal  value 65.399809 \nstopped after 100 iterations\n# weights:  41\ninitial  value 125.026725 \niter  10 value 98.907334\niter  20 value 86.032995\niter  30 value 74.111858\niter  40 value 63.705256\niter  50 value 59.048704\niter  60 value 58.495258\niter  70 value 58.452187\niter  80 value 58.449514\niter  80 value 58.449513\niter  80 value 58.449513\nfinal  value 58.449513 \nconverged\n# weights:  9\ninitial  value 118.430029 \niter  10 value 101.598983\niter  20 value 99.644954\niter  30 value 92.137328\niter  40 value 91.761574\nfinal  value 91.761571 \nconverged\n# weights:  25\ninitial  value 117.257375 \niter  10 value 101.192637\niter  20 value 90.960869\niter  30 value 90.294391\niter  40 value 89.901921\niter  50 value 89.400869\niter  60 value 89.352244\niter  70 value 89.351348\nfinal  value 89.351346 \nconverged\n# weights:  41\ninitial  value 110.427806 \niter  10 value 98.575933\niter  20 value 89.968442\niter  30 value 89.576401\niter  40 value 89.510379\niter  50 value 89.372658\niter  60 value 89.304976\niter  70 value 89.301421\niter  80 value 89.266709\niter  90 value 89.261545\nfinal  value 89.261230 \nconverged\n# weights:  9\ninitial  value 102.124936 \niter  10 value 90.222971\niter  20 value 88.369268\niter  30 value 87.920039\niter  40 value 87.752667\niter  50 value 87.746021\niter  60 value 87.736035\niter  70 value 87.734732\niter  80 value 87.734576\niter  90 value 87.734525\nfinal  value 87.734187 \nconverged\n# weights:  25\ninitial  value 102.248036 \niter  10 value 97.917290\niter  20 value 82.900462\niter  30 value 70.430190\niter  40 value 68.296006\niter  50 value 67.198207\niter  60 value 66.980581\niter  70 value 66.132680\niter  80 value 65.791029\niter  90 value 65.747461\niter 100 value 65.723707\nfinal  value 65.723707 \nstopped after 100 iterations\n# weights:  41\ninitial  value 103.842072 \niter  10 value 89.104515\niter  20 value 85.766891\niter  30 value 73.497712\niter  40 value 64.374918\niter  50 value 60.285499\niter  60 value 57.643591\niter  70 value 57.035928\niter  80 value 56.407282\niter  90 value 56.230432\niter 100 value 54.955636\nfinal  value 54.955636 \nstopped after 100 iterations\n# weights:  9\ninitial  value 111.195962 \niter  10 value 103.631393\niter  20 value 99.293962\niter  30 value 95.181684\niter  40 value 93.715659\niter  50 value 93.684934\niter  60 value 93.671815\niter  70 value 93.526944\niter  80 value 93.198364\niter  90 value 92.994010\niter 100 value 92.914733\nfinal  value 92.914733 \nstopped after 100 iterations\n# weights:  25\ninitial  value 108.647794 \niter  10 value 99.107794\niter  20 value 83.285077\niter  30 value 78.399993\niter  40 value 70.246339\niter  50 value 69.153740\niter  60 value 68.330322\niter  70 value 67.993108\niter  80 value 67.777853\niter  90 value 67.720480\niter 100 value 67.711864\nfinal  value 67.711864 \nstopped after 100 iterations\n# weights:  41\ninitial  value 109.965893 \niter  10 value 95.572915\niter  20 value 84.028631\niter  30 value 75.849617\niter  40 value 69.079545\niter  50 value 66.857279\niter  60 value 66.262929\niter  70 value 66.032199\niter  80 value 66.021074\niter  90 value 66.020650\nfinal  value 66.020580 \nconverged\n# weights:  9\ninitial  value 112.378214 \niter  10 value 102.997910\niter  20 value 92.096043\nfinal  value 92.007118 \nconverged\n# weights:  25\ninitial  value 105.311381 \niter  10 value 95.516156\niter  20 value 91.238769\niter  30 value 90.599786\niter  40 value 90.587084\nfinal  value 90.586818 \nconverged\n# weights:  41\ninitial  value 114.546708 \niter  10 value 92.560304\niter  20 value 90.629190\niter  30 value 90.157105\niter  40 value 90.010894\niter  50 value 89.957189\niter  60 value 89.952680\niter  70 value 89.946490\niter  80 value 89.946177\nfinal  value 89.945660 \nconverged\n# weights:  9\ninitial  value 107.051323 \niter  10 value 102.841205\niter  20 value 92.097608\niter  30 value 83.886747\niter  40 value 83.848214\niter  50 value 83.781594\niter  60 value 83.773218\niter  70 value 83.769923\niter  80 value 83.769118\niter  90 value 83.769015\niter 100 value 83.768917\nfinal  value 83.768917 \nstopped after 100 iterations\n# weights:  25\ninitial  value 120.383754 \niter  10 value 100.775823\niter  20 value 84.886735\niter  30 value 76.886403\niter  40 value 74.313414\niter  50 value 72.044681\niter  60 value 71.689836\niter  70 value 71.549344\niter  80 value 71.253165\niter  90 value 71.145727\niter 100 value 71.135753\nfinal  value 71.135753 \nstopped after 100 iterations\n# weights:  41\ninitial  value 104.474180 \niter  10 value 94.520270\niter  20 value 81.769901\niter  30 value 72.776897\niter  40 value 58.147874\niter  50 value 48.831882\niter  60 value 46.707921\niter  70 value 45.834100\niter  80 value 44.736668\niter  90 value 42.249606\niter 100 value 41.285357\nfinal  value 41.285357 \nstopped after 100 iterations\n# weights:  9\ninitial  value 113.305721 \niter  10 value 101.406303\niter  20 value 93.299664\niter  30 value 92.409601\niter  40 value 92.385036\niter  50 value 92.381084\nfinal  value 92.368835 \nconverged\n# weights:  25\ninitial  value 114.598220 \niter  10 value 101.341048\niter  20 value 87.310406\niter  30 value 75.993820\niter  40 value 63.500057\niter  50 value 60.491777\niter  60 value 60.416351\nfinal  value 60.416301 \nconverged\n# weights:  41\ninitial  value 103.959300 \niter  10 value 88.483715\niter  20 value 84.090874\niter  30 value 72.420823\niter  40 value 62.096063\niter  50 value 60.430002\niter  60 value 57.951754\niter  70 value 56.228748\niter  80 value 56.202403\nfinal  value 56.201809 \nconverged\n# weights:  9\ninitial  value 102.949434 \niter  10 value 101.630675\niter  20 value 94.462279\niter  30 value 93.675053\nfinal  value 93.675042 \nconverged\n# weights:  25\ninitial  value 127.437237 \niter  10 value 102.605453\niter  20 value 94.035143\niter  30 value 93.682611\niter  40 value 92.342141\niter  50 value 91.690791\niter  60 value 91.666708\nfinal  value 91.665848 \nconverged\n# weights:  41\ninitial  value 108.653707 \niter  10 value 98.375505\niter  20 value 92.088089\niter  30 value 91.474192\niter  40 value 91.346733\niter  50 value 91.189006\niter  60 value 91.019628\niter  70 value 90.915666\niter  80 value 90.904310\niter  90 value 90.901730\niter 100 value 90.897943\nfinal  value 90.897943 \nstopped after 100 iterations\n# weights:  9\ninitial  value 108.499706 \niter  10 value 99.392853\niter  20 value 91.157547\niter  30 value 90.038980\niter  40 value 89.527558\niter  50 value 88.988958\niter  60 value 88.888388\niter  70 value 88.877754\niter  80 value 88.536629\niter  90 value 88.311075\niter 100 value 88.292575\nfinal  value 88.292575 \nstopped after 100 iterations\n# weights:  25\ninitial  value 102.703298 \niter  10 value 90.857648\niter  20 value 86.041794\niter  30 value 75.539670\niter  40 value 70.886568\niter  50 value 69.048212\niter  60 value 68.606145\niter  70 value 66.803674\niter  80 value 65.737692\niter  90 value 63.999639\niter 100 value 63.647419\nfinal  value 63.647419 \nstopped after 100 iterations\n# weights:  41\ninitial  value 102.766763 \niter  10 value 94.009669\niter  20 value 81.117764\niter  30 value 70.468961\niter  40 value 68.691483\niter  50 value 68.530773\niter  60 value 68.305637\niter  70 value 67.413290\niter  80 value 67.146369\niter  90 value 65.134959\niter 100 value 60.082611\nfinal  value 60.082611 \nstopped after 100 iterations\n# weights:  9\ninitial  value 111.377148 \niter  10 value 99.494183\niter  20 value 86.747310\niter  30 value 83.018570\niter  40 value 83.001220\niter  50 value 82.998176\nfinal  value 82.998068 \nconverged\n# weights:  25\ninitial  value 104.224068 \niter  10 value 90.573554\niter  20 value 85.045015\niter  30 value 83.378629\niter  40 value 83.114425\niter  50 value 82.997652\niter  60 value 82.997429\niter  60 value 82.997429\niter  60 value 82.997429\nfinal  value 82.997429 \nconverged\n# weights:  41\ninitial  value 105.514446 \niter  10 value 100.328498\niter  20 value 84.300067\niter  30 value 82.210642\niter  40 value 80.208987\niter  50 value 79.338958\niter  60 value 79.333741\nfinal  value 79.333708 \nconverged\n# weights:  9\ninitial  value 103.730936 \niter  10 value 102.271008\niter  20 value 99.097404\niter  30 value 99.068544\niter  30 value 99.068544\niter  30 value 99.068544\nfinal  value 99.068544 \nconverged\n# weights:  25\ninitial  value 121.045800 \niter  10 value 99.673919\niter  20 value 97.725112\niter  30 value 96.474114\niter  40 value 96.400756\niter  50 value 96.373082\niter  50 value 96.373082\niter  50 value 96.373082\nfinal  value 96.373082 \nconverged\n# weights:  41\ninitial  value 104.621490 \niter  10 value 99.409186\niter  20 value 97.233854\niter  30 value 96.992896\niter  40 value 96.799306\niter  50 value 96.459660\niter  60 value 96.412163\niter  70 value 96.398535\niter  80 value 96.368545\niter  90 value 96.289688\niter 100 value 96.262664\nfinal  value 96.262664 \nstopped after 100 iterations\n# weights:  9\ninitial  value 104.162876 \niter  10 value 102.796813\niter  20 value 88.797225\niter  30 value 85.322729\niter  40 value 84.096847\niter  50 value 84.005291\niter  60 value 83.856722\niter  70 value 83.822966\niter  80 value 83.786066\niter  90 value 83.757684\niter 100 value 83.746251\nfinal  value 83.746251 \nstopped after 100 iterations\n# weights:  25\ninitial  value 113.506076 \niter  10 value 99.218725\niter  20 value 87.340026\niter  30 value 79.247111\niter  40 value 72.761177\niter  50 value 70.776008\niter  60 value 70.264059\niter  70 value 70.170396\niter  80 value 70.129300\niter  90 value 70.085077\niter 100 value 70.055943\nfinal  value 70.055943 \nstopped after 100 iterations\n# weights:  41\ninitial  value 102.223534 \niter  10 value 95.341698\niter  20 value 84.668002\niter  30 value 81.313258\niter  40 value 80.448972\niter  50 value 77.395053\niter  60 value 76.915831\niter  70 value 76.467017\niter  80 value 76.007523\niter  90 value 74.989704\niter 100 value 71.918005\nfinal  value 71.918005 \nstopped after 100 iterations\n# weights:  9\ninitial  value 104.557593 \niter  10 value 102.315399\niter  20 value 87.190212\niter  30 value 85.507287\niter  40 value 84.113734\niter  50 value 83.228349\niter  60 value 82.368074\niter  70 value 82.340656\niter  80 value 82.313423\niter  90 value 82.273514\niter 100 value 82.242115\nfinal  value 82.242115 \nstopped after 100 iterations\n# weights:  25\ninitial  value 105.828791 \niter  10 value 89.990805\niter  20 value 84.482291\niter  30 value 79.251643\niter  40 value 68.555749\niter  50 value 66.321811\niter  60 value 62.838678\niter  70 value 62.247769\niter  80 value 62.011839\niter  90 value 61.847179\niter 100 value 61.732168\nfinal  value 61.732168 \nstopped after 100 iterations\n# weights:  41\ninitial  value 114.108906 \niter  10 value 90.054571\niter  20 value 86.498760\niter  30 value 84.736525\niter  40 value 83.951992\niter  50 value 83.797259\niter  60 value 82.007306\niter  70 value 81.354878\niter  80 value 81.290812\nfinal  value 81.280547 \nconverged\n# weights:  9\ninitial  value 120.416161 \niter  10 value 103.677449\niter  20 value 91.446714\niter  30 value 90.433827\nfinal  value 90.433695 \nconverged\n# weights:  25\ninitial  value 105.865844 \niter  10 value 99.109522\niter  20 value 91.005693\niter  30 value 90.667466\niter  40 value 90.629207\niter  50 value 90.410167\niter  60 value 90.401435\nfinal  value 90.401128 \nconverged\n# weights:  41\ninitial  value 111.281637 \niter  10 value 95.211767\niter  20 value 90.860464\niter  30 value 90.073844\niter  40 value 89.877084\niter  50 value 89.777481\niter  60 value 89.731136\niter  70 value 89.697806\niter  80 value 89.682324\niter  90 value 89.664683\niter 100 value 89.655648\nfinal  value 89.655648 \nstopped after 100 iterations\n# weights:  9\ninitial  value 104.269594 \niter  10 value 103.945841\niter  20 value 103.219996\niter  30 value 94.114073\niter  40 value 88.021715\niter  50 value 84.730631\niter  60 value 81.952292\niter  70 value 81.849688\niter  80 value 81.624590\niter  90 value 81.408312\niter 100 value 81.265305\nfinal  value 81.265305 \nstopped after 100 iterations\n# weights:  25\ninitial  value 112.507125 \niter  10 value 103.419114\niter  20 value 88.278116\niter  30 value 78.783057\niter  40 value 75.291677\niter  50 value 75.032030\niter  60 value 74.628805\niter  70 value 74.487110\niter  80 value 74.453244\niter  90 value 74.395006\niter 100 value 74.356549\nfinal  value 74.356549 \nstopped after 100 iterations\n# weights:  41\ninitial  value 106.072914 \niter  10 value 90.511286\niter  20 value 83.934595\niter  30 value 79.141349\niter  40 value 65.844198\niter  50 value 57.639818\niter  60 value 56.162519\niter  70 value 55.167549\niter  80 value 53.810988\niter  90 value 52.769620\niter 100 value 52.467044\nfinal  value 52.467044 \nstopped after 100 iterations\n# weights:  9\ninitial  value 110.354024 \niter  10 value 103.905126\niter  20 value 98.957779\niter  30 value 98.948753\nfinal  value 98.948564 \nconverged\n# weights:  25\ninitial  value 109.239604 \niter  10 value 103.506404\niter  20 value 95.527217\niter  30 value 89.360733\niter  40 value 81.286628\niter  50 value 78.105920\niter  60 value 76.689969\niter  70 value 76.588274\niter  80 value 76.537937\niter  90 value 76.394298\niter 100 value 76.388076\nfinal  value 76.388076 \nstopped after 100 iterations\n# weights:  41\ninitial  value 108.862925 \niter  10 value 103.842901\niter  20 value 100.458723\niter  30 value 96.235177\niter  40 value 95.443177\niter  50 value 95.422853\nfinal  value 95.422297 \nconverged\n# weights:  9\ninitial  value 104.481958 \niter  10 value 99.081650\niter  20 value 98.179600\nfinal  value 98.179209 \nconverged\n# weights:  25\ninitial  value 105.324945 \niter  10 value 98.843441\niter  20 value 96.987836\niter  30 value 96.932677\niter  40 value 96.931619\nfinal  value 96.931611 \nconverged\n# weights:  41\ninitial  value 107.056645 \niter  10 value 99.447824\niter  20 value 97.313633\niter  30 value 96.847440\niter  40 value 96.807979\niter  50 value 96.770473\niter  60 value 96.692784\niter  70 value 96.610286\niter  80 value 96.574738\niter  90 value 96.555445\niter 100 value 96.534595\nfinal  value 96.534595 \nstopped after 100 iterations\n# weights:  9\ninitial  value 104.608149 \niter  10 value 96.801556\niter  20 value 92.395967\niter  30 value 90.984529\niter  40 value 88.845447\niter  50 value 87.998051\niter  60 value 87.920183\niter  70 value 87.887484\niter  80 value 87.866044\niter  90 value 87.849534\niter 100 value 87.844385\nfinal  value 87.844385 \nstopped after 100 iterations\n# weights:  25\ninitial  value 113.289486 \niter  10 value 102.615305\niter  20 value 94.708710\niter  30 value 89.184707\niter  40 value 84.242963\niter  50 value 82.753511\niter  60 value 79.676577\niter  70 value 79.088927\niter  80 value 78.273518\niter  90 value 78.174796\niter 100 value 78.131195\nfinal  value 78.131195 \nstopped after 100 iterations\n# weights:  41\ninitial  value 111.950148 \niter  10 value 95.734759\niter  20 value 89.026511\niter  30 value 84.303684\niter  40 value 83.346041\niter  50 value 81.559488\niter  60 value 80.682378\niter  70 value 80.521186\niter  80 value 79.049832\niter  90 value 78.438848\niter 100 value 78.356452\nfinal  value 78.356452 \nstopped after 100 iterations\n# weights:  9\ninitial  value 107.066447 \niter  10 value 102.648538\niter  20 value 91.956674\niter  30 value 90.512874\niter  40 value 89.985835\niter  50 value 89.567583\niter  60 value 86.065502\niter  70 value 83.508667\niter  80 value 81.764932\niter  90 value 81.403200\niter 100 value 80.557697\nfinal  value 80.557697 \nstopped after 100 iterations\n# weights:  25\ninitial  value 103.008392 \niter  10 value 90.830978\niter  20 value 88.123758\niter  30 value 86.315369\niter  40 value 77.522980\niter  50 value 74.012829\niter  60 value 73.939267\niter  70 value 73.897810\niter  80 value 73.893873\niter  90 value 73.873753\niter 100 value 73.799580\nfinal  value 73.799580 \nstopped after 100 iterations\n# weights:  41\ninitial  value 112.624862 \niter  10 value 98.479657\niter  20 value 85.879176\niter  30 value 78.988055\niter  40 value 67.049381\niter  50 value 65.157797\nfinal  value 65.134558 \nconverged\n# weights:  9\ninitial  value 107.674353 \niter  10 value 102.797037\niter  20 value 94.058475\niter  30 value 93.577430\nfinal  value 93.575836 \nconverged\n# weights:  25\ninitial  value 110.746564 \niter  10 value 96.422589\niter  20 value 92.182824\niter  30 value 92.027371\niter  40 value 91.932994\niter  50 value 91.927236\nfinal  value 91.927215 \nconverged\n# weights:  41\ninitial  value 105.584385 \niter  10 value 97.144919\niter  20 value 92.012307\niter  30 value 91.810689\niter  40 value 91.792490\niter  50 value 91.783672\niter  60 value 91.774781\nfinal  value 91.773713 \nconverged\n# weights:  9\ninitial  value 111.293916 \niter  10 value 100.952335\niter  20 value 90.686849\niter  30 value 89.657253\niter  40 value 83.334054\niter  50 value 81.698644\niter  60 value 81.189406\niter  70 value 81.108587\niter  80 value 81.083751\niter  90 value 80.996935\niter 100 value 80.953021\nfinal  value 80.953021 \nstopped after 100 iterations\n# weights:  25\ninitial  value 105.175106 \niter  10 value 94.107689\niter  20 value 90.813688\niter  30 value 88.216541\niter  40 value 87.647127\niter  50 value 85.459323\niter  60 value 82.327998\niter  70 value 81.324163\niter  80 value 78.577431\niter  90 value 73.965496\niter 100 value 70.635489\nfinal  value 70.635489 \nstopped after 100 iterations\n# weights:  41\ninitial  value 110.854617 \niter  10 value 96.102044\niter  20 value 80.979906\niter  30 value 77.936488\niter  40 value 75.315977\niter  50 value 68.566723\niter  60 value 64.896091\niter  70 value 63.002311\niter  80 value 61.196403\niter  90 value 60.185934\niter 100 value 58.820535\nfinal  value 58.820535 \nstopped after 100 iterations\n# weights:  9\ninitial  value 113.352822 \niter  10 value 102.716462\niter  20 value 85.221190\niter  30 value 83.405449\niter  40 value 82.368969\niter  50 value 80.393298\niter  60 value 80.338870\niter  70 value 80.323251\niter  80 value 80.303991\niter  90 value 80.286528\niter 100 value 80.277505\nfinal  value 80.277505 \nstopped after 100 iterations\n# weights:  25\ninitial  value 120.432111 \niter  10 value 89.601873\niter  20 value 83.294294\niter  30 value 82.675311\niter  40 value 82.599852\niter  50 value 82.519347\niter  60 value 82.490053\niter  70 value 82.489645\niter  80 value 82.483384\niter  90 value 82.465135\niter 100 value 82.457197\nfinal  value 82.457197 \nstopped after 100 iterations\n# weights:  41\ninitial  value 120.135819 \niter  10 value 97.009833\niter  20 value 82.946686\niter  30 value 78.600086\niter  40 value 77.056843\niter  50 value 75.102042\niter  60 value 68.145188\niter  70 value 67.620650\niter  80 value 66.647852\niter  90 value 63.362697\niter 100 value 62.170397\nfinal  value 62.170397 \nstopped after 100 iterations\n# weights:  9\ninitial  value 115.312689 \niter  10 value 102.113835\niter  20 value 89.701937\niter  30 value 89.160385\nfinal  value 89.160284 \nconverged\n# weights:  25\ninitial  value 125.076170 \niter  10 value 99.569827\niter  20 value 90.050465\niter  30 value 89.511105\niter  40 value 88.203869\niter  50 value 87.857018\niter  60 value 87.598117\niter  70 value 87.576036\nfinal  value 87.575768 \nconverged\n# weights:  41\ninitial  value 104.845040 \niter  10 value 92.008839\niter  20 value 88.862601\niter  30 value 88.107758\niter  40 value 87.552224\niter  50 value 87.477185\niter  60 value 87.465993\niter  70 value 87.465507\niter  80 value 87.459230\niter  90 value 87.453494\niter 100 value 87.445864\nfinal  value 87.445864 \nstopped after 100 iterations\n# weights:  9\ninitial  value 103.145717 \niter  10 value 102.457665\niter  20 value 87.907761\niter  30 value 83.219945\niter  40 value 82.796617\niter  50 value 82.684299\niter  60 value 82.637501\niter  70 value 82.602123\niter  80 value 82.597226\niter  90 value 82.595767\niter 100 value 82.594763\nfinal  value 82.594763 \nstopped after 100 iterations\n# weights:  25\ninitial  value 105.946550 \niter  10 value 95.244500\niter  20 value 83.968393\niter  30 value 82.259221\niter  40 value 75.412839\niter  50 value 65.155423\niter  60 value 64.347743\niter  70 value 64.108228\niter  80 value 63.626579\niter  90 value 62.671444\niter 100 value 59.317661\nfinal  value 59.317661 \nstopped after 100 iterations\n# weights:  41\ninitial  value 125.955893 \niter  10 value 102.047863\niter  20 value 81.214449\niter  30 value 65.446032\niter  40 value 60.287907\niter  50 value 59.497267\niter  60 value 59.124843\niter  70 value 58.881753\niter  80 value 58.350310\niter  90 value 57.416495\niter 100 value 56.337970\nfinal  value 56.337970 \nstopped after 100 iterations\n# weights:  9\ninitial  value 105.367874 \niter  10 value 92.468600\niter  20 value 88.523370\niter  30 value 86.302647\niter  40 value 83.960735\niter  50 value 82.639255\niter  60 value 82.383252\niter  70 value 82.139280\niter  80 value 82.033779\niter  90 value 81.869654\niter 100 value 81.691738\nfinal  value 81.691738 \nstopped after 100 iterations\n# weights:  25\ninitial  value 131.142268 \niter  10 value 102.858986\niter  20 value 91.690145\niter  30 value 85.733986\niter  40 value 85.268562\niter  50 value 85.032213\niter  60 value 85.018265\niter  70 value 85.016187\niter  80 value 85.016039\nfinal  value 85.015945 \nconverged\n# weights:  41\ninitial  value 114.466016 \niter  10 value 101.926688\niter  20 value 87.129819\niter  30 value 83.636503\niter  40 value 73.402572\niter  50 value 65.993038\niter  60 value 63.831849\niter  70 value 63.810975\nfinal  value 63.810963 \nconverged\n# weights:  9\ninitial  value 104.185182 \niter  10 value 96.246872\niter  20 value 94.923937\nfinal  value 94.923934 \nconverged\n# weights:  25\ninitial  value 124.822433 \niter  10 value 99.369257\niter  20 value 93.911937\niter  30 value 93.291777\niter  40 value 93.275245\niter  50 value 93.261428\niter  60 value 93.241664\niter  70 value 92.889197\niter  80 value 92.591388\niter  90 value 92.582669\nfinal  value 92.582667 \nconverged\n# weights:  41\ninitial  value 116.292698 \niter  10 value 102.309541\niter  20 value 94.456620\niter  30 value 93.360629\niter  40 value 93.229592\niter  50 value 92.725525\niter  60 value 92.692127\niter  70 value 92.649582\niter  80 value 92.588286\niter  90 value 92.557319\niter 100 value 92.550673\nfinal  value 92.550673 \nstopped after 100 iterations\n# weights:  9\ninitial  value 105.556247 \niter  10 value 102.462883\niter  20 value 93.892803\niter  30 value 90.077735\niter  40 value 89.916938\niter  50 value 89.767505\niter  60 value 89.536446\niter  70 value 89.335049\niter  80 value 89.261387\niter  90 value 89.137735\niter 100 value 88.929230\nfinal  value 88.929230 \nstopped after 100 iterations\n# weights:  25\ninitial  value 114.101099 \niter  10 value 98.081811\niter  20 value 96.571494\niter  30 value 96.384800\niter  40 value 96.341327\niter  50 value 96.322469\niter  60 value 95.851594\niter  70 value 87.027944\niter  80 value 84.878402\niter  90 value 79.775288\niter 100 value 71.422141\nfinal  value 71.422141 \nstopped after 100 iterations\n# weights:  41\ninitial  value 123.931876 \niter  10 value 90.993060\niter  20 value 85.407939\niter  30 value 80.142124\niter  40 value 75.125022\niter  50 value 73.715239\niter  60 value 73.107166\niter  70 value 72.789159\niter  80 value 72.597333\niter  90 value 72.267732\niter 100 value 72.088580\nfinal  value 72.088580 \nstopped after 100 iterations\n# weights:  9\ninitial  value 105.291560 \niter  10 value 89.196191\niter  20 value 85.776782\niter  30 value 85.735442\niter  40 value 85.693904\nfinal  value 85.693768 \nconverged\n# weights:  25\ninitial  value 104.325193 \niter  10 value 90.158029\niter  20 value 87.780088\niter  30 value 87.195698\nfinal  value 87.193634 \nconverged\n# weights:  41\ninitial  value 103.315687 \niter  10 value 88.215737\niter  20 value 80.542299\niter  30 value 65.512306\niter  40 value 60.797313\nfinal  value 60.776149 \nconverged\n# weights:  9\ninitial  value 106.061504 \niter  10 value 98.229074\niter  20 value 95.920560\niter  30 value 95.156761\nfinal  value 95.156755 \nconverged\n# weights:  25\ninitial  value 103.261372 \niter  10 value 98.350808\niter  20 value 95.425213\niter  30 value 91.399039\niter  40 value 89.803715\niter  50 value 89.606737\niter  60 value 89.599241\nfinal  value 89.599142 \nconverged\n# weights:  41\ninitial  value 116.208300 \niter  10 value 97.865664\niter  20 value 93.204803\niter  30 value 91.584686\niter  40 value 89.785698\niter  50 value 89.608563\niter  60 value 89.445968\niter  70 value 89.393797\niter  80 value 89.360380\niter  90 value 89.341844\niter 100 value 89.326541\nfinal  value 89.326541 \nstopped after 100 iterations\n# weights:  9\ninitial  value 115.903392 \niter  10 value 96.213201\niter  20 value 91.567285\niter  30 value 90.071697\niter  40 value 86.730592\niter  50 value 86.203249\niter  60 value 85.871824\niter  70 value 85.719429\niter  80 value 85.651837\niter  90 value 85.282620\niter 100 value 85.273235\nfinal  value 85.273235 \nstopped after 100 iterations\n# weights:  25\ninitial  value 105.901971 \niter  10 value 88.432332\niter  20 value 73.757497\niter  30 value 69.764234\niter  40 value 69.109692\niter  50 value 67.878280\niter  60 value 66.934945\niter  70 value 66.484901\niter  80 value 66.269763\niter  90 value 66.110858\niter 100 value 66.091672\nfinal  value 66.091672 \nstopped after 100 iterations\n# weights:  41\ninitial  value 104.817452 \niter  10 value 96.569172\niter  20 value 79.960575\niter  30 value 72.541734\niter  40 value 68.885763\niter  50 value 67.640904\niter  60 value 66.613898\niter  70 value 66.095600\niter  80 value 65.894759\niter  90 value 65.327791\niter 100 value 64.969393\nfinal  value 64.969393 \nstopped after 100 iterations\n# weights:  9\ninitial  value 115.347481 \niter  10 value 101.302670\niter  20 value 86.795003\niter  30 value 85.282128\niter  40 value 84.845566\niter  50 value 84.674959\niter  60 value 84.664098\niter  70 value 84.649118\niter  80 value 84.638856\niter  90 value 84.638032\niter 100 value 84.637151\nfinal  value 84.637151 \nstopped after 100 iterations\n# weights:  25\ninitial  value 105.632227 \niter  10 value 90.197694\niter  20 value 85.871096\niter  30 value 82.380089\niter  40 value 77.624170\niter  50 value 75.548974\niter  60 value 75.387391\niter  70 value 75.381445\niter  80 value 75.378154\niter  90 value 75.360654\niter 100 value 75.353533\nfinal  value 75.353533 \nstopped after 100 iterations\n# weights:  41\ninitial  value 104.106658 \niter  10 value 94.812081\niter  20 value 86.383854\niter  30 value 84.476664\niter  40 value 80.661040\niter  50 value 79.485476\niter  60 value 79.037171\niter  70 value 78.414252\niter  80 value 78.372939\niter  90 value 77.926241\niter 100 value 77.648023\nfinal  value 77.648023 \nstopped after 100 iterations\n# weights:  9\ninitial  value 107.885273 \niter  10 value 99.331601\niter  20 value 94.796257\nfinal  value 94.796159 \nconverged\n# weights:  25\ninitial  value 106.002444 \niter  10 value 98.813401\niter  20 value 94.256402\niter  30 value 93.916026\niter  40 value 93.864907\nfinal  value 93.864842 \nconverged\n# weights:  41\ninitial  value 118.710221 \niter  10 value 97.022858\niter  20 value 93.718662\niter  30 value 93.365364\niter  40 value 93.285761\niter  50 value 93.276922\niter  60 value 93.260019\niter  70 value 93.259512\niter  80 value 93.259161\nfinal  value 93.259138 \nconverged\n# weights:  9\ninitial  value 108.550635 \niter  10 value 101.979891\niter  20 value 86.158937\niter  30 value 85.584070\niter  40 value 84.986753\niter  50 value 84.942541\niter  60 value 84.937185\nfinal  value 84.937142 \nconverged\n# weights:  25\ninitial  value 106.091637 \niter  10 value 98.193888\niter  20 value 86.117514\niter  30 value 84.807442\niter  40 value 82.902300\niter  50 value 78.167841\niter  60 value 70.105598\niter  70 value 66.058204\niter  80 value 65.496466\niter  90 value 65.270214\niter 100 value 65.153851\nfinal  value 65.153851 \nstopped after 100 iterations\n# weights:  41\ninitial  value 104.652345 \niter  10 value 101.765618\niter  20 value 86.558167\niter  30 value 83.397409\niter  40 value 82.025761\niter  50 value 81.546306\niter  60 value 81.205044\niter  70 value 81.156956\niter  80 value 81.025446\niter  90 value 81.015073\niter 100 value 81.001986\nfinal  value 81.001986 \nstopped after 100 iterations\n# weights:  25\ninitial  value 104.014071 \niter  10 value 102.092923\niter  20 value 94.809335\niter  30 value 94.620849\niter  40 value 94.613984\niter  50 value 94.594203\niter  60 value 94.527069\niter  70 value 94.417257\niter  80 value 94.414058\nfinal  value 94.414038 \nconverged\n\n\nOnce we’ve trained this model, we can test it on our testing dataset, and see how well it does:\n\npred <- predict(mod_fit, testing)\nconfusionMatrix(pred, as.factor(testing$christmas), positive = \"yes\")\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction no yes\n       no  24  13\n       yes 10  17\n                                         \n               Accuracy : 0.6406         \n                 95% CI : (0.511, 0.7568)\n    No Information Rate : 0.5312         \n    P-Value [Acc > NIR] : 0.0509         \n                                         \n                  Kappa : 0.2742         \n                                         \n Mcnemar's Test P-Value : 0.6767         \n                                         \n            Sensitivity : 0.5667         \n            Specificity : 0.7059         \n         Pos Pred Value : 0.6296         \n         Neg Pred Value : 0.6486         \n             Prevalence : 0.4688         \n         Detection Rate : 0.2656         \n   Detection Prevalence : 0.4219         \n      Balanced Accuracy : 0.6363         \n                                         \n       'Positive' Class : yes            \n                                         \n\n\nSo what does this all mean? Let’s define some terms.\n\nAccuracy:\n\nthe accuracy rate. Just how many things it got right.\n\n95% CI:\n\nthe confidence interval of the accuracy.\n\nNo information rate:\n\ngiven no more information other than the overall distribution, how likely are you to be correct if you just pick the “majority class.”\nif you have an accuracy rate of 80%, but the majority class is 80%, then your model isn’t terribly useful.\n\nP-Value:\n\nlikelihood of chance.\n\nKappa:\n\nmeasures the agreement between two raters and ratings. Here it’s looking at the difference between observed accuracy and random chance given the distribution in the dataset.\n\nMcNemar’s Test P-Value:\n\nthis is looking at the two distributions (from a 2x2 table), and determines if they are significantly different,\n\nSensitivity:\n\ngiven that a result is actually a thing, what is the probability that our model will predict that event’s results?\n\nSpecificity:\n\ngiven that a result is not actually a thing, what is the probability that our model will predict that?\n\nPos Predictive Value:\n\nthe probability that a predicted ‘positive’ class is actually positive.\n\nNeg Predictive Value:\n\nthe probability that a predicted ‘negative’ class is actually negative.\n\nPrevalence:\n\nthe prevalence of the ‘positive event’\n\nDetection Rate:\n\nthe rate of true events also predicted to be events\n\nDetection Prevalence\n\nthe prevalence of predicted events\n\nBalanced Accuracy:\n\nthe average of the proportion corrects of each class individually\n\n\n\n\nWe can look at which features the model is using…\n\nplot(varImp(mod_fit))\n\n\n\n\n\n\n\n\n\nUse PCA to explore the works of two artists. How well do they “separate”?\nRun a classifier on two groups (it can be the same two artists, or two distinct groups). How well does your model do?"
  }
]