[
  {
    "objectID": "class_notes/week_4.html",
    "href": "class_notes/week_4.html",
    "title": "Week 4: Conceptual Debates: Key-Finding",
    "section": "",
    "text": "talk about key-finding\nplay with the different weightings\nwhat would it look like to devise your own key-finding algorithm?\n\n\n\n\n\n\nWhen we hear this ringtone, it sounds as though it’s in C, but why?\n\n\n\nNokia\n\n\n\nIt doesn’t begin with C, it begins with G.\nC isn’t the most common note–in fact, it only occurs once before the final bar, and it’s on the “and” of 2 in the third measure (a pretty weak position metrically).\nIs a key just whatever key the piece ends in? If we ended this on A, would it sound like it’s in A minor? It would be the same key signature, and we’d actually have a nice cadential ascent to the final A from the G in the third measure.\n\nSo what gives? Why do we hear this as being in C?\nPerhaps a follow-up question might simply be: what makes us hear something as being in a key?\n\n\n\n\n\nThis approach used what we might call an exclusionary approach, eliminating different key possibilities as pitch classes were introduced over the course of a musical passage.\nFor example, with the Nokia theme, the opening G would fit into seven major keys (G, C, D, F, B-flats, A-flat, E-flat;); six of those keys would include the opening two notes; and three of those six would still be possible when presented with the first three notes. By the end of the first measure, however, the only major key that would encompass all four melody notes would be C major. If more than one key was still available however, the algorithm would place more weight on the pitches present at the start of the piece. This worked quite well on pieces that were overtly tonal, but it was less effective for pieces that contained non-diatonic pitches (which is most pieces!)\n\n\n\nLonguet-Higgins and Steedman's 1971 Key-Finding Algorithm\n\n\n\n\n\nAs you might guess, the Longuet-Higgins and Steedman would miss a lot of musical instances. For example, pieces that have non-harmonic chords would struggle, as would pieces that had a lot of chromatic ornamentations. Ideally an algorithm would allow for these pitches to occur, but acknowledge that pitches in the key might be a better fit than those outside of the key, and that certain pitches in the key should be more heavily weighted than others.\nCarol Krumhansl and Mark Schmuckler (Krumhansl, 1990) would devise an algorithm that tallied up the pitch classes of an excerpt and compared the distribution of these pitch classes to ratings from an earlier probe-tone experiment. (Krumhansl and Kessler, 1982). The weightings can be seen below.\nWe might think of this as a correlational approach. We tally up all of the pitches in a corpus, and then run a correlation on this key-profile. We run this over all of the keys, and the one that best fits is then labeled as “the key”.\n\nks_major_key <-\n  c(6.35, \n   2.23, \n   3.48, \n   2.33, \n   4.38, \n   4.09, \n   2.52, \n   5.19, \n   2.39, \n   3.66, \n   2.29, \n   2.88)\n\nks_minor_key <-\n  c(6.33, \n  2.68, \n  3.52, \n  5.38, \n  2.60, \n  3.53, \n  2.54, \n  4.75, \n  3.98, \n  2.69, \n  3.34, \n  3.17)\n\nAn interesting distinction here is that of experiment-derived vs. corpus-derived weightings. Should a key-finding algorithm intend to match how we hear key in a controlled lab environment (with basic harmonic progression stimuli), or should they use real music as a starting point? If they use real music, which music?\n\n\n\nHector Bellman created a key-finding algorithm that used Helen Budge’s dissertation from the 1940s as a starting point. Budge tallied up note occurrences in composers from the classical music canon, looking at the tonal makeup of a large collection of pieces. Bellman then used these frequencies as the starting point for his own key-finding algorithm.\n\nmajor <- c(16.80, \n            0.86,\n            12.95,\n            1.41,\n            13.49,\n            11.93,\n            1.25,\n            20.28,\n            1.80,\n            8.04,\n            0.62,\n            10.57)\n\nminor <- c(18.16,\n            0.69,\n            12.99,\n            13.34,\n            1.07,\n            11.15,\n            1.38,\n            21.07,\n            7.49,\n            1.53,\n            0.92,\n            10.21)\n\n\n\n\nDavid Temperley (2001) also employed Western classical music as a starting point for his early key-finding work (not to be confused with his more dynamic Bayesian-informed later work). He used examples from a commonly used music theory textbook (Stefan Kostka and Dorothy Payne’s Tonal Harmony).\n\nmajor <- c(0.748, \n            0.060, \n            0.488,\n            0.082, \n            0.670, \n            0.460, \n            0.096, \n            0.715, \n            0.104, \n            0.366,\n            0.057, \n            0.400)\n\nminor <- c(0.712, \n            0.084, \n            0.474, \n            0.618, \n            0.049, \n            0.460, \n            0.105, \n            0.747, \n            0.404, \n            0.067, \n            0.133, \n            0.330)\n\n\n\n\nBret Aarden (2003) argued that folk music would be a better fit than those generated from classical music. He used the Essen Folksong collection (consisting of thousands of folksongs throughout Europe, although with an uneven balance toward German folksong), to come up with the weightings below.\n\nmajor <- c(17.7661, \n            0.145624, \n            14.9265, \n            0.160186, \n            19.8049, \n            11.3587, \n            0.291248, \n            22.062, \n            0.145624, \n            8.15494, \n            0.232998, \n            4.95122)\n            \nminor <- c(18.2648, \n            0.737619, \n            14.0499, \n            16.8599, \n            0.702494, \n            14.4362, \n            0.702494, \n            18.6161, \n            4.56621, \n            1.93186, \n            7.37619, \n            1.75623)\n\n\n\n\nCraig Sapp argued that we probably didn’t even need to get frequencies from corpora or experiments. If we just assume that the tonic and the dominant (scale degrees 1 and 5) are the most important, and the other pitches in the key are less important, but more important than those not in the key, then we have a pretty simple weighting system (that works quite well!).\n\nmajor <- c(2, 0, 1, 0, 1, 1, 0, 2, 0, 1, 0, 1)\n\nminor <- c(2, 0, 1, 1, 0, 1, 0, 2, 1, 0, 0.5, 0.5)\n\n\n\n\nJosh Albrecht and I tried our hands at this problem, and picked a set of classical works from the Humdrum corpus, looking at only the first and last eight measures of each. The numbers are below.\n\nmajor <- c(0.238, \n            0.006, \n            0.111, \n            0.006, \n            0.137, \n            0.094, \n            0.016, \n            0.214, \n            0.009, \n            0.080, \n            0.008, \n            0.081) \n\nminor <- c(0.220, \n            0.006, \n            0.104, \n            0.123, \n            0.019, \n            0.103, \n            0.012, \n            0.214, \n            0.062, \n            0.022, \n            0.061, \n            0.052)\n\nWe also tried a Euclidean distance approach, rather than a correlational approach.\nWe tried to explain it as follows:\n\nIn a two-dimensional space, if there were 70% of pitch X and 30% pitch Y, the Cartesian location of the point representing this pitch-class distribution would be at X 1⁄4 0.7 and Y 1⁄4 0.3. In this case, we are examining the distribution of 12 pitch classes, resulting in a 12-dimensional Cartesian space. The pitch-class distribution of each piece is represented by a point in that 12-dimensional space. The distance is then measured between this point and the 24 points representing the 12 major and 12 minor key pitch-class distributions, and the key separated by the shortest distance is taken to be the key of the work.\n\nBelow is a table comparing how well this did to the others.\n\n\n\nComparing the Albrecht and Shanahan to others\n\n\n\n\n\nThey each perform a bit differently on different types of tasks.\n\n\n\nComparing key-finding algorithms in major, minor, and overall (from Albrecht and Shanahan, 2013)\n\n\n\n\ncorrplot 0.92 loaded\n\n\n\ncircshift <- function(v, n) {\n  if (n == 0) v else c(tail(v, n), head(v, -n))\n}\n# \n# # ### uses the Krumhansl Schmuckler Profiles\nmajor_key <- c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)\nminor_key <- c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)\n\n##sapp's simple weightings\n# major_key <- c(2, 0, 1, 0, 1, 1, 0, 2, 0, 1, 0, 1)\n# \n# minor_key <- c(2, 0, 1, 1, 0, 1, 0, 2, 1, 0, 0.5, 0.5)\n\nkey_templates <-\n  tribble(\n    ~name, ~template,\n    \"Gb:maj\", circshift(major_key, 6),\n    \"Bb:min\", circshift(minor_key, 10),\n    \"Db:maj\", circshift(major_key, 1),\n    \"F:min\", circshift(minor_key, 5),\n    \"Ab:maj\", circshift(major_key, 8),\n    \"C:min\", circshift(minor_key, 0),\n    \"Eb:maj\", circshift(major_key, 3),\n    \"G:min\", circshift(minor_key, 7),\n    \"Bb:maj\", circshift(major_key, 10),\n    \"D:min\", circshift(minor_key, 2),\n    \"F:maj\", circshift(major_key, 5),\n    \"A:min\", circshift(minor_key, 9),\n    \"C:maj\", circshift(major_key, 0),\n    \"E:min\", circshift(minor_key, 4),\n    \"G:maj\", circshift(major_key, 7),\n    \"B:min\", circshift(minor_key, 11),\n    \"D:maj\", circshift(major_key, 2),\n    \"F#:min\", circshift(minor_key, 6),\n    \"A:maj\", circshift(major_key, 9),\n    \"C#:min\", circshift(minor_key, 1),\n    \"E:maj\", circshift(major_key, 4),\n    \"G#:min\", circshift(minor_key, 8),\n    \"B:maj\", circshift(major_key, 11),\n    \"D#:min\", circshift(minor_key, 3)\n  )\n\nLet’s look at Lucy Dacus’s “Night Shift”.\nThis grabs the track and does all the magic:\n\nnight_shift <-\n  get_tidy_audio_analysis(\"1yYlpGuBiRRf33e1gY61bN\") %>%\n  compmus_align(sections, segments) %>%\n  select(sections) %>%\n  unnest(sections) %>%\n  mutate(\n    pitches =\n      map(segments,\n          compmus_summarise, pitches,\n          method = \"mean\", norm = \"manhattan\"\n      )\n  )\n\nAnd this is just a plotting function:\n\nnight_shift %>% \n  compmus_match_pitch_template(\n    key_templates,         # Change to chord_templates if descired\n    method = \"euclidean\",  # Try different distance metrics\n    norm = \"manhattan\"     # Try different norms\n  ) %>%\n  ggplot(\n    aes(x = start + duration / 2, width = duration, y = name, fill = d)\n  ) +\n  geom_tile() +\n  scale_fill_viridis_c(guide = \"none\") +\n  theme_minimal() +\n  labs(x = \"Time (s)\", y = \"\")\n\n\n\n\n\nnight_shift <-\n  get_tidy_audio_analysis(\"1yYlpGuBiRRf33e1gY61bN\") %>%\n  compmus_align(sections, segments) %>%\n  select(sections) %>%\n  unnest(sections) %>%\n  mutate(\n    pitches =\n      map(segments,\n          compmus_summarise, pitches,\n          method = \"mean\", norm = \"manhattan\"\n      )\n  )\n\nLet’s do some exercises:\n\nVisualize a song with all of these weightings.\n\nHow do the algorithms differ?\n\nCan you write a function that would call each weighting as an argument? What would that look like?"
  },
  {
    "objectID": "class_notes/week_4.html#whats-the-key-profile-for-indie-pop",
    "href": "class_notes/week_4.html#whats-the-key-profile-for-indie-pop",
    "title": "Week 4: Conceptual Debates: Key-Finding",
    "section": "What’s the Key-Profile for “Indie-Pop”",
    "text": "What’s the Key-Profile for “Indie-Pop”\nThe basic code for getting a key-profile from a playlist is below. The process is as follows:\n\nGet the audio features from a playlist, and add the audio analysis onto the datafame.\nWe then create a “segments” column by using a map function from the tidyverse. Map functions basically apply a function over each element in a list. Here, we are saying “apply the compmus_c_transpose function to the key and segments lists from the add_audio_analysis function.”\n\nWhat does the compmus_c_transpose function do? It takes all of the chroma vectors and transposes them to the key of C, so that we can construct a single set of weightings from pieces in different keys.\n\nWe then only grab this transposed segments column and turn it into a more readable list with the unnest function.\n\nWe then grab the start, duration, and pitches info.\n\nWe then create a “pitches” column, and normalize these raw pitch counts. There are a few ways to do this, and there are different options for this.\nWe then used the compmus_gather_chroma function to take all of those chroma vectors and turn them into a list.\nWe then use the group_by and summarise functions from tidyverse, and get the mean count of each pitch class in the distribution.\n\n\n### grabs the key-profile of the indie-pop playlist.\nindie_pop_key_profile <- get_playlist_audio_features(\"\", \"37i9dQZF1DWWEcRhUVtL8n\") |>\n  \n  add_audio_analysis() |>\n  ## transpose all the chroma vectors to C.\n  mutate(segments = map2(segments, key, compmus_c_transpose)) |>\n  ## grab the segments data and unnest it, then only grabbing the start, duration, and pitches info.\n  select(segments) |>\n  unnest(segments) |> \n  select(start, duration, pitches) |> \n  mutate(pitches = map(pitches, compmus_normalise, \"euclidean\")) |>\n  compmus_gather_chroma() |>\n  group_by(pitch_class) |>\n  summarise(mean_value = mean(value)) \n\nindie_pop_key_profile\n\n# A tibble: 12 × 2\n   pitch_class mean_value\n   <fct>            <dbl>\n 1 C                0.310\n 2 C#|Db            0.169\n 3 D                0.236\n 4 D#|Eb            0.190\n 5 E                0.227\n 6 F                0.225\n 7 F#|Gb            0.175\n 8 G                0.279\n 9 G#|Ab            0.179\n10 A                0.193\n11 A#|Bb            0.173\n12 B                0.215\n\n\nIdeally, we’d be able to turn this into a more reusable function. Below we’ve just turned made the playlist URI an argument:\n\nget_key_profile_broad <- function(uri){\n   get_playlist_audio_features(\"\", uri) |>\n   add_audio_analysis() |>\n   ## transpose all the chroma vectors to C. (have I mentioned how great Burgoyne's library is??)\n   mutate(segments = map2(segments, key, compmus_c_transpose)) |>\n   ## grab the segments data and unnest it, then only grabbing the start, duration, and pitches info.\n   select(segments) |>\n   unnest(segments) |> \n   select(start, duration, pitches) |> \n   mutate(pitches = map(pitches, compmus_normalise, \"euclidean\")) |>\n   compmus_gather_chroma() |>\n   group_by(pitch_class) |>\n   summarise(mean_value = mean(value)) \n}\n\nAnd now we can just run the function like so:\n\nindie_pop <- get_key_profile_broad(\"37i9dQZF1DWWEcRhUVtL8n\")\nindie_pop\n\n# A tibble: 12 × 2\n   pitch_class mean_value\n   <fct>            <dbl>\n 1 C                0.310\n 2 C#|Db            0.169\n 3 D                0.236\n 4 D#|Eb            0.190\n 5 E                0.227\n 6 F                0.225\n 7 F#|Gb            0.175\n 8 G                0.279\n 9 G#|Ab            0.179\n10 A                0.193\n11 A#|Bb            0.173\n12 B                0.215\n\n\nand we can plot it in a pretty straightforward way:\n\nbarplot(indie_pop$mean_value)\n\n\n\n\nSo we can look at other genres pretty easily. Here is me looking at Spotify’s “EDM 2023” playlist:"
  },
  {
    "objectID": "class_notes/week_4.html#whats-the-key-profile-for-edm",
    "href": "class_notes/week_4.html#whats-the-key-profile-for-edm",
    "title": "Week 4: Conceptual Debates: Key-Finding",
    "section": "What’s the Key Profile for EDM?",
    "text": "What’s the Key Profile for EDM?\n\nedm <- get_key_profile_broad(\"37i9dQZF1DX1kCIzMYtzum\")\nedm\n\n# A tibble: 12 × 2\n   pitch_class mean_value\n   <fct>            <dbl>\n 1 C                0.313\n 2 C#|Db            0.180\n 3 D                0.214\n 4 D#|Eb            0.217\n 5 E                0.214\n 6 F                0.233\n 7 F#|Gb            0.194\n 8 G                0.265\n 9 G#|Ab            0.203\n10 A                0.196\n11 A#|Bb            0.203\n12 B                0.230\n\n\nand once again we can plot it:\n\nbarplot(edm$mean_value)\n\n\n\n\n\nSome points of interest\n\nFor both of these distributions, we see a strong showing for scale degrees 1 and 5 (they aren’t really labeled in these quickie plots, but it would be the first and seventh column, respectively).\nWith the “Indie Pop” plot, we see a strong showing of scale degrees 1 and 5, and are followed by the diatonic pitches, but with the “EDM” list, scale degrees 2, flat 3, and 3 occur with pretty much the same frequency. It might be worth splitting the major and minor pieces up a bit?"
  },
  {
    "objectID": "class_notes/week_4.html#getting-separate-major-and-minor-key-profiles",
    "href": "class_notes/week_4.html#getting-separate-major-and-minor-key-profiles",
    "title": "Week 4: Conceptual Debates: Key-Finding",
    "section": "Getting separate major and minor key-profiles",
    "text": "Getting separate major and minor key-profiles\nWe could break this into a few parts for our own comfort. Let’s start by just creating a function that grabs the data. As that’s the one that’s quite time intensive, and calls to the API, let’s try to run it only once.\n\ngrab_playlist_info <- function(uri){\n   get_playlist_audio_features(\"\", uri) |>\n   add_audio_analysis() \n}\n\nOnce we have that in place, we can create a variable, and then subset it from there. Here, I’m saving the full list, and then creating a major and a minor variable.\n\nplaylist <- grab_playlist_info(\"37i9dQZF1DX1kCIzMYtzum\")  \nminor <- playlist |> filter(mode == 0)\nmajor <- playlist |> filter(mode == 1)\n\n\nget_pitch_list <- function(input){\n   input |>     \n   ## transpose all the chroma vectors to C. (have I mentioned how great Burgoyne's library is??)\n   mutate(segments = map2(segments, key, compmus_c_transpose)) |>\n   ## grab the segments data and unnest it, then only grabbing the start, duration, and pitches info.\n   select(segments) |>\n   unnest(segments) |> \n   select(start, duration, pitches) |> \n   mutate(pitches = map(pitches, compmus_normalise, \"euclidean\")) |>\n   compmus_gather_chroma() |>\n   group_by(pitch_class) |>\n   summarise(mean_value = mean(value))\n}\n\nAnd now we can get separate pitch lists for major and minor:\n\nminor_key <- get_pitch_list(minor)\nmajor_key <- get_pitch_list(major)\n\nand then of course we can use these to inform our own key mapping.\nWe can start by putting this all into a super quick and inefficient function like this (hoping to improve it as we go along):\n\nkey_plotter <- function(uri, major, minor){\n   major_key <- major\n   minor_key <- minor\n   key_templates <-\n   tribble(\n      ~name, ~template,\n      \"Gb:maj\", circshift(major_key, 6),\n      \"Bb:min\", circshift(minor_key, 10),\n      \"Db:maj\", circshift(major_key, 1),\n      \"F:min\", circshift(minor_key, 5),\n      \"Ab:maj\", circshift(major_key, 8),\n      \"C:min\", circshift(minor_key, 0),\n      \"Eb:maj\", circshift(major_key, 3),\n      \"G:min\", circshift(minor_key, 7),\n      \"Bb:maj\", circshift(major_key, 10),\n      \"D:min\", circshift(minor_key, 2),\n      \"F:maj\", circshift(major_key, 5),\n      \"A:min\", circshift(minor_key, 9),\n      \"C:maj\", circshift(major_key, 0),\n      \"E:min\", circshift(minor_key, 4),\n      \"G:maj\", circshift(major_key, 7),\n      \"B:min\", circshift(minor_key, 11),\n      \"D:maj\", circshift(major_key, 2),\n      \"F#:min\", circshift(minor_key, 6),\n      \"A:maj\", circshift(major_key, 9),\n      \"C#:min\", circshift(minor_key, 1),\n      \"E:maj\", circshift(major_key, 4),\n      \"G#:min\", circshift(minor_key, 8),\n      \"B:maj\", circshift(major_key, 11),\n      \"D#:min\", circshift(minor_key, 3)\n  )\n\ntune <-\n  get_tidy_audio_analysis(uri) %>%\n  compmus_align(sections, segments) %>%\n  select(sections) %>%\n  unnest(sections) %>%\n  mutate(\n    pitches =\n      map(segments,\n          compmus_summarise, pitches,\n          method = \"mean\", norm = \"manhattan\"\n      )\n  ) \n\ntune |> compmus_match_pitch_template(\n    key_templates,         # Change to chord_templates if descired\n    method = \"euclidean\",  # Try different distance metrics\n    norm = \"manhattan\"     # Try different norms\n  ) %>%\n  ggplot(\n    aes(x = start + duration / 2, width = duration, y = name, fill = d)\n  ) +\n  geom_tile() +\n  scale_fill_viridis_c(guide = \"none\") +\n  theme_minimal() +\n  labs(x = \"Time (s)\", y = \"\")\n\n}"
  },
  {
    "objectID": "class_notes/week_4.html#one-piece-and-many-key-profiles",
    "href": "class_notes/week_4.html#one-piece-and-many-key-profiles",
    "title": "Week 4: Conceptual Debates: Key-Finding",
    "section": "One Piece and Many Key Profiles",
    "text": "One Piece and Many Key Profiles\nLooking at Lucy Dacus’s “Night Shift” with EDM Key Profiles:\n\nedm_major_key <- c(0.2949827,0.1842662, 0.2249348, 0.1796559, 0.2532545, 0.2391564, 0.2028676, 0.2607747, 0.1765553, 0.2105823, 0.1806760, 0.2562869)\n# \nedm_minor_key <- c(0.3247214, 0.1767437, 0.2066454, 0.2482824, 0.1811887, 0.2263670, 0.1830838, 0.2662832, 0.2340293, 0.1888321, 0.2203257, 0.2047107)\nkey_plotter(\"1yYlpGuBiRRf33e1gY61bN\", edm_major_key, edm_minor_key)\n\n\n\n\nAnd here is the piece with the more traditional Krumhansl-Schmuckler key profiles:\n\nkey_plotter(\"1yYlpGuBiRRf33e1gY61bN\", ks_major_key, ks_minor_key)\n\n\n\n\nWe can load our “indie pop” but now in major and minor:\n\nplaylist <- grab_playlist_info(\"37i9dQZF1DWWEcRhUVtL8n\")  \nindie_minor <- playlist |> filter(mode == 0)\nindie_major <- playlist |> filter(mode == 1)\nindie_minor <- get_pitch_list(indie_minor)\nindie_major <- get_pitch_list(indie_major)\n\n\n\n\nAnd then we put these weightings into the plotter:\n\nkey_plotter(\"1yYlpGuBiRRf33e1gY61bN\", ks_major_key, ks_minor_key)"
  },
  {
    "objectID": "class_notes/week_4.html#exercise",
    "href": "class_notes/week_4.html#exercise",
    "title": "Week 4: Conceptual Debates: Key-Finding",
    "section": "Exercise:",
    "text": "Exercise:\n\nPick one piece and construct a genre-specific key-profile that might be used to explain its tonal make-up.\n\nExplain this musically."
  },
  {
    "objectID": "class_notes/week_5.html#looking-at-tempo-over-time",
    "href": "class_notes/week_5.html#looking-at-tempo-over-time",
    "title": "Week 5: Regression and Clustering",
    "section": "Looking at tempo over time",
    "text": "Looking at tempo over time\nWe can start by eyeballing the data. Here is how we’d do it with base R (no ggplot/tidyverse):\n\nplot(tempo ~ album_release_year, data=jayz)\nabline(lm(tempo ~ album_release_year, data=jayz), col=\"red\")\n\n\n\n\nIf we’d like to use ggplot it can give us some confidence bars (the default here is a 95% confidence interval):\n\nggplot(jayz, aes(album_release_year, tempo) ) + geom_point() +\n  stat_smooth(method = lm) +\n  theme_bw()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nSo it looks promising. We can run a linear regression with a simple lm command. Here we can get a summary of the model pretty easily, as well.\n\nsummary(lm(tempo ~ album_release_year, data=jayz))\n\n\nCall:\nlm(formula = tempo ~ album_release_year, data = jayz)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-44.84 -21.17 -11.79  12.22  92.18 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(>|t|)    \n(Intercept)        -2158.9919   396.7564  -5.442 6.99e-08 ***\nalbum_release_year     1.1306     0.1979   5.714 1.55e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 30.94 on 815 degrees of freedom\nMultiple R-squared:  0.03852,   Adjusted R-squared:  0.03734 \nF-statistic: 32.65 on 1 and 815 DF,  p-value: 1.547e-08\n\n\nSo, as we can see from the results here, it’s significant (p < .001), but it really doesn’t account for much of the variance (an adjusted R-squared of .037).\n\nPost-Hoc Analyses\nPerhaps we can look at how other variables might be predictive of the year of the recording.\nLet’s look at how tempo, danceability, valence, speechiness, and energy might improve the model.\n\nsummary(lm(album_release_year ~ tempo + danceability + valence + speechiness + energy, data=jayz))\n\n\nCall:\nlm(formula = album_release_year ~ tempo + danceability + valence + \n    speechiness + energy, data = jayz)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.2530  -3.4825  -0.2837   3.5690  18.2393 \n\nCoefficients:\n               Estimate Std. Error  t value Pr(>|t|)    \n(Intercept)   2.016e+03  1.826e+00 1104.163  < 2e-16 ***\ntempo         2.861e-02  5.603e-03    5.105 4.12e-07 ***\ndanceability -5.986e+00  1.519e+00   -3.942 8.77e-05 ***\nvalence      -6.849e+00  9.630e-01   -7.112 2.51e-12 ***\nspeechiness  -7.808e+00  1.160e+00   -6.730 3.20e-11 ***\nenergy       -4.484e+00  1.348e+00   -3.326 0.000921 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.885 on 811 degrees of freedom\nMultiple R-squared:  0.2084,    Adjusted R-squared:  0.2035 \nF-statistic: 42.69 on 5 and 811 DF,  p-value: < 2.2e-16\n\n\nSo we have a more predictive model, with an adjusted R-squared of about .20.\nThere are some remaining questions, however. Firstly, is there covariance at play?\nWe can use the variance inflation factor (VIF) to look at this. Typically, each variable should be less than 5.\n\nlibrary(car)\n\nLoading required package: carData\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\njayz_model <- lm(album_release_year ~ tempo + danceability + valence + speechiness + energy, data=jayz)\nvif(jayz_model)\n\n       tempo danceability      valence  speechiness       energy \n    1.067352     1.350387     1.228451     1.093354     1.253967 \n\n\nA correlation plot can help us to visualize this a bit more.\n\nlibrary(corrplot)\n\ncorrplot 0.92 loaded\n\njz <- jayz %>% \n    select(c(\"acousticness\", \"liveness\", \"danceability\", \"loudness\", \"speechiness\", \"valence\"))\n  x <- as.matrix(cor(jz))\n  round(x, 2)\n\n             acousticness liveness danceability loudness speechiness valence\nacousticness         1.00     0.14         0.07    -0.12        0.33    0.07\nliveness             0.14     1.00        -0.20     0.01        0.03   -0.11\ndanceability         0.07    -0.20         1.00    -0.17       -0.01    0.28\nloudness            -0.12     0.01        -0.17     1.00       -0.20    0.03\nspeechiness          0.33     0.03        -0.01    -0.20        1.00    0.18\nvalence              0.07    -0.11         0.28     0.03        0.18    1.00\n\n  corrplot(x, method=\"pie\")"
  },
  {
    "objectID": "class_notes/week_5.html#sidenote-isare-the-data-normal",
    "href": "class_notes/week_5.html#sidenote-isare-the-data-normal",
    "title": "Week 5: Regression and Clustering",
    "section": "Sidenote: Is/Are the data normal?",
    "text": "Sidenote: Is/Are the data normal?\nWe can test to see if the tempo data is normally distributed:\n\nqqnorm(jayz$tempo)\n\n\n\nhist(jayz$tempo)\n\n\n\nshapiro.test(jayz$tempo)\n\n\n    Shapiro-Wilk normality test\n\ndata:  jayz$tempo\nW = 0.78869, p-value < 2.2e-16\n\nks.test(jayz$tempo, \"pnorm\")\n\nWarning in ks.test.default(jayz$tempo, \"pnorm\"): ties should not be present for\nthe Kolmogorov-Smirnov test\n\n\n\n    Asymptotic one-sample Kolmogorov-Smirnov test\n\ndata:  jayz$tempo\nD = 1, p-value < 2.2e-16\nalternative hypothesis: two-sided\n\n\nAt the moment, it doesn’t seem to be…"
  },
  {
    "objectID": "class_notes/week_5.html#stepwise-entry-regression",
    "href": "class_notes/week_5.html#stepwise-entry-regression",
    "title": "Week 5: Regression and Clustering",
    "section": "Stepwise Entry Regression",
    "text": "Stepwise Entry Regression\n\nsummary(step(lm(album_release_year ~ danceability + \n          tempo + acousticness + \n          speechiness + valence, data=jayz), \n     direction=\"backward\"))\n\nStart:  AIC=2573.56\nalbum_release_year ~ danceability + tempo + acousticness + speechiness + \n    valence\n\n               Df Sum of Sq   RSS    AIC\n<none>                      18788 2573.6\n- danceability  1    243.73 19032 2582.1\n- tempo         1    714.38 19502 2602.1\n- acousticness  1    826.79 19614 2606.7\n- speechiness   1   1422.54 20210 2631.2\n- valence       1   1687.89 20476 2641.8\n\n\n\nCall:\nlm(formula = album_release_year ~ danceability + tempo + acousticness + \n    speechiness + valence, data = jayz)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-12.9398  -3.6858  -0.2579   3.4408  16.3345 \n\nCoefficients:\n               Estimate Std. Error  t value Pr(>|t|)    \n(Intercept)   2.011e+03  1.257e+00 1599.612  < 2e-16 ***\ndanceability -4.496e+00  1.386e+00   -3.244  0.00123 ** \ntempo         3.075e-02  5.538e-03    5.553 3.80e-08 ***\nacousticness  6.752e+00  1.130e+00    5.974 3.46e-09 ***\nspeechiness  -9.233e+00  1.178e+00   -7.836 1.46e-14 ***\nvalence      -7.750e+00  9.080e-01   -8.536  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.813 on 811 degrees of freedom\nMultiple R-squared:  0.2314,    Adjusted R-squared:  0.2267 \nF-statistic: 48.83 on 5 and 811 DF,  p-value: < 2.2e-16\n\nsummary(step(lm(album_release_year ~ danceability + \n          tempo + acousticness + \n          speechiness + valence, data=jayz), \n     direction=\"forward\"))\n\nStart:  AIC=2573.56\nalbum_release_year ~ danceability + tempo + acousticness + speechiness + \n    valence\n\n\n\nCall:\nlm(formula = album_release_year ~ danceability + tempo + acousticness + \n    speechiness + valence, data = jayz)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-12.9398  -3.6858  -0.2579   3.4408  16.3345 \n\nCoefficients:\n               Estimate Std. Error  t value Pr(>|t|)    \n(Intercept)   2.011e+03  1.257e+00 1599.612  < 2e-16 ***\ndanceability -4.496e+00  1.386e+00   -3.244  0.00123 ** \ntempo         3.075e-02  5.538e-03    5.553 3.80e-08 ***\nacousticness  6.752e+00  1.130e+00    5.974 3.46e-09 ***\nspeechiness  -9.233e+00  1.178e+00   -7.836 1.46e-14 ***\nvalence      -7.750e+00  9.080e-01   -8.536  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.813 on 811 degrees of freedom\nMultiple R-squared:  0.2314,    Adjusted R-squared:  0.2267 \nF-statistic: 48.83 on 5 and 811 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "class_notes/week_5.html#comparing-fits",
    "href": "class_notes/week_5.html#comparing-fits",
    "title": "Week 5: Regression and Clustering",
    "section": "Comparing Fits:",
    "text": "Comparing Fits:\nWe could construct a few models But how can we tell which of these is more predictable? For this, we can look at Akaike’s ‘An Information Criterion’(or AIC). The lower number is better, and any difference of more than 2 is considered a statistically significant difference fit.\n\ndance_model <- lm(danceability ~ album_release_year, data=jayz)\nacoustic_model <- lm(acousticness ~ album_release_year, data=jayz)\nspeech_model <- lm(speechiness ~ album_release_year, data=jayz)\nvalence_model <- lm(valence ~ album_release_year, data=jayz)\ntempo_model <- lm(tempo ~ album_release_year, data=jayz)\ncombined_model <- lm(album_release_year ~ tempo + acousticness + speechiness + valence, data=jayz)\n\n\nAIC(dance_model, \n    acoustic_model, \n    tempo_model,\n    speech_model, \n    valence_model, \n    combined_model)\n\n               df        AIC\ndance_model     3 -1036.7120\nacoustic_model  3  -686.8298\ntempo_model     3  7930.2701\nspeech_model    3  -782.5959\nvalence_model   3  -438.7779\ncombined_model  6  4902.6326\n\n\nThe combined model doesn’t seem to do terribly well here, which seems to muddy the question up a bit."
  },
  {
    "objectID": "class_notes/week_5.html#is-a-linear-model-the-best-approach",
    "href": "class_notes/week_5.html#is-a-linear-model-the-best-approach",
    "title": "Week 5: Regression and Clustering",
    "section": "Is a linear model the best approach?",
    "text": "Is a linear model the best approach?\nWe can adjust how we are viewing this data (linear or polynomial fit) a bit with the code below. Here we can plot our data as a line:\n\nggplot(jayz, aes(album_release_year, tempo) ) + geom_point() +\n  stat_smooth(method = lm, formula = y ~ poly(x, 1, raw = TRUE)) +\n  theme_bw()\n\n\n\n\nAnd here we have it as a second order polynomial:\n\nggplot(jayz, aes(album_release_year, tempo) ) + geom_point() +\n  stat_smooth(method = lm, formula = y ~ poly(x, 2, raw = TRUE)) +\n  theme_bw()\n\n\n\n\nAnd we can compare fits here:\n\nlinear <- lm(album_release_year ~ tempo, data = jayz)\npoly_2 <- lm(album_release_year ~ tempo + I(album_release_year^2), data = jayz)\n\nAIC(linear, \n    poly_2)\n\n       df       AIC\nlinear  3  5069.029\npoly_2  4 -5640.652"
  },
  {
    "objectID": "class_notes/week_5.html#predicting-a-categorical-variable",
    "href": "class_notes/week_5.html#predicting-a-categorical-variable",
    "title": "Week 5: Regression and Clustering",
    "section": "Predicting a categorical variable",
    "text": "Predicting a categorical variable\nWhat does it look like to predict a categorical variable in R? We might explore this question by hypothesizing that musical features might be predictive of mode (labeled in Spotify as major or minor: 1 or 0).\nHere is what a binomial logistic regression would look like:\n\njayz.log <- glm(mode ~ tempo + danceability + valence +\n                     speechiness + acousticness, family = binomial, data = jayz)\n\nAnd it looks like “speechiness” is the most predictive of mode here.\n\nsummary(jayz.log)\n\n\nCall:\nglm(formula = mode ~ tempo + danceability + valence + speechiness + \n    acousticness, family = binomial, data = jayz)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.8836  -1.2367   0.9073   1.0739   1.3806  \n\nCoefficients:\n               Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  -1.468e-01  5.364e-01  -0.274   0.7843    \ntempo         3.210e-06  2.365e-03   0.001   0.9989    \ndanceability  1.062e-01  5.893e-01   0.180   0.8570    \nvalence      -5.733e-01  3.911e-01  -1.466   0.1427    \nspeechiness   2.561e+00  5.483e-01   4.671    3e-06 ***\nacousticness -9.680e-01  4.849e-01  -1.996   0.0459 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1120.6  on 816  degrees of freedom\nResidual deviance: 1095.8  on 811  degrees of freedom\nAIC: 1107.8\n\nNumber of Fisher Scoring iterations: 4\n\n\nWe can plot the log odds ratios as well:\n\nCI <- exp(confint(jayz.log))[-1,]\n\nWaiting for profiling to be done...\n\nsjPlot::plot_model(jayz.log,\n                   axis.lim = c(min(CI), max(CI)),\n                   auto.label = F,\n                   show.values = T) +\n                   theme_bw()"
  },
  {
    "objectID": "class_notes/week_5.html#clustering",
    "href": "class_notes/week_5.html#clustering",
    "title": "Week 5: Regression and Clustering",
    "section": "Clustering",
    "text": "Clustering\nCluster analysis is a form of statistical data analysis in which subsets (called “clusters”) are formed according to some notion of similarity. There are many different variants of cluster analysis, but most are hierarchical–in which low-level clusters are successively joined together to make larger clusters, and so on, until everything is clustered into one large group. The result is a cluster tree or dendrogram.\n\nHow does the R hclust function work?\nThe hclust function is part of the default package in R, and it clusters based on dissimilarities in the data. There are different algorithms it can use, but the default is Ward’s minimum variance. It requires some distance to be calculated first, so the dist function is used on the data. Again there are many options here, but the default is to simply calculate the Euclidean distance between the values.\nThe documentation states:\n\nThis function performs a hierarchical cluster analysis using a set of dissimilarities for the n objects being clustered. Initially, each object is assigned to its own cluster and then the algorithm proceeds iteratively, at each stage joining the two most similar clusters, continuing until there is just a single cluster. At each stage distances between clusters are recomputed by the Lance–Williams dissimilarity update formula according to the particular clustering method being used.\n\nThe default is Ward’s minimum variance method, which:\n\naims at finding compact, spherical clusters. The complete linkage method finds similar clusters.\n\nAnother method is the “single linkage method”.\n\nThe single linkage method (which is closely related to the minimal spanning tree) adopts a ‘friends of friends’ clustering strategy. The other methods can be regarded as aiming for clusters with characteristics somewhere between the single and complete link methods. Note however, that methods “median” and “centroid” are not leading to a monotone distance measure, or equivalently the resulting dendrograms can have so called inversions or reversals which are hard to interpret, but note the trichotomies in Legendre and Legendre (2012).\n\n\n# cluster demo modified from here: \n### https://www.r-bloggers.com/2021/04/cluster-analysis-in-r/\nlibrary(tidyverse)\nlibrary(cluster)\nbeyonce <- read.csv(\"beyonce.csv\")\ntaylor <- read.csv(\"taylor.csv\")\n\n\ndf <- beyonce %>% \n  filter(album_name == \"4\") %>%\n  select(c(\"track_name\", \"acousticness\", \"liveness\", \"danceability\", \"loudness\", \"speechiness\", \"valence\"))\n\n\ndf <- df %>% distinct(track_name, .keep_all = TRUE)\n\n## cleaning up the data.\nz <- df[,-c(1,1)]\n\n### getting means of each category.\nmeans <- apply(z,2,mean)\n### getting standard deviation of each category.\nsds <- apply(z,2,sd)\n\n### scales the data in the matrix.\nscaled_data <- scale(z,center=means,scale=sds)\ndistance <- dist(scaled_data)\n\nAnd we can plot the data like this:\n\n### helps with the size of the image.\npar(mar = c(5, 4, 4, 1))\n\n### creates the cluster\ndf.hclust <- hclust(distance)\n\n### plots the data but with row numbers.\nplot(df.hclust)\n\n\n\n\nAnd we can add the track name like so:\n\nplot(df.hclust,labels=df$track_name,main='Default from hclust')\n\n\n\n\nWe can clean up the plot the be along a single x-axis with the hang argument.\n\nnodePar <- list(lab.cex = 0.6, pch = c(NA, 19), \n                cex = 0.7, col = \"blue\")\nplot(df.hclust,hang=-1, labels=df$track_name,main='Default from hclust')"
  },
  {
    "objectID": "class_notes/week_5.html#which-track-belongs-to-which-cluster",
    "href": "class_notes/week_5.html#which-track-belongs-to-which-cluster",
    "title": "Week 5: Regression and Clustering",
    "section": "Which track belongs to which cluster?",
    "text": "Which track belongs to which cluster?\nIt might be helpful with this analysis to look at how each of the songs fits on the tree. We can use the cutree function, which “cuts a tree” from the cluster based on how many groups we ask it for.\nThe following code can tell us how many fall into each broader tree, assuming we think that the tree should be cut into three. Notice that the third branch is the most populous, with the second being the most sparsely populated.\n\nmember <- cutree(df.hclust,3)\ntable(member)\n\nmember\n 1  2  3 \n 4  3 11 \n\n\nBut how is each category being weighted? The code below shows that acousticness and danceability do a fair bit of work in separating groups 1 and 3, and valence separates 1 and 2 from one another.\n\n##but how are these clusters calculated?\naggregate(scaled_data,list(member),mean)\n\n  Group.1 acousticness   liveness danceability   loudness speechiness\n1       1    1.2293770 -0.3037203   -1.5341655 -0.4289046   0.3971934\n2       2    0.6165508 -0.5097641    0.5152708 -1.0002027   1.0862774\n3       3   -0.6151964  0.2494703    0.4173500  0.4287478  -0.4406914\n     valence\n1 -1.2506018\n2  1.0310423\n3  0.1735709\n\n\nA slightly more even split occurs if we break it into four groups rather than three.\n\nmember <- cutree(df.hclust,4)\ntable(member)\n\nmember\n1 2 3 4 \n4 3 9 2 \n\n\nAnd that how they’re split into four is a bit different from how we might split them into three, but danceability and acousticness still playing a strong role.\n\naggregate(scaled_data,list(member),mean)\n\n  Group.1 acousticness   liveness danceability   loudness speechiness\n1       1    1.2293770 -0.3037203   -1.5341655 -0.4289046   0.3971934\n2       2    0.6165508 -0.5097641    0.5152708 -1.0002027   1.0862774\n3       3   -0.6115250 -0.1547202    0.4044253  0.3613888  -0.5628163\n4       4   -0.6317177  2.0683279    0.4755110  0.7318636   0.1088705\n      valence\n1 -1.25060184\n2  1.03104230\n3  0.05196371\n4  0.72080353\n\n\n\nK-Means Clustering\nWe can also run a simple k-means clustering on the data. With this, we are clustering the data into k groups. R’s documentation explains it like so:\n\naims to partition the points into k groups such that the sum of squares from points to the assigned cluster centres is minimized. At the minimum, all cluster centres are at the mean of their Voronoi sets (the set of data points which are nearest to the cluster centre).\n\nThere are a few algorithms to pick from. R uses the Hartigan and Wong (1979) algorithm by default.\n\n###split it into three groups\nset.seed(123)\nkc <- kmeans(scaled_data,3)\n\n### add labels.\nrow.names(scaled_data) <- df$track_name\n\n###get the shortest distance.\ndatadistshortset<-dist(scaled_data,method = \"euclidean\")\n\nThe code below will cluster it based on this k-means clustering distance, and plot them into the amount of groups listed (here 3).\n\nhc1 <- hclust(datadistshortset, method = \"complete\" )\npamvshortset <- pam(datadistshortset,3, diss = FALSE)\n\nclusplot(pamvshortset, shade = FALSE,labels=2,col.clus=\"blue\",col.p=\"red\",span=FALSE,main=\"Cluster Mapping\",cex=1.2)"
  },
  {
    "objectID": "class_notes/week_5.html#example-1-looking-at-nirvana-albumss",
    "href": "class_notes/week_5.html#example-1-looking-at-nirvana-albumss",
    "title": "Week 5: Regression and Clustering",
    "section": "Example 1: Looking at Nirvana Albumss",
    "text": "Example 1: Looking at Nirvana Albumss\nI’m going to get the global features from Nirvana, and specifically I’m just going to look at the Unplugged in New York album. I do this in two, rather inefficient, steps: I get all of the Nirvana data and put those in a dataframe, and then I create a variable that has filtered out only the specific album I’m looking for.\n\nnirvana <- get_artist_audio_features('nirvana')\nunplugged <- filter(nirvana, album_name == \"MTV Unplugged In New York\")\nboth <- filter(nirvana, album_name == \"MTV Unplugged In New York\"  | album_name == \"Nevermind\")\n\nThis gets lots of data, and I’m just interested in their global measures (tempo, danceability, liveness, etc.). Here, I’ve gone with column number rather than name, but the other version might be a bit easier/cleaner. Nevertheless, this is another way of doing it:\n\n###i've just picked out the columns I want.\nselected <- c(9,10,12,14,15,16,17,18,19,30)\n### This subsets the data based on only the columns I want.\nunplugged <- unplugged[,selected]\n\n### I assign the track name column (30) with the rownames, to have a labeled cluster.\nrownames(unplugged) <- unplugged$track_name\n\n\nhc <- hclust(dist(unplugged), method = \"complete\", members = NULL)\n\nWarning in dist(unplugged): NAs introduced by coercion\n\n\n\nPlotting the cluster\nTechnically, you could just use the plot function at this point, but there are some long title names, so I added these extra plot options to make the text smaller and increase the marins.\n\npar(cex=0.5, mar=c(5, 8, 4, 1))\nplot(hc, xlab=\"\", ylab=\"\", main=\"\", sub=\"\", axes=FALSE)\npar(cex=1)\ntitle(xlab=\"tunes\", ylab=\"height\", main=\"Nirvana unplugged\")\naxis(2)\n\n\n\n\nThis plot is a bit strange now, as we have a pretty big negative number on the y-axis. Nevertheless, we see some cool things. The songs written by the Meat Puppets cluster together, for example.\n\n\nk-means clustering\nOur next type of clustering analysis is be a k-means cluster. We will start off by using a scree plot to see how many clusters we should use. There are a number of ways of analyzing where an “elbow” on this plot might be, but many people actually just eyeball it.\n\nunplugged <- filter(nirvana, album_name == \"MTV Unplugged In New York\" )\ntitles <- unplugged$track_name\nunplugged <- unplugged[,selected]\nunplugged <- scale(unplugged[,-10]) # standardize variables\nunplugged <- as.data.frame(unplugged) # standardize variables\n\n# Determine number of clusters\nwss <- (nrow(unplugged)-1)*sum(apply(unplugged,2,var))\nfor (i in 2:9) wss[i] <- sum(kmeans(unplugged, \n                                    centers=i)$withinss)\n  plot(1:9, wss, type=\"b\", xlab=\"Number of Clusters\",\n    ylab=\"Within groups sum of squares\")\n\n\n\n\nAnd now we can look at the k-means clustering based on however many clusters we think are necessary.\n\n# K-Means Cluster Analysis\nfit <- kmeans(unplugged, 3) # 3 cluster solution\n# get cluster means \naggregate(unplugged,by=list(fit$cluster),FUN=mean)\n\n  Group.1 danceability     energy   loudness speechiness acousticness\n1       1     1.026773  0.1850689  0.4603061 -0.67242123    0.5744139\n2       2    -0.466715  0.7286156  0.3748797  0.92850767   -1.1595582\n3       3    -0.653401 -0.7679614 -0.7602098 -0.07038491    0.3532327\n  instrumentalness    liveness    valence      tempo\n1       -0.1528028  0.59048330  0.5986320  0.1965621\n2       -0.3318123  0.09159538  0.4107587  0.7767115\n3        0.4182526 -0.66375960 -0.9272390 -0.8179312\n\n# append cluster assignment\nunplugged_appended <- data.frame(unplugged, fit$cluster)\n\n\nrownames(unplugged_appended) = titles\nclusplot(unplugged_appended, fit$cluster, color=TRUE, shade=TRUE, \n   labels=3, lines=0)"
  },
  {
    "objectID": "class_notes/week_5.html#conditional-inference-tree-with-party",
    "href": "class_notes/week_5.html#conditional-inference-tree-with-party",
    "title": "Week 5: Regression and Clustering",
    "section": "Conditional Inference Tree with Party",
    "text": "Conditional Inference Tree with Party\nA conditional inference tree is basically a regression tree, and it tells you exactly how it picks apart the data in a pretty clear way.\nI’ve always thought that Weezer was a bit derivative, so we might look at how we can separate them other (much better) bands, like Pavement…\n\npavement <- get_artist_audio_features('pavement')\nweezer <- get_artist_audio_features('weezer')\npavement_weezer <-rbind(pavement, weezer)\n\nHere’s a regression tree that tries to account for the variance between deciding whether a piece is from Pavement or Weezer.\n\nlibrary(rpart.plot)\n\nLoading required package: rpart\n\n# grow tree \nfit <- rpart(as.factor(artist_name) ~ danceability + valence + tempo + liveness,  data=pavement_weezer)\n\nprintcp(fit) # display the results \n\n\nClassification tree:\nrpart(formula = as.factor(artist_name) ~ danceability + valence + \n    tempo + liveness, data = pavement_weezer)\n\nVariables actually used in tree construction:\n[1] danceability liveness     valence     \n\nRoot node error: 349/979 = 0.35649\n\nn= 979 \n\n        CP nsplit rel error  xerror     xstd\n1 0.047755      0   1.00000 1.00000 0.042940\n2 0.022923      4   0.80516 0.89398 0.041776\n3 0.020057      5   0.78223 0.85960 0.041331\n4 0.010000      6   0.76218 0.85673 0.041293\n\nplotcp(fit) # visualize cross-validation results \n\n\n\nsummary(fit) # detailed summary of splits\n\nCall:\nrpart(formula = as.factor(artist_name) ~ danceability + valence + \n    tempo + liveness, data = pavement_weezer)\n  n= 979 \n\n          CP nsplit rel error    xerror       xstd\n1 0.04775549      0 1.0000000 1.0000000 0.04294041\n2 0.02292264      4 0.8051576 0.8939828 0.04177567\n3 0.02005731      5 0.7822350 0.8595989 0.04133127\n4 0.01000000      6 0.7621777 0.8567335 0.04129270\n\nVariable importance\ndanceability      valence     liveness        tempo \n          61           28           10            1 \n\nNode number 1: 979 observations,    complexity param=0.04775549\n  predicted class=Weezer    expected loss=0.3564862  P(node) =1\n    class counts:   349   630\n   probabilities: 0.356 0.644 \n  left son=2 (618 obs) right son=3 (361 obs)\n  Primary splits:\n      danceability < 0.522    to the left,  improve=43.858290, (0 missing)\n      valence      < 0.2975   to the left,  improve=13.962690, (0 missing)\n      tempo        < 142.843  to the right, improve= 7.641565, (0 missing)\n      liveness     < 0.05625  to the right, improve= 6.520634, (0 missing)\n  Surrogate splits:\n      valence  < 0.5905   to the left,  agree=0.697, adj=0.177, (0 split)\n      liveness < 0.05625  to the right, agree=0.647, adj=0.042, (0 split)\n      tempo    < 76.103   to the right, agree=0.635, adj=0.011, (0 split)\n\nNode number 2: 618 observations,    complexity param=0.04775549\n  predicted class=Weezer    expected loss=0.4708738  P(node) =0.6312564\n    class counts:   291   327\n   probabilities: 0.471 0.529 \n  left son=4 (53 obs) right son=5 (565 obs)\n  Primary splits:\n      liveness     < 0.5765   to the right, improve=7.022553, (0 missing)\n      valence      < 0.1925   to the left,  improve=6.959549, (0 missing)\n      danceability < 0.404    to the left,  improve=5.352002, (0 missing)\n      tempo        < 125.6825 to the left,  improve=4.458552, (0 missing)\n\nNode number 3: 361 observations\n  predicted class=Weezer    expected loss=0.1606648  P(node) =0.3687436\n    class counts:    58   303\n   probabilities: 0.161 0.839 \n\nNode number 4: 53 observations\n  predicted class=Pavement  expected loss=0.2830189  P(node) =0.05413687\n    class counts:    38    15\n   probabilities: 0.717 0.283 \n\nNode number 5: 565 observations,    complexity param=0.04775549\n  predicted class=Weezer    expected loss=0.4477876  P(node) =0.5771195\n    class counts:   253   312\n   probabilities: 0.448 0.552 \n  left son=10 (61 obs) right son=11 (504 obs)\n  Primary splits:\n      valence      < 0.6655   to the right, improve=10.232180, (0 missing)\n      danceability < 0.5165   to the right, improve= 4.949092, (0 missing)\n      tempo        < 77.8915  to the left,  improve= 3.823105, (0 missing)\n      liveness     < 0.09715  to the left,  improve= 3.710828, (0 missing)\n\nNode number 10: 61 observations\n  predicted class=Pavement  expected loss=0.2786885  P(node) =0.06230848\n    class counts:    44    17\n   probabilities: 0.721 0.279 \n\nNode number 11: 504 observations,    complexity param=0.04775549\n  predicted class=Weezer    expected loss=0.4146825  P(node) =0.514811\n    class counts:   209   295\n   probabilities: 0.415 0.585 \n  left son=22 (98 obs) right son=23 (406 obs)\n  Primary splits:\n      valence      < 0.192    to the left,  improve=7.635957, (0 missing)\n      danceability < 0.5165   to the right, improve=4.863906, (0 missing)\n      tempo        < 77.8915  to the left,  improve=3.964739, (0 missing)\n      liveness     < 0.3565   to the left,  improve=3.188013, (0 missing)\n  Surrogate splits:\n      tempo        < 202.119  to the right, agree=0.813, adj=0.041, (0 split)\n      danceability < 0.196    to the left,  agree=0.812, adj=0.031, (0 split)\n\nNode number 22: 98 observations,    complexity param=0.02292264\n  predicted class=Pavement  expected loss=0.4081633  P(node) =0.1001021\n    class counts:    58    40\n   probabilities: 0.592 0.408 \n  left son=44 (90 obs) right son=45 (8 obs)\n  Primary splits:\n      danceability < 0.497    to the left,  improve=6.102494, (0 missing)\n      liveness     < 0.09655  to the left,  improve=4.191436, (0 missing)\n      tempo        < 130.712  to the right, improve=3.409439, (0 missing)\n      valence      < 0.113    to the right, improve=1.896793, (0 missing)\n\nNode number 23: 406 observations,    complexity param=0.02005731\n  predicted class=Weezer    expected loss=0.3719212  P(node) =0.4147089\n    class counts:   151   255\n   probabilities: 0.372 0.628 \n  left son=46 (7 obs) right son=47 (399 obs)\n  Primary splits:\n      danceability < 0.5165   to the right, improve=5.619653, (0 missing)\n      valence      < 0.22     to the right, improve=3.489861, (0 missing)\n      liveness     < 0.363    to the left,  improve=2.817898, (0 missing)\n      tempo        < 124.612  to the left,  improve=2.773645, (0 missing)\n\nNode number 44: 90 observations\n  predicted class=Pavement  expected loss=0.3555556  P(node) =0.09193054\n    class counts:    58    32\n   probabilities: 0.644 0.356 \n\nNode number 45: 8 observations\n  predicted class=Weezer    expected loss=0  P(node) =0.008171604\n    class counts:     0     8\n   probabilities: 0.000 1.000 \n\nNode number 46: 7 observations\n  predicted class=Pavement  expected loss=0  P(node) =0.007150153\n    class counts:     7     0\n   probabilities: 1.000 0.000 \n\nNode number 47: 399 observations\n  predicted class=Weezer    expected loss=0.3609023  P(node) =0.4075587\n    class counts:   144   255\n   probabilities: 0.361 0.639 \n\n# plot tree \nrpart.plot(fit, uniform=TRUE, \n   main=\"Regression Tree for Pavement/Weezer\")"
  },
  {
    "objectID": "class_notes/week_5.html#christmas-or-not",
    "href": "class_notes/week_5.html#christmas-or-not",
    "title": "Week 5: Regression and Clustering",
    "section": "Christmas or Not?",
    "text": "Christmas or Not?\n\nchristmas <- get_playlist_audio_features(\"\", \"5OP7itTh52BMfZS1DJrdlv\")\nchristmas$christmas <- \"yes\"\n\nnot <- get_playlist_audio_features(\"\", \"6i2Qd6OpeRBAzxfscNXeWp\")\nnot$christmas <- \"no\"\nchristmas_not <-rbind(christmas, not)\n\nfit <- rpart(as.factor(christmas) ~ danceability + valence + tempo + liveness + tempo + mode, data=christmas_not)\n\n# plot tree \nrpart.plot(fit, uniform=TRUE, \n   main=\"Regression Tree for Christmas/Not\")\n\n\n\n\n\ntable(not$mode)\n\n\n 0  1 \n34 80"
  },
  {
    "objectID": "class_notes/week_6.html",
    "href": "class_notes/week_6.html",
    "title": "Week 6: Classifying",
    "section": "",
    "text": "Look at running a principal components analysis for authorship\nWork on some models for classifying data\nDiscuss how we might evaluate our models\n\n\n\nWe will be using a of libraries today:\n\n\n\n\n\n\n\n\n\nPCAs are often used for reducing dimensions when we have lots of variables but a model might be better suited from combining those variables. PCAs have also been used a fair bit to explore questions of authorship. Here we have a question of authorship using symbolic data taken from scores. We are trying to explore the music of Josquin.\nHere we load the data in:\n\ncomplete_data <- read.csv(\"attribution_data_new.csv\", na.strings=c(\"\",\"NA\"), header=T)\ncomplete_data <- complete_data[,-62]\n\nJesse Rodin’s Josquin Research Project has given levels of security for attribution, including pieces that we know are Josquin’s, those we think might be, and those which are more questionable.\n\n# Josquin attribution level 1 and palestrina\n\njosquin <- complete_data[complete_data$Composer == 'Josquin des Prez',-12]\n\njosquin_secure <- josquin[josquin$Attribution.Level <= 2 ,]\njosquin_secure$Composer <- as.character(josquin_secure$Composer)\njosquin_less_secure <- josquin[ josquin$Attribution.Level >= 3,]\n\n\n####Other composers\nbach <- complete_data[complete_data$Composer == \"Bach_Johann Sebastian\",-12]\nlarue <- complete_data[complete_data$Composer == \"la Rue_Pierre de\",-12]\npalestrina <- complete_data[complete_data$Composer == \"Palestrina_Giovanni Perluigi da\",-12]\nockeghem <- complete_data[complete_data$Composer == \"Johannes Ockeghem\",-12]\norto <- complete_data[complete_data$Composer == \"de Orto_Marbrianus\",-12]\ndufay <- complete_data[complete_data$Composer == \"Du Fay_Guillaume\",-12]\n\njosquin_bach <- rbind(josquin_secure, bach)\njosquin_palestrina <- rbind(josquin_secure, palestrina)\njosquin_larue <- rbind(josquin_secure, larue)\n\ncomparison <- rbind(josquin_secure, bach)\n\n\ncolumns_wanted <- c(5:11)  \nMatrix <- comparison[,columns_wanted]\nMatrix <- as.matrix(Matrix)\nMatrix[is.na(Matrix)] <- 0\n# log.pieces <- log(Matrix)\nlog.pieces <- log(Matrix)\n\nWarning in log(Matrix): NaNs produced\n\ncomposer <- comparison[,1]\n\nThis code runs the actual principal components analysis.\nIt also provides a scree plot, allowing us to see which components are the most heavily weighted. This can allow us to reduce the dimensions as we see fit.\n\n####principle component analysis.\n\npieces.pca <- prcomp(Matrix,\n                 center = TRUE,\n                 scale. = TRUE) \nplot(pieces.pca, type = \"l\", main=\"Principal Components Analysis\")\n\n\n\n\nIt’s worth taking some time to explore what each of these components actually means and how they’re weighted. PCA is weighting instances of parallel motion and similar motion pretty heavily, but negatively weighting pitch entropy and oblique motion. PC2 seems to be looking at nPVI and 9-8 suspensions.\n\nprint(pieces.pca)\n\nStandard deviations (1, .., p=7):\n[1] 1.8907847 0.9923828 0.8705046 0.8298104 0.7104739 0.5567648 0.4230672\n\nRotation (n x k) = (7 x 7):\n                         PC1         PC2        PC3         PC4         PC5\nnPVI_Entire       -0.2826479  0.52894566 -0.2336756  0.74429280 -0.17804201\nNine_Eight        -0.2553594  0.61806193  0.5670902 -0.41414816 -0.23123893\npitch_correlation -0.3244143  0.05847133 -0.7184471 -0.47933237 -0.36850440\npitch_entropy     -0.4038052  0.19724082 -0.1329848 -0.14687309  0.77362200\nparallel_motion    0.4444947  0.24809410 -0.1263420 -0.08782873 -0.20277222\nsimilar_motion     0.4682238  0.29107268 -0.1026235 -0.05294267  0.04450771\noblique_motion    -0.4120550 -0.38680631  0.2519115  0.11252328 -0.37073657\n                           PC6          PC7\nnPVI_Entire        0.006914729  0.001955825\nNine_Eight         0.076657435 -0.018255504\npitch_correlation  0.076213006  0.061699313\npitch_entropy     -0.387728754  0.099780551\nparallel_motion   -0.750907432 -0.334991080\nsimilar_motion     0.016773922  0.824891734\noblique_motion    -0.523249921  0.439584522\n\n\nAs we can see, about 65% of the variance is accounted for with the first two principal components:\n\nsummary(pieces.pca)\n\nImportance of components:\n                          PC1    PC2    PC3     PC4     PC5     PC6     PC7\nStandard deviation     1.8908 0.9924 0.8705 0.82981 0.71047 0.55676 0.42307\nProportion of Variance 0.5107 0.1407 0.1082 0.09837 0.07211 0.04428 0.02557\nCumulative Proportion  0.5107 0.6514 0.7597 0.85804 0.93015 0.97443 1.00000\n\n\nPlotting our two composers with the first two principal components.\n\ng <- ggbiplot(pieces.pca, obs.scale = 1, var.scale = 1, \n              groups = composer, ellipse = TRUE, \n              circle = TRUE)\ng <- g + scale_color_discrete(name = '')\ng <- g + theme(legend.direction = 'horizontal', \n               legend.position = 'top') +\n               theme_bw()\nprint(g)\n\n\n\n# we can change the number of components\n# seven_component_model <- data.frame(pieces.pca$x[,1:8])\n\nWe can also look at how much each of these features is being weighted within the first two components.\n\ntheta <- seq(0,2*pi,length.out = 100)\ncircle <- data.frame(x = cos(theta), y = sin(theta))\np <- ggplot(circle,aes(x,y)) + geom_path()\n\nloadings <- data.frame(pieces.pca$rotation, \n                       .names = row.names(pieces.pca$rotation))\np + geom_text(data=loadings, \n              mapping=aes(x = PC1, y = PC2, label = .names, colour = .names)) +\n  coord_fixed(ratio=1) +\n  labs(x = \"PC1\", y = \"PC2\") +\n  theme_bw()\n\n\n\n\n\n\n\nA classifier is a model that assigns a label to data based on the input. There are many types of classifiers, and we will be evaluating various models throughout the week.\nOur goal will be to train a model on the features generally associated with a category, and then test the accuracy of that model. For now, a good starting point might be our Christmas Song question from last week.\n\n\n\nFirst, let’s get the data and add a column that tells us whether it’s a Christmas song or not\n\n### get the data and add yes/no column.\nchristmas <- get_playlist_audio_features(\"\", \"5OP7itTh52BMfZS1DJrdlv\")\nchristmas$christmas <- \"yes\"\n\nnot <- get_playlist_audio_features(\"\", \"6i2Qd6OpeRBAzxfscNXeWp\")\nnot$christmas <- \"no\"\n\n## combine the two datasets and get the columns we want to use.\nchristmas_subset <-rbind(christmas, not)\nchristmas_subset <- christmas_subset %>% \n    select(c(\"christmas\", \"acousticness\", \"liveness\", \"danceability\", \"loudness\", \"speechiness\", \"valence\"))\n\nNow we can use the createDataPartition function from the caret library to create a testing and a training dataset. Here, I’ve chosen a 70/30 partition of training and testing, but you can adjust as you see fit.\n\nTrain <- createDataPartition(christmas_subset$christmas, p=0.7, list=FALSE)\ntraining <- christmas_subset[ Train, ]\ntesting <- christmas_subset[ -Train, ]\n\nWe can pretty easily implement something like a neural network, using our training dataset to train it:\n\nmod_fit <- caret::train(christmas ~ .,  \n                 data=training, method=\"nnet\", importance = \"christmas\")\n\nOnce we’ve trained this model, we can test it on our testing dataset, and see how well it does:\n\npred <- predict(mod_fit, testing)\nconfusionMatrix(pred, as.factor(testing$christmas), positive = \"yes\")\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction no yes\n       no  25  14\n       yes  9  16\n                                         \n               Accuracy : 0.6406         \n                 95% CI : (0.511, 0.7568)\n    No Information Rate : 0.5312         \n    P-Value [Acc > NIR] : 0.0509         \n                                         \n                  Kappa : 0.2713         \n                                         \n Mcnemar's Test P-Value : 0.4042         \n                                         \n            Sensitivity : 0.5333         \n            Specificity : 0.7353         \n         Pos Pred Value : 0.6400         \n         Neg Pred Value : 0.6410         \n             Prevalence : 0.4688         \n         Detection Rate : 0.2500         \n   Detection Prevalence : 0.3906         \n      Balanced Accuracy : 0.6343         \n                                         \n       'Positive' Class : yes            \n                                         \n\n\nSo what does this all mean? Let’s define some terms.\n\nAccuracy:\n\nthe accuracy rate. Just how many things it got right.\n\n95% CI:\n\nthe confidence interval of the accuracy.\n\nNo information rate:\n\ngiven no more information other than the overall distribution, how likely are you to be correct if you just pick the “majority class.”\nif you have an accuracy rate of 80%, but the majority class is 80%, then your model isn’t terribly useful.\n\nP-Value:\n\nlikelihood of chance.\n\nKappa:\n\nmeasures the agreement between two raters and ratings. Here it’s looking at the difference between observed accuracy and random chance given the distribution in the dataset.\n\nMcNemar’s Test P-Value:\n\nthis is looking at the two distributions (from a 2x2 table), and determines if they are significantly different,\n\nSensitivity:\n\ngiven that a result is actually a thing, what is the probability that our model will predict that event’s results?\n\nSpecificity:\n\ngiven that a result is not actually a thing, what is the probability that our model will predict that?\n\nPos Predictive Value:\n\nthe probability that a predicted ‘positive’ class is actually positive.\n\nNeg Predictive Value:\n\nthe probability that a predicted ‘negative’ class is actually negative.\n\nPrevalence:\n\nthe prevalence of the ‘positive event’\n\nDetection Rate:\n\nthe rate of true events also predicted to be events\n\nDetection Prevalence\n\nthe prevalence of predicted events\n\nBalanced Accuracy:\n\nthe average of the proportion corrects of each class individually\n\n\n\n\nWe can look at which features the model is using…\n\nplot(varImp(mod_fit))\n\n\n\n\n\n\n\n\n\nUse PCA to explore the works of two artists. How well do they “separate”?\nRun a classifier on two groups (it can be the same two artists, or two distinct groups). How well does your model do?"
  },
  {
    "objectID": "class_notes/other_files.qmd/sampling_exercises.html",
    "href": "class_notes/other_files.qmd/sampling_exercises.html",
    "title": "Sampling Exercise",
    "section": "",
    "text": "For each case, identify the kind of sampling employed.\n\nA researcher walks into a music library with a question: Are sharp keys more common than flat keys? Wandering through the stacks, she blindly grabs volumes off the shelves and allows each volume to open spontaneously to some page. She takes note of the key signature.\nA professional music marketer is interested in carrying out a detailed survey of musical tastes in Britain. The marketer decides to use the ACORN geodemographic profile. British households will be sampled in proportion to the second-level ACORN categories: wealthy executives (8.6 percent of the population), affluent greys (7.7%), flourishing families (8.8%), prosperous professionals (2.2%), educated urbanites (4.6%), aspiring singles (3.9%), starting out (2.5%), secured families (15.5%), settled surburbia (6.0%), prudent pensioners (2.6%), asian communities (1.6%), post-industrial families (4.8%), blue collar roots (8.0%), struggling families (14.1%), burdened singles (4.5%), high rise hardship (1.6%), and inner city adversity (2.1%).\nA researcher is interested in assembling a random sample of “classical” keyboard music. She has determined that she needs roughly 20 pieces for her study. In order to maximize data independence, she wants each piece to be written by a different composer. Using Wikipedia, she finds an alphabetical list of “classical composers.” For each letter of the alphabet, she selects the first composer who she knows has written for piano: Isaac Albéniz, Carl Philipp Emanuel Bach, Alfredo Casella, Claude Debussy, Edward Elgar, Manuel de Falla, etc.\nIn piloting an experiment, a graduate student recruits her graduate student colleagues as experimental participants.\nA team of researchers is interested in emotional expression in Hindustani film music. Indian participants are asked to characterize the emotional tenor of various film scenes. Using the descriptions, the researchers then classify the scenes into 14 categories — such as romantic, humorous, physical conflict, emotional tension, etc. The researchers then select four scenes for each of the 14 categories and analyse the associated background music. Their goal is to identify musical features in Hindustani culture that signal romance, humor, etc.\nA medievalist thinks that the Dorian mode was more likely to have been heard as comparatively “happy” whereas the Phrygian mode was more likely to have been heard as comparative “sad” for medieval listeners. In order to test this notion, the scholar examines all of the Glorias (nominally “happy” text) and Kyries (nominally “sad” text) in the Liber Usualis. The prediction is that Dorian will predominate for Glorias while Phrygian will be more likely to occur for Kyries.\nA researcher is interested in changing harmonic patterns in the masses of Palestrina. The researcher makes us of the Humdrum database of the scores for the complete 103 masses assembled by musicologist John Miller.\nPaul von Hippel and David Huron (2000) carried out a study to test the idea that melodies tend to change direction following a leap, and that this pattern is ubiquitous in musical melodies around the world. In order to test this idea, they made use of two musical samples. The first sample selected music spanning five centuries. The second sample selected music spanning five continents: Africa, Asia, Europe, North and South America.\nUnsure of the contents of a box, an archivist reaches in and grabs a couple of documents, which he then examines.\nA researcher wants to know whether there is anything Italian, French or German about augmented sixth chords. Using large computer databases, the researcher uses Humdrum to isolate 900 sonorities in which the lowered sixth and raised fourth appear concurrently (including enharmonic spellings): 300 each written by Italian, French and German composers. Each of the sonorities is then classified as either Italian, French, German or Other.\n\n\n\nPaul von Hippel & David Huron (2000). Why do skips precede reversals? The effect of tessitura on melodic structure. Music Perception, Vol. 18, No. 1, pp. 59-85."
  },
  {
    "objectID": "class_notes/week_2.html#chords",
    "href": "class_notes/week_2.html#chords",
    "title": "Week 2: Pitch",
    "section": "Chords",
    "text": "Chords\nBurgoyne’s chordogram functions allow us to look at the likely chordal spaces for specific piecses. The code below does a few things:\n\nFirst we define what a major, minor, and seventh chord looks like in terms of pitch space.\nWe then use the key-profiles from the Krumhansl-Kessler article on the probe tone experiments and store them into major_key and minor_key variables.\nThe circshift function rotates these key profiles through the chord variables and provides the best fit for that moment. This is done through the key_templates variable (Notice the compmus_match_pitch_template below).\n\n\n#      C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B\nmajor_chord <-\n  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)\nminor_chord <-\n  c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)\nseventh_chord <-\n  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)\n\nmajor_key <-\n  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)\nminor_key <-\n  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)\n\ncircshift <- function(v, n) {\n  if (n == 0) v else c(tail(v, n), head(v, -n))\n}\nchord_templates <-\n  tribble(\n    ~name, ~template,\n    \"Gb:7\", circshift(seventh_chord, 6),\n    \"Gb:maj\", circshift(major_chord, 6),\n    \"Bb:min\", circshift(minor_chord, 10),\n    \"Db:maj\", circshift(major_chord, 1),\n    \"F:min\", circshift(minor_chord, 5),\n    \"Ab:7\", circshift(seventh_chord, 8),\n    \"Ab:maj\", circshift(major_chord, 8),\n    \"C:min\", circshift(minor_chord, 0),\n    \"Eb:7\", circshift(seventh_chord, 3),\n    \"Eb:maj\", circshift(major_chord, 3),\n    \"G:min\", circshift(minor_chord, 7),\n    \"Bb:7\", circshift(seventh_chord, 10),\n    \"Bb:maj\", circshift(major_chord, 10),\n    \"D:min\", circshift(minor_chord, 2),\n    \"F:7\", circshift(seventh_chord, 5),\n    \"F:maj\", circshift(major_chord, 5),\n    \"A:min\", circshift(minor_chord, 9),\n    \"C:7\", circshift(seventh_chord, 0),\n    \"C:maj\", circshift(major_chord, 0),\n    \"E:min\", circshift(minor_chord, 4),\n    \"G:7\", circshift(seventh_chord, 7),\n    \"G:maj\", circshift(major_chord, 7),\n    \"B:min\", circshift(minor_chord, 11),\n    \"D:7\", circshift(seventh_chord, 2),\n    \"D:maj\", circshift(major_chord, 2),\n    \"F#:min\", circshift(minor_chord, 6),\n    \"A:7\", circshift(seventh_chord, 9),\n    \"A:maj\", circshift(major_chord, 9),\n    \"C#:min\", circshift(minor_chord, 1),\n    \"E:7\", circshift(seventh_chord, 4),\n    \"E:maj\", circshift(major_chord, 4),\n    \"G#:min\", circshift(minor_chord, 8),\n    \"B:7\", circshift(seventh_chord, 11),\n    \"B:maj\", circshift(major_chord, 11),\n    \"D#:min\", circshift(minor_chord, 3)\n  )\n\nkey_templates <-\n  tribble(\n    ~name, ~template,\n    \"Gb:maj\", circshift(major_key, 6),\n    \"Bb:min\", circshift(minor_key, 10),\n    \"Db:maj\", circshift(major_key, 1),\n    \"F:min\", circshift(minor_key, 5),\n    \"Ab:maj\", circshift(major_key, 8),\n    \"C:min\", circshift(minor_key, 0),\n    \"Eb:maj\", circshift(major_key, 3),\n    \"G:min\", circshift(minor_key, 7),\n    \"Bb:maj\", circshift(major_key, 10),\n    \"D:min\", circshift(minor_key, 2),\n    \"F:maj\", circshift(major_key, 5),\n    \"A:min\", circshift(minor_key, 9),\n    \"C:maj\", circshift(major_key, 0),\n    \"E:min\", circshift(minor_key, 4),\n    \"G:maj\", circshift(major_key, 7),\n    \"B:min\", circshift(minor_key, 11),\n    \"D:maj\", circshift(major_key, 2),\n    \"F#:min\", circshift(minor_key, 6),\n    \"A:maj\", circshift(major_key, 9),\n    \"C#:min\", circshift(minor_key, 1),\n    \"E:maj\", circshift(major_key, 4),\n    \"G#:min\", circshift(minor_key, 8),\n    \"B:maj\", circshift(major_key, 11),\n    \"D#:min\", circshift(minor_key, 3)\n  )\n\nHere we have a piece of code that grabs a single audio file (“Those magic changes”). In class we listened to it while going through the chordogram. Can you spot the modulation? Why do we get that yellowish color at the end of the graph?\n\nthose_magic_changes <-\n  get_tidy_audio_analysis(\"1WHauHX7U6FqOWh46lK4IV\") %>%\n  compmus_align(sections, segments) %>%\n  select(sections) %>%\n  unnest(sections) %>%\n  mutate(\n    pitches =\n      map(segments,\n          compmus_summarise, pitches,\n          method = \"mean\", norm = \"manhattan\"\n      )\n  )\n\nthose_magic_changes %>% \n  compmus_match_pitch_template(\n    key_templates,         # Change to chord_templates if descired\n    method = \"euclidean\",  # Try different distance metrics\n    norm = \"manhattan\"     # Try different norms\n  ) %>%\n  ggplot(\n    aes(x = start + duration / 2, width = duration, y = name, fill = d)\n  ) +\n  geom_tile() +\n  scale_fill_viridis_c(guide = \"none\") +\n  theme_minimal() +\n  labs(x = \"Time (s)\", y = \"\")\n\n\n\n\nSome activities:\n\nGo through some songs that you know. How close is the chordogram of providing some sort of brief explanatory analysis?\nWhat does this look like on music that might be considered less adherent to notions of western tonal music?"
  },
  {
    "objectID": "class_notes/week_3.html#midterms",
    "href": "class_notes/week_3.html#midterms",
    "title": "Week 3: Time",
    "section": "Midterms",
    "text": "Midterms\n\nDue April 28th\n1-3 pages long.\nconstruct a literature review of the topic you’re interested in writing about. This could culminate in a hypothesis, and a discussion of how your work fills the current gaps in the literature.\nBe sure to include an overview of previous research related to the topic. This should include both empirical and non-empirical work. So if you’re focusing on the memory for jazz licks, for example, research on improvisational styles should be included alongside work on memory for musical ideas. It need not be all-encompassing, but it should try to cover as much ground as possible.\nAddress a gap in the current literature, or that between theory and research. End with a discussion of your study, and how it hopes to fill in these gaps.\nFeel free to meet with me if you have any questions."
  },
  {
    "objectID": "class_notes/week_3.html#populations",
    "href": "class_notes/week_3.html#populations",
    "title": "Week 3: Time",
    "section": "Populations",
    "text": "Populations\n\nA population is everything or everyone that you’re interested in.\ne.g. all the world’s people\nall the world’s people including living and deceased\nall Western-enculturated people\nall people who enjoy listening to music\nall clarinet players\n\nA “population” does not refer only to people: Other examples:\n\nall of the music written by Vivaldi\nall solo flute music (both with and without accompaniment)\nall music in the minor mode\nall of the jazz scores available in the New York Public Library\nall performances of Rachmaninov’s 2nd piano concerto"
  },
  {
    "objectID": "class_notes/week_3.html#sample",
    "href": "class_notes/week_3.html#sample",
    "title": "Week 3: Time",
    "section": "Sample",
    "text": "Sample\n\nSample: a subset of the population that you hope closely resembles the population as a whole.\nA sample is said to be representative when the property of interest is identical in both the sample and the population."
  },
  {
    "objectID": "class_notes/week_3.html#bias",
    "href": "class_notes/week_3.html#bias",
    "title": "Week 3: Time",
    "section": "Bias",
    "text": "Bias\n\nA sample is said to be biased when the property of interest differs between the sample and the population."
  },
  {
    "objectID": "class_notes/week_3.html#weird",
    "href": "class_notes/week_3.html#weird",
    "title": "Week 3: Time",
    "section": "WEIRD",
    "text": "WEIRD\n\nWestern\nEducated\nIndustrialized\nRich\nDemocratic\n\nsee Henrich’s Work on this"
  },
  {
    "objectID": "class_notes/week_3.html#defining-your-population",
    "href": "class_notes/week_3.html#defining-your-population",
    "title": "Week 3: Time",
    "section": "Defining Your Population",
    "text": "Defining Your Population\n\nYou can’t sample a population unless you have a clear idea of what constitutes the population of interest.\n\nSuppose, for example, that you are a political pollster. Your aim is to predict the likely election results for a national election in Denmark. What, precisely, is the population you are interested in?"
  },
  {
    "objectID": "class_notes/week_3.html#defining-your-population-continued",
    "href": "class_notes/week_3.html#defining-your-population-continued",
    "title": "Week 3: Time",
    "section": "Defining Your Population (continued)",
    "text": "Defining Your Population (continued)\n\nAll Danish citizens?\nAll people living in Denmark?\nAll people living in Denmark eligible to vote?\nAll people eligible to vote in Danish elections?\nAll people likely to vote in Danish elections?"
  },
  {
    "objectID": "class_notes/week_3.html#sampling-method",
    "href": "class_notes/week_3.html#sampling-method",
    "title": "Week 3: Time",
    "section": "Sampling Method",
    "text": "Sampling Method\n\nSampling method: the way you recruit or assemble your sample. When your population consists of people, sampling methods might include soliciting information by telephone (telephone sampling), street sampling, mail sampling, web sampling, classroom sampling, concert sampling, etc."
  },
  {
    "objectID": "class_notes/week_3.html#sampling-bias",
    "href": "class_notes/week_3.html#sampling-bias",
    "title": "Week 3: Time",
    "section": "Sampling Bias",
    "text": "Sampling Bias\n\nSampling bias: when the sampling method introduces differences that cause the sample not to be representative. We try to avoid or minimize sampling bias.\nWhen conducting a telephone survey, a pollster may be tempted to ask to speak to a respondent’s spouse. However, spouses are likely to share many things in common (such as political views) so the sampling method will introduce a bias."
  },
  {
    "objectID": "class_notes/week_3.html#simple-random-sampling",
    "href": "class_notes/week_3.html#simple-random-sampling",
    "title": "Week 3: Time",
    "section": "Simple Random Sampling",
    "text": "Simple Random Sampling\n\nSimple Random Sampling. Suppose we want to know about musical instrument sales in the City of Nashville. We could use the phone book to identify all of the shops within the city boundaries that sell musical instruments. Perhaps we discover that there are 131 retailers. From this list, we might randomly select 25 retailers in order to carry out our survey."
  },
  {
    "objectID": "class_notes/week_3.html#systematic-sampling",
    "href": "class_notes/week_3.html#systematic-sampling",
    "title": "Week 3: Time",
    "section": "Systematic Sampling",
    "text": "Systematic Sampling\nSuppose that we have a questionnaire we want to distribute to people who attended a concert. There might be 500 audience members, but we have only 50 surveys to distribute. One approach would be to distribute the questionnaires to the first 50 people leaving the concert hall."
  },
  {
    "objectID": "class_notes/week_3.html#matched-random-sampling",
    "href": "class_notes/week_3.html#matched-random-sampling",
    "title": "Week 3: Time",
    "section": "Matched Random Sampling",
    "text": "Matched Random Sampling\n\nA way of linking members from two or more samples. For example, a study might involve matching each professional musician with an amateur musician who plays the same instrument."
  },
  {
    "objectID": "class_notes/week_3.html#convenience-sampling",
    "href": "class_notes/week_3.html#convenience-sampling",
    "title": "Week 3: Time",
    "section": "Convenience Sampling",
    "text": "Convenience Sampling\n\nConvenience Sampling. A convenience sample simply takes advantage of whatever might be available. For example, a sample of organ music by Gabriel Fauré might simply consist of all of the scores available in a music library. Similarly, we might stand on a street corner and ask whoever passes by to answer questions on a survey."
  },
  {
    "objectID": "class_notes/week_3.html#stratified-sampling",
    "href": "class_notes/week_3.html#stratified-sampling",
    "title": "Week 3: Time",
    "section": "Stratified Sampling",
    "text": "Stratified Sampling\n\nWhen we have reason to suspect that differences in sub-populations might influence the results, it is common to sample in such a way to ensure that each of the main sub-populations is represented.\nPost and Huron (2009) were interested in common-practice era tonal classical music. So we decided to use a stratified sample consisting of music from three periods: Baroque, Classical and Romantic. Our overall sample consisted of equivalent numbers of works from each of these historical eras."
  },
  {
    "objectID": "class_notes/week_3.html#quota-sampling",
    "href": "class_notes/week_3.html#quota-sampling",
    "title": "Week 3: Time",
    "section": "Quota Sampling",
    "text": "Quota Sampling\n\nA type of stratified sampling in which sub-samples are weighted according to their prevalence in the population.\nSuppose that we find that 52% of instrumentalists are most accomplished on guitar, 33% are most accomplished on keyboards, 12% on flute, 9% on trumpet, 8% on violin, etc. In quota sampling, we would aim to sample the same proportions for each instrument."
  },
  {
    "objectID": "class_notes/week_3.html#exercise",
    "href": "class_notes/week_3.html#exercise",
    "title": "Week 3: Time",
    "section": "Exercise",
    "text": "Exercise\nGroup Exercise"
  },
  {
    "objectID": "class_notes/week_3.html#tempo-average",
    "href": "class_notes/week_3.html#tempo-average",
    "title": "Week 3: Time",
    "section": "Tempo Average",
    "text": "Tempo Average\nThe general default at the track level is an averaging of an entire piece. This can be useful at times, but it should be noted that it is a broad average for a parameter in which the variability is often quite meaningful.\nWhat if we wanted to see how the songs of Daft Punk changed in tempo changed over time?\nFirst we would make sure that Spotify had your user access token, and then get the artist audio features:\n\naccess_token <- get_spotify_access_token()\ndaft_punk <- get_artist_audio_features('daft punk')\n\nThen we would be able to simply plot the album’s mean tempo with the variance (and outliers) with ggplot, as below:\n\nggplot(daft_punk, aes(x=album_release_year, y=tempo, group = album_name)) + geom_boxplot() +\ntheme_bw()\n\n\n\n\nWe could also explore the variance of tempo within an album by looking at the standard deviation.\n\nalbum_sd <- daft_punk %>% \n    group_by(album_name, album_release_year) %>%\n    summarise(sd_tempo = sd(tempo))\n\n`summarise()` has grouped output by 'album_name'. You can override using the\n`.groups` argument.\n\n\nAnd then we can similarly plot this information:\n\nggplot(album_sd, aes(x=album_release_year, y=sd_tempo, group = album_name)) + geom_point() +\n  geom_label(\n    aes(label=album_name)) +\ntheme_bw()\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\n\n\nWarning: Removed 1 rows containing missing values (`geom_label()`)."
  },
  {
    "objectID": "class_notes/week_3.html#other-aspects-of-tempo",
    "href": "class_notes/week_3.html#other-aspects-of-tempo",
    "title": "Week 3: Time",
    "section": "Other aspects of tempo:",
    "text": "Other aspects of tempo:\nWe can also look at other elements of tempo, such as variability within sections. Here we have a question about the differences in tempo between punk in the 1980s and later punk (1990s and 2000s). I’m interested not just in the tempo, but also the variation of tempo.\nThere are a couple of points to notice in this code:\n\nNote how we are able to get data from a playlist. A playlist can be a good way for you to construct a sample.\nNote the add_audio_analysis function from the compmus library. This adds track level analysis information to the broader list of global information. It’s great.\n\nSit for a minute with this data. You’ll see the columns at the end that provided the specific audio analysis for each piece.\n\neighties_punk <-\n  get_playlist_audio_features(\n    \"kristian\",\n    \"5sxuwIQlaByb6Sx2OEwWTx\"\n  ) %>%\n  slice(1:30) %>%\n  add_audio_analysis()\n\nnineties_and_aughts_punk <-\n  get_playlist_audio_features(\n    \"CW\",\n    \"39sVxPTg7BKwrf2MfgrtcD\"\n  ) %>%\n  slice(1:30) %>%\n  add_audio_analysis()\n\nHere we bind both playlists together as a single data frame:\n\npunk <- \n  eighties_punk %>%\n  mutate(genre = \"eighties\") %>%\n  bind_rows(nineties_and_aughts_punk %>% mutate(genre = \"newer\"))\n\nThe spotify analysis gives us section markers as well, and we can use the code below to summarise the tempo, loudness, and duration for each section. Note the use of the map function, which takes the input and applies a function to that input (here the summarise_at function, and the summarise_at itself, which provides a summary of each of these columns.\nHere we are storing it in a variable called summarised_punk.\n\nsummarised_punk <- punk %>%\n  mutate(\n    sections =\n      map(\n        sections,                                    # sections or segments\n        summarise_at,\n        vars(tempo, loudness, duration),             # features of interest\n        list(section_mean = mean, section_sd = sd)   # aggregation functions\n      )\n  )\n\nNow we take this variable and plot it using ggplot.\nThe process below is as follows:\n\ntake the table above with summarized section information and unnest it (this takes the sections list of information and turns it into rows and columns).\nPipe that into ggplot, with the aesthetics function plotting the tempo on the x-axis, the standard deviation on the y-axis, the color being which genre we used (eighties or not). The color saturation is set to the loudness variable.\nWe then tell ggplot that we want this to be a scatterplot with the geom_point function, and that the size of each point should be the duration of the piece (divided by 60 as Spotify just gives it in seconds).\nWe then add a rug plot which gives the ticks on both axes to show the distribution of events.\nWe then add a black and white theme because nobody likes default graphics.\nWe then add the size of the graph and the axis labels.\n\n\n  summarised_punk %>%\n  unnest(sections) %>%\n  ggplot(\n    aes(\n      x = tempo,\n      y = tempo_section_sd,\n      colour = genre,\n      alpha = loudness\n    )\n  ) +\n  geom_point(aes(size = duration / 60)) +\n  geom_rug() +\n  theme_bw() +\n  ylim(0, 5) +\n  labs(\n    x = \"Mean Tempo (bpm)\",\n    y = \"SD Tempo\",\n    colour = \"Genre\",\n    size = \"Duration (min)\",\n    alpha = \"Volume (dBFS)\"\n  )  \n\nWarning: Removed 7 rows containing missing values (`geom_point()`).\n\n\n\n\n\nIn-class exercises:\n\nHow is tempo treated differently across the albums of the Beatles?"
  },
  {
    "objectID": "class_notes/week_3.html#plan-for-the-day",
    "href": "class_notes/week_3.html#plan-for-the-day",
    "title": "Week 3: Time",
    "section": "Plan for the day:",
    "text": "Plan for the day:\n\nTalk about your homework a bit.\nLook at some tap data, and what we might actually be able to do with it.\nTalk about how Spotify (might) calculate tempo.\n\nHow might we look at tempo across pieces?\nOne way to get this is to just tap the tempo, and then align it to the onsets. But how do we find onsets?\nThis is where a novelty function comes in. (see Müller on Fourier Tempograms). Put (extremely) succinctly, a novelty function detects changes in the energy or the spectrum of the signal. So looking for energy peaks might be a good marker for “peaks in energy”.\n\n\n\nMüller’s Onset Detection Example (p.311)\n\n\nAfter finding these onsets, it then examines a correlation between various sinusoids and picks the most likely one. There are many different ways of approaching this.\nOne issue is the presence of so-called “tempo octaves”. That is, it finds tempos at twice the beat, half the beat, etc..\nHere’s a graph of AJR’s “World’s Smallest Violin”:\n\nget_tidy_audio_analysis(\"3jHBgKdLCf46aP3HRI0WYv\") %>%\n  tempogram(window_size = 8, hop_size = 2) %>%\n   ggplot(aes(x = time, y = bpm, fill = power)) + \n    geom_raster() + \n    scale_fill_viridis_c(guide = 'none') +\n    labs(x = 'Time (s)', y = 'Tempo (BPM)') +\n    theme_classic()\n\n\n\n\nThis seems quite strange, though. It’s not really a great indicator of tempo…\nMüller points out that the Fourier-based method tends to struggle with these tempo-octaves, and a cyclic model, which look at “subharmonics” rather than harmonics, and are a bit better for mid-level tempo finding. The example below seems to work a bit better. Notice how the cyclic option has been switched to TRUE.\n\nget_tidy_audio_analysis(\"3jHBgKdLCf46aP3HRI0WYv\") %>%\n  tempogram(window_size = 8, hop_size = 2, cyclic=TRUE) %>%\n   ggplot(aes(x = time, y = bpm, fill = power)) + \n    geom_raster() + \n    scale_fill_viridis_c(guide = 'none') +\n    labs(x = 'Time (s)', y = 'Tempo (BPM)') +\n    theme_classic()"
  },
  {
    "objectID": "class_notes/week_1.html",
    "href": "class_notes/week_1.html",
    "title": "Week 1: Representing Musical Data",
    "section": "",
    "text": "In the first week, we worked through basic introductions for the class, and went through the syllabus and the course structure.\n\n\nHere, we install the necessary library. As you can see, you will need to install devtools, which will allow you to install packages that aren’t on CRAN from github.\nThen, we install the package (you can uncomment these installation lines as necessary for you).\n\n### installing everything as needed\n# library(devtools)\n# devtools::install_github(\"Computational-Cognitive-Musicology-Lab/humdrumR\", build_vignettes = TRUE)\nlibrary(humdrumR)\n\nIn the code below, you can see how we load all of the Chopin files into a preludes variable with the readHumdrum function.\nThen we subset it by spines. We are interested in various ways of calculating pitch, so we looked at pc (pitch class), as well as solfa and deg, which gave us solfege syllables and scale degrees, respectively.\nWe then plot this data in a barplot. Note the |> or “pipe” that we are using. The older tidyverse-style pipe (%>%) will also work here.\n\n### Load in Chopin preludes, grab the left hand and see all the scale degrees.\npreludes <- readHumdrum(\"~/gitcloud/corpora/humdrum_scores/Chopin/Preludes/*.krn\")\nleft_hand <- subset(preludes, Spine == 1)\n###solfa, deg, pc\ntable_data <- with(left_hand, pc(Token,simple=TRUE)) |> table() \nbarplot(table_data)\n\n\n\n\nYou can use a similar with syntax to get rhythm variables, as seen below:\n\n## rhythminterval\nrhythms <- with(preludes[2], duration(Token))\n\nWarning in is.na(x) || is.na(y): 'length(x) = 93 > 1' in coercion to\n'logical(1)'\n\nWarning in is.na(x) || is.na(y): 'length(x) = 93 > 1' in coercion to\n'logical(1)'\n\nWarning in is.na(x) || is.na(y): 'length(x) = 93 > 1' in coercion to\n'logical(1)'\n\nWarning in is.na(x) || is.na(y): 'length(x) = 93 > 1' in coercion to\n'logical(1)'\n\n\nWarning in is.na(x) || is.na(y): 'length(x) = 84 > 1' in coercion to\n'logical(1)'\n\nWarning in is.na(x) || is.na(y): 'length(x) = 84 > 1' in coercion to\n'logical(1)'\n\n\nWarning in is.na(x) || is.na(y): 'length(x) = 93 > 1' in coercion to\n'logical(1)'\n\nWarning in is.na(x) || is.na(y): 'length(x) = 93 > 1' in coercion to\n'logical(1)'\n\n\nWarning in is.na(x) || is.na(y): 'length(x) = 11 > 1' in coercion to\n'logical(1)'\n\nWarning in is.na(x) || is.na(y): 'length(x) = 11 > 1' in coercion to\n'logical(1)'\n\n\nWarning in is.na(x) || is.na(y): 'length(x) = 2 > 1' in coercion to\n'logical(1)'\n\nWarning in is.na(x) || is.na(y): 'length(x) = 2 > 1' in coercion to\n'logical(1)'\n\n\nWarning in is.na(x) || is.na(y): 'length(x) = 93 > 1' in coercion to\n'logical(1)'\n\nWarning in is.na(x) || is.na(y): 'length(x) = 93 > 1' in coercion to\n'logical(1)'\n\n\nWarning in is.na(x) || is.na(y): 'length(x) = 84 > 1' in coercion to\n'logical(1)'\n\nWarning in is.na(x) || is.na(y): 'length(x) = 84 > 1' in coercion to\n'logical(1)'\n\n\nWarning in is.na(x) || is.na(y): 'length(x) = 11 > 1' in coercion to\n'logical(1)'\n\nWarning in is.na(x) || is.na(y): 'length(x) = 11 > 1' in coercion to\n'logical(1)'\n\n\nWarning in is.na(x) || is.na(y): 'length(x) = 5 > 1' in coercion to\n'logical(1)'\n\nWarning in is.na(x) || is.na(y): 'length(x) = 5 > 1' in coercion to\n'logical(1)'\n\n#### group exercise:\n#### using a repertoire in the Humdrum scores collection, \n#### print a table of most common musical events.\n\n\n\n\nWe can start by loading our spotifyr library, and tidyverse for good measure:\n\nlibrary(spotifyr)\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.2     ✔ purrr   1.0.1\n✔ tibble  3.2.1     ✔ dplyr   1.1.1\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.4     ✔ forcats 1.0.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ purrr::%@%()         masks rlang::%@%()\n✖ dplyr::changes()     masks humdrumR::changes()\n✖ dplyr::count()       masks humdrumR::count()\n✖ tidyr::expand()      masks humdrumR::expand()\n✖ dplyr::filter()      masks stats::filter()\n✖ purrr::flatten()     masks rlang::flatten()\n✖ purrr::flatten_chr() masks rlang::flatten_chr()\n✖ purrr::flatten_dbl() masks rlang::flatten_dbl()\n✖ purrr::flatten_int() masks rlang::flatten_int()\n✖ purrr::flatten_lgl() masks rlang::flatten_lgl()\n✖ purrr::flatten_raw() masks rlang::flatten_raw()\n✖ humdrumR::int()      masks rlang::int()\n✖ purrr::invoke()      masks rlang::invoke()\n✖ dplyr::lag()         masks humdrumR::lag(), stats::lag()\n✖ dplyr::lead()        masks humdrumR::lead()\n✖ purrr::splice()      masks rlang::splice()\n✖ dplyr::symdiff()     masks bit::symdiff()\n✖ purrr::transpose()   masks humdrumR::transpose()\n\n\nYou will need your own spotify client ID and client secret. You can get them by filling out the brief online form here.\n\n### setting up spotify\nSys.setenv(SPOTIFY_CLIENT_ID = YOUR SPOTIFY CLIENT ID)\nSys.setenv(SPOTIFY_CLIENT_SECRET = YOUR SPOTIFY CLIENT SECRET)\naccess_token <- get_spotify_access_token()\n\n\n\n\nFor the most part, in this class we will be looking at global features data (the “danceability” of a song), and track-level analysis features, such as chroma vectors.\nHere we see how you might grab artist features for Ryan Adams and Taylor Swift, comparing the performances of each of their 1989 albums.\n\n###getting artist level data\nryan_adams <- get_artist_audio_features('ryan adams')\ntaylor_swift <- get_artist_audio_features('taylor swift')\n\n### cleaning up the data\nadams_swift <- rbind(ryan_adams, taylor_swift)\nadams_swift_1989 <- adams_swift %>% filter(album_name == \"1989\") \nadams_swift_1989$track_name <- tolower(adams_swift_1989$track_name)\n\n## comparing energy\nggplot(adams_swift_1989, aes(x=track_name, y=energy, group=artist_name)) +\n  geom_line(aes(linetype=artist_name, color=artist_name))+\n  geom_point(aes(color=artist_name))+\n  theme(legend.position=\"top\", axis.text.x = element_text(angle = 90, hjust = 1))"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Corpus Studies and Music",
    "section": "",
    "text": "Welcome!\nWelcome to the Corpus Studies and Music class."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is the class site for Corpus Studies in Music."
  },
  {
    "objectID": "course-schedule.html",
    "href": "course-schedule.html",
    "title": "Course Schedule",
    "section": "",
    "text": "Unit\nWeek\nDate (Date)\nTopic\nImportant Dates\n\n\n\n\nRepresenting, Searching, and Counting with Symbolic Data\n1\nT (3/28; Northwestern Monday)\nIntroductions; a history of corpus-based approaches to music.\n\n\n\n\n1\nW (3/29)\nRepresenting musical data (kern, MIDI, XML, MEI)\nAssignment #1 Due\n\n\n\n2\nM (4/3)\nPitches, melodic intervals and scale degrees\nResponse #1 Due\n\n\n\n2\nW (4/5)\nRepresenting Harmony\nAssignment #2 Due\n\n\n\n3\nM (4/10)\nRepresenting Rhythm and Meter\nResponse #2 Due\n\n\n\n3\nW (4/12)\nCharacteristic Features and TF-IDF\nAssignment #3 Due\n\n\n\n4\nM (4/17)\nn-grams and Transition Probabilities\nResponse #3 Due\n\n\n\n4\nW (4/19)\nFinding patterns\nAssignment #4 Due\n\n\nConceptual Debates\n5\nM (4/24)\nMeasurements of Similarity\nMidterm Literature Review Due\n\n\n\n5\nW (4/26)\nEntropy and Variability\nAssignment #5 Due\n\n\n\n6\nM (5/1)\nKey-Finding Algorithms\nResponse #4 Due\n\n\n\n6\nW (5/3)\nKey-Finding Algorithms (continued)\nAssignment #6 Due\n\n\n\n7\nM (5/8)\nFast Fourier Transforms\nResponse #5 Due\n\n\n\n7\nW (5/10)\nClustering and Authorship\nAssignment #7 Due\n\n\nAudio Data\n8\nM (5/15)\nPerformance Data/the Mazurka Project\nFirst Draft of Final Project Due\n\n\n\n8\nW (5/17)\nTempo flexibility, and other performance data\nAssignment #8 Due\n\n\n\n9\nM (5/22)\nThe Spotify API\nResponse #6 Due; Peer Reviews Due\n\n\n\n9\nW (5/24)\nThe Spotify API (continued)\nAssignment #9 Due\n\n\n\n10\nM (5/29)\nNo Classes (Memorial Day)\nFinal Presentations\n\n\n\n10\nW (5/31)\nFinal Presentations\nFinal Papers due on Friday, 6/2"
  },
  {
    "objectID": "course-syllabus.html",
    "href": "course-syllabus.html",
    "title": "Corpus Studies (Music 348/448) Syllabus",
    "section": "",
    "text": "Instructor: Dr. Daniel Shanahan\nContact: daniel.shanahan@northwestern.edu"
  },
  {
    "objectID": "course-syllabus.html#overview",
    "href": "course-syllabus.html#overview",
    "title": "Corpus Studies (Music 348/448) Syllabus",
    "section": "Overview",
    "text": "Overview\nCorpus studies, or distant readings of multiple musical works, are often employed as a way of better understanding issues such as the relationships between pieces, authorship, trends over time, or differences and similarities between genres. In this class, we will explore the techniques, history, and philosophy of such approaches, and will construct and analyze our own corpora. For the most part, this class will deal with notated scores, and students will be encouraged to ask their own research questions of the music that they are most interested in."
  },
  {
    "objectID": "course-syllabus.html#course-info",
    "href": "course-syllabus.html#course-info",
    "title": "Corpus Studies (Music 348/448) Syllabus",
    "section": "Course info",
    "text": "Course info\n\n\n\n\n\n\n\n\n\n\nDay\nTime\nLocation\n\n\n\n\nMeeting Times:\nMon & Weds\n12:30pm - 1:50 pm\nRCMA 1-164\n\n\nOffice Hours\nTBD (and by appointment)\nTBD (and by appointment)\nRCMA 4-181"
  },
  {
    "objectID": "course-syllabus.html#learning-objectives",
    "href": "course-syllabus.html#learning-objectives",
    "title": "Corpus Studies (Music 348/448) Syllabus",
    "section": "Learning objectives",
    "text": "Learning objectives\nBy the end of the quarter, you will…\n\nhave an understanding of how music has been examined through distant readings of scores and recordings\nbe able to explore how the concepts of concordances, schemata, key-finding, clustering, and introductory machine learning approaches can be applied to music analysis\nhave a working introductory knowledge of the R programming language and the HumDrumR package."
  },
  {
    "objectID": "course-syllabus.html#course-policies",
    "href": "course-syllabus.html#course-policies",
    "title": "Corpus Studies (Music 348/448) Syllabus",
    "section": "Course Policies",
    "text": "Course Policies\n\nAcademic Integrity\nStudents in this course are required to comply with the policies found in the booklet, “Academic Integrity at Northwestern University: A Basic Guide”. All papers submitted for credit in this course must be submitted electronically unless otherwise instructed by the professor. Your written work may be tested for plagiarized content. For details regarding academic integrity at Northwestern or to download the guide, visit this page.\n\n\nAccesibility\nNorthwestern University is committed to providing the most accessible learning environment as possible for students with disabilities. Should you anticipate or experience disability-related barriers in the academic setting, please contact AccessibleNU to move forward with the university’s established accommodation process (email: accessiblenu@northwestern.edu; p: 847-467-5530). If you already have established accommodations with AccessibleNU, please let me know as soon as possible, preferably within the first two weeks of the term, so we can work together to implement your disability accommodations. Disability information, including academic accommodations, is confidential under the Family Educational Rights and Privacy Act.\n\n\nCOVID-19 Classroom Expectations\nStudents, faculty and staff must comply with University expectations regarding appropriate classroom behavior, including those outlined below and in the COVID-19 Expectations for Students. With respect to classroom procedures, this includes:\nPolicies regarding masking, social distancing and other public health measures evolve as the situation changes. Students are responsible for understanding and complying with current University, state and city requirements. In some classes, masking and/or social distancing may be required as a result of an Americans with Disabilities Act (ADA) accommodation for the instructor or a student in the class even when not generally required on campus. In such cases, the instructor will notify the class.\nIf a student fails to comply with the COVID-19 Expectations for Students or other University expectations related to COVID-19, the instructor may ask the student to leave the class. The instructor is asked to report the incident to the Office of Community Standards for additional follow-up.\n\nIf you’re feeling sick…\nMaintaining the health of the community remains our priority. If you are experiencing any symptoms of COVID do not attend class. Follow the steps outlined on the NU sites for testing, isolation and reporting a positive case. Next, contact me as soon as possible to arrange to complete coursework.\nShould public health recommendations prevent in-person class from being held on a given day, I or the university will notify students.\n\n\n\nDiversity, Equity, and Inclusion\nThis course strives to be an inclusive learning community, respecting those of differing backgrounds and beliefs. As a community, we aim to be respectful to all students in this class, regardless of race, ethnicity, socio-economic status, religion, gender identity or sexual orientation."
  },
  {
    "objectID": "course-syllabus.html#textbooks",
    "href": "course-syllabus.html#textbooks",
    "title": "Corpus Studies (Music 348/448) Syllabus",
    "section": "Textbooks",
    "text": "Textbooks\nThere is no textbook for this course, and most of the materials will be available on Canvas or the course website (or both). Many of the readings will be taken from the forthcoming Oxford Handbook of Music and Corpus Studies, edited by Daniel Shanahan, Ashley Burgoyne, and Ian Quinn.\nHaving said that, you should sign up for a free account for Posit Cloud (formerly RStudio Cloud), where many of the class notebooks will be held.I would also recommend downloading R and RStudio onto your personal machine, if possible.\nAlthough not required, I would highly recommend having a look at:\n\nR for Data Science by Garret Grolemund and Hadley Wickham\nThe Humdrum User Guide\nThe music21 documentation\nThe Oxford Handbook of Music and Corpus Studies"
  },
  {
    "objectID": "course-syllabus.html#support-for-wellness-and-mental-health",
    "href": "course-syllabus.html#support-for-wellness-and-mental-health",
    "title": "Corpus Studies (Music 348/448) Syllabus",
    "section": "Support for Wellness and Mental Health",
    "text": "Support for Wellness and Mental Health\nNorthwestern University is committed to supporting the wellness of our students. Student Affairs has multiple resources to support student wellness and mental health. If you are feeling distressed or overwhelmed, please reach out for help. Students can access confidential resources through the Counseling and Psychological Services (CAPS), Religious and Spiritual Life (RSL) and the Center for Awareness, Response and Education (CARE). Additional information on all of the resources mentioned above can be found here:\nhttps://www.northwestern.edu/counseling/\nhttps://www.northwestern.edu/religious-life/\nhttps://www.northwestern.edu/care/\n\nHomework\nThere will be regular assignments in which you will be asked to respond to do one of the following:\n\nCritically reflect upon a reading about the history, methods, and dilemmas commonly found in corpus studies.\nWrite code that addresses a musical question (e.g. what’s the most common pitch transition in this group of pieces?)\nAnalyze a given collection of musical data.\n\nTypically, we will have reading reflections due on Mondays, and code-related questions relevant to those readings due on Wednesdays.\n\n\nMidterm Project\nThe goal of this class is for you to both understand corpus studies as a method with a long history, and for you to be able to incorporate these methods in your own research. There will be a midterm project that is primarily used a stepping stone into your final project, and it will consist of presenting a literature review in which you situate your own research question within the existing literature and propose a study that examines this question. You may use existing data, but you might find it more relevant to you if you use your own dataset. Therefore, this would be a good time to have a bulk of your data encoded, so that you are aware of the time needed to construct your corpus.\n\n\nFinal Project\nThe final project will be focused on a research question of your choosing, and will be broken up into several a peer-reviewed first draft, a presentation, and a final paper."
  },
  {
    "objectID": "course-syllabus.html#grading",
    "href": "course-syllabus.html#grading",
    "title": "Corpus Studies (Music 348/448) Syllabus",
    "section": "Grading",
    "text": "Grading\nThe final course grade will be calculated as follows:\n\n\n\nCategory\nPercentage\n\n\n\n\nReading Reflection Questions\n20%\n\n\nCode-focused assignments\n30%\n\n\nMidterm Literature Review\n15%\n\n\nFinal Project (First Draft)\n10%\n\n\nPresentation\n10%\n\n\nFinal Project (Final draft)\n15%\n\n\n\nThe final letter grade will be determined based on the following thresholds:\n\n\n\nLetter Grade\nFinal Course Grade\n\n\n\n\nA\n>= 93\n\n\nA-\n90 - 92.99\n\n\nB+\n87 - 89.99\n\n\nB\n83 - 86.99\n\n\nB-\n80 - 82.99\n\n\nC+\n77 - 79.99\n\n\nC\n73 - 76.99\n\n\nC-\n70 - 72.99\n\n\nD+\n67 - 69.99\n\n\nD\n63 - 66.99\n\n\nD-\n60 - 62.99\n\n\nF\n< 60"
  },
  {
    "objectID": "course-syllabus.html#schedule",
    "href": "course-syllabus.html#schedule",
    "title": "Corpus Studies (Music 348/448) Syllabus",
    "section": "Schedule",
    "text": "Schedule\n\n\n\n\n\n\n\n\n\nUnit\nWeek\nDate (Date)\nTopic\n\n\n\n\nRepresenting, Searching, and Counting with Symbolic Data\n1\nT (3/28; Northwestern Monday)\nIntroductions; a history of corpus-based approaches to music.\n\n\n\n1\nW (3/29)\nRepresenting musical data (kern, MIDI, XML, MEI)\n\n\n\n2\nM (4/3)\nPitches, melodic intervals and scale degrees\n\n\n\n2\nW (4/5)\nRepresenting Harmony\n\n\n\n3\nM (4/10)\nRepresenting Rhythm and Meter\n\n\n\n3\nW (4/12)\nCharacteristic Features and TF-IDF\n\n\n\n4\nM (4/17)\nn-grams and Transition Probabilities\n\n\n\n4\nW (4/19)\nFinding patterns\n\n\nConceptual Debates\n5\nM (4/24)\nMeasurements of Similarity\n\n\n\n5\nW (4/26)\nEntropy and Variability\n\n\n\n6\nM (5/1)\nKey-Finding Algorithms\n\n\n\n6\nW (5/3)\nKey-Finding Algorithms (continued)\n\n\n\n7\nM (5/8)\nFast Fourier Transforms\n\n\n\n7\nW (5/10)\nClustering and Authorship\n\n\nAudio Data\n8\nM (5/15)\nPerformance Data/the Mazurka Project\n\n\n\n8\nW (5/17)\nTempo flexibility, and other performance data\n\n\n\n9\nM (5/22)\nThe Spotify API\n\n\n\n9\nW (5/24)\nThe Spotify API (continued)\n\n\n\n10\nM (5/29)\nNo Classes (Memorial Day)\n\n\n\n10\nW (5/31)\nFinal Presentations"
  },
  {
    "objectID": "class_notes/week_6.html#john-or-paul",
    "href": "class_notes/week_6.html#john-or-paul",
    "title": "Week 6: Classifying",
    "section": "John or Paul?",
    "text": "John or Paul?\nOur research process will follow a simple trajectory:\n\nGet songs by each artist’s solo career (this can be our ‘ground truth’, as it were).\nTrain the model on these pieces, and evaluate the various models.\nApply the various models to some songs by the Beatles.\n\n\nGetting the Data\n\njohn <- get_artist_audio_features('john lennon')\npaul <- get_artist_audio_features('paul mccartney')\nboth <- rbind(john, paul)\n\nWhat is the balance of pieces like? It looks like we have far more McCartney than Lennon pieces. What does this mean for our model?\n\ntable(both$artist_name)\n\n\n   John Lennon Paul McCartney \n           422           1387 \n\n\nWe then can grab only the features that we want to explore for this model.\n\nboth_subset <- both %>% select(c(\"artist_name\", \"acousticness\", \"liveness\", \"danceability\", \"loudness\", \"speechiness\", \"valence\"))\n\nBefore running a clustering, PCA, or a classifier such as a k-nearest neighbor, it’s probably good to standardize your data. This means that the data is consistent, and prevents wide ranges from dominating the results. Here we’ve scaled all of our data with the z-score of the data according with the rest of the data for that category.\nI’ve also (temporarily) split the data from the artist, and then brought it all back together with cbind.\n\ndata <- both_subset[,-1]\nartists <- both_subset[,1]\ndata <- data %>% mutate_all(~(scale(.) %>% as.vector))\nboth_artists <- cbind(artists, data)"
  },
  {
    "objectID": "class_notes/week_6.html#cross-validation",
    "href": "class_notes/week_6.html#cross-validation",
    "title": "Week 6: Classifying",
    "section": "Cross-Validation",
    "text": "Cross-Validation\nCross-validation splits the data up into a testing and training set, and evaluates it.\n\nK-folds cross validation:\nK refers to the number of groups that data is split into.\n\nIt randomizes the data\nsplits it into the specified number of groups\nfor each group, split into a training and testing set, and then evaluate\n\n\nctrl <- trainControl(method = \"repeatedcv\", number = 2, savePredictions = TRUE)\n\n\nTrain <- createDataPartition(both_artists$artists, p=0.7, list=FALSE)\ntraining <- both_artists[ Train, ]\ntesting <- both_artists[ -Train, ]\n\nLet’s look at our results with a logistic regression:\n\nmod_fit <- train(artists ~ .,  data=both_artists, method=\"glm\", family=\"binomial\",\n                 trControl = ctrl, tuneLength = 10)\n\ntesting$artists <- as.factor(testing$artists)\npred <- predict(mod_fit, newdata=testing)\nconfusionMatrix(data=pred, testing$artists)\n\nConfusion Matrix and Statistics\n\n                Reference\nPrediction       John Lennon Paul McCartney\n  John Lennon             11             14\n  Paul McCartney         115            402\n                                          \n               Accuracy : 0.762           \n                 95% CI : (0.7238, 0.7973)\n    No Information Rate : 0.7675          \n    P-Value [Acc > NIR] : 0.642           \n                                          \n                  Kappa : 0.0744          \n                                          \n Mcnemar's Test P-Value : <2e-16          \n                                          \n            Sensitivity : 0.08730         \n            Specificity : 0.96635         \n         Pos Pred Value : 0.44000         \n         Neg Pred Value : 0.77756         \n             Prevalence : 0.23247         \n         Detection Rate : 0.02030         \n   Detection Prevalence : 0.04613         \n      Balanced Accuracy : 0.52682         \n                                          \n       'Positive' Class : John Lennon     \n                                          \n\n\nIt looks like the accuracy is about 76%, but pay attention to the sensitivity and the specificity values.\nRecall that sensitivity is a measurement of how well the model can detect a “positive” instance, and specificity measures how well the model is finding true negatives.\nSensitivity can be defined as follows:\n\nSensitivity = (True Positive)/(True Positive + False Negative)\n\nand specificity can be defined as follows:\n\nSpecificity = (True Negative)/(True Negative + False Positive)\n\nSo this model is quite good at finding the negative class (here defined as McCartney), but not great at finding the positive class (Lennon)."
  },
  {
    "objectID": "class_notes/week_6.html#other-models",
    "href": "class_notes/week_6.html#other-models",
    "title": "Week 6: Classifying",
    "section": "Other Models",
    "text": "Other Models\nLet’s run the same code again, but now with a k-nearest neighbor. For our sanity, let’s put it into a function.\n\nmodel_evaluation <- function(method){\n    Train <- createDataPartition(both_artists$artists, p=0.7, list=FALSE)\n    training <- both_artists[ Train, ]\n    testing <- both_artists[ -Train, ]\n    mod_fit <- train(artists ~ .,  \n                     data=training, method=method)\n    pred <- predict(mod_fit, newdata=testing)\n\n    accuracy <- table(pred, testing[,\"artists\"])\n    sum(diag(accuracy))/sum(accuracy)\n    testing$artists <- as.factor(testing$artists)\n    confusionMatrix(data=pred, testing$artists)\n    \n}\nmodel_evaluation(\"kknn\")\n\nConfusion Matrix and Statistics\n\n                Reference\nPrediction       John Lennon Paul McCartney\n  John Lennon             51             33\n  Paul McCartney          75            383\n                                          \n               Accuracy : 0.8007          \n                 95% CI : (0.7646, 0.8336)\n    No Information Rate : 0.7675          \n    P-Value [Acc > NIR] : 0.03586         \n                                          \n                  Kappa : 0.3682          \n                                          \n Mcnemar's Test P-Value : 7.972e-05       \n                                          \n            Sensitivity : 0.4048          \n            Specificity : 0.9207          \n         Pos Pred Value : 0.6071          \n         Neg Pred Value : 0.8362          \n             Prevalence : 0.2325          \n         Detection Rate : 0.0941          \n   Detection Prevalence : 0.1550          \n      Balanced Accuracy : 0.6627          \n                                          \n       'Positive' Class : John Lennon     \n                                          \n\n\nNote that it performs quite well! It’s better at finding the “John Lennon” model.\nWhy do we think this model performed better? A comparison of models can be found here.\n\nNeural Net\nA neural net doesn’t seem to do as well.\n\nmodel_evaluation(\"nnet\")"
  },
  {
    "objectID": "class_notes/week_6.html#comparing-models",
    "href": "class_notes/week_6.html#comparing-models",
    "title": "Week 6: Classifying",
    "section": "Comparing Models",
    "text": "Comparing Models\n\nLogistic Regression\nK-nearest neighbor\nneural net\nLearning Vector Quantization\ngradient boosted machine\nsupport vector machine\n\nWe can train different models explicitly (without a function) for now.\n\nset.seed(1234)\ncontrol <- trainControl(method=\"repeatedcv\", number=10, repeats=3)\n\n# train logistic regression\nmodelglm <- train(artists ~ ., data=both_artists, method=\"glm\", trControl=control)\n\n# train knn\nmodelknn <- train(artists ~ ., data=both_artists, method=\"kknn\", trControl=control)\n\n# train nnet\nmodelnnet <- train(artists ~ ., data=both_artists, method=\"nnet\", trControl=control)\n\n# train the LVQ model\nmodelLvq <- train(artists ~ ., data=both_artists, method=\"lvq\", trControl=control)\n\n# train the GBM model\nset.seed(7)\nmodelGbm <- train(artists ~ ., data=both_artists, method=\"gbm\", trControl=control)\n\n# train the SVM model\nset.seed(7)\nmodelSvm <- train(artists ~., data=both_artists, method=\"svmRadial\", trControl=control)\n\n# train the random forest\nrandomforest <- train(artists~., data=both_artists, method=\"ranger\", trControl=control)\n\nWe can actually look at the resampling of the dataset for each model, and get the results for each model:\n\n# collect resamples\nresults <- resamples(list(LVQ=modelLvq, GBM=modelGbm, SVM=modelSvm,knn=modelknn, nnet=modelnnet, glm=modelglm, rf=randomforest))\n\n# summarize the distributions\nsummary(results)\n\n\nCall:\nsummary.resamples(object = results)\n\nModels: LVQ, GBM, SVM, knn, nnet, glm, rf \nNumber of resamples: 30 \n\nAccuracy \n          Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nLVQ  0.6850829 0.7555556 0.7645488 0.7617583 0.7734807 0.7888889    0\nGBM  0.7845304 0.8088079 0.8254911 0.8229457 0.8395565 0.8508287    0\nSVM  0.7679558 0.7790055 0.7900552 0.7888529 0.7988950 0.8111111    0\nknn  0.7833333 0.8179300 0.8397790 0.8361661 0.8506215 0.8895028    0\nnnet 0.7403315 0.7725368 0.7873016 0.7936387 0.8099908 0.8500000    0\nglm  0.7458564 0.7572400 0.7624309 0.7658017 0.7734807 0.7900552    0\nrf   0.8111111 0.8613029 0.8729282 0.8697308 0.8893493 0.9060773    0\n\nKappa \n             Min.    1st Qu.     Median       Mean   3rd Qu.      Max. NA's\nLVQ  -0.009392427 0.02663193 0.07100195 0.07419685 0.1190372 0.1670796    0\nGBM   0.248634477 0.36345672 0.42324938 0.41069041 0.4661853 0.5403931    0\nSVM   0.071114370 0.15186812 0.19162277 0.19665202 0.2298367 0.3126685    0\nknn   0.389352818 0.47400876 0.52339123 0.52601569 0.5683450 0.6899623    0\nnnet  0.091489847 0.25010786 0.30093890 0.32095848 0.4082260 0.5212766    0\nglm   0.000000000 0.02688172 0.07086253 0.06823155 0.1020063 0.1595797    0\nrf    0.413793103 0.55728402 0.60804017 0.58941910 0.6453960 0.7041763    0\n\n\nIt might be better to look at the accuracy for each model. Here we have the accuracy rating as well as Cohen’s Kappa, which is like accuracy but also incorporates the imbalance of the dataset.\n\n# boxplots of results\nbwplot(results)\n\n\n\n\nHere’s another plot:\n\n# dot plots of results\ndotplot(results)\n\n\n\n\nIs it possible to use this for a research question??\nWhat if we use our neural net model but on a different dataset? How about the beatles dataset that is available on Spotify (which admittedly isn’t as much as we’d like).\n\nGrabbing Beatles Data\nWe can start by getting the data from Spotify:\n\nbeatles <- get_artist_audio_features('the beatles')\nbeatles_subset <- beatles %>% select(c(\"artist_name\", \"acousticness\", \"energy\", \"instrumentalness\", \"liveness\", \"danceability\", \"loudness\", \"speechiness\", \"valence\"))\n\n\n\nPredicting\nNow we can use the models that we’ve trained, but on new data. Here we use the random forest and the k-nearest neighbor models.\n\nbeatles_knn <- predict(modelknn, newdata=beatles_subset)\nbeatles_rf <- predict(randomforest, newdata=beatles_subset)\n\nNow, we are going to create a data frame of the track name, and both models.\n\nclassified_data <- as.data.frame(cbind(beatles_knn, beatles_rf, beatles$track_name))\n\nThis returns data as either 1 or 2, so we can clean up the columns a bit. Here, if it’s a 2, I label it as “Paul”, otherwise, it’s a “John”.\n\nclassified_data$beatles_knn <- if_else(classified_data$beatles_knn == 2, \"Paul\", \"John\")\nclassified_data$beatles_rf<- if_else(classified_data$beatles_rf == 2, \"Paul\", \"John\")\n\nWith the caret package, we can extract the probabilities of each guess. We can also add track info here as needed:\n\nprobabilities <- extractProb(list(modelnnet), unkX = beatles_subset)\ncolnames(classified_data)[3] <- \"track\"\nprobability_data <- cbind(probabilities[,1:2], classified_data)\nprobability_data %>% datatable(filter =\"top\") \n\n\n\n\n\n\n(Note that this table doesn’t seem to be rendering correctly when pushed online)."
  },
  {
    "objectID": "class_notes/week_6.html#summary",
    "href": "class_notes/week_6.html#summary",
    "title": "Week 6: Classifying",
    "section": "Summary",
    "text": "Summary\nWhat I like about this is that we can take something about authorship that we know, and then use it to explore authorship of things that are a little more ambiguous. It can also teach us a fair bit about the specific models. Why do we think some performed so much better than others?\n\nExercise:\nLet’s try to build an east/west coast rap classifier:\nSteps!:\n\nGrab data\npartition and train model\ncompare models\nuse it to predict a new dataset.\n\n\neast_coast <- get_playlist_audio_features(\"\", \"3pu8tsqTW52aUtYFZN3g4A\")\neast_coast$coast <- \"east\"\nwest_coast <- get_playlist_audio_features(\"\", \"6lAOSVxxvGuEhPtZguaeav\")\nwest_coast$coast <- \"west\"\nboth <- rbind(east_coast, west_coast)\n\n####standardize and clean a bit\n\nboth <- both %>% select(c(\"coast\", \"acousticness\", \"energy\", \"instrumentalness\", \"liveness\", \"danceability\", \"loudness\", \"speechiness\", \"valence\"))\n\ndata <- both[,-1]\ncoast <- both[,1]\ndata <- data %>% mutate_all(~(scale(.) %>% as.vector))\nboth <- cbind(coast, data)\n\nset.seed(1234)\ncontrol <- trainControl(method=\"repeatedcv\", number=10, repeats=3)\n\n# train logistic regression\nmodelglm <- train(coast ~ ., data=both, method=\"glm\", trControl=control)\n\n# train knn\nmodelknn <- train(coast ~ ., data=both, method=\"kknn\", trControl=control)\n\n# train nnet\nmodelnnet <- train(coast ~ ., data=both, method=\"nnet\", trControl=control)\n\n# weights:  11\ninitial  value 179.713406 \niter  10 value 139.274270\niter  20 value 138.418159\niter  30 value 138.311368\nfinal  value 138.294899 \nconverged\n# weights:  31\ninitial  value 159.063233 \niter  10 value 136.332036\niter  20 value 126.730188\niter  30 value 119.138385\niter  40 value 114.350246\niter  50 value 110.525689\niter  60 value 109.317970\niter  70 value 109.034288\niter  80 value 109.025799\niter  90 value 109.024934\nfinal  value 109.024850 \nconverged\n# weights:  51\ninitial  value 180.830976 \niter  10 value 133.260420\niter  20 value 118.436490\niter  30 value 104.516811\niter  40 value 97.422974\niter  50 value 95.549203\niter  60 value 95.406004\niter  70 value 95.404046\nfinal  value 95.404045 \nconverged\n# weights:  11\ninitial  value 160.997220 \niter  10 value 140.103830\niter  20 value 139.649981\nfinal  value 139.649000 \nconverged\n# weights:  31\ninitial  value 183.358389 \niter  10 value 138.135160\niter  20 value 133.504773\niter  30 value 132.232569\niter  40 value 131.823810\niter  50 value 131.813630\niter  50 value 131.813629\niter  50 value 131.813629\nfinal  value 131.813629 \nconverged\n# weights:  51\ninitial  value 188.375733 \niter  10 value 137.715389\niter  20 value 133.857201\niter  30 value 127.299427\niter  40 value 122.821534\niter  50 value 122.276359\niter  60 value 122.234125\niter  70 value 122.231352\nfinal  value 122.231336 \nconverged\n# weights:  11\ninitial  value 184.044708 \niter  10 value 138.367892\niter  20 value 137.987598\niter  30 value 136.782436\niter  40 value 134.813322\niter  50 value 134.429955\niter  60 value 134.318182\niter  70 value 134.314470\niter  80 value 134.311792\niter  90 value 134.306794\niter 100 value 134.293605\nfinal  value 134.293605 \nstopped after 100 iterations\n# weights:  31\ninitial  value 161.912438 \niter  10 value 138.527760\niter  20 value 132.896082\niter  30 value 120.564804\niter  40 value 116.472165\niter  50 value 114.106373\niter  60 value 113.931795\niter  70 value 113.703512\niter  80 value 113.583707\niter  90 value 113.522543\niter 100 value 113.334040\nfinal  value 113.334040 \nstopped after 100 iterations\n# weights:  51\ninitial  value 163.530519 \niter  10 value 135.397425\niter  20 value 117.760201\niter  30 value 98.757667\niter  40 value 90.530530\niter  50 value 85.899673\niter  60 value 85.639340\niter  70 value 85.567108\niter  80 value 85.461082\niter  90 value 85.337051\niter 100 value 85.329269\nfinal  value 85.329269 \nstopped after 100 iterations\n# weights:  11\ninitial  value 179.506932 \niter  10 value 137.970252\niter  20 value 137.646657\niter  30 value 137.290045\niter  40 value 135.628243\niter  50 value 134.060107\niter  60 value 134.054853\nfinal  value 134.054593 \nconverged\n# weights:  31\ninitial  value 162.801024 \niter  10 value 136.586035\niter  20 value 122.061440\niter  30 value 112.709036\niter  40 value 105.942660\niter  50 value 104.607989\niter  60 value 104.550472\nfinal  value 104.550340 \nconverged\n# weights:  51\ninitial  value 162.840044 \niter  10 value 133.804152\niter  20 value 122.210161\niter  30 value 107.034765\niter  40 value 97.298628\niter  50 value 95.311873\niter  60 value 95.178014\niter  70 value 95.175154\nfinal  value 95.175133 \nconverged\n# weights:  11\ninitial  value 172.786887 \niter  10 value 142.144810\niter  20 value 139.504127\nfinal  value 139.499624 \nconverged\n# weights:  31\ninitial  value 162.282873 \niter  10 value 138.859534\niter  20 value 136.843247\niter  30 value 135.628344\niter  40 value 135.496862\nfinal  value 135.495788 \nconverged\n# weights:  51\ninitial  value 159.568153 \niter  10 value 138.887708\niter  20 value 133.007936\niter  30 value 125.811940\niter  40 value 123.319577\niter  50 value 122.654103\niter  60 value 122.410055\niter  70 value 122.392654\nfinal  value 122.391815 \nconverged\n# weights:  11\ninitial  value 227.394362 \niter  10 value 145.684059\niter  20 value 141.780969\niter  30 value 138.889227\niter  40 value 138.764292\niter  50 value 138.719265\niter  60 value 136.671320\niter  70 value 136.099361\niter  80 value 136.067891\niter  90 value 136.060026\niter 100 value 136.055339\nfinal  value 136.055339 \nstopped after 100 iterations\n# weights:  31\ninitial  value 163.796501 \niter  10 value 136.031707\niter  20 value 122.770172\niter  30 value 117.282544\niter  40 value 116.151263\niter  50 value 116.017031\niter  60 value 115.982232\niter  70 value 115.924025\niter  80 value 115.869099\niter  90 value 115.850331\niter 100 value 115.797430\nfinal  value 115.797430 \nstopped after 100 iterations\n# weights:  51\ninitial  value 168.938205 \niter  10 value 134.137211\niter  20 value 115.865328\niter  30 value 102.191187\niter  40 value 98.907005\niter  50 value 95.491122\niter  60 value 95.082852\niter  70 value 94.871496\niter  80 value 94.699871\niter  90 value 94.283373\niter 100 value 94.098959\nfinal  value 94.098959 \nstopped after 100 iterations\n# weights:  11\ninitial  value 209.149800 \niter  10 value 143.053952\niter  20 value 139.006978\niter  30 value 138.965950\niter  40 value 135.981395\niter  50 value 135.626680\niter  60 value 135.624187\niter  70 value 135.622937\niter  70 value 135.622937\nfinal  value 135.622937 \nconverged\n# weights:  31\ninitial  value 171.706598 \niter  10 value 137.283422\niter  20 value 131.926435\niter  30 value 122.643111\niter  40 value 118.227293\niter  50 value 116.775266\niter  60 value 111.365769\niter  70 value 110.602586\niter  80 value 110.502305\niter  90 value 110.427176\niter 100 value 110.112560\nfinal  value 110.112560 \nstopped after 100 iterations\n# weights:  51\ninitial  value 197.280762 \niter  10 value 135.837539\niter  20 value 121.848493\niter  30 value 107.636327\niter  40 value 101.682352\niter  50 value 99.657649\niter  60 value 95.500436\niter  70 value 94.331351\niter  80 value 94.302994\niter  90 value 94.302073\nfinal  value 94.302069 \nconverged\n# weights:  11\ninitial  value 160.483879 \niter  10 value 142.714774\niter  20 value 141.077035\niter  30 value 140.882819\nfinal  value 140.881232 \nconverged\n# weights:  31\ninitial  value 166.601724 \niter  10 value 139.006468\niter  20 value 135.760186\niter  30 value 134.678546\niter  40 value 134.194038\niter  50 value 134.164016\nfinal  value 134.163997 \nconverged\n# weights:  51\ninitial  value 166.890655 \niter  10 value 140.817858\niter  20 value 136.191013\niter  30 value 132.232099\niter  40 value 131.138807\niter  50 value 129.909766\niter  60 value 129.352231\niter  70 value 129.169821\niter  80 value 129.159334\nfinal  value 129.159301 \nconverged\n# weights:  11\ninitial  value 178.516491 \niter  10 value 140.111926\niter  20 value 139.024288\niter  30 value 138.976948\niter  40 value 136.754672\niter  50 value 136.533183\niter  60 value 136.370358\niter  70 value 136.331510\niter  80 value 136.328674\niter  90 value 136.327950\niter 100 value 136.326126\nfinal  value 136.326126 \nstopped after 100 iterations\n# weights:  31\ninitial  value 160.121464 \niter  10 value 136.538567\niter  20 value 126.361109\niter  30 value 118.613099\niter  40 value 116.316348\niter  50 value 115.910778\niter  60 value 115.852292\niter  70 value 115.754953\niter  80 value 115.731127\niter  90 value 115.697254\niter 100 value 115.678638\nfinal  value 115.678638 \nstopped after 100 iterations\n# weights:  51\ninitial  value 165.226667 \niter  10 value 136.842955\niter  20 value 124.877308\niter  30 value 112.642368\niter  40 value 100.743099\niter  50 value 99.533481\niter  60 value 99.441176\niter  70 value 99.385135\niter  80 value 99.326497\niter  90 value 99.252275\niter 100 value 99.177754\nfinal  value 99.177754 \nstopped after 100 iterations\n# weights:  11\ninitial  value 213.104004 \niter  10 value 140.199463\niter  20 value 139.576207\niter  30 value 137.322258\niter  40 value 135.241842\niter  50 value 134.322859\niter  60 value 134.269538\niter  70 value 134.261580\niter  80 value 134.258660\niter  90 value 134.064392\niter 100 value 133.670879\nfinal  value 133.670879 \nstopped after 100 iterations\n# weights:  31\ninitial  value 161.865379 \niter  10 value 139.751517\niter  20 value 131.801229\niter  30 value 122.854584\niter  40 value 116.644115\niter  50 value 116.320947\niter  60 value 116.316760\nfinal  value 116.316755 \nconverged\n# weights:  51\ninitial  value 162.573078 \niter  10 value 131.870883\niter  20 value 112.170607\niter  30 value 100.847820\niter  40 value 95.371014\niter  50 value 89.976699\niter  60 value 89.195852\niter  70 value 89.051934\niter  80 value 88.946802\niter  90 value 88.736371\niter 100 value 88.425572\nfinal  value 88.425572 \nstopped after 100 iterations\n# weights:  11\ninitial  value 191.374109 \niter  10 value 142.619027\niter  20 value 141.754410\nfinal  value 141.752217 \nconverged\n# weights:  31\ninitial  value 160.527288 \niter  10 value 142.278147\niter  20 value 139.405239\niter  30 value 136.654675\niter  40 value 135.677475\niter  50 value 135.247570\niter  60 value 135.193250\niter  70 value 134.989266\niter  80 value 134.589903\niter  90 value 134.577755\niter  90 value 134.577754\niter  90 value 134.577754\nfinal  value 134.577754 \nconverged\n# weights:  51\ninitial  value 168.921008 \niter  10 value 139.050284\niter  20 value 135.370327\niter  30 value 129.473189\niter  40 value 127.682591\niter  50 value 125.500818\niter  60 value 124.951181\niter  70 value 124.748899\niter  80 value 124.734874\nfinal  value 124.734714 \nconverged\n# weights:  11\ninitial  value 173.675872 \niter  10 value 142.732003\niter  20 value 139.426080\niter  30 value 135.432780\niter  40 value 135.169714\niter  50 value 135.142446\niter  60 value 135.103959\niter  70 value 135.088078\niter  80 value 135.079005\nfinal  value 135.078999 \nconverged\n# weights:  31\ninitial  value 210.396138 \niter  10 value 135.342801\niter  20 value 124.368749\niter  30 value 119.571781\niter  40 value 119.203721\niter  50 value 119.078813\niter  60 value 119.012281\niter  70 value 118.997014\niter  80 value 118.971654\niter  90 value 118.936224\niter 100 value 116.825862\nfinal  value 116.825862 \nstopped after 100 iterations\n# weights:  51\ninitial  value 184.677163 \niter  10 value 134.100112\niter  20 value 118.392853\niter  30 value 104.115461\niter  40 value 98.682711\niter  50 value 96.908988\niter  60 value 96.071214\niter  70 value 95.574754\niter  80 value 95.242041\niter  90 value 94.816766\niter 100 value 94.757510\nfinal  value 94.757510 \nstopped after 100 iterations\n# weights:  11\ninitial  value 172.278040 \niter  10 value 138.203789\niter  20 value 133.404888\niter  30 value 132.766527\niter  40 value 132.172883\niter  50 value 131.900267\niter  60 value 131.819210\niter  70 value 131.574998\niter  80 value 131.573080\niter  90 value 131.506629\niter 100 value 131.479077\nfinal  value 131.479077 \nstopped after 100 iterations\n# weights:  31\ninitial  value 175.782901 \niter  10 value 130.165434\niter  20 value 118.981631\niter  30 value 109.802911\niter  40 value 106.231973\niter  50 value 103.878418\niter  60 value 103.782520\nfinal  value 103.782330 \nconverged\n# weights:  51\ninitial  value 158.969842 \niter  10 value 125.557857\niter  20 value 106.694682\niter  30 value 93.753454\niter  40 value 85.325967\niter  50 value 79.053747\niter  60 value 78.007393\niter  70 value 77.873553\niter  80 value 77.759841\niter  90 value 77.724905\niter 100 value 77.707215\nfinal  value 77.707215 \nstopped after 100 iterations\n# weights:  11\ninitial  value 163.718986 \niter  10 value 135.759288\niter  20 value 135.096530\nfinal  value 135.091882 \nconverged\n# weights:  31\ninitial  value 162.804409 \niter  10 value 132.873090\niter  20 value 128.688435\niter  30 value 128.377188\niter  40 value 128.368205\nfinal  value 128.368176 \nconverged\n# weights:  51\ninitial  value 159.843967 \niter  10 value 131.070641\niter  20 value 123.919180\niter  30 value 117.729124\niter  40 value 116.201406\niter  50 value 115.729271\niter  60 value 115.689759\niter  70 value 115.687639\nfinal  value 115.687630 \nconverged\n# weights:  11\ninitial  value 227.548862 \niter  10 value 134.924409\niter  20 value 133.318694\niter  30 value 132.486381\niter  40 value 132.220948\niter  50 value 131.710555\niter  60 value 131.626297\niter  70 value 131.584002\niter  80 value 131.557200\niter  90 value 131.556912\niter  90 value 131.556911\niter  90 value 131.556911\nfinal  value 131.556911 \nconverged\n# weights:  31\ninitial  value 191.239772 \niter  10 value 129.081148\niter  20 value 120.608759\niter  30 value 115.350887\niter  40 value 112.488760\niter  50 value 111.840081\niter  60 value 111.295902\niter  70 value 111.177832\niter  80 value 111.059110\niter  90 value 110.825157\niter 100 value 110.519145\nfinal  value 110.519145 \nstopped after 100 iterations\n# weights:  51\ninitial  value 165.458685 \niter  10 value 130.787917\niter  20 value 112.820324\niter  30 value 97.869772\niter  40 value 89.474738\niter  50 value 81.219272\niter  60 value 79.136512\niter  70 value 78.932527\niter  80 value 78.883020\niter  90 value 78.845330\niter 100 value 78.746223\nfinal  value 78.746223 \nstopped after 100 iterations\n# weights:  11\ninitial  value 162.055938 \niter  10 value 140.589535\niter  20 value 140.072731\niter  30 value 139.934847\niter  40 value 139.897693\niter  50 value 139.888327\nfinal  value 139.888302 \nconverged\n# weights:  31\ninitial  value 177.576410 \niter  10 value 136.595687\niter  20 value 130.478324\niter  30 value 120.010050\niter  40 value 116.672710\niter  50 value 116.438731\nfinal  value 116.438040 \nconverged\n# weights:  51\ninitial  value 174.292085 \niter  10 value 137.714523\niter  20 value 125.522290\niter  30 value 107.084484\niter  40 value 95.305070\niter  50 value 91.426701\niter  60 value 91.105481\niter  70 value 91.098087\nfinal  value 91.098030 \nconverged\n# weights:  11\ninitial  value 211.564194 \niter  10 value 141.501721\niter  20 value 141.399229\nfinal  value 141.398724 \nconverged\n# weights:  31\ninitial  value 160.131240 \niter  10 value 140.891852\niter  20 value 137.304058\niter  30 value 134.735777\niter  40 value 133.688406\niter  50 value 133.588249\nfinal  value 133.588185 \nconverged\n# weights:  51\ninitial  value 171.501655 \niter  10 value 139.596755\niter  20 value 131.067752\niter  30 value 126.421477\niter  40 value 126.057469\niter  50 value 125.089448\niter  60 value 124.977675\niter  70 value 124.956850\nfinal  value 124.956412 \nconverged\n# weights:  11\ninitial  value 170.905104 \niter  10 value 140.241009\niter  20 value 140.059992\niter  30 value 139.973805\niter  40 value 139.896534\niter  50 value 139.891493\niter  60 value 139.890571\nfinal  value 139.890465 \nconverged\n# weights:  31\ninitial  value 169.542161 \niter  10 value 136.542952\niter  20 value 126.030673\niter  30 value 118.378674\niter  40 value 115.065697\niter  50 value 114.963238\niter  60 value 114.930833\niter  70 value 114.914701\niter  80 value 114.884295\niter  90 value 114.766143\niter 100 value 114.725576\nfinal  value 114.725576 \nstopped after 100 iterations\n# weights:  51\ninitial  value 184.485265 \niter  10 value 138.103760\niter  20 value 119.882299\niter  30 value 103.685543\niter  40 value 95.462899\niter  50 value 91.187148\niter  60 value 90.780603\niter  70 value 90.681210\niter  80 value 90.518048\niter  90 value 90.398916\niter 100 value 90.297934\nfinal  value 90.297934 \nstopped after 100 iterations\n# weights:  11\ninitial  value 171.592321 \niter  10 value 140.781938\niter  20 value 140.422259\niter  30 value 140.289618\niter  40 value 140.131506\niter  50 value 140.103460\niter  60 value 138.035852\nfinal  value 137.724228 \nconverged\n# weights:  31\ninitial  value 164.460259 \niter  10 value 140.543957\niter  20 value 136.226058\niter  30 value 128.977413\niter  40 value 122.608601\niter  50 value 116.566615\niter  60 value 114.686383\niter  70 value 114.184220\niter  80 value 113.881354\niter  90 value 113.695505\niter 100 value 113.668043\nfinal  value 113.668043 \nstopped after 100 iterations\n# weights:  51\ninitial  value 161.403042 \niter  10 value 136.865772\niter  20 value 126.108083\niter  30 value 116.241422\niter  40 value 111.907289\niter  50 value 111.682080\niter  60 value 111.657440\nfinal  value 111.657247 \nconverged\n# weights:  11\ninitial  value 182.278156 \niter  10 value 141.824502\niter  20 value 141.658782\nfinal  value 141.658742 \nconverged\n# weights:  31\ninitial  value 190.329850 \niter  10 value 139.801605\niter  20 value 135.845564\niter  30 value 135.443615\niter  40 value 135.084678\niter  50 value 134.996332\nfinal  value 134.996052 \nconverged\n# weights:  51\ninitial  value 212.318761 \niter  10 value 140.380880\niter  20 value 134.225340\niter  30 value 128.598883\niter  40 value 127.470551\niter  50 value 126.069289\niter  60 value 125.915148\niter  70 value 125.912537\nfinal  value 125.912530 \nconverged\n# weights:  11\ninitial  value 190.986317 \niter  10 value 140.797753\niter  20 value 139.271855\niter  30 value 138.339032\niter  40 value 138.038192\niter  50 value 137.834263\niter  60 value 137.437737\niter  70 value 137.432604\niter  80 value 137.427188\niter  90 value 137.425442\niter 100 value 137.424602\nfinal  value 137.424602 \nstopped after 100 iterations\n# weights:  31\ninitial  value 163.115806 \niter  10 value 136.900695\niter  20 value 129.052346\niter  30 value 119.348552\niter  40 value 117.912775\niter  50 value 115.451590\niter  60 value 115.105045\niter  70 value 114.274047\niter  80 value 113.884471\niter  90 value 113.860675\niter 100 value 113.841186\nfinal  value 113.841186 \nstopped after 100 iterations\n# weights:  51\ninitial  value 165.005277 \niter  10 value 133.585182\niter  20 value 120.341988\niter  30 value 110.776771\niter  40 value 105.623942\niter  50 value 105.049834\niter  60 value 104.943054\niter  70 value 104.853483\niter  80 value 104.696672\niter  90 value 104.530329\niter 100 value 104.426164\nfinal  value 104.426164 \nstopped after 100 iterations\n# weights:  11\ninitial  value 200.313719 \niter  10 value 139.350658\niter  20 value 136.098793\niter  30 value 134.191895\niter  40 value 133.963233\niter  50 value 133.953509\niter  60 value 133.941539\niter  70 value 133.040667\niter  80 value 133.019420\niter  90 value 132.967278\niter 100 value 132.957064\nfinal  value 132.957064 \nstopped after 100 iterations\n# weights:  31\ninitial  value 164.684300 \niter  10 value 137.563149\niter  20 value 127.532291\niter  30 value 124.122823\niter  40 value 123.789976\niter  50 value 123.787727\niter  50 value 123.787725\niter  50 value 123.787725\nfinal  value 123.787725 \nconverged\n# weights:  51\ninitial  value 159.570397 \niter  10 value 138.379599\niter  20 value 126.041458\niter  30 value 110.774525\niter  40 value 103.415620\niter  50 value 102.669115\niter  60 value 102.667208\nfinal  value 102.667201 \nconverged\n# weights:  11\ninitial  value 158.727093 \niter  10 value 143.737524\niter  20 value 141.397593\niter  30 value 140.828659\nfinal  value 140.826885 \nconverged\n# weights:  31\ninitial  value 159.780949 \niter  10 value 140.310630\niter  20 value 138.052977\niter  30 value 135.245764\niter  40 value 134.695327\niter  50 value 134.660598\nfinal  value 134.660031 \nconverged\n# weights:  51\ninitial  value 232.326623 \niter  10 value 138.591008\niter  20 value 133.583988\niter  30 value 129.609159\niter  40 value 128.010409\niter  50 value 127.132771\niter  60 value 126.891068\nfinal  value 126.889661 \nconverged\n# weights:  11\ninitial  value 173.357705 \niter  10 value 140.184672\niter  20 value 138.776170\niter  30 value 136.417203\niter  40 value 135.751818\niter  50 value 135.727716\niter  60 value 135.727622\nfinal  value 135.727374 \nconverged\n# weights:  31\ninitial  value 163.383543 \niter  10 value 139.610489\niter  20 value 128.071009\niter  30 value 119.613759\niter  40 value 114.206623\niter  50 value 113.179046\niter  60 value 113.043883\niter  70 value 113.024941\niter  80 value 113.020857\niter  90 value 113.009616\niter 100 value 113.000503\nfinal  value 113.000503 \nstopped after 100 iterations\n# weights:  51\ninitial  value 238.940138 \niter  10 value 134.952253\niter  20 value 114.937840\niter  30 value 102.695858\niter  40 value 96.296091\niter  50 value 95.023270\niter  60 value 94.796545\niter  70 value 94.690275\niter  80 value 94.625081\niter  90 value 94.533119\niter 100 value 94.456990\nfinal  value 94.456990 \nstopped after 100 iterations\n# weights:  11\ninitial  value 165.027060 \niter  10 value 139.875130\niter  20 value 139.077223\niter  30 value 138.971226\niter  40 value 138.970471\niter  40 value 138.970470\niter  40 value 138.970470\nfinal  value 138.970470 \nconverged\n# weights:  31\ninitial  value 179.225982 \niter  10 value 135.469367\niter  20 value 120.012899\niter  30 value 115.696459\niter  40 value 110.558677\niter  50 value 106.791605\niter  60 value 106.244906\nfinal  value 106.243952 \nconverged\n# weights:  51\ninitial  value 167.807338 \niter  10 value 135.034359\niter  20 value 114.510593\niter  30 value 101.278542\niter  40 value 91.645607\niter  50 value 85.392046\niter  60 value 84.012795\niter  70 value 83.949672\niter  80 value 83.949243\niter  80 value 83.949242\niter  80 value 83.949242\nfinal  value 83.949242 \nconverged\n# weights:  11\ninitial  value 174.279384 \niter  10 value 145.451949\niter  20 value 140.834317\niter  30 value 140.532603\nfinal  value 140.531101 \nconverged\n# weights:  31\ninitial  value 161.342593 \niter  10 value 140.593117\niter  20 value 138.779082\niter  30 value 136.503457\niter  40 value 135.335674\niter  50 value 134.466010\niter  60 value 134.048104\niter  70 value 133.993586\nfinal  value 133.993574 \nconverged\n# weights:  51\ninitial  value 168.037064 \niter  10 value 138.323100\niter  20 value 134.352953\niter  30 value 132.511169\niter  40 value 131.902917\niter  50 value 131.199685\niter  60 value 129.261727\niter  70 value 127.711129\niter  80 value 127.611576\niter  90 value 127.609367\nfinal  value 127.609363 \nconverged\n# weights:  11\ninitial  value 194.152603 \niter  10 value 139.425041\niter  20 value 139.137106\niter  30 value 139.054512\niter  40 value 139.039935\niter  50 value 138.774238\niter  60 value 137.404061\niter  70 value 137.247646\niter  80 value 137.239981\nfinal  value 137.238340 \nconverged\n# weights:  31\ninitial  value 158.266381 \niter  10 value 132.791252\niter  20 value 124.811292\niter  30 value 118.036323\niter  40 value 112.974656\niter  50 value 112.447767\niter  60 value 112.261851\niter  70 value 112.193535\niter  80 value 111.616625\niter  90 value 111.464448\niter 100 value 111.349567\nfinal  value 111.349567 \nstopped after 100 iterations\n# weights:  51\ninitial  value 165.468073 \niter  10 value 133.687798\niter  20 value 122.590838\niter  30 value 111.973761\niter  40 value 105.859130\niter  50 value 97.431079\niter  60 value 94.452660\niter  70 value 94.153811\niter  80 value 94.079744\niter  90 value 93.998486\niter 100 value 93.850135\nfinal  value 93.850135 \nstopped after 100 iterations\n# weights:  11\ninitial  value 165.014561 \niter  10 value 137.899484\niter  20 value 136.232056\niter  30 value 134.549308\niter  40 value 134.513446\niter  50 value 134.508829\niter  60 value 134.500676\niter  70 value 134.401937\niter  80 value 134.340127\niter  90 value 134.330952\niter 100 value 134.324818\nfinal  value 134.324818 \nstopped after 100 iterations\n# weights:  31\ninitial  value 160.203566 \niter  10 value 132.832429\niter  20 value 122.158532\niter  30 value 115.134376\niter  40 value 109.170960\niter  50 value 108.128700\niter  60 value 108.115362\nfinal  value 108.115353 \nconverged\n# weights:  51\ninitial  value 250.231304 \niter  10 value 135.167939\niter  20 value 120.744735\niter  30 value 111.907151\niter  40 value 97.843055\niter  50 value 89.226454\niter  60 value 83.736963\niter  70 value 81.527434\niter  80 value 81.198832\niter  90 value 81.197683\niter  90 value 81.197682\niter  90 value 81.197682\nfinal  value 81.197682 \nconverged\n# weights:  11\ninitial  value 190.116711 \niter  10 value 142.070813\niter  20 value 140.747631\nfinal  value 140.745228 \nconverged\n# weights:  31\ninitial  value 161.641477 \niter  10 value 139.415694\niter  20 value 134.327968\niter  30 value 132.965416\niter  40 value 132.859049\niter  50 value 132.858283\niter  50 value 132.858282\niter  50 value 132.858282\nfinal  value 132.858282 \nconverged\n# weights:  51\ninitial  value 157.243854 \niter  10 value 138.879049\niter  20 value 132.686310\niter  30 value 130.953890\niter  40 value 128.504956\niter  50 value 128.118506\niter  60 value 128.097075\niter  70 value 128.095948\niter  80 value 128.094710\niter  90 value 128.087408\niter 100 value 128.051531\nfinal  value 128.051531 \nstopped after 100 iterations\n# weights:  11\ninitial  value 179.029806 \niter  10 value 139.357902\niter  20 value 138.341402\niter  30 value 138.004403\niter  40 value 136.193411\niter  50 value 135.997240\niter  60 value 135.663070\niter  70 value 135.662275\niter  80 value 135.662003\niter  90 value 135.661573\niter 100 value 135.661147\nfinal  value 135.661147 \nstopped after 100 iterations\n# weights:  31\ninitial  value 188.396607 \niter  10 value 137.597390\niter  20 value 126.095361\niter  30 value 122.394763\niter  40 value 121.749206\niter  50 value 121.656930\niter  60 value 121.447927\niter  70 value 121.391625\niter  80 value 121.329699\niter  90 value 121.294809\niter 100 value 121.262329\nfinal  value 121.262329 \nstopped after 100 iterations\n# weights:  51\ninitial  value 211.304252 \niter  10 value 133.154680\niter  20 value 119.582469\niter  30 value 109.802660\niter  40 value 106.671920\niter  50 value 106.129939\niter  60 value 106.034590\niter  70 value 106.018875\niter  80 value 106.001439\niter  90 value 105.984382\niter 100 value 105.969542\nfinal  value 105.969542 \nstopped after 100 iterations\n# weights:  11\ninitial  value 159.878783 \niter  10 value 141.053876\niter  20 value 140.741966\niter  30 value 138.104921\niter  40 value 137.425453\niter  50 value 137.421048\niter  60 value 137.419952\nfinal  value 137.419355 \nconverged\n# weights:  31\ninitial  value 162.328277 \niter  10 value 139.682112\niter  20 value 130.505981\niter  30 value 120.717799\niter  40 value 112.203372\niter  50 value 109.695482\niter  60 value 109.666428\nfinal  value 109.666397 \nconverged\n# weights:  51\ninitial  value 240.785893 \niter  10 value 132.634613\niter  20 value 118.844433\niter  30 value 107.335508\niter  40 value 99.729768\niter  50 value 95.095891\niter  60 value 94.632741\niter  70 value 94.629487\nfinal  value 94.629481 \nconverged\n# weights:  11\ninitial  value 168.785632 \niter  10 value 143.488428\niter  20 value 140.813386\niter  30 value 140.626454\nfinal  value 140.621415 \nconverged\n# weights:  31\ninitial  value 162.191817 \niter  10 value 142.182906\niter  20 value 139.047756\niter  30 value 137.108331\niter  40 value 135.690700\niter  50 value 134.829325\niter  60 value 134.526339\niter  70 value 134.394948\niter  80 value 134.374081\niter  90 value 134.371950\nfinal  value 134.371795 \nconverged\n# weights:  51\ninitial  value 171.847601 \niter  10 value 140.190920\niter  20 value 131.645432\niter  30 value 128.515127\niter  40 value 127.791904\niter  50 value 127.697450\niter  60 value 127.372297\niter  70 value 127.281962\niter  80 value 127.277110\niter  80 value 127.277109\niter  80 value 127.277109\nfinal  value 127.277109 \nconverged\n# weights:  11\ninitial  value 229.184361 \niter  10 value 140.558727\niter  20 value 139.200319\niter  30 value 139.040693\niter  40 value 138.992673\nfinal  value 138.991916 \nconverged\n# weights:  31\ninitial  value 159.022505 \niter  10 value 139.290037\niter  20 value 126.427195\niter  30 value 114.027045\niter  40 value 112.523801\niter  50 value 112.352736\niter  60 value 112.170449\niter  70 value 112.131646\niter  80 value 112.032048\niter  90 value 111.629188\niter 100 value 111.539967\nfinal  value 111.539967 \nstopped after 100 iterations\n# weights:  51\ninitial  value 206.077132 \niter  10 value 134.210703\niter  20 value 114.742557\niter  30 value 100.597042\niter  40 value 95.026590\niter  50 value 93.328071\niter  60 value 93.198664\niter  70 value 93.153741\niter  80 value 92.975239\niter  90 value 92.741525\niter 100 value 92.609810\nfinal  value 92.609810 \nstopped after 100 iterations\n# weights:  11\ninitial  value 160.482924 \niter  10 value 141.143759\niter  20 value 140.641476\niter  30 value 140.606369\nfinal  value 140.606302 \nconverged\n# weights:  31\ninitial  value 253.981764 \niter  10 value 141.359966\niter  20 value 131.040887\niter  30 value 127.889537\niter  40 value 123.099625\niter  50 value 121.697706\niter  60 value 121.657556\niter  70 value 121.235104\niter  80 value 121.214890\nfinal  value 121.214883 \nconverged\n# weights:  51\ninitial  value 233.073560 \niter  10 value 138.019072\niter  20 value 126.263984\niter  30 value 113.162843\niter  40 value 107.090041\niter  50 value 106.607210\niter  60 value 106.583600\niter  70 value 106.580318\nfinal  value 106.580296 \nconverged\n# weights:  11\ninitial  value 197.495517 \niter  10 value 142.487626\niter  20 value 141.805689\nfinal  value 141.796821 \nconverged\n# weights:  31\ninitial  value 160.559031 \niter  10 value 139.431362\niter  20 value 136.243653\niter  30 value 135.391244\niter  40 value 133.561877\niter  50 value 133.376007\nfinal  value 133.373608 \nconverged\n# weights:  51\ninitial  value 160.877038 \niter  10 value 141.242832\niter  20 value 138.323231\niter  30 value 132.060545\niter  40 value 128.332430\niter  50 value 126.467795\niter  60 value 124.437730\niter  70 value 123.569221\niter  80 value 122.948680\niter  90 value 122.940604\nfinal  value 122.940562 \nconverged\n# weights:  11\ninitial  value 159.074580 \niter  10 value 143.054856\niter  20 value 139.328905\niter  30 value 138.029209\niter  40 value 137.878093\niter  50 value 137.797731\niter  60 value 137.693232\niter  70 value 137.678310\niter  80 value 137.664265\niter  90 value 137.646541\niter 100 value 137.627387\nfinal  value 137.627387 \nstopped after 100 iterations\n# weights:  31\ninitial  value 168.166449 \niter  10 value 138.195648\niter  20 value 123.351662\niter  30 value 120.927095\niter  40 value 120.103912\niter  50 value 119.976717\niter  60 value 119.898606\niter  70 value 119.886486\niter  80 value 119.871386\niter  90 value 119.838751\niter 100 value 119.836236\nfinal  value 119.836236 \nstopped after 100 iterations\n# weights:  51\ninitial  value 159.127970 \niter  10 value 136.977079\niter  20 value 118.214752\niter  30 value 103.067679\niter  40 value 93.802645\niter  50 value 90.361081\niter  60 value 89.733079\niter  70 value 89.708205\niter  80 value 89.672917\niter  90 value 89.606588\niter 100 value 89.592164\nfinal  value 89.592164 \nstopped after 100 iterations\n# weights:  11\ninitial  value 188.478411 \niter  10 value 137.804643\niter  20 value 136.945184\niter  30 value 133.782148\niter  40 value 133.195165\niter  50 value 133.190954\niter  60 value 133.188769\niter  70 value 133.187599\nfinal  value 133.187429 \nconverged\n# weights:  31\ninitial  value 159.990338 \niter  10 value 132.314341\niter  20 value 121.645500\niter  30 value 114.189288\niter  40 value 108.426521\niter  50 value 108.276882\niter  60 value 108.271997\nfinal  value 108.271734 \nconverged\n# weights:  51\ninitial  value 160.804983 \niter  10 value 134.560690\niter  20 value 116.894522\niter  30 value 107.316336\niter  40 value 101.068077\niter  50 value 94.915218\niter  60 value 94.125586\niter  70 value 94.108013\niter  80 value 94.107608\niter  80 value 94.107607\niter  80 value 94.107607\nfinal  value 94.107607 \nconverged\n# weights:  11\ninitial  value 165.352922 \niter  10 value 140.019650\niter  20 value 139.943021\niter  20 value 139.943021\niter  20 value 139.943021\nfinal  value 139.943021 \nconverged\n# weights:  31\ninitial  value 163.534768 \niter  10 value 140.089518\niter  20 value 135.911395\niter  30 value 133.397132\niter  40 value 132.665077\niter  50 value 132.643633\nfinal  value 132.643632 \nconverged\n# weights:  51\ninitial  value 162.632690 \niter  10 value 139.891031\niter  20 value 134.466897\niter  30 value 125.833300\niter  40 value 124.044554\niter  50 value 123.950305\niter  60 value 123.946686\nfinal  value 123.946651 \nconverged\n# weights:  11\ninitial  value 161.521217 \niter  10 value 138.352578\niter  20 value 134.926945\niter  30 value 134.335225\niter  40 value 134.181046\niter  50 value 133.400835\niter  60 value 133.396980\niter  70 value 133.396382\niter  80 value 133.395956\niter  90 value 133.395424\niter 100 value 133.394751\nfinal  value 133.394751 \nstopped after 100 iterations\n# weights:  31\ninitial  value 169.286557 \niter  10 value 134.915879\niter  20 value 131.520632\niter  30 value 127.634272\niter  40 value 121.184546\niter  50 value 120.108939\niter  60 value 120.064399\niter  70 value 119.941217\niter  80 value 119.724805\niter  90 value 119.694112\niter 100 value 119.679920\nfinal  value 119.679920 \nstopped after 100 iterations\n# weights:  51\ninitial  value 164.834544 \niter  10 value 136.340300\niter  20 value 120.441165\niter  30 value 110.457060\niter  40 value 106.524903\niter  50 value 104.342035\niter  60 value 104.233242\niter  70 value 104.099063\niter  80 value 103.986700\niter  90 value 103.971069\niter 100 value 103.947646\nfinal  value 103.947646 \nstopped after 100 iterations\n# weights:  11\ninitial  value 188.973656 \niter  10 value 142.657304\niter  20 value 141.820403\niter  30 value 138.071023\niter  40 value 137.962318\niter  50 value 137.541287\niter  60 value 137.466606\niter  70 value 137.425139\niter  80 value 137.378779\niter  90 value 137.371160\niter 100 value 137.351561\nfinal  value 137.351561 \nstopped after 100 iterations\n# weights:  31\ninitial  value 170.464505 \niter  10 value 140.185394\niter  20 value 133.232531\niter  30 value 129.806869\niter  40 value 128.476237\niter  50 value 128.457513\nfinal  value 128.457453 \nconverged\n# weights:  51\ninitial  value 152.418441 \niter  10 value 138.481721\niter  20 value 117.397102\niter  30 value 102.145633\niter  40 value 96.091086\niter  50 value 87.581813\niter  60 value 85.359877\niter  70 value 84.857822\niter  80 value 84.789513\niter  90 value 84.787848\nfinal  value 84.787843 \nconverged\n# weights:  11\ninitial  value 179.923406 \niter  10 value 144.237452\niter  20 value 143.500036\nfinal  value 143.498922 \nconverged\n# weights:  31\ninitial  value 183.085748 \niter  10 value 140.308456\niter  20 value 134.859115\niter  30 value 133.874012\niter  40 value 132.885265\niter  50 value 132.685579\nfinal  value 132.685319 \nconverged\n# weights:  51\ninitial  value 198.202592 \niter  10 value 139.326123\niter  20 value 133.994387\niter  30 value 130.370564\niter  40 value 127.960817\niter  50 value 127.746961\niter  60 value 127.728360\niter  70 value 127.725337\nfinal  value 127.725335 \nconverged\n# weights:  11\ninitial  value 171.552962 \niter  10 value 141.392085\niter  20 value 140.374135\niter  30 value 138.353199\niter  40 value 138.339886\niter  50 value 137.758753\niter  60 value 137.656876\niter  70 value 137.515082\niter  80 value 137.510236\nfinal  value 137.510208 \nconverged\n# weights:  31\ninitial  value 195.362939 \niter  10 value 138.607181\niter  20 value 132.888136\niter  30 value 125.211658\niter  40 value 118.113902\niter  50 value 116.754478\niter  60 value 115.693866\niter  70 value 114.858249\niter  80 value 114.800139\niter  90 value 114.719976\niter 100 value 114.702740\nfinal  value 114.702740 \nstopped after 100 iterations\n# weights:  51\ninitial  value 177.411284 \niter  10 value 138.948381\niter  20 value 119.965976\niter  30 value 106.346815\niter  40 value 100.431078\niter  50 value 99.481461\niter  60 value 99.418376\niter  70 value 99.188020\niter  80 value 98.894783\niter  90 value 98.878053\niter 100 value 98.776687\nfinal  value 98.776687 \nstopped after 100 iterations\n# weights:  11\ninitial  value 199.450269 \niter  10 value 140.042863\niter  20 value 137.776892\niter  30 value 137.311594\niter  40 value 134.205065\niter  50 value 134.185100\niter  60 value 134.183479\nfinal  value 134.183476 \nconverged\n# weights:  31\ninitial  value 159.087716 \niter  10 value 136.968804\niter  20 value 129.729952\niter  30 value 115.557341\niter  40 value 106.585443\niter  50 value 105.254936\niter  60 value 104.892019\niter  70 value 104.819847\niter  80 value 104.785683\niter  90 value 104.781488\niter 100 value 104.780549\nfinal  value 104.780549 \nstopped after 100 iterations\n# weights:  51\ninitial  value 164.990982 \niter  10 value 133.353509\niter  20 value 120.604817\niter  30 value 110.203450\niter  40 value 102.744935\niter  50 value 101.383507\niter  60 value 100.979434\niter  70 value 100.749418\niter  80 value 100.604613\niter  90 value 100.532027\niter 100 value 100.284556\nfinal  value 100.284556 \nstopped after 100 iterations\n# weights:  11\ninitial  value 165.432812 \niter  10 value 145.097720\niter  20 value 139.793307\niter  30 value 139.767833\nfinal  value 139.767802 \nconverged\n# weights:  31\ninitial  value 157.985748 \niter  10 value 138.639982\niter  20 value 134.248349\niter  30 value 132.465309\niter  40 value 130.575138\niter  50 value 130.038744\niter  60 value 130.005944\niter  70 value 129.997270\niter  80 value 129.997025\nfinal  value 129.997012 \nconverged\n# weights:  51\ninitial  value 177.003743 \niter  10 value 137.341333\niter  20 value 130.515669\niter  30 value 126.393040\niter  40 value 124.299342\niter  50 value 123.695119\niter  60 value 123.473894\niter  70 value 123.462742\nfinal  value 123.462711 \nconverged\n# weights:  11\ninitial  value 161.228963 \niter  10 value 138.154203\niter  20 value 137.741239\niter  30 value 137.512607\niter  40 value 135.323190\niter  50 value 135.289905\niter  60 value 135.233821\niter  70 value 135.233271\niter  80 value 135.233202\nfinal  value 135.233190 \nconverged\n# weights:  31\ninitial  value 156.985964 \niter  10 value 134.450941\niter  20 value 121.976187\niter  30 value 117.060237\niter  40 value 114.954745\niter  50 value 114.526206\niter  60 value 114.430027\niter  70 value 114.341575\niter  80 value 114.306580\niter  90 value 114.215755\niter 100 value 114.153284\nfinal  value 114.153284 \nstopped after 100 iterations\n# weights:  51\ninitial  value 163.792023 \niter  10 value 131.591132\niter  20 value 111.429147\niter  30 value 104.805306\niter  40 value 100.571772\niter  50 value 95.677671\niter  60 value 94.455232\niter  70 value 94.113590\niter  80 value 93.832495\niter  90 value 93.323754\niter 100 value 93.133845\nfinal  value 93.133845 \nstopped after 100 iterations\n# weights:  11\ninitial  value 172.043858 \niter  10 value 138.538278\niter  20 value 136.023042\niter  30 value 134.509999\niter  40 value 134.493950\niter  50 value 134.476281\niter  60 value 134.461501\niter  70 value 134.241261\niter  80 value 133.806204\niter  90 value 133.736226\niter 100 value 133.725576\nfinal  value 133.725576 \nstopped after 100 iterations\n# weights:  31\ninitial  value 204.421492 \niter  10 value 135.098335\niter  20 value 123.682456\niter  30 value 116.573724\niter  40 value 116.359482\nfinal  value 116.359085 \nconverged\n# weights:  51\ninitial  value 176.939858 \niter  10 value 133.275214\niter  20 value 124.037579\niter  30 value 104.534173\niter  40 value 95.374454\niter  50 value 91.888598\niter  60 value 89.447472\niter  70 value 89.266625\niter  80 value 89.247950\nfinal  value 89.247650 \nconverged\n# weights:  11\ninitial  value 160.524320 \niter  10 value 141.928302\niter  20 value 140.806229\niter  30 value 140.717047\nfinal  value 140.717014 \nconverged\n# weights:  31\ninitial  value 162.141736 \niter  10 value 139.464477\niter  20 value 136.865960\niter  30 value 135.965222\niter  40 value 135.788236\niter  50 value 135.677602\niter  60 value 135.547734\nfinal  value 135.547416 \nconverged\n# weights:  51\ninitial  value 188.957579 \niter  10 value 138.731326\niter  20 value 133.359283\niter  30 value 129.574382\niter  40 value 127.640660\niter  50 value 127.504270\niter  60 value 127.391312\niter  70 value 127.383785\niter  80 value 127.292676\niter  90 value 126.782620\niter 100 value 126.726321\nfinal  value 126.726321 \nstopped after 100 iterations\n# weights:  11\ninitial  value 161.548062 \niter  10 value 138.675210\niter  20 value 136.289494\niter  30 value 133.858997\niter  40 value 133.847805\niter  50 value 133.846844\niter  60 value 133.846266\niter  70 value 133.843881\niter  80 value 133.842867\niter  90 value 133.842751\nfinal  value 133.842645 \nconverged\n# weights:  31\ninitial  value 204.213290 \niter  10 value 137.714825\niter  20 value 121.097166\niter  30 value 111.242819\niter  40 value 109.002456\niter  50 value 108.966775\niter  60 value 108.942534\niter  70 value 108.901155\niter  80 value 108.699595\niter  90 value 108.189617\niter 100 value 108.150521\nfinal  value 108.150521 \nstopped after 100 iterations\n# weights:  51\ninitial  value 197.029950 \niter  10 value 136.948449\niter  20 value 117.561507\niter  30 value 104.601184\niter  40 value 99.952924\niter  50 value 99.610563\niter  60 value 99.549544\niter  70 value 99.488656\niter  80 value 99.461512\niter  90 value 99.431786\niter 100 value 99.399898\nfinal  value 99.399898 \nstopped after 100 iterations\n# weights:  11\ninitial  value 179.253490 \niter  10 value 139.850354\niter  20 value 138.033637\niter  30 value 137.373329\niter  40 value 131.785956\niter  50 value 129.176868\niter  60 value 128.902993\niter  70 value 128.597681\niter  80 value 128.523481\niter  90 value 128.518768\niter 100 value 128.515672\nfinal  value 128.515672 \nstopped after 100 iterations\n# weights:  31\ninitial  value 160.477444 \niter  10 value 135.784723\niter  20 value 120.603473\niter  30 value 115.084747\niter  40 value 112.484926\niter  50 value 112.407475\niter  60 value 112.325669\niter  70 value 112.322721\niter  80 value 112.322050\nfinal  value 112.321973 \nconverged\n# weights:  51\ninitial  value 163.818419 \niter  10 value 130.276743\niter  20 value 110.833405\niter  30 value 99.257209\niter  40 value 90.525321\niter  50 value 83.110115\niter  60 value 78.135598\niter  70 value 77.930262\nfinal  value 77.929812 \nconverged\n# weights:  11\ninitial  value 166.537722 \niter  10 value 139.862768\niter  20 value 139.513229\nfinal  value 139.513212 \nconverged\n# weights:  31\ninitial  value 169.288777 \niter  10 value 138.762995\niter  20 value 136.603748\niter  30 value 134.713523\niter  40 value 133.531498\niter  50 value 132.610076\niter  60 value 130.737598\niter  70 value 129.874160\niter  80 value 129.807507\nfinal  value 129.805891 \nconverged\n# weights:  51\ninitial  value 173.493044 \niter  10 value 140.358971\niter  20 value 129.542664\niter  30 value 125.051827\niter  40 value 122.719351\niter  50 value 121.321995\niter  60 value 120.970558\niter  70 value 120.883849\niter  80 value 120.878019\nfinal  value 120.877947 \nconverged\n# weights:  11\ninitial  value 175.515378 \niter  10 value 137.186811\niter  20 value 134.661196\niter  30 value 134.205658\niter  40 value 134.038036\niter  50 value 133.903787\niter  60 value 133.850532\niter  70 value 133.759368\niter  80 value 133.743357\niter  90 value 133.718864\niter 100 value 133.713921\nfinal  value 133.713921 \nstopped after 100 iterations\n# weights:  31\ninitial  value 181.861933 \niter  10 value 134.011042\niter  20 value 124.301660\niter  30 value 117.861414\niter  40 value 116.765006\niter  50 value 116.285166\niter  60 value 116.204280\niter  70 value 116.177031\niter  80 value 116.146976\niter  90 value 116.094104\niter 100 value 115.167654\nfinal  value 115.167654 \nstopped after 100 iterations\n# weights:  51\ninitial  value 167.131195 \niter  10 value 130.522665\niter  20 value 103.907639\niter  30 value 92.480401\niter  40 value 86.193654\niter  50 value 83.610350\niter  60 value 83.388546\niter  70 value 82.926186\niter  80 value 82.618307\niter  90 value 82.415946\niter 100 value 82.365126\nfinal  value 82.365126 \nstopped after 100 iterations\n# weights:  11\ninitial  value 192.960373 \niter  10 value 138.930574\niter  20 value 137.145455\niter  30 value 135.996406\niter  40 value 134.921534\niter  50 value 134.744369\niter  60 value 134.726931\niter  70 value 134.668643\niter  80 value 134.631432\niter  90 value 134.618217\niter 100 value 134.610438\nfinal  value 134.610438 \nstopped after 100 iterations\n# weights:  31\ninitial  value 168.388198 \niter  10 value 136.619693\niter  20 value 126.731615\niter  30 value 118.109688\niter  40 value 110.295249\niter  50 value 109.958570\niter  60 value 109.955609\nfinal  value 109.955600 \nconverged\n# weights:  51\ninitial  value 163.188638 \niter  10 value 135.460481\niter  20 value 110.623367\niter  30 value 97.617879\niter  40 value 90.094343\niter  50 value 87.808715\niter  60 value 87.447817\niter  70 value 87.431836\nfinal  value 87.431811 \nconverged\n# weights:  11\ninitial  value 162.425778 \niter  10 value 140.152438\niter  20 value 140.081018\niter  20 value 140.081018\niter  20 value 140.081018\nfinal  value 140.081018 \nconverged\n# weights:  31\ninitial  value 165.768558 \niter  10 value 139.564719\niter  20 value 135.203046\niter  30 value 132.031708\niter  40 value 131.354449\niter  50 value 131.300072\nfinal  value 131.299990 \nconverged\n# weights:  51\ninitial  value 159.493701 \niter  10 value 139.191401\niter  20 value 134.302744\niter  30 value 130.757610\niter  40 value 127.696812\niter  50 value 127.098708\niter  60 value 127.010605\niter  70 value 127.005975\niter  80 value 127.005351\niter  80 value 127.005350\niter  80 value 127.005350\nfinal  value 127.005350 \nconverged\n# weights:  11\ninitial  value 159.709223 \niter  10 value 139.377435\niter  20 value 138.792912\niter  30 value 138.790753\niter  40 value 138.770492\niter  50 value 138.761045\nfinal  value 138.761040 \nconverged\n# weights:  31\ninitial  value 244.290896 \niter  10 value 138.377358\niter  20 value 131.610197\niter  30 value 120.378975\niter  40 value 113.457179\niter  50 value 112.583421\niter  60 value 112.336146\niter  70 value 112.103140\niter  80 value 111.874296\niter  90 value 111.339151\niter 100 value 111.047659\nfinal  value 111.047659 \nstopped after 100 iterations\n# weights:  51\ninitial  value 157.842271 \niter  10 value 131.424923\niter  20 value 107.582657\niter  30 value 95.062526\niter  40 value 88.139524\niter  50 value 87.097306\niter  60 value 86.767221\niter  70 value 86.607616\niter  80 value 85.459441\niter  90 value 84.207332\niter 100 value 83.900425\nfinal  value 83.900425 \nstopped after 100 iterations\n# weights:  11\ninitial  value 158.886957 \niter  10 value 140.747054\niter  20 value 138.784156\niter  30 value 138.003545\niter  40 value 134.947623\nfinal  value 134.930883 \nconverged\n# weights:  31\ninitial  value 171.023965 \niter  10 value 132.207412\niter  20 value 124.607151\niter  30 value 117.828446\niter  40 value 117.398787\niter  50 value 117.390610\nfinal  value 117.390595 \nconverged\n# weights:  51\ninitial  value 165.954075 \niter  10 value 133.097489\niter  20 value 114.049486\niter  30 value 105.858696\niter  40 value 101.675715\niter  50 value 96.506613\niter  60 value 94.524772\niter  70 value 94.165235\niter  80 value 94.053774\niter  90 value 94.050563\niter 100 value 94.049204\nfinal  value 94.049204 \nstopped after 100 iterations\n# weights:  11\ninitial  value 159.651221 \niter  10 value 141.825865\niter  20 value 140.593177\nfinal  value 140.592534 \nconverged\n# weights:  31\ninitial  value 168.999914 \niter  10 value 141.129249\niter  20 value 134.493849\niter  30 value 132.538362\niter  40 value 131.000960\niter  50 value 130.386917\niter  60 value 130.348890\nfinal  value 130.348888 \nconverged\n# weights:  51\ninitial  value 170.710788 \niter  10 value 137.776025\niter  20 value 132.374971\niter  30 value 127.050974\niter  40 value 125.879472\niter  50 value 125.804386\niter  60 value 125.782448\nfinal  value 125.782313 \nconverged\n# weights:  11\ninitial  value 176.052363 \niter  10 value 138.995477\niter  20 value 138.420920\niter  30 value 138.293440\niter  40 value 135.486838\niter  50 value 135.297066\niter  60 value 135.286745\nfinal  value 135.286731 \nconverged\n# weights:  31\ninitial  value 164.096424 \niter  10 value 136.062938\niter  20 value 123.147557\niter  30 value 118.836426\niter  40 value 116.672736\niter  50 value 116.500910\niter  60 value 116.493838\niter  70 value 116.488380\niter  80 value 116.486064\niter  90 value 116.480212\niter 100 value 116.478667\nfinal  value 116.478667 \nstopped after 100 iterations\n# weights:  51\ninitial  value 179.392223 \niter  10 value 133.317270\niter  20 value 113.929925\niter  30 value 98.934877\niter  40 value 93.892153\niter  50 value 93.259076\niter  60 value 93.028304\niter  70 value 92.454578\niter  80 value 92.372844\niter  90 value 92.350128\niter 100 value 92.289403\nfinal  value 92.289403 \nstopped after 100 iterations\n# weights:  11\ninitial  value 185.258810 \niter  10 value 133.283563\niter  20 value 131.211703\niter  30 value 129.689138\niter  40 value 129.515441\niter  50 value 129.373386\niter  60 value 129.318147\niter  70 value 129.014230\niter  80 value 128.983550\niter  90 value 128.914114\niter 100 value 128.864089\nfinal  value 128.864089 \nstopped after 100 iterations\n# weights:  31\ninitial  value 157.546860 \niter  10 value 131.852098\niter  20 value 124.807883\niter  30 value 111.534495\niter  40 value 101.803934\niter  50 value 99.514141\niter  60 value 98.979709\niter  70 value 98.905709\niter  80 value 98.874512\niter  90 value 98.863353\niter 100 value 98.855647\nfinal  value 98.855647 \nstopped after 100 iterations\n# weights:  51\ninitial  value 169.662196 \niter  10 value 129.598821\niter  20 value 112.814737\niter  30 value 101.511669\niter  40 value 94.600833\niter  50 value 85.601699\niter  60 value 83.665374\niter  70 value 83.612945\niter  80 value 83.608179\nfinal  value 83.607928 \nconverged\n# weights:  11\ninitial  value 165.454179 \niter  10 value 134.568201\niter  20 value 134.107537\nfinal  value 134.107055 \nconverged\n# weights:  31\ninitial  value 215.968242 \niter  10 value 134.289951\niter  20 value 131.063527\niter  30 value 129.875716\niter  40 value 129.524260\niter  50 value 129.027629\niter  60 value 127.955828\niter  70 value 127.842109\niter  80 value 127.711335\niter  90 value 127.705794\nfinal  value 127.705790 \nconverged\n# weights:  51\ninitial  value 165.465306 \niter  10 value 131.703062\niter  20 value 128.749655\niter  30 value 125.115273\niter  40 value 122.294830\niter  50 value 120.879908\niter  60 value 120.767062\niter  70 value 120.763734\nfinal  value 120.763644 \nconverged\n# weights:  11\ninitial  value 204.498422 \niter  10 value 134.166043\niter  20 value 132.696060\niter  30 value 132.640134\niter  40 value 132.219410\niter  50 value 131.988483\niter  60 value 131.875898\niter  70 value 131.720163\niter  80 value 131.715775\niter  90 value 131.684932\niter 100 value 131.674434\nfinal  value 131.674434 \nstopped after 100 iterations\n# weights:  31\ninitial  value 153.697756 \niter  10 value 130.362273\niter  20 value 117.032633\niter  30 value 107.888303\niter  40 value 103.177739\niter  50 value 102.816678\niter  60 value 102.798527\niter  70 value 102.780593\niter  80 value 102.701685\niter  90 value 102.401169\niter 100 value 101.887210\nfinal  value 101.887210 \nstopped after 100 iterations\n# weights:  51\ninitial  value 157.450712 \niter  10 value 130.539306\niter  20 value 114.809118\niter  30 value 108.153641\niter  40 value 101.419837\niter  50 value 100.579741\niter  60 value 100.269641\niter  70 value 100.070267\niter  80 value 99.975498\niter  90 value 99.811741\niter 100 value 99.239919\nfinal  value 99.239919 \nstopped after 100 iterations\n# weights:  11\ninitial  value 161.066526 \niter  10 value 136.470473\niter  20 value 133.757187\niter  30 value 133.194788\niter  40 value 133.031777\niter  50 value 133.027072\niter  60 value 133.023832\niter  70 value 133.020348\niter  80 value 133.019230\niter  90 value 133.014640\niter 100 value 133.011087\nfinal  value 133.011087 \nstopped after 100 iterations\n# weights:  31\ninitial  value 165.307845 \niter  10 value 134.843564\niter  20 value 130.583705\niter  30 value 123.290255\niter  40 value 119.500010\niter  50 value 118.444734\niter  60 value 118.419911\nfinal  value 118.419807 \nconverged\n# weights:  51\ninitial  value 193.143138 \niter  10 value 128.179869\niter  20 value 101.119992\niter  30 value 93.716822\niter  40 value 84.859905\niter  50 value 78.399057\niter  60 value 73.294573\niter  70 value 72.978280\niter  80 value 72.970780\nfinal  value 72.970752 \nconverged\n# weights:  11\ninitial  value 166.155609 \niter  10 value 138.922057\niter  20 value 138.667544\nfinal  value 138.667364 \nconverged\n# weights:  31\ninitial  value 167.884501 \niter  10 value 137.561965\niter  20 value 132.336256\niter  30 value 130.004184\niter  40 value 129.927683\niter  50 value 129.924877\nfinal  value 129.924873 \nconverged\n# weights:  51\ninitial  value 159.393294 \niter  10 value 133.906208\niter  20 value 128.433492\niter  30 value 125.605620\niter  40 value 123.865515\niter  50 value 122.295517\niter  60 value 122.040838\niter  70 value 121.997458\niter  80 value 121.637260\niter  90 value 121.470053\niter 100 value 121.418455\nfinal  value 121.418455 \nstopped after 100 iterations\n# weights:  11\ninitial  value 157.412168 \niter  10 value 137.253113\niter  20 value 136.915892\niter  30 value 136.914603\nfinal  value 136.914529 \nconverged\n# weights:  31\ninitial  value 169.690699 \niter  10 value 134.110178\niter  20 value 125.993208\niter  30 value 116.630057\niter  40 value 116.263499\niter  50 value 116.237284\niter  60 value 116.172863\niter  70 value 115.980614\niter  80 value 115.842686\niter  90 value 115.791500\niter 100 value 115.679051\nfinal  value 115.679051 \nstopped after 100 iterations\n# weights:  51\ninitial  value 179.720238 \niter  10 value 133.826000\niter  20 value 120.024196\niter  30 value 111.261796\niter  40 value 109.150467\niter  50 value 108.741650\niter  60 value 108.682688\niter  70 value 108.651245\niter  80 value 108.369641\niter  90 value 108.002617\niter 100 value 107.808805\nfinal  value 107.808805 \nstopped after 100 iterations\n# weights:  11\ninitial  value 158.031740 \niter  10 value 137.451264\niter  20 value 135.526927\niter  30 value 134.473945\niter  40 value 134.165042\niter  50 value 133.997902\niter  60 value 133.963669\niter  70 value 133.956969\niter  80 value 133.951293\niter  90 value 133.941256\niter 100 value 133.915174\nfinal  value 133.915174 \nstopped after 100 iterations\n# weights:  31\ninitial  value 230.367166 \niter  10 value 134.031586\niter  20 value 122.754717\niter  30 value 115.638333\niter  40 value 113.876328\niter  50 value 113.657571\niter  60 value 113.656668\nfinal  value 113.656665 \nconverged\n# weights:  51\ninitial  value 214.352008 \niter  10 value 129.803031\niter  20 value 110.115605\niter  30 value 101.964495\niter  40 value 91.688336\niter  50 value 84.763727\niter  60 value 84.539872\niter  70 value 84.539169\nfinal  value 84.539154 \nconverged\n# weights:  11\ninitial  value 194.569584 \niter  10 value 138.709173\niter  20 value 138.225912\nfinal  value 138.210797 \nconverged\n# weights:  31\ninitial  value 158.027512 \niter  10 value 137.410169\niter  20 value 134.359795\niter  30 value 133.080559\niter  40 value 132.955911\niter  50 value 132.201494\niter  60 value 132.086507\niter  70 value 132.084570\nfinal  value 132.084568 \nconverged\n# weights:  51\ninitial  value 167.998534 \niter  10 value 137.647499\niter  20 value 132.868126\niter  30 value 127.824018\niter  40 value 126.638547\niter  50 value 125.917193\niter  60 value 125.573917\niter  70 value 125.570306\nfinal  value 125.570256 \nconverged\n# weights:  11\ninitial  value 162.148945 \niter  10 value 137.016020\niter  20 value 136.732457\nfinal  value 136.724127 \nconverged\n# weights:  31\ninitial  value 184.299774 \niter  10 value 133.164163\niter  20 value 120.577551\niter  30 value 116.020345\niter  40 value 115.238015\niter  50 value 115.125957\niter  60 value 114.792754\niter  70 value 114.355880\niter  80 value 114.290537\niter  90 value 114.284517\niter 100 value 114.261756\nfinal  value 114.261756 \nstopped after 100 iterations\n# weights:  51\ninitial  value 193.839725 \niter  10 value 130.611202\niter  20 value 123.025621\niter  30 value 112.905778\niter  40 value 106.468083\niter  50 value 104.427426\niter  60 value 103.786928\niter  70 value 103.531138\niter  80 value 103.274126\niter  90 value 103.071072\niter 100 value 102.854595\nfinal  value 102.854595 \nstopped after 100 iterations\n# weights:  11\ninitial  value 192.385911 \niter  10 value 138.302345\niter  20 value 137.887375\niter  30 value 137.658180\niter  40 value 137.548979\niter  50 value 137.519346\niter  60 value 137.518645\nfinal  value 137.518588 \nconverged\n# weights:  31\ninitial  value 172.186688 \niter  10 value 137.432596\niter  20 value 131.388236\niter  30 value 122.589390\niter  40 value 117.202986\niter  50 value 117.133234\niter  60 value 117.129652\nfinal  value 117.129622 \nconverged\n# weights:  51\ninitial  value 157.466280 \niter  10 value 130.833156\niter  20 value 114.331861\niter  30 value 102.225126\niter  40 value 92.498676\niter  50 value 90.657914\niter  60 value 90.579621\niter  70 value 90.576582\nfinal  value 90.576567 \nconverged\n# weights:  11\ninitial  value 195.341170 \niter  10 value 139.392200\niter  20 value 138.994820\nfinal  value 138.994063 \nconverged\n# weights:  31\ninitial  value 165.027320 \niter  10 value 137.699420\niter  20 value 135.079906\niter  30 value 132.447683\niter  40 value 131.770743\niter  50 value 131.767812\niter  50 value 131.767812\niter  50 value 131.767812\nfinal  value 131.767812 \nconverged\n# weights:  51\ninitial  value 183.500839 \niter  10 value 135.754439\niter  20 value 127.955239\niter  30 value 126.773046\niter  40 value 126.609896\niter  50 value 126.578859\niter  60 value 126.528087\niter  70 value 126.525470\nfinal  value 126.525467 \nconverged\n# weights:  11\ninitial  value 160.164210 \niter  10 value 139.055363\niter  20 value 138.065782\niter  30 value 137.736037\niter  40 value 137.558172\niter  50 value 137.524293\niter  60 value 137.521901\nfinal  value 137.521432 \nconverged\n# weights:  31\ninitial  value 187.087121 \niter  10 value 136.747986\niter  20 value 130.310272\niter  30 value 127.255031\niter  40 value 120.241031\niter  50 value 119.043547\niter  60 value 118.691796\niter  70 value 118.634546\niter  80 value 118.590792\niter  90 value 118.428798\niter 100 value 118.399020\nfinal  value 118.399020 \nstopped after 100 iterations\n# weights:  51\ninitial  value 169.872463 \niter  10 value 127.393512\niter  20 value 114.098134\niter  30 value 102.710133\niter  40 value 98.798822\niter  50 value 98.219617\niter  60 value 97.962040\niter  70 value 97.728683\niter  80 value 97.450258\niter  90 value 97.270700\niter 100 value 97.184010\nfinal  value 97.184010 \nstopped after 100 iterations\n# weights:  11\ninitial  value 166.985413 \niter  10 value 139.107309\niter  20 value 137.769111\niter  30 value 135.203706\niter  40 value 135.103015\niter  50 value 133.051640\niter  60 value 132.876397\niter  70 value 132.867414\niter  80 value 132.866472\niter  90 value 132.865928\nfinal  value 132.865799 \nconverged\n# weights:  31\ninitial  value 168.552537 \niter  10 value 135.130659\niter  20 value 125.687145\niter  30 value 116.728520\niter  40 value 111.746496\niter  50 value 111.632175\niter  60 value 111.629306\nfinal  value 111.629302 \nconverged\n# weights:  51\ninitial  value 200.972068 \niter  10 value 132.016602\niter  20 value 112.889723\niter  30 value 104.480879\niter  40 value 94.650616\niter  50 value 88.853679\niter  60 value 88.033839\niter  70 value 87.965818\niter  80 value 87.964708\nfinal  value 87.964706 \nconverged\n# weights:  11\ninitial  value 193.741891 \niter  10 value 141.297141\niter  20 value 140.752942\niter  30 value 140.739887\niter  30 value 140.739886\niter  30 value 140.739886\nfinal  value 140.739886 \nconverged\n# weights:  31\ninitial  value 177.824415 \niter  10 value 140.962166\niter  20 value 138.288412\niter  30 value 134.161894\niter  40 value 132.909172\niter  50 value 131.173979\niter  60 value 130.633489\niter  70 value 130.598256\niter  80 value 130.594995\niter  90 value 130.586618\nfinal  value 130.586604 \nconverged\n# weights:  51\ninitial  value 199.933953 \niter  10 value 138.280564\niter  20 value 132.653649\niter  30 value 130.288621\niter  40 value 125.639637\niter  50 value 125.178239\niter  60 value 125.116492\niter  70 value 125.100756\niter  80 value 125.084387\nfinal  value 125.084384 \nconverged\n# weights:  11\ninitial  value 172.644308 \niter  10 value 139.327190\niter  20 value 137.820228\niter  30 value 137.185074\niter  40 value 136.912586\niter  50 value 136.369262\nfinal  value 136.369212 \nconverged\n# weights:  31\ninitial  value 171.800914 \niter  10 value 137.900419\niter  20 value 129.513765\niter  30 value 124.268187\niter  40 value 123.421270\niter  50 value 123.317372\niter  60 value 123.260872\niter  70 value 123.226965\niter  80 value 123.196956\niter  90 value 123.058609\niter 100 value 122.953587\nfinal  value 122.953587 \nstopped after 100 iterations\n# weights:  51\ninitial  value 164.124541 \niter  10 value 136.637861\niter  20 value 121.595952\niter  30 value 108.288435\niter  40 value 99.219966\niter  50 value 94.092358\niter  60 value 91.215321\niter  70 value 89.058843\niter  80 value 86.662898\niter  90 value 86.147353\niter 100 value 86.071616\nfinal  value 86.071616 \nstopped after 100 iterations\n# weights:  11\ninitial  value 163.390851 \niter  10 value 139.571850\niter  20 value 139.040922\niter  30 value 139.027112\nfinal  value 139.026431 \nconverged\n# weights:  31\ninitial  value 173.075321 \niter  10 value 136.369643\niter  20 value 130.718505\niter  30 value 124.650492\niter  40 value 123.675048\niter  50 value 123.632925\nfinal  value 123.632825 \nconverged\n# weights:  51\ninitial  value 182.535968 \niter  10 value 134.029641\niter  20 value 119.993783\niter  30 value 106.797251\niter  40 value 96.983994\niter  50 value 92.858233\niter  60 value 92.594900\niter  70 value 92.557038\niter  80 value 92.556553\nfinal  value 92.556551 \nconverged\n# weights:  11\ninitial  value 186.774968 \niter  10 value 141.724860\niter  20 value 140.435123\niter  30 value 140.395580\nfinal  value 140.395455 \nconverged\n# weights:  31\ninitial  value 179.934703 \niter  10 value 140.080841\niter  20 value 137.440497\niter  30 value 136.136702\niter  40 value 135.420412\niter  50 value 135.238030\nfinal  value 135.237563 \nconverged\n# weights:  51\ninitial  value 174.445735 \niter  10 value 140.174187\niter  20 value 135.740836\niter  30 value 131.382119\niter  40 value 129.812133\niter  50 value 129.307728\niter  60 value 129.267017\niter  70 value 129.264552\nfinal  value 129.264514 \nconverged\n# weights:  11\ninitial  value 159.616296 \niter  10 value 139.623133\niter  20 value 139.036754\nfinal  value 139.028404 \nconverged\n# weights:  31\ninitial  value 178.918303 \niter  10 value 137.844351\niter  20 value 127.119562\niter  30 value 122.513720\niter  40 value 118.950388\niter  50 value 118.509583\niter  60 value 118.442395\niter  70 value 118.405116\niter  80 value 118.383254\niter  90 value 118.358159\niter 100 value 118.276157\nfinal  value 118.276157 \nstopped after 100 iterations\n# weights:  51\ninitial  value 177.222003 \niter  10 value 134.443130\niter  20 value 121.256057\niter  30 value 112.548047\niter  40 value 102.909855\niter  50 value 95.009068\niter  60 value 93.564746\niter  70 value 91.411537\niter  80 value 90.491554\niter  90 value 90.072906\niter 100 value 89.894156\nfinal  value 89.894156 \nstopped after 100 iterations\n# weights:  11\ninitial  value 192.290896 \niter  10 value 139.667973\niter  20 value 137.895977\niter  30 value 137.862761\nfinal  value 137.862678 \nconverged\n# weights:  31\ninitial  value 160.723298 \niter  10 value 127.657535\niter  20 value 116.337159\niter  30 value 112.799218\niter  40 value 109.526983\niter  50 value 108.231725\niter  60 value 108.160944\niter  70 value 108.131158\niter  80 value 108.113056\niter  90 value 108.110564\niter 100 value 108.091503\nfinal  value 108.091503 \nstopped after 100 iterations\n# weights:  51\ninitial  value 163.325417 \niter  10 value 133.211130\niter  20 value 122.610045\niter  30 value 108.338887\niter  40 value 100.192216\niter  50 value 95.207553\niter  60 value 93.291101\niter  70 value 93.065137\niter  80 value 93.061179\nfinal  value 93.061174 \nconverged\n# weights:  11\ninitial  value 177.284999 \niter  10 value 139.500891\niter  20 value 138.909103\nfinal  value 138.903623 \nconverged\n# weights:  31\ninitial  value 168.158514 \niter  10 value 137.260594\niter  20 value 135.159137\niter  30 value 133.582067\niter  40 value 133.435124\niter  50 value 133.433156\nfinal  value 133.433154 \nconverged\n# weights:  51\ninitial  value 199.354172 \niter  10 value 137.387085\niter  20 value 133.746317\niter  30 value 127.688698\niter  40 value 124.774655\niter  50 value 120.972303\niter  60 value 120.144036\niter  70 value 119.987111\niter  80 value 119.977193\nfinal  value 119.977165 \nconverged\n# weights:  11\ninitial  value 161.323282 \niter  10 value 138.773432\niter  20 value 137.672594\niter  30 value 137.096381\niter  40 value 136.775918\niter  50 value 136.125603\niter  60 value 134.346182\niter  70 value 134.311336\niter  80 value 134.310593\nfinal  value 134.310517 \nconverged\n# weights:  31\ninitial  value 168.919100 \niter  10 value 135.300854\niter  20 value 128.051848\niter  30 value 118.564249\niter  40 value 117.422638\niter  50 value 117.405780\niter  60 value 117.369532\niter  70 value 117.323691\niter  80 value 117.301846\niter  90 value 117.289995\niter 100 value 117.253483\nfinal  value 117.253483 \nstopped after 100 iterations\n# weights:  51\ninitial  value 164.577881 \niter  10 value 134.774738\niter  20 value 111.460832\niter  30 value 95.564925\niter  40 value 93.257003\niter  50 value 92.489338\niter  60 value 92.437247\niter  70 value 92.309387\niter  80 value 92.240287\niter  90 value 92.122207\niter 100 value 92.006815\nfinal  value 92.006815 \nstopped after 100 iterations\n# weights:  11\ninitial  value 169.824202 \niter  10 value 141.746071\niter  20 value 138.467693\niter  30 value 138.304236\nfinal  value 138.302472 \nconverged\n# weights:  31\ninitial  value 163.574016 \niter  10 value 139.077907\niter  20 value 129.613850\niter  30 value 123.319693\niter  40 value 120.086325\niter  50 value 119.255451\niter  60 value 119.039298\niter  70 value 118.913198\niter  80 value 118.864788\niter  90 value 118.850236\niter 100 value 118.768583\nfinal  value 118.768583 \nstopped after 100 iterations\n# weights:  51\ninitial  value 223.961211 \niter  10 value 134.757272\niter  20 value 116.738157\niter  30 value 106.733267\niter  40 value 97.457509\niter  50 value 96.147611\niter  60 value 96.077496\niter  70 value 96.061017\nfinal  value 96.060890 \nconverged\n# weights:  11\ninitial  value 185.723274 \niter  10 value 141.253510\niter  20 value 139.942334\nfinal  value 139.933120 \nconverged\n# weights:  31\ninitial  value 171.915669 \niter  10 value 139.405029\niter  20 value 134.054367\niter  30 value 133.204872\niter  40 value 133.130672\nfinal  value 133.128761 \nconverged\n# weights:  51\ninitial  value 181.089174 \niter  10 value 138.321605\niter  20 value 130.978590\niter  30 value 129.168416\niter  40 value 127.916672\niter  50 value 127.185515\niter  60 value 126.215050\niter  70 value 125.825239\niter  80 value 125.783343\niter  90 value 125.768557\nfinal  value 125.767998 \nconverged\n# weights:  11\ninitial  value 159.702364 \niter  10 value 138.806617\niter  20 value 138.340943\niter  30 value 138.304558\nfinal  value 138.304172 \nconverged\n# weights:  31\ninitial  value 158.164570 \niter  10 value 134.863525\niter  20 value 121.192495\niter  30 value 110.147814\niter  40 value 109.150683\niter  50 value 109.067781\niter  60 value 109.039674\niter  70 value 109.037979\niter  80 value 109.037407\niter  90 value 109.037016\niter 100 value 109.036296\nfinal  value 109.036296 \nstopped after 100 iterations\n# weights:  51\ninitial  value 183.563732 \niter  10 value 133.250253\niter  20 value 115.596484\niter  30 value 104.942684\niter  40 value 98.561275\niter  50 value 94.516263\niter  60 value 93.889302\niter  70 value 93.746517\niter  80 value 93.694930\niter  90 value 93.579447\niter 100 value 93.467051\nfinal  value 93.467051 \nstopped after 100 iterations\n# weights:  11\ninitial  value 210.990993 \niter  10 value 141.109460\niter  20 value 139.621994\niter  30 value 139.426722\niter  40 value 136.771645\niter  50 value 136.252088\niter  60 value 135.785086\niter  70 value 135.367111\niter  80 value 134.407288\niter  90 value 133.901785\niter 100 value 133.895034\nfinal  value 133.895034 \nstopped after 100 iterations\n# weights:  31\ninitial  value 157.651897 \niter  10 value 136.458347\niter  20 value 123.777234\niter  30 value 114.820138\niter  40 value 113.118869\niter  50 value 112.996671\niter  60 value 112.995785\niter  70 value 112.754022\nfinal  value 112.753800 \nconverged\n# weights:  51\ninitial  value 230.200381 \niter  10 value 135.891189\niter  20 value 117.156106\niter  30 value 106.768876\niter  40 value 99.532140\niter  50 value 93.432001\niter  60 value 90.172620\niter  70 value 89.633791\niter  80 value 89.627348\nfinal  value 89.627330 \nconverged\n# weights:  11\ninitial  value 184.526505 \niter  10 value 141.204785\niter  20 value 140.164955\nfinal  value 140.157158 \nconverged\n# weights:  31\ninitial  value 166.799756 \niter  10 value 139.158473\niter  20 value 135.640213\niter  30 value 134.766965\niter  40 value 133.582029\niter  50 value 133.236587\niter  60 value 133.091750\niter  70 value 133.070559\niter  80 value 133.068584\nfinal  value 133.068550 \nconverged\n# weights:  51\ninitial  value 169.428181 \niter  10 value 139.738504\niter  20 value 132.438426\niter  30 value 127.451973\niter  40 value 124.865205\niter  50 value 122.080170\niter  60 value 121.296683\niter  70 value 121.031949\niter  80 value 120.999335\nfinal  value 120.999295 \nconverged\n# weights:  11\ninitial  value 164.571641 \niter  10 value 139.360855\niter  20 value 138.670350\niter  30 value 138.667470\nfinal  value 138.667033 \nconverged\n# weights:  31\ninitial  value 165.861375 \niter  10 value 138.844446\niter  20 value 131.780867\niter  30 value 125.848215\niter  40 value 122.540201\niter  50 value 121.980379\niter  60 value 121.786424\niter  70 value 121.602115\niter  80 value 121.560017\niter  90 value 121.552885\niter 100 value 121.520548\nfinal  value 121.520548 \nstopped after 100 iterations\n# weights:  51\ninitial  value 238.640662 \niter  10 value 130.980648\niter  20 value 113.472931\niter  30 value 100.853090\niter  40 value 91.094117\niter  50 value 87.320387\niter  60 value 87.024841\niter  70 value 86.930212\niter  80 value 86.816340\niter  90 value 86.664265\niter 100 value 86.572372\nfinal  value 86.572372 \nstopped after 100 iterations\n# weights:  11\ninitial  value 161.057672 \niter  10 value 138.830377\niter  20 value 138.170028\niter  30 value 136.155530\niter  40 value 135.243775\niter  50 value 135.074975\niter  60 value 134.200119\niter  60 value 134.200119\nfinal  value 134.200119 \nconverged\n# weights:  31\ninitial  value 162.411411 \niter  10 value 136.802346\niter  20 value 123.948532\niter  30 value 116.964535\niter  40 value 109.705617\niter  50 value 108.482442\niter  60 value 108.439423\niter  70 value 108.357160\niter  80 value 108.354690\nfinal  value 108.354685 \nconverged\n# weights:  51\ninitial  value 176.541849 \niter  10 value 136.056560\niter  20 value 120.137106\niter  30 value 108.114876\niter  40 value 98.576863\niter  50 value 97.971905\niter  60 value 97.563713\niter  70 value 97.523066\niter  80 value 97.520911\nfinal  value 97.520909 \nconverged\n# weights:  11\ninitial  value 162.029816 \niter  10 value 140.419571\nfinal  value 140.237497 \nconverged\n# weights:  31\ninitial  value 191.671741 \niter  10 value 139.625633\niter  20 value 137.304904\niter  30 value 135.211444\niter  40 value 134.547239\niter  50 value 134.532262\niter  60 value 134.532043\nfinal  value 134.532041 \nconverged\n# weights:  51\ninitial  value 172.786659 \niter  10 value 138.400859\niter  20 value 131.102035\niter  30 value 127.784060\niter  40 value 126.770777\niter  50 value 126.516858\niter  60 value 126.470576\nfinal  value 126.467714 \nconverged\n# weights:  11\ninitial  value 191.969613 \niter  10 value 138.782044\niter  20 value 138.224327\niter  30 value 137.038323\niter  40 value 135.308439\niter  50 value 135.214981\niter  60 value 135.208777\niter  70 value 135.208637\niter  70 value 135.208636\nfinal  value 135.208636 \nconverged\n# weights:  31\ninitial  value 243.948943 \niter  10 value 135.769423\niter  20 value 124.989991\niter  30 value 120.416680\niter  40 value 117.017163\niter  50 value 115.188238\niter  60 value 114.540144\niter  70 value 114.420292\niter  80 value 114.389780\niter  90 value 114.329952\niter 100 value 114.018324\nfinal  value 114.018324 \nstopped after 100 iterations\n# weights:  51\ninitial  value 171.944841 \niter  10 value 133.377313\niter  20 value 107.473021\niter  30 value 94.066943\niter  40 value 87.917467\niter  50 value 86.813106\niter  60 value 86.569195\niter  70 value 86.325505\niter  80 value 86.213916\niter  90 value 86.094817\niter 100 value 86.028035\nfinal  value 86.028035 \nstopped after 100 iterations\n# weights:  11\ninitial  value 188.315370 \niter  10 value 143.404626\niter  20 value 141.366131\niter  30 value 139.680898\niter  40 value 139.477025\niter  50 value 139.117295\niter  60 value 138.593612\niter  70 value 137.534325\niter  80 value 137.522494\niter  90 value 137.514763\niter  90 value 137.514763\nfinal  value 137.514763 \nconverged\n# weights:  31\ninitial  value 162.885925 \niter  10 value 143.349381\niter  20 value 135.564946\niter  30 value 128.448168\niter  40 value 121.786271\niter  50 value 115.581435\niter  60 value 113.190182\niter  70 value 111.366912\niter  80 value 111.257158\niter  90 value 111.217949\niter 100 value 110.923213\nfinal  value 110.923213 \nstopped after 100 iterations\n# weights:  51\ninitial  value 176.896822 \niter  10 value 136.548981\niter  20 value 119.848119\niter  30 value 106.868679\niter  40 value 99.815682\niter  50 value 93.925531\niter  60 value 88.589394\niter  70 value 86.511931\niter  80 value 86.417887\nfinal  value 86.417375 \nconverged\n# weights:  11\ninitial  value 167.894742 \niter  10 value 144.306502\niter  20 value 143.791539\nfinal  value 143.790575 \nconverged\n# weights:  31\ninitial  value 157.545477 \niter  10 value 143.681840\niter  20 value 139.193325\niter  30 value 137.162291\niter  40 value 136.927269\niter  50 value 136.925902\niter  50 value 136.925901\niter  50 value 136.925901\nfinal  value 136.925901 \nconverged\n# weights:  51\ninitial  value 161.749304 \niter  10 value 143.764260\niter  20 value 137.474766\niter  30 value 131.416877\niter  40 value 127.402761\niter  50 value 126.234765\niter  60 value 126.156524\niter  70 value 126.146580\nfinal  value 126.146259 \nconverged\n# weights:  11\ninitial  value 158.100678 \niter  10 value 144.358215\niter  20 value 143.585232\niter  30 value 142.695660\niter  40 value 140.158364\niter  50 value 139.241596\niter  60 value 139.192504\niter  70 value 139.182992\niter  80 value 139.006052\niter  90 value 138.584068\niter 100 value 138.577200\nfinal  value 138.577200 \nstopped after 100 iterations\n# weights:  31\ninitial  value 161.855349 \niter  10 value 138.050067\niter  20 value 124.504136\niter  30 value 120.512197\niter  40 value 119.624601\niter  50 value 119.460695\niter  60 value 119.326458\niter  70 value 119.231707\niter  80 value 119.081087\niter  90 value 118.670458\niter 100 value 118.420921\nfinal  value 118.420921 \nstopped after 100 iterations\n# weights:  51\ninitial  value 193.055188 \niter  10 value 136.806948\niter  20 value 112.941903\niter  30 value 101.689611\niter  40 value 96.312764\niter  50 value 93.224586\niter  60 value 92.561492\niter  70 value 92.214721\niter  80 value 92.036365\niter  90 value 91.933025\niter 100 value 91.641332\nfinal  value 91.641332 \nstopped after 100 iterations\n# weights:  11\ninitial  value 181.058950 \niter  10 value 156.847456\niter  20 value 156.021342\nfinal  value 156.014187 \nconverged\n\n# train the LVQ model\nmodelLvq <- train(coast ~ ., data=both, method=\"lvq\", trControl=control)\n\n# train the GBM model\nmodelGbm <- train(coast ~ ., data=both, method=\"gbm\", trControl=control)\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2795             nan     0.1000    0.0106\n     2        1.2633             nan     0.1000    0.0020\n     3        1.2463             nan     0.1000    0.0062\n     4        1.2329             nan     0.1000    0.0060\n     5        1.2173             nan     0.1000    0.0059\n     6        1.2081             nan     0.1000    0.0030\n     7        1.1972             nan     0.1000    0.0006\n     8        1.1905             nan     0.1000    0.0025\n     9        1.1847             nan     0.1000    0.0010\n    10        1.1769             nan     0.1000    0.0028\n    20        1.1240             nan     0.1000   -0.0013\n    40        1.0687             nan     0.1000   -0.0029\n    60        1.0297             nan     0.1000   -0.0035\n    80        0.9996             nan     0.1000   -0.0003\n   100        0.9746             nan     0.1000   -0.0010\n   120        0.9506             nan     0.1000   -0.0016\n   140        0.9320             nan     0.1000   -0.0020\n   150        0.9198             nan     0.1000   -0.0020\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2739             nan     0.1000    0.0131\n     2        1.2511             nan     0.1000    0.0051\n     3        1.2348             nan     0.1000    0.0018\n     4        1.2191             nan     0.1000    0.0033\n     5        1.2074             nan     0.1000   -0.0018\n     6        1.1852             nan     0.1000    0.0049\n     7        1.1709             nan     0.1000   -0.0016\n     8        1.1552             nan     0.1000    0.0022\n     9        1.1451             nan     0.1000    0.0027\n    10        1.1316             nan     0.1000    0.0020\n    20        1.0671             nan     0.1000   -0.0021\n    40        0.9843             nan     0.1000   -0.0061\n    60        0.9161             nan     0.1000   -0.0024\n    80        0.8360             nan     0.1000   -0.0030\n   100        0.7942             nan     0.1000   -0.0042\n   120        0.7504             nan     0.1000   -0.0013\n   140        0.7107             nan     0.1000   -0.0033\n   150        0.6962             nan     0.1000   -0.0034\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2748             nan     0.1000    0.0091\n     2        1.2447             nan     0.1000    0.0095\n     3        1.2170             nan     0.1000    0.0132\n     4        1.1923             nan     0.1000    0.0039\n     5        1.1762             nan     0.1000    0.0009\n     6        1.1618             nan     0.1000   -0.0063\n     7        1.1465             nan     0.1000    0.0011\n     8        1.1304             nan     0.1000   -0.0021\n     9        1.1189             nan     0.1000    0.0018\n    10        1.1015             nan     0.1000   -0.0021\n    20        1.0066             nan     0.1000   -0.0022\n    40        0.8816             nan     0.1000   -0.0086\n    60        0.7828             nan     0.1000   -0.0075\n    80        0.7165             nan     0.1000   -0.0035\n   100        0.6561             nan     0.1000   -0.0021\n   120        0.5956             nan     0.1000   -0.0019\n   140        0.5563             nan     0.1000   -0.0027\n   150        0.5321             nan     0.1000   -0.0016\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2840             nan     0.1000    0.0071\n     2        1.2697             nan     0.1000    0.0042\n     3        1.2526             nan     0.1000    0.0063\n     4        1.2423             nan     0.1000    0.0060\n     5        1.2281             nan     0.1000    0.0024\n     6        1.2173             nan     0.1000    0.0018\n     7        1.2104             nan     0.1000    0.0019\n     8        1.2023             nan     0.1000    0.0027\n     9        1.2003             nan     0.1000   -0.0018\n    10        1.1928             nan     0.1000    0.0004\n    20        1.1517             nan     0.1000    0.0018\n    40        1.1071             nan     0.1000   -0.0047\n    60        1.0747             nan     0.1000   -0.0012\n    80        1.0414             nan     0.1000   -0.0013\n   100        1.0127             nan     0.1000   -0.0036\n   120        0.9884             nan     0.1000   -0.0016\n   140        0.9666             nan     0.1000   -0.0019\n   150        0.9552             nan     0.1000   -0.0021\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2786             nan     0.1000    0.0117\n     2        1.2605             nan     0.1000    0.0043\n     3        1.2479             nan     0.1000   -0.0010\n     4        1.2287             nan     0.1000    0.0058\n     5        1.2155             nan     0.1000    0.0051\n     6        1.2054             nan     0.1000   -0.0014\n     7        1.1875             nan     0.1000    0.0029\n     8        1.1803             nan     0.1000   -0.0021\n     9        1.1717             nan     0.1000    0.0011\n    10        1.1649             nan     0.1000   -0.0028\n    20        1.0932             nan     0.1000   -0.0041\n    40        0.9915             nan     0.1000   -0.0063\n    60        0.9221             nan     0.1000   -0.0032\n    80        0.8659             nan     0.1000   -0.0030\n   100        0.8238             nan     0.1000   -0.0043\n   120        0.7819             nan     0.1000   -0.0025\n   140        0.7433             nan     0.1000   -0.0078\n   150        0.7191             nan     0.1000   -0.0032\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2757             nan     0.1000    0.0000\n     2        1.2447             nan     0.1000    0.0096\n     3        1.2216             nan     0.1000    0.0027\n     4        1.1958             nan     0.1000    0.0012\n     5        1.1748             nan     0.1000    0.0023\n     6        1.1602             nan     0.1000    0.0017\n     7        1.1513             nan     0.1000   -0.0018\n     8        1.1405             nan     0.1000   -0.0020\n     9        1.1244             nan     0.1000   -0.0040\n    10        1.1165             nan     0.1000   -0.0045\n    20        1.0374             nan     0.1000    0.0005\n    40        0.9224             nan     0.1000   -0.0057\n    60        0.8244             nan     0.1000   -0.0025\n    80        0.7424             nan     0.1000   -0.0046\n   100        0.6821             nan     0.1000   -0.0046\n   120        0.6353             nan     0.1000   -0.0029\n   140        0.5884             nan     0.1000   -0.0040\n   150        0.5632             nan     0.1000   -0.0042\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2844             nan     0.1000    0.0105\n     2        1.2645             nan     0.1000    0.0083\n     3        1.2414             nan     0.1000    0.0095\n     4        1.2259             nan     0.1000    0.0084\n     5        1.2225             nan     0.1000   -0.0008\n     6        1.2124             nan     0.1000    0.0009\n     7        1.1972             nan     0.1000    0.0045\n     8        1.1842             nan     0.1000    0.0052\n     9        1.1736             nan     0.1000    0.0034\n    10        1.1682             nan     0.1000    0.0003\n    20        1.1202             nan     0.1000   -0.0027\n    40        1.0646             nan     0.1000   -0.0011\n    60        1.0208             nan     0.1000   -0.0046\n    80        0.9966             nan     0.1000   -0.0036\n   100        0.9679             nan     0.1000   -0.0052\n   120        0.9392             nan     0.1000   -0.0032\n   140        0.9215             nan     0.1000   -0.0016\n   150        0.9102             nan     0.1000   -0.0034\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2802             nan     0.1000    0.0093\n     2        1.2467             nan     0.1000    0.0113\n     3        1.2203             nan     0.1000    0.0078\n     4        1.1995             nan     0.1000    0.0028\n     5        1.1833             nan     0.1000    0.0034\n     6        1.1697             nan     0.1000    0.0026\n     7        1.1599             nan     0.1000   -0.0061\n     8        1.1537             nan     0.1000   -0.0011\n     9        1.1468             nan     0.1000   -0.0021\n    10        1.1361             nan     0.1000   -0.0009\n    20        1.0618             nan     0.1000   -0.0017\n    40        0.9737             nan     0.1000   -0.0026\n    60        0.9057             nan     0.1000   -0.0017\n    80        0.8439             nan     0.1000   -0.0028\n   100        0.7820             nan     0.1000   -0.0046\n   120        0.7411             nan     0.1000   -0.0036\n   140        0.7067             nan     0.1000   -0.0023\n   150        0.6913             nan     0.1000   -0.0035\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2714             nan     0.1000    0.0135\n     2        1.2442             nan     0.1000    0.0065\n     3        1.2227             nan     0.1000    0.0040\n     4        1.1998             nan     0.1000    0.0059\n     5        1.1769             nan     0.1000    0.0067\n     6        1.1545             nan     0.1000    0.0041\n     7        1.1393             nan     0.1000    0.0012\n     8        1.1253             nan     0.1000   -0.0040\n     9        1.1181             nan     0.1000   -0.0027\n    10        1.1090             nan     0.1000   -0.0034\n    20        1.0147             nan     0.1000   -0.0044\n    40        0.8857             nan     0.1000   -0.0047\n    60        0.7907             nan     0.1000   -0.0019\n    80        0.7140             nan     0.1000   -0.0027\n   100        0.6560             nan     0.1000   -0.0045\n   120        0.5938             nan     0.1000   -0.0022\n   140        0.5513             nan     0.1000   -0.0051\n   150        0.5312             nan     0.1000   -0.0015\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2788             nan     0.1000    0.0109\n     2        1.2475             nan     0.1000    0.0122\n     3        1.2267             nan     0.1000    0.0094\n     4        1.2081             nan     0.1000    0.0058\n     5        1.1940             nan     0.1000    0.0049\n     6        1.1841             nan     0.1000    0.0037\n     7        1.1715             nan     0.1000    0.0025\n     8        1.1621             nan     0.1000    0.0024\n     9        1.1526             nan     0.1000    0.0001\n    10        1.1435             nan     0.1000    0.0031\n    20        1.1006             nan     0.1000   -0.0026\n    40        1.0496             nan     0.1000   -0.0042\n    60        1.0155             nan     0.1000   -0.0028\n    80        0.9843             nan     0.1000   -0.0013\n   100        0.9544             nan     0.1000   -0.0037\n   120        0.9273             nan     0.1000   -0.0035\n   140        0.9017             nan     0.1000   -0.0025\n   150        0.8902             nan     0.1000   -0.0041\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2747             nan     0.1000    0.0134\n     2        1.2419             nan     0.1000    0.0092\n     3        1.2154             nan     0.1000    0.0094\n     4        1.1938             nan     0.1000    0.0050\n     5        1.1713             nan     0.1000    0.0054\n     6        1.1563             nan     0.1000    0.0020\n     7        1.1445             nan     0.1000    0.0038\n     8        1.1319             nan     0.1000    0.0013\n     9        1.1176             nan     0.1000   -0.0041\n    10        1.1104             nan     0.1000   -0.0022\n    20        1.0437             nan     0.1000   -0.0037\n    40        0.9475             nan     0.1000   -0.0063\n    60        0.8563             nan     0.1000   -0.0026\n    80        0.8057             nan     0.1000   -0.0033\n   100        0.7549             nan     0.1000   -0.0038\n   120        0.7105             nan     0.1000   -0.0017\n   140        0.6702             nan     0.1000   -0.0043\n   150        0.6528             nan     0.1000   -0.0020\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2684             nan     0.1000    0.0111\n     2        1.2337             nan     0.1000    0.0084\n     3        1.2066             nan     0.1000    0.0045\n     4        1.1825             nan     0.1000    0.0057\n     5        1.1553             nan     0.1000    0.0061\n     6        1.1315             nan     0.1000    0.0057\n     7        1.1156             nan     0.1000    0.0061\n     8        1.0998             nan     0.1000    0.0014\n     9        1.0844             nan     0.1000    0.0014\n    10        1.0744             nan     0.1000   -0.0019\n    20        0.9674             nan     0.1000   -0.0031\n    40        0.8383             nan     0.1000   -0.0038\n    60        0.7533             nan     0.1000   -0.0048\n    80        0.6764             nan     0.1000   -0.0058\n   100        0.6270             nan     0.1000   -0.0032\n   120        0.5733             nan     0.1000   -0.0044\n   140        0.5216             nan     0.1000   -0.0031\n   150        0.4979             nan     0.1000   -0.0037\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2810             nan     0.1000    0.0104\n     2        1.2607             nan     0.1000    0.0062\n     3        1.2321             nan     0.1000    0.0081\n     4        1.2176             nan     0.1000    0.0061\n     5        1.2106             nan     0.1000   -0.0022\n     6        1.2031             nan     0.1000    0.0021\n     7        1.1956             nan     0.1000    0.0006\n     8        1.1869             nan     0.1000    0.0034\n     9        1.1793             nan     0.1000    0.0006\n    10        1.1752             nan     0.1000   -0.0031\n    20        1.1249             nan     0.1000   -0.0025\n    40        1.0699             nan     0.1000   -0.0029\n    60        1.0319             nan     0.1000   -0.0033\n    80        0.9991             nan     0.1000   -0.0030\n   100        0.9779             nan     0.1000   -0.0006\n   120        0.9548             nan     0.1000   -0.0029\n   140        0.9269             nan     0.1000   -0.0023\n   150        0.9174             nan     0.1000   -0.0041\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2755             nan     0.1000    0.0097\n     2        1.2454             nan     0.1000    0.0096\n     3        1.2264             nan     0.1000    0.0078\n     4        1.2095             nan     0.1000    0.0046\n     5        1.1966             nan     0.1000    0.0013\n     6        1.1879             nan     0.1000    0.0002\n     7        1.1776             nan     0.1000   -0.0004\n     8        1.1629             nan     0.1000    0.0028\n     9        1.1512             nan     0.1000    0.0010\n    10        1.1338             nan     0.1000    0.0046\n    20        1.0572             nan     0.1000   -0.0011\n    40        0.9708             nan     0.1000   -0.0031\n    60        0.8919             nan     0.1000   -0.0034\n    80        0.8454             nan     0.1000   -0.0020\n   100        0.7855             nan     0.1000   -0.0053\n   120        0.7443             nan     0.1000   -0.0033\n   140        0.7024             nan     0.1000   -0.0041\n   150        0.6791             nan     0.1000   -0.0042\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2701             nan     0.1000    0.0100\n     2        1.2395             nan     0.1000    0.0041\n     3        1.2116             nan     0.1000    0.0054\n     4        1.1900             nan     0.1000    0.0003\n     5        1.1725             nan     0.1000    0.0009\n     6        1.1565             nan     0.1000    0.0029\n     7        1.1386             nan     0.1000    0.0025\n     8        1.1258             nan     0.1000    0.0008\n     9        1.1106             nan     0.1000   -0.0019\n    10        1.0995             nan     0.1000   -0.0009\n    20        1.0028             nan     0.1000   -0.0036\n    40        0.8822             nan     0.1000   -0.0064\n    60        0.7885             nan     0.1000   -0.0043\n    80        0.7075             nan     0.1000   -0.0042\n   100        0.6370             nan     0.1000   -0.0060\n   120        0.5855             nan     0.1000   -0.0035\n   140        0.5364             nan     0.1000   -0.0016\n   150        0.5176             nan     0.1000   -0.0030\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2871             nan     0.1000    0.0040\n     2        1.2591             nan     0.1000    0.0128\n     3        1.2380             nan     0.1000    0.0100\n     4        1.2212             nan     0.1000    0.0070\n     5        1.2032             nan     0.1000    0.0055\n     6        1.1841             nan     0.1000    0.0043\n     7        1.1747             nan     0.1000    0.0023\n     8        1.1655             nan     0.1000    0.0028\n     9        1.1541             nan     0.1000    0.0031\n    10        1.1459             nan     0.1000    0.0016\n    20        1.1111             nan     0.1000   -0.0035\n    40        1.0576             nan     0.1000   -0.0036\n    60        1.0140             nan     0.1000   -0.0004\n    80        0.9774             nan     0.1000   -0.0032\n   100        0.9491             nan     0.1000   -0.0021\n   120        0.9241             nan     0.1000   -0.0025\n   140        0.9014             nan     0.1000   -0.0010\n   150        0.8864             nan     0.1000   -0.0025\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2748             nan     0.1000    0.0110\n     2        1.2406             nan     0.1000    0.0104\n     3        1.2234             nan     0.1000    0.0037\n     4        1.2018             nan     0.1000    0.0095\n     5        1.1733             nan     0.1000    0.0062\n     6        1.1576             nan     0.1000    0.0024\n     7        1.1413             nan     0.1000    0.0046\n     8        1.1312             nan     0.1000   -0.0028\n     9        1.1205             nan     0.1000    0.0013\n    10        1.1084             nan     0.1000   -0.0007\n    20        1.0355             nan     0.1000   -0.0032\n    40        0.9402             nan     0.1000   -0.0013\n    60        0.8757             nan     0.1000   -0.0067\n    80        0.8247             nan     0.1000   -0.0027\n   100        0.7743             nan     0.1000   -0.0029\n   120        0.7323             nan     0.1000   -0.0038\n   140        0.6914             nan     0.1000   -0.0036\n   150        0.6727             nan     0.1000   -0.0031\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2878             nan     0.1000   -0.0042\n     2        1.2475             nan     0.1000    0.0134\n     3        1.2203             nan     0.1000    0.0093\n     4        1.1929             nan     0.1000    0.0065\n     5        1.1627             nan     0.1000    0.0083\n     6        1.1445             nan     0.1000    0.0035\n     7        1.1249             nan     0.1000    0.0048\n     8        1.1042             nan     0.1000   -0.0011\n     9        1.0906             nan     0.1000    0.0002\n    10        1.0785             nan     0.1000    0.0022\n    20        0.9859             nan     0.1000   -0.0038\n    40        0.8745             nan     0.1000   -0.0004\n    60        0.7791             nan     0.1000   -0.0028\n    80        0.7041             nan     0.1000   -0.0030\n   100        0.6415             nan     0.1000   -0.0032\n   120        0.5936             nan     0.1000   -0.0042\n   140        0.5446             nan     0.1000   -0.0035\n   150        0.5225             nan     0.1000   -0.0041\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2896             nan     0.1000    0.0042\n     2        1.2802             nan     0.1000    0.0028\n     3        1.2599             nan     0.1000    0.0056\n     4        1.2371             nan     0.1000    0.0078\n     5        1.2319             nan     0.1000   -0.0004\n     6        1.2264             nan     0.1000   -0.0013\n     7        1.2127             nan     0.1000    0.0065\n     8        1.1988             nan     0.1000    0.0066\n     9        1.1924             nan     0.1000   -0.0007\n    10        1.1840             nan     0.1000    0.0031\n    20        1.1338             nan     0.1000   -0.0016\n    40        1.0639             nan     0.1000   -0.0010\n    60        1.0139             nan     0.1000   -0.0040\n    80        0.9753             nan     0.1000   -0.0002\n   100        0.9494             nan     0.1000   -0.0038\n   120        0.9211             nan     0.1000   -0.0020\n   140        0.8976             nan     0.1000   -0.0038\n   150        0.8844             nan     0.1000   -0.0023\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2724             nan     0.1000    0.0127\n     2        1.2509             nan     0.1000    0.0077\n     3        1.2285             nan     0.1000    0.0090\n     4        1.2136             nan     0.1000    0.0052\n     5        1.1951             nan     0.1000    0.0066\n     6        1.1893             nan     0.1000   -0.0057\n     7        1.1743             nan     0.1000   -0.0059\n     8        1.1620             nan     0.1000    0.0013\n     9        1.1508             nan     0.1000    0.0023\n    10        1.1416             nan     0.1000   -0.0008\n    20        1.0625             nan     0.1000   -0.0027\n    40        0.9557             nan     0.1000   -0.0033\n    60        0.8795             nan     0.1000   -0.0049\n    80        0.8139             nan     0.1000   -0.0036\n   100        0.7654             nan     0.1000   -0.0037\n   120        0.7215             nan     0.1000   -0.0019\n   140        0.6700             nan     0.1000   -0.0030\n   150        0.6525             nan     0.1000   -0.0028\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2784             nan     0.1000    0.0087\n     2        1.2490             nan     0.1000    0.0029\n     3        1.2263             nan     0.1000    0.0068\n     4        1.2062             nan     0.1000    0.0040\n     5        1.1849             nan     0.1000   -0.0002\n     6        1.1657             nan     0.1000    0.0071\n     7        1.1478             nan     0.1000   -0.0026\n     8        1.1329             nan     0.1000   -0.0007\n     9        1.1212             nan     0.1000    0.0001\n    10        1.1019             nan     0.1000    0.0017\n    20        0.9914             nan     0.1000    0.0001\n    40        0.8663             nan     0.1000   -0.0004\n    60        0.7735             nan     0.1000   -0.0022\n    80        0.6857             nan     0.1000   -0.0016\n   100        0.6252             nan     0.1000   -0.0021\n   120        0.5680             nan     0.1000   -0.0015\n   140        0.5229             nan     0.1000   -0.0031\n   150        0.4987             nan     0.1000   -0.0021\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2755             nan     0.1000    0.0074\n     2        1.2589             nan     0.1000    0.0064\n     3        1.2424             nan     0.1000    0.0050\n     4        1.2328             nan     0.1000    0.0041\n     5        1.2206             nan     0.1000    0.0021\n     6        1.2127             nan     0.1000   -0.0028\n     7        1.2069             nan     0.1000   -0.0051\n     8        1.1958             nan     0.1000   -0.0006\n     9        1.1897             nan     0.1000    0.0004\n    10        1.1852             nan     0.1000   -0.0054\n    20        1.1438             nan     0.1000   -0.0021\n    40        1.1066             nan     0.1000   -0.0010\n    60        1.0683             nan     0.1000   -0.0012\n    80        1.0368             nan     0.1000   -0.0013\n   100        1.0113             nan     0.1000   -0.0033\n   120        0.9922             nan     0.1000   -0.0033\n   140        0.9720             nan     0.1000   -0.0034\n   150        0.9625             nan     0.1000   -0.0034\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2833             nan     0.1000   -0.0051\n     2        1.2589             nan     0.1000    0.0088\n     3        1.2352             nan     0.1000    0.0073\n     4        1.2171             nan     0.1000    0.0052\n     5        1.2034             nan     0.1000    0.0026\n     6        1.1896             nan     0.1000    0.0024\n     7        1.1776             nan     0.1000   -0.0001\n     8        1.1655             nan     0.1000   -0.0012\n     9        1.1563             nan     0.1000   -0.0009\n    10        1.1488             nan     0.1000   -0.0010\n    20        1.0800             nan     0.1000   -0.0063\n    40        0.9859             nan     0.1000   -0.0022\n    60        0.9311             nan     0.1000   -0.0039\n    80        0.8777             nan     0.1000   -0.0033\n   100        0.8255             nan     0.1000   -0.0020\n   120        0.7821             nan     0.1000   -0.0027\n   140        0.7398             nan     0.1000   -0.0022\n   150        0.7229             nan     0.1000   -0.0032\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2686             nan     0.1000    0.0073\n     2        1.2443             nan     0.1000    0.0035\n     3        1.2218             nan     0.1000    0.0030\n     4        1.2001             nan     0.1000    0.0042\n     5        1.1880             nan     0.1000   -0.0046\n     6        1.1712             nan     0.1000   -0.0009\n     7        1.1551             nan     0.1000   -0.0026\n     8        1.1430             nan     0.1000   -0.0037\n     9        1.1276             nan     0.1000    0.0013\n    10        1.1157             nan     0.1000   -0.0016\n    20        1.0173             nan     0.1000   -0.0032\n    40        0.8996             nan     0.1000   -0.0056\n    60        0.8067             nan     0.1000   -0.0041\n    80        0.7396             nan     0.1000   -0.0020\n   100        0.6663             nan     0.1000   -0.0055\n   120        0.6140             nan     0.1000   -0.0029\n   140        0.5684             nan     0.1000   -0.0053\n   150        0.5473             nan     0.1000   -0.0030\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2845             nan     0.1000    0.0127\n     2        1.2700             nan     0.1000    0.0081\n     3        1.2502             nan     0.1000    0.0076\n     4        1.2342             nan     0.1000    0.0044\n     5        1.2219             nan     0.1000    0.0058\n     6        1.2112             nan     0.1000    0.0032\n     7        1.2004             nan     0.1000   -0.0001\n     8        1.1992             nan     0.1000   -0.0059\n     9        1.1916             nan     0.1000    0.0014\n    10        1.1879             nan     0.1000   -0.0020\n    20        1.1399             nan     0.1000   -0.0012\n    40        1.0907             nan     0.1000   -0.0018\n    60        1.0471             nan     0.1000   -0.0018\n    80        1.0092             nan     0.1000   -0.0039\n   100        0.9832             nan     0.1000   -0.0072\n   120        0.9556             nan     0.1000   -0.0023\n   140        0.9284             nan     0.1000   -0.0023\n   150        0.9117             nan     0.1000   -0.0009\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2763             nan     0.1000    0.0113\n     2        1.2565             nan     0.1000    0.0087\n     3        1.2339             nan     0.1000    0.0036\n     4        1.2141             nan     0.1000    0.0008\n     5        1.2014             nan     0.1000    0.0015\n     6        1.1871             nan     0.1000   -0.0013\n     7        1.1808             nan     0.1000   -0.0039\n     8        1.1686             nan     0.1000    0.0008\n     9        1.1598             nan     0.1000   -0.0016\n    10        1.1505             nan     0.1000   -0.0010\n    20        1.0889             nan     0.1000   -0.0043\n    40        0.9911             nan     0.1000   -0.0022\n    60        0.9250             nan     0.1000   -0.0034\n    80        0.8652             nan     0.1000   -0.0025\n   100        0.8024             nan     0.1000   -0.0036\n   120        0.7558             nan     0.1000   -0.0016\n   140        0.7138             nan     0.1000   -0.0027\n   150        0.6936             nan     0.1000   -0.0027\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2760             nan     0.1000    0.0096\n     2        1.2527             nan     0.1000    0.0123\n     3        1.2429             nan     0.1000   -0.0090\n     4        1.2185             nan     0.1000    0.0116\n     5        1.1966             nan     0.1000    0.0010\n     6        1.1860             nan     0.1000   -0.0026\n     7        1.1643             nan     0.1000    0.0070\n     8        1.1528             nan     0.1000   -0.0010\n     9        1.1343             nan     0.1000    0.0051\n    10        1.1229             nan     0.1000    0.0003\n    20        1.0420             nan     0.1000   -0.0036\n    40        0.8942             nan     0.1000   -0.0024\n    60        0.8025             nan     0.1000   -0.0044\n    80        0.7349             nan     0.1000   -0.0054\n   100        0.6652             nan     0.1000   -0.0042\n   120        0.6034             nan     0.1000   -0.0027\n   140        0.5595             nan     0.1000   -0.0044\n   150        0.5380             nan     0.1000   -0.0032\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2712             nan     0.1000    0.0140\n     2        1.2535             nan     0.1000    0.0083\n     3        1.2372             nan     0.1000    0.0059\n     4        1.2103             nan     0.1000    0.0041\n     5        1.2011             nan     0.1000   -0.0012\n     6        1.1846             nan     0.1000    0.0011\n     7        1.1744             nan     0.1000    0.0020\n     8        1.1721             nan     0.1000   -0.0015\n     9        1.1628             nan     0.1000    0.0037\n    10        1.1567             nan     0.1000    0.0024\n    20        1.1009             nan     0.1000   -0.0012\n    40        1.0576             nan     0.1000   -0.0019\n    60        1.0153             nan     0.1000   -0.0039\n    80        0.9862             nan     0.1000   -0.0034\n   100        0.9634             nan     0.1000   -0.0008\n   120        0.9403             nan     0.1000   -0.0022\n   140        0.9097             nan     0.1000   -0.0024\n   150        0.8952             nan     0.1000   -0.0024\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2702             nan     0.1000    0.0164\n     2        1.2425             nan     0.1000    0.0141\n     3        1.2225             nan     0.1000    0.0067\n     4        1.2065             nan     0.1000    0.0043\n     5        1.1863             nan     0.1000    0.0064\n     6        1.1664             nan     0.1000    0.0083\n     7        1.1505             nan     0.1000   -0.0000\n     8        1.1339             nan     0.1000    0.0059\n     9        1.1224             nan     0.1000    0.0005\n    10        1.1126             nan     0.1000   -0.0023\n    20        1.0338             nan     0.1000   -0.0047\n    40        0.9557             nan     0.1000   -0.0031\n    60        0.8818             nan     0.1000   -0.0068\n    80        0.8231             nan     0.1000   -0.0016\n   100        0.7713             nan     0.1000   -0.0048\n   120        0.7200             nan     0.1000   -0.0011\n   140        0.6830             nan     0.1000   -0.0017\n   150        0.6564             nan     0.1000   -0.0028\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2702             nan     0.1000    0.0125\n     2        1.2390             nan     0.1000    0.0106\n     3        1.2069             nan     0.1000    0.0096\n     4        1.1904             nan     0.1000    0.0007\n     5        1.1661             nan     0.1000    0.0080\n     6        1.1475             nan     0.1000    0.0017\n     7        1.1316             nan     0.1000    0.0021\n     8        1.1141             nan     0.1000    0.0015\n     9        1.1052             nan     0.1000   -0.0032\n    10        1.1001             nan     0.1000   -0.0043\n    20        0.9980             nan     0.1000   -0.0005\n    40        0.8655             nan     0.1000   -0.0019\n    60        0.7835             nan     0.1000   -0.0067\n    80        0.7066             nan     0.1000   -0.0052\n   100        0.6428             nan     0.1000   -0.0037\n   120        0.5931             nan     0.1000   -0.0078\n   140        0.5449             nan     0.1000   -0.0054\n   150        0.5234             nan     0.1000   -0.0031\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2787             nan     0.1000    0.0113\n     2        1.2661             nan     0.1000    0.0004\n     3        1.2441             nan     0.1000    0.0061\n     4        1.2294             nan     0.1000    0.0046\n     5        1.2186             nan     0.1000    0.0025\n     6        1.2045             nan     0.1000    0.0044\n     7        1.1950             nan     0.1000    0.0044\n     8        1.1878             nan     0.1000    0.0007\n     9        1.1792             nan     0.1000    0.0032\n    10        1.1721             nan     0.1000    0.0001\n    20        1.1319             nan     0.1000   -0.0024\n    40        1.0815             nan     0.1000   -0.0005\n    60        1.0555             nan     0.1000   -0.0021\n    80        1.0223             nan     0.1000   -0.0058\n   100        0.9945             nan     0.1000   -0.0011\n   120        0.9762             nan     0.1000   -0.0022\n   140        0.9482             nan     0.1000   -0.0018\n   150        0.9359             nan     0.1000   -0.0009\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2735             nan     0.1000    0.0087\n     2        1.2553             nan     0.1000    0.0044\n     3        1.2351             nan     0.1000    0.0066\n     4        1.2130             nan     0.1000    0.0072\n     5        1.1999             nan     0.1000   -0.0006\n     6        1.1864             nan     0.1000   -0.0013\n     7        1.1702             nan     0.1000    0.0022\n     8        1.1587             nan     0.1000    0.0029\n     9        1.1527             nan     0.1000   -0.0016\n    10        1.1438             nan     0.1000    0.0002\n    20        1.0789             nan     0.1000   -0.0003\n    40        0.9903             nan     0.1000   -0.0017\n    60        0.9261             nan     0.1000   -0.0017\n    80        0.8705             nan     0.1000   -0.0081\n   100        0.8264             nan     0.1000   -0.0039\n   120        0.7754             nan     0.1000   -0.0055\n   140        0.7337             nan     0.1000   -0.0032\n   150        0.7104             nan     0.1000   -0.0010\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2802             nan     0.1000    0.0035\n     2        1.2473             nan     0.1000    0.0103\n     3        1.2196             nan     0.1000    0.0057\n     4        1.2091             nan     0.1000   -0.0039\n     5        1.1848             nan     0.1000    0.0044\n     6        1.1673             nan     0.1000    0.0013\n     7        1.1449             nan     0.1000    0.0032\n     8        1.1268             nan     0.1000    0.0034\n     9        1.1191             nan     0.1000   -0.0041\n    10        1.1138             nan     0.1000   -0.0054\n    20        1.0284             nan     0.1000   -0.0052\n    40        0.9058             nan     0.1000   -0.0042\n    60        0.8172             nan     0.1000   -0.0033\n    80        0.7404             nan     0.1000   -0.0033\n   100        0.6852             nan     0.1000   -0.0091\n   120        0.6297             nan     0.1000   -0.0038\n   140        0.5824             nan     0.1000   -0.0012\n   150        0.5619             nan     0.1000   -0.0056\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2772             nan     0.1000    0.0132\n     2        1.2516             nan     0.1000    0.0133\n     3        1.2321             nan     0.1000    0.0084\n     4        1.2109             nan     0.1000    0.0094\n     5        1.1970             nan     0.1000    0.0038\n     6        1.1881             nan     0.1000    0.0013\n     7        1.1771             nan     0.1000    0.0044\n     8        1.1628             nan     0.1000    0.0052\n     9        1.1539             nan     0.1000    0.0018\n    10        1.1448             nan     0.1000    0.0012\n    20        1.1055             nan     0.1000   -0.0026\n    40        1.0575             nan     0.1000   -0.0015\n    60        1.0159             nan     0.1000   -0.0028\n    80        0.9769             nan     0.1000   -0.0004\n   100        0.9463             nan     0.1000   -0.0016\n   120        0.9250             nan     0.1000   -0.0028\n   140        0.9066             nan     0.1000   -0.0034\n   150        0.8948             nan     0.1000   -0.0016\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2617             nan     0.1000    0.0159\n     2        1.2284             nan     0.1000    0.0112\n     3        1.2033             nan     0.1000    0.0045\n     4        1.1818             nan     0.1000    0.0095\n     5        1.1586             nan     0.1000    0.0056\n     6        1.1532             nan     0.1000   -0.0043\n     7        1.1387             nan     0.1000    0.0025\n     8        1.1335             nan     0.1000   -0.0043\n     9        1.1231             nan     0.1000   -0.0030\n    10        1.1111             nan     0.1000   -0.0003\n    20        1.0518             nan     0.1000   -0.0018\n    40        0.9561             nan     0.1000   -0.0031\n    60        0.8926             nan     0.1000   -0.0015\n    80        0.8379             nan     0.1000   -0.0048\n   100        0.7872             nan     0.1000    0.0008\n   120        0.7464             nan     0.1000   -0.0031\n   140        0.6987             nan     0.1000   -0.0032\n   150        0.6834             nan     0.1000    0.0004\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2613             nan     0.1000    0.0149\n     2        1.2414             nan     0.1000    0.0030\n     3        1.2167             nan     0.1000    0.0027\n     4        1.1935             nan     0.1000    0.0064\n     5        1.1747             nan     0.1000    0.0038\n     6        1.1577             nan     0.1000   -0.0024\n     7        1.1332             nan     0.1000    0.0058\n     8        1.1128             nan     0.1000    0.0028\n     9        1.0980             nan     0.1000    0.0022\n    10        1.0838             nan     0.1000   -0.0001\n    20        0.9811             nan     0.1000   -0.0051\n    40        0.8589             nan     0.1000   -0.0025\n    60        0.7694             nan     0.1000   -0.0055\n    80        0.6881             nan     0.1000   -0.0024\n   100        0.6246             nan     0.1000   -0.0026\n   120        0.5714             nan     0.1000   -0.0048\n   140        0.5255             nan     0.1000   -0.0045\n   150        0.5036             nan     0.1000   -0.0047\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2920             nan     0.1000    0.0021\n     2        1.2640             nan     0.1000    0.0095\n     3        1.2467             nan     0.1000    0.0025\n     4        1.2346             nan     0.1000    0.0031\n     5        1.2252             nan     0.1000    0.0031\n     6        1.2121             nan     0.1000    0.0072\n     7        1.2010             nan     0.1000    0.0039\n     8        1.1948             nan     0.1000    0.0017\n     9        1.1856             nan     0.1000    0.0018\n    10        1.1786             nan     0.1000    0.0024\n    20        1.1383             nan     0.1000   -0.0051\n    40        1.0789             nan     0.1000   -0.0015\n    60        1.0339             nan     0.1000   -0.0013\n    80        0.9994             nan     0.1000   -0.0008\n   100        0.9735             nan     0.1000   -0.0007\n   120        0.9535             nan     0.1000   -0.0027\n   140        0.9331             nan     0.1000   -0.0020\n   150        0.9233             nan     0.1000   -0.0019\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2769             nan     0.1000    0.0110\n     2        1.2539             nan     0.1000    0.0053\n     3        1.2313             nan     0.1000    0.0056\n     4        1.2136             nan     0.1000    0.0042\n     5        1.2012             nan     0.1000   -0.0026\n     6        1.1899             nan     0.1000    0.0036\n     7        1.1783             nan     0.1000   -0.0008\n     8        1.1658             nan     0.1000    0.0028\n     9        1.1598             nan     0.1000   -0.0041\n    10        1.1493             nan     0.1000   -0.0006\n    20        1.0699             nan     0.1000   -0.0017\n    40        0.9831             nan     0.1000   -0.0045\n    60        0.9162             nan     0.1000   -0.0016\n    80        0.8520             nan     0.1000   -0.0038\n   100        0.7991             nan     0.1000   -0.0024\n   120        0.7478             nan     0.1000   -0.0024\n   140        0.7130             nan     0.1000   -0.0027\n   150        0.6945             nan     0.1000   -0.0055\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2808             nan     0.1000    0.0011\n     2        1.2555             nan     0.1000    0.0077\n     3        1.2201             nan     0.1000    0.0138\n     4        1.1917             nan     0.1000    0.0037\n     5        1.1661             nan     0.1000    0.0016\n     6        1.1478             nan     0.1000    0.0059\n     7        1.1323             nan     0.1000   -0.0030\n     8        1.1173             nan     0.1000   -0.0013\n     9        1.1003             nan     0.1000   -0.0028\n    10        1.0844             nan     0.1000    0.0028\n    20        0.9837             nan     0.1000   -0.0048\n    40        0.8600             nan     0.1000   -0.0023\n    60        0.7679             nan     0.1000   -0.0031\n    80        0.6962             nan     0.1000   -0.0013\n   100        0.6312             nan     0.1000   -0.0034\n   120        0.5838             nan     0.1000   -0.0051\n   140        0.5332             nan     0.1000   -0.0028\n   150        0.5119             nan     0.1000   -0.0016\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2870             nan     0.1000    0.0105\n     2        1.2618             nan     0.1000    0.0118\n     3        1.2427             nan     0.1000    0.0075\n     4        1.2262             nan     0.1000    0.0037\n     5        1.2178             nan     0.1000    0.0009\n     6        1.2029             nan     0.1000    0.0023\n     7        1.1896             nan     0.1000    0.0034\n     8        1.1809             nan     0.1000    0.0015\n     9        1.1747             nan     0.1000    0.0011\n    10        1.1674             nan     0.1000    0.0029\n    20        1.1317             nan     0.1000   -0.0002\n    40        1.0866             nan     0.1000   -0.0031\n    60        1.0443             nan     0.1000   -0.0002\n    80        1.0090             nan     0.1000   -0.0015\n   100        0.9742             nan     0.1000   -0.0021\n   120        0.9524             nan     0.1000   -0.0027\n   140        0.9304             nan     0.1000   -0.0027\n   150        0.9192             nan     0.1000   -0.0040\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2815             nan     0.1000    0.0135\n     2        1.2562             nan     0.1000    0.0071\n     3        1.2326             nan     0.1000    0.0048\n     4        1.2092             nan     0.1000    0.0038\n     5        1.1999             nan     0.1000    0.0023\n     6        1.1838             nan     0.1000    0.0063\n     7        1.1712             nan     0.1000    0.0025\n     8        1.1608             nan     0.1000   -0.0008\n     9        1.1502             nan     0.1000    0.0010\n    10        1.1460             nan     0.1000   -0.0031\n    20        1.0658             nan     0.1000   -0.0041\n    40        0.9692             nan     0.1000   -0.0028\n    60        0.8978             nan     0.1000   -0.0047\n    80        0.8523             nan     0.1000   -0.0057\n   100        0.8082             nan     0.1000   -0.0023\n   120        0.7669             nan     0.1000   -0.0038\n   140        0.7298             nan     0.1000   -0.0050\n   150        0.7161             nan     0.1000   -0.0020\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2604             nan     0.1000    0.0129\n     2        1.2366             nan     0.1000    0.0032\n     3        1.2173             nan     0.1000    0.0030\n     4        1.1970             nan     0.1000    0.0040\n     5        1.1786             nan     0.1000    0.0030\n     6        1.1600             nan     0.1000   -0.0028\n     7        1.1474             nan     0.1000   -0.0008\n     8        1.1306             nan     0.1000    0.0009\n     9        1.1132             nan     0.1000    0.0033\n    10        1.1008             nan     0.1000    0.0000\n    20        1.0216             nan     0.1000   -0.0061\n    40        0.8960             nan     0.1000   -0.0053\n    60        0.7947             nan     0.1000   -0.0034\n    80        0.7205             nan     0.1000   -0.0055\n   100        0.6503             nan     0.1000   -0.0012\n   120        0.6027             nan     0.1000   -0.0027\n   140        0.5547             nan     0.1000   -0.0009\n   150        0.5422             nan     0.1000   -0.0028\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2854             nan     0.1000    0.0095\n     2        1.2618             nan     0.1000    0.0043\n     3        1.2476             nan     0.1000    0.0028\n     4        1.2310             nan     0.1000    0.0081\n     5        1.2249             nan     0.1000    0.0007\n     6        1.2161             nan     0.1000    0.0037\n     7        1.2091             nan     0.1000    0.0017\n     8        1.1973             nan     0.1000    0.0036\n     9        1.1884             nan     0.1000    0.0011\n    10        1.1849             nan     0.1000   -0.0013\n    20        1.1377             nan     0.1000    0.0007\n    40        1.0845             nan     0.1000   -0.0056\n    60        1.0474             nan     0.1000   -0.0046\n    80        1.0162             nan     0.1000   -0.0037\n   100        0.9857             nan     0.1000   -0.0022\n   120        0.9649             nan     0.1000   -0.0007\n   140        0.9395             nan     0.1000   -0.0023\n   150        0.9313             nan     0.1000   -0.0023\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2761             nan     0.1000    0.0051\n     2        1.2473             nan     0.1000    0.0080\n     3        1.2325             nan     0.1000    0.0030\n     4        1.2211             nan     0.1000    0.0018\n     5        1.2029             nan     0.1000    0.0051\n     6        1.1939             nan     0.1000    0.0009\n     7        1.1780             nan     0.1000   -0.0007\n     8        1.1634             nan     0.1000    0.0005\n     9        1.1585             nan     0.1000   -0.0032\n    10        1.1543             nan     0.1000   -0.0024\n    20        1.0729             nan     0.1000    0.0012\n    40        0.9788             nan     0.1000   -0.0002\n    60        0.9255             nan     0.1000   -0.0069\n    80        0.8605             nan     0.1000   -0.0056\n   100        0.8028             nan     0.1000   -0.0014\n   120        0.7631             nan     0.1000   -0.0024\n   140        0.7148             nan     0.1000   -0.0019\n   150        0.6992             nan     0.1000   -0.0031\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2718             nan     0.1000    0.0097\n     2        1.2427             nan     0.1000    0.0102\n     3        1.2203             nan     0.1000    0.0025\n     4        1.1984             nan     0.1000    0.0051\n     5        1.1743             nan     0.1000    0.0000\n     6        1.1584             nan     0.1000   -0.0004\n     7        1.1430             nan     0.1000   -0.0061\n     8        1.1298             nan     0.1000   -0.0005\n     9        1.1123             nan     0.1000   -0.0028\n    10        1.1051             nan     0.1000   -0.0057\n    20        1.0163             nan     0.1000   -0.0035\n    40        0.8893             nan     0.1000   -0.0048\n    60        0.8073             nan     0.1000   -0.0073\n    80        0.7154             nan     0.1000   -0.0050\n   100        0.6491             nan     0.1000   -0.0022\n   120        0.5913             nan     0.1000   -0.0021\n   140        0.5508             nan     0.1000   -0.0029\n   150        0.5350             nan     0.1000   -0.0016\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2782             nan     0.1000    0.0102\n     2        1.2605             nan     0.1000    0.0070\n     3        1.2422             nan     0.1000    0.0065\n     4        1.2272             nan     0.1000    0.0041\n     5        1.2128             nan     0.1000    0.0029\n     6        1.1992             nan     0.1000    0.0015\n     7        1.1937             nan     0.1000   -0.0015\n     8        1.1861             nan     0.1000    0.0017\n     9        1.1800             nan     0.1000    0.0006\n    10        1.1721             nan     0.1000    0.0012\n    20        1.1343             nan     0.1000   -0.0035\n    40        1.0779             nan     0.1000   -0.0021\n    60        1.0454             nan     0.1000   -0.0022\n    80        1.0146             nan     0.1000   -0.0032\n   100        0.9835             nan     0.1000   -0.0016\n   120        0.9619             nan     0.1000   -0.0032\n   140        0.9338             nan     0.1000   -0.0043\n   150        0.9212             nan     0.1000   -0.0065\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2779             nan     0.1000    0.0058\n     2        1.2626             nan     0.1000    0.0038\n     3        1.2340             nan     0.1000    0.0081\n     4        1.2090             nan     0.1000    0.0077\n     5        1.1878             nan     0.1000    0.0040\n     6        1.1776             nan     0.1000    0.0003\n     7        1.1685             nan     0.1000    0.0013\n     8        1.1561             nan     0.1000    0.0002\n     9        1.1504             nan     0.1000   -0.0041\n    10        1.1415             nan     0.1000    0.0008\n    20        1.0846             nan     0.1000   -0.0079\n    40        0.9791             nan     0.1000   -0.0008\n    60        0.9120             nan     0.1000   -0.0007\n    80        0.8621             nan     0.1000   -0.0021\n   100        0.8059             nan     0.1000   -0.0059\n   120        0.7629             nan     0.1000   -0.0042\n   140        0.7249             nan     0.1000   -0.0072\n   150        0.7039             nan     0.1000   -0.0038\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2700             nan     0.1000    0.0119\n     2        1.2474             nan     0.1000    0.0069\n     3        1.2310             nan     0.1000    0.0018\n     4        1.2037             nan     0.1000    0.0047\n     5        1.1844             nan     0.1000    0.0039\n     6        1.1688             nan     0.1000    0.0007\n     7        1.1490             nan     0.1000    0.0028\n     8        1.1356             nan     0.1000    0.0015\n     9        1.1244             nan     0.1000   -0.0039\n    10        1.1092             nan     0.1000    0.0006\n    20        1.0146             nan     0.1000   -0.0052\n    40        0.8902             nan     0.1000   -0.0056\n    60        0.7981             nan     0.1000   -0.0057\n    80        0.7159             nan     0.1000   -0.0020\n   100        0.6572             nan     0.1000   -0.0028\n   120        0.6033             nan     0.1000   -0.0043\n   140        0.5538             nan     0.1000   -0.0014\n   150        0.5380             nan     0.1000   -0.0029\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2792             nan     0.1000    0.0144\n     2        1.2496             nan     0.1000    0.0081\n     3        1.2398             nan     0.1000   -0.0010\n     4        1.2208             nan     0.1000    0.0041\n     5        1.2075             nan     0.1000    0.0036\n     6        1.1966             nan     0.1000    0.0027\n     7        1.1804             nan     0.1000    0.0040\n     8        1.1734             nan     0.1000    0.0005\n     9        1.1683             nan     0.1000    0.0001\n    10        1.1622             nan     0.1000   -0.0013\n    20        1.1223             nan     0.1000   -0.0019\n    40        1.0641             nan     0.1000   -0.0014\n    60        1.0312             nan     0.1000   -0.0014\n    80        0.9941             nan     0.1000   -0.0017\n   100        0.9688             nan     0.1000   -0.0022\n   120        0.9459             nan     0.1000   -0.0022\n   140        0.9341             nan     0.1000   -0.0038\n   150        0.9236             nan     0.1000   -0.0020\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2751             nan     0.1000    0.0070\n     2        1.2476             nan     0.1000    0.0102\n     3        1.2193             nan     0.1000    0.0076\n     4        1.1943             nan     0.1000    0.0081\n     5        1.1782             nan     0.1000    0.0054\n     6        1.1623             nan     0.1000    0.0015\n     7        1.1478             nan     0.1000    0.0018\n     8        1.1397             nan     0.1000   -0.0021\n     9        1.1298             nan     0.1000   -0.0042\n    10        1.1230             nan     0.1000   -0.0015\n    20        1.0620             nan     0.1000   -0.0035\n    40        0.9566             nan     0.1000   -0.0053\n    60        0.8833             nan     0.1000    0.0001\n    80        0.8308             nan     0.1000   -0.0009\n   100        0.7934             nan     0.1000   -0.0014\n   120        0.7444             nan     0.1000   -0.0040\n   140        0.7002             nan     0.1000   -0.0049\n   150        0.6826             nan     0.1000   -0.0016\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2715             nan     0.1000    0.0131\n     2        1.2389             nan     0.1000    0.0126\n     3        1.2111             nan     0.1000    0.0029\n     4        1.1973             nan     0.1000    0.0006\n     5        1.1806             nan     0.1000    0.0023\n     6        1.1667             nan     0.1000    0.0011\n     7        1.1501             nan     0.1000    0.0006\n     8        1.1464             nan     0.1000   -0.0065\n     9        1.1305             nan     0.1000    0.0025\n    10        1.1130             nan     0.1000    0.0019\n    20        1.0180             nan     0.1000   -0.0035\n    40        0.8859             nan     0.1000   -0.0014\n    60        0.7697             nan     0.1000   -0.0054\n    80        0.7000             nan     0.1000   -0.0028\n   100        0.6335             nan     0.1000   -0.0035\n   120        0.5823             nan     0.1000   -0.0031\n   140        0.5355             nan     0.1000   -0.0019\n   150        0.5128             nan     0.1000   -0.0055\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2773             nan     0.1000    0.0105\n     2        1.2578             nan     0.1000    0.0049\n     3        1.2461             nan     0.1000    0.0060\n     4        1.2348             nan     0.1000    0.0064\n     5        1.2332             nan     0.1000   -0.0032\n     6        1.2204             nan     0.1000    0.0047\n     7        1.2104             nan     0.1000    0.0025\n     8        1.1982             nan     0.1000    0.0014\n     9        1.1893             nan     0.1000    0.0028\n    10        1.1846             nan     0.1000    0.0010\n    20        1.1347             nan     0.1000    0.0003\n    40        1.0790             nan     0.1000   -0.0014\n    60        1.0371             nan     0.1000   -0.0023\n    80        1.0061             nan     0.1000   -0.0018\n   100        0.9748             nan     0.1000   -0.0002\n   120        0.9512             nan     0.1000   -0.0043\n   140        0.9330             nan     0.1000   -0.0010\n   150        0.9253             nan     0.1000   -0.0016\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2763             nan     0.1000    0.0091\n     2        1.2527             nan     0.1000    0.0106\n     3        1.2275             nan     0.1000    0.0044\n     4        1.2096             nan     0.1000    0.0045\n     5        1.1917             nan     0.1000    0.0030\n     6        1.1768             nan     0.1000    0.0020\n     7        1.1602             nan     0.1000    0.0039\n     8        1.1489             nan     0.1000    0.0006\n     9        1.1430             nan     0.1000   -0.0031\n    10        1.1333             nan     0.1000    0.0038\n    20        1.0562             nan     0.1000   -0.0025\n    40        0.9591             nan     0.1000   -0.0032\n    60        0.8889             nan     0.1000   -0.0016\n    80        0.8247             nan     0.1000   -0.0039\n   100        0.7784             nan     0.1000   -0.0036\n   120        0.7253             nan     0.1000   -0.0024\n   140        0.6812             nan     0.1000   -0.0030\n   150        0.6646             nan     0.1000   -0.0019\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2819             nan     0.1000    0.0084\n     2        1.2462             nan     0.1000    0.0142\n     3        1.2230             nan     0.1000    0.0033\n     4        1.2076             nan     0.1000   -0.0007\n     5        1.1919             nan     0.1000   -0.0009\n     6        1.1672             nan     0.1000   -0.0003\n     7        1.1500             nan     0.1000    0.0016\n     8        1.1332             nan     0.1000   -0.0000\n     9        1.1208             nan     0.1000    0.0018\n    10        1.1040             nan     0.1000   -0.0046\n    20        1.0095             nan     0.1000   -0.0073\n    40        0.8993             nan     0.1000   -0.0016\n    60        0.8087             nan     0.1000   -0.0016\n    80        0.7282             nan     0.1000   -0.0058\n   100        0.6677             nan     0.1000   -0.0030\n   120        0.6111             nan     0.1000   -0.0028\n   140        0.5565             nan     0.1000   -0.0019\n   150        0.5342             nan     0.1000   -0.0022\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2766             nan     0.1000    0.0102\n     2        1.2605             nan     0.1000    0.0080\n     3        1.2437             nan     0.1000    0.0076\n     4        1.2275             nan     0.1000    0.0046\n     5        1.2134             nan     0.1000    0.0037\n     6        1.2005             nan     0.1000   -0.0020\n     7        1.1937             nan     0.1000    0.0009\n     8        1.1882             nan     0.1000    0.0008\n     9        1.1817             nan     0.1000   -0.0015\n    10        1.1750             nan     0.1000   -0.0039\n    20        1.1277             nan     0.1000   -0.0030\n    40        1.0671             nan     0.1000   -0.0022\n    60        1.0288             nan     0.1000   -0.0014\n    80        0.9893             nan     0.1000   -0.0020\n   100        0.9583             nan     0.1000   -0.0011\n   120        0.9370             nan     0.1000   -0.0017\n   140        0.9164             nan     0.1000   -0.0031\n   150        0.8993             nan     0.1000   -0.0025\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2750             nan     0.1000    0.0088\n     2        1.2425             nan     0.1000    0.0069\n     3        1.2180             nan     0.1000    0.0025\n     4        1.2000             nan     0.1000    0.0017\n     5        1.1817             nan     0.1000    0.0009\n     6        1.1703             nan     0.1000    0.0007\n     7        1.1633             nan     0.1000   -0.0049\n     8        1.1567             nan     0.1000   -0.0009\n     9        1.1454             nan     0.1000   -0.0033\n    10        1.1313             nan     0.1000    0.0015\n    20        1.0557             nan     0.1000   -0.0041\n    40        0.9577             nan     0.1000   -0.0064\n    60        0.8947             nan     0.1000   -0.0022\n    80        0.8355             nan     0.1000   -0.0021\n   100        0.7849             nan     0.1000   -0.0055\n   120        0.7372             nan     0.1000   -0.0051\n   140        0.6992             nan     0.1000   -0.0050\n   150        0.6803             nan     0.1000   -0.0032\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2711             nan     0.1000    0.0125\n     2        1.2425             nan     0.1000    0.0019\n     3        1.2106             nan     0.1000    0.0071\n     4        1.2011             nan     0.1000   -0.0157\n     5        1.1797             nan     0.1000    0.0041\n     6        1.1629             nan     0.1000    0.0005\n     7        1.1440             nan     0.1000   -0.0000\n     8        1.1282             nan     0.1000   -0.0015\n     9        1.1130             nan     0.1000    0.0009\n    10        1.1015             nan     0.1000    0.0012\n    20        1.0028             nan     0.1000   -0.0026\n    40        0.8874             nan     0.1000   -0.0087\n    60        0.7833             nan     0.1000   -0.0065\n    80        0.7057             nan     0.1000   -0.0059\n   100        0.6373             nan     0.1000   -0.0030\n   120        0.5809             nan     0.1000   -0.0034\n   140        0.5311             nan     0.1000   -0.0034\n   150        0.5078             nan     0.1000   -0.0038\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2718             nan     0.1000    0.0141\n     2        1.2470             nan     0.1000    0.0124\n     3        1.2229             nan     0.1000    0.0078\n     4        1.2056             nan     0.1000    0.0080\n     5        1.1851             nan     0.1000    0.0064\n     6        1.1814             nan     0.1000   -0.0018\n     7        1.1702             nan     0.1000    0.0021\n     8        1.1624             nan     0.1000    0.0035\n     9        1.1536             nan     0.1000    0.0014\n    10        1.1469             nan     0.1000    0.0021\n    20        1.0928             nan     0.1000   -0.0010\n    40        1.0314             nan     0.1000   -0.0047\n    60        0.9929             nan     0.1000   -0.0044\n    80        0.9619             nan     0.1000   -0.0009\n   100        0.9329             nan     0.1000   -0.0026\n   120        0.9047             nan     0.1000   -0.0012\n   140        0.8809             nan     0.1000   -0.0001\n   150        0.8718             nan     0.1000   -0.0026\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2806             nan     0.1000    0.0117\n     2        1.2356             nan     0.1000    0.0052\n     3        1.2081             nan     0.1000    0.0075\n     4        1.1881             nan     0.1000    0.0023\n     5        1.1680             nan     0.1000    0.0048\n     6        1.1526             nan     0.1000    0.0027\n     7        1.1504             nan     0.1000   -0.0066\n     8        1.1339             nan     0.1000    0.0050\n     9        1.1199             nan     0.1000    0.0015\n    10        1.1080             nan     0.1000    0.0005\n    20        1.0372             nan     0.1000   -0.0004\n    40        0.9376             nan     0.1000   -0.0026\n    60        0.8630             nan     0.1000   -0.0037\n    80        0.8031             nan     0.1000   -0.0032\n   100        0.7599             nan     0.1000   -0.0005\n   120        0.7124             nan     0.1000   -0.0032\n   140        0.6662             nan     0.1000   -0.0005\n   150        0.6451             nan     0.1000   -0.0033\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2634             nan     0.1000    0.0196\n     2        1.2270             nan     0.1000    0.0133\n     3        1.1922             nan     0.1000    0.0099\n     4        1.1627             nan     0.1000    0.0093\n     5        1.1393             nan     0.1000    0.0069\n     6        1.1150             nan     0.1000    0.0039\n     7        1.1011             nan     0.1000    0.0016\n     8        1.0905             nan     0.1000    0.0002\n     9        1.0806             nan     0.1000    0.0001\n    10        1.0697             nan     0.1000   -0.0032\n    20        0.9760             nan     0.1000   -0.0019\n    40        0.8442             nan     0.1000   -0.0015\n    60        0.7463             nan     0.1000   -0.0040\n    80        0.6666             nan     0.1000   -0.0049\n   100        0.6006             nan     0.1000   -0.0027\n   120        0.5556             nan     0.1000   -0.0057\n   140        0.5067             nan     0.1000   -0.0025\n   150        0.4835             nan     0.1000   -0.0037\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2841             nan     0.1000    0.0068\n     2        1.2749             nan     0.1000   -0.0016\n     3        1.2624             nan     0.1000    0.0028\n     4        1.2427             nan     0.1000    0.0088\n     5        1.2315             nan     0.1000    0.0028\n     6        1.2130             nan     0.1000    0.0036\n     7        1.1985             nan     0.1000    0.0027\n     8        1.1949             nan     0.1000   -0.0012\n     9        1.1872             nan     0.1000    0.0013\n    10        1.1805             nan     0.1000    0.0009\n    20        1.1367             nan     0.1000   -0.0003\n    40        1.0823             nan     0.1000   -0.0018\n    60        1.0389             nan     0.1000   -0.0020\n    80        1.0066             nan     0.1000   -0.0046\n   100        0.9809             nan     0.1000   -0.0024\n   120        0.9566             nan     0.1000   -0.0024\n   140        0.9296             nan     0.1000   -0.0022\n   150        0.9175             nan     0.1000   -0.0029\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2745             nan     0.1000    0.0094\n     2        1.2514             nan     0.1000    0.0092\n     3        1.2302             nan     0.1000    0.0062\n     4        1.2166             nan     0.1000   -0.0000\n     5        1.2068             nan     0.1000   -0.0015\n     6        1.1904             nan     0.1000    0.0057\n     7        1.1805             nan     0.1000   -0.0000\n     8        1.1700             nan     0.1000   -0.0031\n     9        1.1625             nan     0.1000   -0.0026\n    10        1.1495             nan     0.1000    0.0019\n    20        1.0639             nan     0.1000   -0.0013\n    40        0.9645             nan     0.1000   -0.0021\n    60        0.8953             nan     0.1000   -0.0033\n    80        0.8298             nan     0.1000   -0.0030\n   100        0.7894             nan     0.1000   -0.0008\n   120        0.7381             nan     0.1000   -0.0014\n   140        0.6983             nan     0.1000   -0.0028\n   150        0.6810             nan     0.1000   -0.0030\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2633             nan     0.1000    0.0058\n     2        1.2417             nan     0.1000    0.0063\n     3        1.2170             nan     0.1000    0.0057\n     4        1.1961             nan     0.1000    0.0014\n     5        1.1706             nan     0.1000    0.0016\n     6        1.1507             nan     0.1000    0.0057\n     7        1.1410             nan     0.1000   -0.0025\n     8        1.1287             nan     0.1000   -0.0009\n     9        1.1159             nan     0.1000    0.0004\n    10        1.1089             nan     0.1000   -0.0031\n    20        1.0256             nan     0.1000    0.0007\n    40        0.9105             nan     0.1000   -0.0034\n    60        0.8166             nan     0.1000   -0.0054\n    80        0.7482             nan     0.1000   -0.0008\n   100        0.6824             nan     0.1000   -0.0014\n   120        0.6221             nan     0.1000   -0.0034\n   140        0.5661             nan     0.1000   -0.0029\n   150        0.5461             nan     0.1000   -0.0034\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2679             nan     0.1000    0.0091\n     2        1.2484             nan     0.1000    0.0074\n     3        1.2318             nan     0.1000    0.0060\n     4        1.2183             nan     0.1000    0.0041\n     5        1.2067             nan     0.1000    0.0026\n     6        1.1969             nan     0.1000   -0.0002\n     7        1.1878             nan     0.1000    0.0018\n     8        1.1783             nan     0.1000    0.0013\n     9        1.1713             nan     0.1000   -0.0005\n    10        1.1631             nan     0.1000    0.0022\n    20        1.1186             nan     0.1000   -0.0035\n    40        1.0649             nan     0.1000   -0.0013\n    60        1.0311             nan     0.1000   -0.0008\n    80        0.9903             nan     0.1000   -0.0029\n   100        0.9696             nan     0.1000   -0.0016\n   120        0.9389             nan     0.1000   -0.0012\n   140        0.9243             nan     0.1000   -0.0032\n   150        0.9168             nan     0.1000   -0.0029\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2689             nan     0.1000    0.0097\n     2        1.2488             nan     0.1000    0.0046\n     3        1.2217             nan     0.1000    0.0095\n     4        1.2010             nan     0.1000    0.0042\n     5        1.1909             nan     0.1000    0.0005\n     6        1.1739             nan     0.1000    0.0052\n     7        1.1574             nan     0.1000    0.0017\n     8        1.1489             nan     0.1000   -0.0042\n     9        1.1348             nan     0.1000    0.0005\n    10        1.1326             nan     0.1000   -0.0058\n    20        1.0478             nan     0.1000   -0.0037\n    40        0.9453             nan     0.1000   -0.0010\n    60        0.8810             nan     0.1000   -0.0065\n    80        0.8183             nan     0.1000   -0.0015\n   100        0.7779             nan     0.1000   -0.0027\n   120        0.7424             nan     0.1000   -0.0023\n   140        0.6971             nan     0.1000   -0.0060\n   150        0.6737             nan     0.1000   -0.0004\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2776             nan     0.1000    0.0059\n     2        1.2428             nan     0.1000    0.0111\n     3        1.2224             nan     0.1000    0.0054\n     4        1.1998             nan     0.1000    0.0077\n     5        1.1728             nan     0.1000    0.0070\n     6        1.1541             nan     0.1000   -0.0001\n     7        1.1406             nan     0.1000    0.0026\n     8        1.1263             nan     0.1000   -0.0014\n     9        1.1119             nan     0.1000   -0.0001\n    10        1.0992             nan     0.1000   -0.0014\n    20        0.9982             nan     0.1000   -0.0041\n    40        0.8639             nan     0.1000   -0.0057\n    60        0.7697             nan     0.1000   -0.0027\n    80        0.7019             nan     0.1000   -0.0027\n   100        0.6335             nan     0.1000   -0.0027\n   120        0.5925             nan     0.1000   -0.0050\n   140        0.5418             nan     0.1000   -0.0029\n   150        0.5169             nan     0.1000   -0.0036\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2801             nan     0.1000    0.0118\n     2        1.2616             nan     0.1000    0.0107\n     3        1.2407             nan     0.1000    0.0090\n     4        1.2216             nan     0.1000    0.0055\n     5        1.2134             nan     0.1000   -0.0015\n     6        1.2004             nan     0.1000    0.0040\n     7        1.1861             nan     0.1000    0.0025\n     8        1.1855             nan     0.1000   -0.0028\n     9        1.1744             nan     0.1000    0.0020\n    10        1.1705             nan     0.1000   -0.0021\n    20        1.1215             nan     0.1000    0.0006\n    40        1.0720             nan     0.1000   -0.0027\n    60        1.0363             nan     0.1000   -0.0006\n    80        1.0093             nan     0.1000   -0.0039\n   100        0.9807             nan     0.1000   -0.0023\n   120        0.9562             nan     0.1000   -0.0043\n   140        0.9411             nan     0.1000   -0.0019\n   150        0.9302             nan     0.1000   -0.0016\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2756             nan     0.1000    0.0111\n     2        1.2511             nan     0.1000    0.0081\n     3        1.2343             nan     0.1000    0.0074\n     4        1.2180             nan     0.1000    0.0067\n     5        1.2063             nan     0.1000   -0.0001\n     6        1.1882             nan     0.1000    0.0048\n     7        1.1770             nan     0.1000    0.0018\n     8        1.1621             nan     0.1000    0.0009\n     9        1.1484             nan     0.1000    0.0001\n    10        1.1400             nan     0.1000    0.0001\n    20        1.0754             nan     0.1000   -0.0003\n    40        0.9805             nan     0.1000   -0.0049\n    60        0.9047             nan     0.1000   -0.0013\n    80        0.8465             nan     0.1000   -0.0030\n   100        0.8005             nan     0.1000   -0.0040\n   120        0.7537             nan     0.1000   -0.0057\n   140        0.7075             nan     0.1000   -0.0034\n   150        0.6875             nan     0.1000   -0.0016\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2713             nan     0.1000    0.0062\n     2        1.2361             nan     0.1000    0.0091\n     3        1.2169             nan     0.1000    0.0017\n     4        1.1982             nan     0.1000   -0.0018\n     5        1.1834             nan     0.1000    0.0009\n     6        1.1685             nan     0.1000   -0.0001\n     7        1.1485             nan     0.1000    0.0028\n     8        1.1357             nan     0.1000    0.0022\n     9        1.1187             nan     0.1000    0.0028\n    10        1.1096             nan     0.1000   -0.0026\n    20        1.0189             nan     0.1000   -0.0020\n    40        0.8825             nan     0.1000   -0.0034\n    60        0.7820             nan     0.1000   -0.0029\n    80        0.7005             nan     0.1000   -0.0055\n   100        0.6419             nan     0.1000   -0.0035\n   120        0.5919             nan     0.1000   -0.0050\n   140        0.5473             nan     0.1000   -0.0052\n   150        0.5283             nan     0.1000   -0.0039\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2779             nan     0.1000    0.0133\n     2        1.2515             nan     0.1000    0.0099\n     3        1.2323             nan     0.1000    0.0086\n     4        1.2173             nan     0.1000    0.0041\n     5        1.2005             nan     0.1000    0.0047\n     6        1.1889             nan     0.1000    0.0040\n     7        1.1745             nan     0.1000    0.0008\n     8        1.1658             nan     0.1000    0.0034\n     9        1.1562             nan     0.1000    0.0029\n    10        1.1549             nan     0.1000   -0.0048\n    20        1.1079             nan     0.1000   -0.0002\n    40        1.0520             nan     0.1000   -0.0045\n    60        1.0129             nan     0.1000   -0.0041\n    80        0.9773             nan     0.1000   -0.0015\n   100        0.9507             nan     0.1000   -0.0050\n   120        0.9297             nan     0.1000   -0.0034\n   140        0.9100             nan     0.1000   -0.0045\n   150        0.9025             nan     0.1000   -0.0019\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2640             nan     0.1000    0.0105\n     2        1.2376             nan     0.1000    0.0082\n     3        1.2146             nan     0.1000    0.0106\n     4        1.1936             nan     0.1000    0.0055\n     5        1.1773             nan     0.1000    0.0041\n     6        1.1645             nan     0.1000    0.0017\n     7        1.1503             nan     0.1000    0.0022\n     8        1.1349             nan     0.1000    0.0004\n     9        1.1253             nan     0.1000    0.0008\n    10        1.1171             nan     0.1000   -0.0024\n    20        1.0346             nan     0.1000    0.0014\n    40        0.9494             nan     0.1000   -0.0035\n    60        0.8749             nan     0.1000   -0.0025\n    80        0.8057             nan     0.1000   -0.0001\n   100        0.7563             nan     0.1000   -0.0037\n   120        0.7148             nan     0.1000   -0.0025\n   140        0.6789             nan     0.1000   -0.0030\n   150        0.6643             nan     0.1000   -0.0045\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2650             nan     0.1000    0.0123\n     2        1.2279             nan     0.1000    0.0105\n     3        1.1959             nan     0.1000    0.0038\n     4        1.1778             nan     0.1000    0.0029\n     5        1.1537             nan     0.1000    0.0026\n     6        1.1353             nan     0.1000    0.0029\n     7        1.1173             nan     0.1000    0.0026\n     8        1.1057             nan     0.1000   -0.0048\n     9        1.0916             nan     0.1000   -0.0040\n    10        1.0747             nan     0.1000    0.0024\n    20        0.9812             nan     0.1000   -0.0044\n    40        0.8566             nan     0.1000   -0.0047\n    60        0.7731             nan     0.1000   -0.0054\n    80        0.6859             nan     0.1000   -0.0037\n   100        0.6262             nan     0.1000   -0.0056\n   120        0.5772             nan     0.1000   -0.0042\n   140        0.5367             nan     0.1000   -0.0029\n   150        0.5138             nan     0.1000   -0.0037\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2828             nan     0.1000    0.0085\n     2        1.2702             nan     0.1000    0.0073\n     3        1.2474             nan     0.1000    0.0090\n     4        1.2325             nan     0.1000    0.0053\n     5        1.2177             nan     0.1000    0.0023\n     6        1.2078             nan     0.1000    0.0023\n     7        1.2017             nan     0.1000   -0.0015\n     8        1.1925             nan     0.1000    0.0028\n     9        1.1843             nan     0.1000    0.0015\n    10        1.1783             nan     0.1000   -0.0003\n    20        1.1371             nan     0.1000   -0.0025\n    40        1.0687             nan     0.1000   -0.0016\n    60        1.0303             nan     0.1000   -0.0036\n    80        0.9979             nan     0.1000   -0.0018\n   100        0.9666             nan     0.1000   -0.0046\n   120        0.9442             nan     0.1000   -0.0028\n   140        0.9181             nan     0.1000   -0.0012\n   150        0.9068             nan     0.1000   -0.0022\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2758             nan     0.1000    0.0130\n     2        1.2477             nan     0.1000    0.0102\n     3        1.2301             nan     0.1000    0.0026\n     4        1.2116             nan     0.1000   -0.0012\n     5        1.1952             nan     0.1000    0.0015\n     6        1.1813             nan     0.1000   -0.0011\n     7        1.1742             nan     0.1000   -0.0007\n     8        1.1682             nan     0.1000   -0.0046\n     9        1.1540             nan     0.1000   -0.0015\n    10        1.1439             nan     0.1000   -0.0018\n    20        1.0645             nan     0.1000   -0.0016\n    40        0.9561             nan     0.1000   -0.0031\n    60        0.8894             nan     0.1000   -0.0049\n    80        0.8237             nan     0.1000   -0.0064\n   100        0.7767             nan     0.1000   -0.0074\n   120        0.7335             nan     0.1000    0.0014\n   140        0.6967             nan     0.1000   -0.0024\n   150        0.6768             nan     0.1000   -0.0038\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2676             nan     0.1000    0.0095\n     2        1.2363             nan     0.1000    0.0039\n     3        1.1993             nan     0.1000    0.0075\n     4        1.1699             nan     0.1000    0.0055\n     5        1.1551             nan     0.1000   -0.0020\n     6        1.1395             nan     0.1000   -0.0039\n     7        1.1258             nan     0.1000   -0.0011\n     8        1.1158             nan     0.1000   -0.0027\n     9        1.1082             nan     0.1000   -0.0092\n    10        1.0938             nan     0.1000   -0.0035\n    20        1.0114             nan     0.1000   -0.0030\n    40        0.8894             nan     0.1000   -0.0020\n    60        0.7920             nan     0.1000   -0.0069\n    80        0.7089             nan     0.1000   -0.0008\n   100        0.6451             nan     0.1000   -0.0052\n   120        0.5815             nan     0.1000   -0.0048\n   140        0.5320             nan     0.1000   -0.0034\n   150        0.5124             nan     0.1000   -0.0018\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2872             nan     0.1000    0.0096\n     2        1.2635             nan     0.1000    0.0108\n     3        1.2482             nan     0.1000    0.0079\n     4        1.2341             nan     0.1000    0.0066\n     5        1.2188             nan     0.1000    0.0049\n     6        1.2149             nan     0.1000   -0.0008\n     7        1.2072             nan     0.1000    0.0007\n     8        1.2061             nan     0.1000   -0.0056\n     9        1.1930             nan     0.1000    0.0046\n    10        1.1876             nan     0.1000   -0.0004\n    20        1.1250             nan     0.1000   -0.0003\n    40        1.0749             nan     0.1000   -0.0030\n    60        1.0467             nan     0.1000   -0.0036\n    80        1.0101             nan     0.1000   -0.0019\n   100        0.9890             nan     0.1000   -0.0013\n   120        0.9553             nan     0.1000   -0.0023\n   140        0.9384             nan     0.1000   -0.0031\n   150        0.9288             nan     0.1000   -0.0025\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2783             nan     0.1000    0.0113\n     2        1.2533             nan     0.1000    0.0034\n     3        1.2356             nan     0.1000    0.0035\n     4        1.2099             nan     0.1000    0.0094\n     5        1.1874             nan     0.1000    0.0058\n     6        1.1777             nan     0.1000    0.0012\n     7        1.1689             nan     0.1000   -0.0030\n     8        1.1608             nan     0.1000   -0.0002\n     9        1.1493             nan     0.1000    0.0034\n    10        1.1414             nan     0.1000   -0.0003\n    20        1.0644             nan     0.1000   -0.0037\n    40        0.9642             nan     0.1000   -0.0025\n    60        0.8922             nan     0.1000   -0.0034\n    80        0.8372             nan     0.1000   -0.0025\n   100        0.7913             nan     0.1000   -0.0042\n   120        0.7435             nan     0.1000   -0.0026\n   140        0.7063             nan     0.1000   -0.0009\n   150        0.6860             nan     0.1000   -0.0026\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2741             nan     0.1000    0.0082\n     2        1.2492             nan     0.1000    0.0065\n     3        1.2168             nan     0.1000    0.0085\n     4        1.1917             nan     0.1000    0.0036\n     5        1.1675             nan     0.1000    0.0070\n     6        1.1494             nan     0.1000    0.0030\n     7        1.1368             nan     0.1000    0.0002\n     8        1.1159             nan     0.1000    0.0034\n     9        1.1024             nan     0.1000   -0.0012\n    10        1.0881             nan     0.1000   -0.0071\n    20        1.0097             nan     0.1000   -0.0060\n    40        0.8813             nan     0.1000   -0.0022\n    60        0.8008             nan     0.1000   -0.0033\n    80        0.7135             nan     0.1000   -0.0057\n   100        0.6495             nan     0.1000   -0.0020\n   120        0.5935             nan     0.1000   -0.0063\n   140        0.5496             nan     0.1000   -0.0032\n   150        0.5262             nan     0.1000   -0.0015\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2845             nan     0.1000    0.0094\n     2        1.2620             nan     0.1000    0.0103\n     3        1.2438             nan     0.1000    0.0075\n     4        1.2302             nan     0.1000    0.0055\n     5        1.2266             nan     0.1000   -0.0040\n     6        1.2126             nan     0.1000    0.0001\n     7        1.1969             nan     0.1000    0.0033\n     8        1.1861             nan     0.1000    0.0024\n     9        1.1780             nan     0.1000    0.0017\n    10        1.1719             nan     0.1000   -0.0032\n    20        1.1283             nan     0.1000   -0.0001\n    40        1.0818             nan     0.1000   -0.0002\n    60        1.0386             nan     0.1000   -0.0028\n    80        1.0082             nan     0.1000   -0.0015\n   100        0.9843             nan     0.1000   -0.0027\n   120        0.9587             nan     0.1000   -0.0020\n   140        0.9353             nan     0.1000   -0.0032\n   150        0.9246             nan     0.1000   -0.0020\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2776             nan     0.1000    0.0070\n     2        1.2594             nan     0.1000    0.0052\n     3        1.2270             nan     0.1000    0.0084\n     4        1.2111             nan     0.1000   -0.0052\n     5        1.1943             nan     0.1000    0.0053\n     6        1.1728             nan     0.1000    0.0039\n     7        1.1565             nan     0.1000    0.0004\n     8        1.1470             nan     0.1000   -0.0023\n     9        1.1374             nan     0.1000    0.0006\n    10        1.1314             nan     0.1000   -0.0021\n    20        1.0629             nan     0.1000   -0.0061\n    40        0.9749             nan     0.1000   -0.0049\n    60        0.9044             nan     0.1000   -0.0039\n    80        0.8523             nan     0.1000   -0.0055\n   100        0.8038             nan     0.1000   -0.0027\n   120        0.7546             nan     0.1000   -0.0022\n   140        0.7185             nan     0.1000   -0.0041\n   150        0.6964             nan     0.1000   -0.0023\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2706             nan     0.1000    0.0114\n     2        1.2435             nan     0.1000    0.0049\n     3        1.2205             nan     0.1000    0.0060\n     4        1.1987             nan     0.1000    0.0044\n     5        1.1764             nan     0.1000    0.0047\n     6        1.1629             nan     0.1000   -0.0035\n     7        1.1469             nan     0.1000    0.0034\n     8        1.1336             nan     0.1000    0.0027\n     9        1.1159             nan     0.1000    0.0050\n    10        1.0990             nan     0.1000    0.0026\n    20        1.0299             nan     0.1000   -0.0024\n    40        0.8810             nan     0.1000   -0.0035\n    60        0.7952             nan     0.1000   -0.0016\n    80        0.7219             nan     0.1000   -0.0042\n   100        0.6539             nan     0.1000   -0.0028\n   120        0.5977             nan     0.1000   -0.0038\n   140        0.5529             nan     0.1000   -0.0022\n   150        0.5355             nan     0.1000   -0.0043\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2833             nan     0.1000    0.0111\n     2        1.2552             nan     0.1000    0.0105\n     3        1.2374             nan     0.1000    0.0081\n     4        1.2180             nan     0.1000    0.0090\n     5        1.2084             nan     0.1000    0.0040\n     6        1.1938             nan     0.1000    0.0036\n     7        1.1810             nan     0.1000    0.0047\n     8        1.1702             nan     0.1000    0.0014\n     9        1.1605             nan     0.1000   -0.0006\n    10        1.1551             nan     0.1000    0.0021\n    20        1.1016             nan     0.1000    0.0014\n    40        1.0464             nan     0.1000   -0.0003\n    60        1.0097             nan     0.1000   -0.0040\n    80        0.9752             nan     0.1000   -0.0009\n   100        0.9508             nan     0.1000   -0.0041\n   120        0.9290             nan     0.1000   -0.0028\n   140        0.9059             nan     0.1000   -0.0035\n   150        0.8947             nan     0.1000   -0.0050\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2854             nan     0.1000    0.0078\n     2        1.2531             nan     0.1000    0.0115\n     3        1.2240             nan     0.1000    0.0096\n     4        1.2072             nan     0.1000    0.0037\n     5        1.1907             nan     0.1000    0.0018\n     6        1.1699             nan     0.1000    0.0004\n     7        1.1567             nan     0.1000    0.0009\n     8        1.1447             nan     0.1000   -0.0012\n     9        1.1383             nan     0.1000   -0.0042\n    10        1.1294             nan     0.1000   -0.0003\n    20        1.0586             nan     0.1000    0.0012\n    40        0.9674             nan     0.1000   -0.0044\n    60        0.8902             nan     0.1000   -0.0010\n    80        0.8261             nan     0.1000   -0.0031\n   100        0.7741             nan     0.1000   -0.0048\n   120        0.7309             nan     0.1000   -0.0037\n   140        0.6982             nan     0.1000   -0.0035\n   150        0.6792             nan     0.1000   -0.0016\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2677             nan     0.1000    0.0152\n     2        1.2308             nan     0.1000    0.0027\n     3        1.2096             nan     0.1000    0.0065\n     4        1.1853             nan     0.1000    0.0039\n     5        1.1628             nan     0.1000    0.0045\n     6        1.1450             nan     0.1000   -0.0006\n     7        1.1336             nan     0.1000   -0.0040\n     8        1.1145             nan     0.1000    0.0028\n     9        1.1032             nan     0.1000   -0.0019\n    10        1.0910             nan     0.1000   -0.0016\n    20        1.0064             nan     0.1000   -0.0044\n    40        0.8770             nan     0.1000   -0.0061\n    60        0.7827             nan     0.1000   -0.0023\n    80        0.7140             nan     0.1000   -0.0029\n   100        0.6417             nan     0.1000   -0.0045\n   120        0.5902             nan     0.1000   -0.0041\n   140        0.5470             nan     0.1000   -0.0030\n   150        0.5213             nan     0.1000   -0.0034\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2855             nan     0.1000    0.0089\n     2        1.2607             nan     0.1000    0.0093\n     3        1.2390             nan     0.1000    0.0032\n     4        1.2284             nan     0.1000    0.0046\n     5        1.2148             nan     0.1000    0.0050\n     6        1.2049             nan     0.1000    0.0018\n     7        1.1941             nan     0.1000   -0.0006\n     8        1.1888             nan     0.1000    0.0019\n     9        1.1785             nan     0.1000    0.0028\n    10        1.1771             nan     0.1000   -0.0038\n    20        1.1351             nan     0.1000   -0.0004\n    40        1.0754             nan     0.1000   -0.0011\n    60        1.0361             nan     0.1000   -0.0020\n    80        1.0051             nan     0.1000   -0.0022\n   100        0.9797             nan     0.1000   -0.0025\n   120        0.9544             nan     0.1000   -0.0020\n   140        0.9305             nan     0.1000   -0.0042\n   150        0.9200             nan     0.1000   -0.0013\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2896             nan     0.1000    0.0065\n     2        1.2583             nan     0.1000    0.0067\n     3        1.2323             nan     0.1000    0.0079\n     4        1.2131             nan     0.1000    0.0077\n     5        1.2004             nan     0.1000    0.0007\n     6        1.1828             nan     0.1000    0.0063\n     7        1.1714             nan     0.1000    0.0016\n     8        1.1624             nan     0.1000   -0.0057\n     9        1.1580             nan     0.1000   -0.0031\n    10        1.1417             nan     0.1000    0.0005\n    20        1.0733             nan     0.1000    0.0002\n    40        0.9894             nan     0.1000   -0.0041\n    60        0.8975             nan     0.1000    0.0019\n    80        0.8316             nan     0.1000   -0.0055\n   100        0.7852             nan     0.1000   -0.0048\n   120        0.7474             nan     0.1000   -0.0020\n   140        0.7114             nan     0.1000   -0.0018\n   150        0.6918             nan     0.1000   -0.0031\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2702             nan     0.1000    0.0114\n     2        1.2460             nan     0.1000    0.0045\n     3        1.2240             nan     0.1000    0.0066\n     4        1.2086             nan     0.1000   -0.0039\n     5        1.1906             nan     0.1000   -0.0016\n     6        1.1724             nan     0.1000   -0.0033\n     7        1.1565             nan     0.1000   -0.0003\n     8        1.1422             nan     0.1000    0.0018\n     9        1.1279             nan     0.1000   -0.0007\n    10        1.1172             nan     0.1000   -0.0001\n    20        1.0246             nan     0.1000    0.0002\n    40        0.8982             nan     0.1000   -0.0031\n    60        0.8085             nan     0.1000   -0.0064\n    80        0.7331             nan     0.1000   -0.0036\n   100        0.6634             nan     0.1000   -0.0029\n   120        0.6090             nan     0.1000   -0.0029\n   140        0.5630             nan     0.1000   -0.0029\n   150        0.5454             nan     0.1000   -0.0025\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2824             nan     0.1000    0.0147\n     2        1.2677             nan     0.1000    0.0081\n     3        1.2448             nan     0.1000    0.0108\n     4        1.2174             nan     0.1000    0.0054\n     5        1.2049             nan     0.1000    0.0022\n     6        1.1926             nan     0.1000    0.0054\n     7        1.1832             nan     0.1000    0.0012\n     8        1.1749             nan     0.1000    0.0022\n     9        1.1686             nan     0.1000    0.0022\n    10        1.1598             nan     0.1000    0.0000\n    20        1.1176             nan     0.1000   -0.0016\n    40        1.0675             nan     0.1000   -0.0020\n    60        1.0189             nan     0.1000   -0.0014\n    80        0.9909             nan     0.1000   -0.0031\n   100        0.9611             nan     0.1000   -0.0028\n   120        0.9310             nan     0.1000   -0.0032\n   140        0.9123             nan     0.1000   -0.0038\n   150        0.9043             nan     0.1000   -0.0009\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2787             nan     0.1000    0.0083\n     2        1.2582             nan     0.1000    0.0067\n     3        1.2368             nan     0.1000    0.0088\n     4        1.2095             nan     0.1000    0.0014\n     5        1.1932             nan     0.1000    0.0031\n     6        1.1826             nan     0.1000   -0.0006\n     7        1.1612             nan     0.1000    0.0041\n     8        1.1466             nan     0.1000    0.0014\n     9        1.1372             nan     0.1000   -0.0023\n    10        1.1321             nan     0.1000   -0.0039\n    20        1.0549             nan     0.1000   -0.0023\n    40        0.9529             nan     0.1000   -0.0046\n    60        0.8851             nan     0.1000   -0.0033\n    80        0.8236             nan     0.1000   -0.0046\n   100        0.7806             nan     0.1000   -0.0036\n   120        0.7354             nan     0.1000   -0.0045\n   140        0.6955             nan     0.1000   -0.0032\n   150        0.6771             nan     0.1000   -0.0024\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2793             nan     0.1000    0.0101\n     2        1.2509             nan     0.1000    0.0080\n     3        1.2285             nan     0.1000    0.0039\n     4        1.2054             nan     0.1000    0.0089\n     5        1.1850             nan     0.1000    0.0036\n     6        1.1671             nan     0.1000    0.0046\n     7        1.1450             nan     0.1000   -0.0037\n     8        1.1288             nan     0.1000    0.0010\n     9        1.1141             nan     0.1000   -0.0031\n    10        1.1005             nan     0.1000   -0.0013\n    20        1.0135             nan     0.1000   -0.0071\n    40        0.8884             nan     0.1000   -0.0059\n    60        0.7986             nan     0.1000   -0.0025\n    80        0.7336             nan     0.1000   -0.0056\n   100        0.6581             nan     0.1000   -0.0028\n   120        0.6009             nan     0.1000   -0.0033\n   140        0.5420             nan     0.1000   -0.0019\n   150        0.5159             nan     0.1000   -0.0035\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2709             nan     0.1000    0.0135\n     2        1.2455             nan     0.1000    0.0066\n     3        1.2227             nan     0.1000    0.0031\n     4        1.2004             nan     0.1000    0.0076\n     5        1.1812             nan     0.1000    0.0035\n     6        1.1593             nan     0.1000    0.0039\n     7        1.1432             nan     0.1000   -0.0003\n     8        1.1265             nan     0.1000    0.0027\n     9        1.1172             nan     0.1000   -0.0006\n    10        1.1070             nan     0.1000   -0.0040\n    20        1.0206             nan     0.1000   -0.0036\n    40        0.8905             nan     0.1000   -0.0030\n    50        0.8376             nan     0.1000   -0.0023\n\n# train the SVM model\nmodelSvm <- train(coast ~., data=both, method=\"svmRadial\", trControl=control)\n\n# train the random forest\nrandomforest <- train(coast~., data=both, method=\"ranger\", trControl=control)\n\n\n# collect resamples\nresults <- resamples(list(LVQ=modelLvq, GBM=modelGbm, SVM=modelSvm,knn=modelknn, nnet=modelnnet, glm=modelglm, rf=randomforest))\n\n# summarize the distributions\nsummary(results)\n\n\nCall:\nsummary.resamples(object = results)\n\nModels: LVQ, GBM, SVM, knn, nnet, glm, rf \nNumber of resamples: 30 \n\nAccuracy \n          Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nLVQ  0.5185185 0.5925926 0.6602564 0.6629121 0.7142857 0.8571429    0\nGBM  0.5185185 0.6127646 0.6666667 0.6730701 0.7266484 0.8148148    0\nSVM  0.5000000 0.6296296 0.6666667 0.6589676 0.7008547 0.7500000    0\nknn  0.4444444 0.5962302 0.6296296 0.6361620 0.6666667 0.7777778    0\nnnet 0.5185185 0.6296296 0.6923077 0.6847544 0.7407407 0.8571429    0\nglm  0.5000000 0.6296296 0.6662088 0.6701601 0.7382479 0.8076923    0\nrf   0.4814815 0.5925926 0.6483516 0.6372473 0.6888736 0.7692308    0\n\nKappa \n            Min.    1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nLVQ  -0.15081967 0.09749683 0.1878083 0.2181786 0.3488372 0.6585366    0\nGBM  -0.15081967 0.12740385 0.2727190 0.2457643 0.3863908 0.5768025    0\nSVM  -0.15081967 0.06046176 0.2016393 0.1627443 0.2477190 0.3875000    0\nknn  -0.32786885 0.07348993 0.1796066 0.1666885 0.2323747 0.5000000    0\nnnet -0.05405405 0.16794479 0.2952255 0.2743689 0.3892479 0.6823529    0\nglm  -0.13422819 0.09395973 0.1706327 0.2139387 0.3782539 0.5390071    0\nrf   -0.21153846 0.06896552 0.1767987 0.1656162 0.2808609 0.4901961    0\n\nbwplot(results)\n\n\n\n\nNow we can test our model on a dataset from outside of our initial training/testing stage:\n\nkendrick <- get_artist_audio_features('kendrick lamar')\n\nkendrick <- kendrick %>% select(c(\"acousticness\", \"energy\", \"instrumentalness\", \"liveness\", \"danceability\", \"loudness\", \"speechiness\", \"valence\"))\n\nkendrick <- kendrick %>% mutate_all(~(scale(.) %>% as.vector))\n\nkendrick_rf <- predict(modelnnet, newdata=kendrick)\ntable(kendrick_rf)\n\nkendrick_rf\neast west \n 327  131 \n\nclassified_data <- as.data.frame(cbind(beatles_knn, beatles_rf, beatles$track_name))"
  }
]