[
  {
    "objectID": "class_notes/week_4.html",
    "href": "class_notes/week_4.html",
    "title": "Week 4: Conceptual Debates: Key-Finding",
    "section": "",
    "text": "talk about key-finding\nplay with the different weightings\nwhat would it look like to devise your own key-finding algorithm?\n\n\n\n\n\n\nWhen we hear this ringtone, it sounds as though it’s in C, but why?\n\n\n\nNokia\n\n\n\nIt doesn’t begin with C, it begins with G.\nC isn’t the most common note–in fact, it only occurs once before the final bar, and it’s on the “and” of 2 in the third measure (a pretty weak position metrically).\nIs a key just whatever key the piece ends in? If we ended this on A, would it sound like it’s in A minor? It would be the same key signature, and we’d actually have a nice cadential ascent to the final A from the G in the third measure.\n\nSo what gives? Why do we hear this as being in C?\nPerhaps a follow-up question might simply be: what makes us hear something as being in a key?\n\n\n\n\n\nThis approach used what we might call an exclusionary approach, eliminating different key possibilities as pitch classes were introduced over the course of a musical passage.\nFor example, with the Nokia theme, the opening G would fit into seven major keys (G, C, D, F, B-flats, A-flat, E-flat;); six of those keys would include the opening two notes; and three of those six would still be possible when presented with the first three notes. By the end of the first measure, however, the only major key that would encompass all four melody notes would be C major. If more than one key was still available however, the algorithm would place more weight on the pitches present at the start of the piece. This worked quite well on pieces that were overtly tonal, but it was less effective for pieces that contained non-diatonic pitches (which is most pieces!)\n\n\n\nLonguet-Higgins and Steedman's 1971 Key-Finding Algorithm\n\n\n\n\n\nAs you might guess, the Longuet-Higgins and Steedman would miss a lot of musical instances. For example, pieces that have non-harmonic chords would struggle, as would pieces that had a lot of chromatic ornamentations. Ideally an algorithm would allow for these pitches to occur, but acknowledge that pitches in the key might be a better fit than those outside of the key, and that certain pitches in the key should be more heavily weighted than others.\nCarol Krumhansl and Mark Schmuckler (Krumhansl, 1990) would devise an algorithm that tallied up the pitch classes of an excerpt and compared the distribution of these pitch classes to ratings from an earlier probe-tone experiment. (Krumhansl and Kessler, 1982). The weightings can be seen below.\nWe might think of this as a correlational approach. We tally up all of the pitches in a corpus, and then run a correlation on this key-profile. We run this over all of the keys, and the one that best fits is then labeled as “the key”.\n\nks_major_key <-\n  c(6.35, \n   2.23, \n   3.48, \n   2.33, \n   4.38, \n   4.09, \n   2.52, \n   5.19, \n   2.39, \n   3.66, \n   2.29, \n   2.88)\n\nks_minor_key <-\n  c(6.33, \n  2.68, \n  3.52, \n  5.38, \n  2.60, \n  3.53, \n  2.54, \n  4.75, \n  3.98, \n  2.69, \n  3.34, \n  3.17)\n\nAn interesting distinction here is that of experiment-derived vs. corpus-derived weightings. Should a key-finding algorithm intend to match how we hear key in a controlled lab environment (with basic harmonic progression stimuli), or should they use real music as a starting point? If they use real music, which music?\n\n\n\nHector Bellman created a key-finding algorithm that used Helen Budge’s dissertation from the 1940s as a starting point. Budge tallied up note occurrences in composers from the classical music canon, looking at the tonal makeup of a large collection of pieces. Bellman then used these frequencies as the starting point for his own key-finding algorithm.\n\nmajor <- c(16.80, \n            0.86,\n            12.95,\n            1.41,\n            13.49,\n            11.93,\n            1.25,\n            20.28,\n            1.80,\n            8.04,\n            0.62,\n            10.57)\n\nminor <- c(18.16,\n            0.69,\n            12.99,\n            13.34,\n            1.07,\n            11.15,\n            1.38,\n            21.07,\n            7.49,\n            1.53,\n            0.92,\n            10.21)\n\n\n\n\nDavid Temperley (2001) also employed Western classical music as a starting point for his early key-finding work (not to be confused with his more dynamic Bayesian-informed later work). He used examples from a commonly used music theory textbook (Stefan Kostka and Dorothy Payne’s Tonal Harmony).\n\nmajor <- c(0.748, \n            0.060, \n            0.488,\n            0.082, \n            0.670, \n            0.460, \n            0.096, \n            0.715, \n            0.104, \n            0.366,\n            0.057, \n            0.400)\n\nminor <- c(0.712, \n            0.084, \n            0.474, \n            0.618, \n            0.049, \n            0.460, \n            0.105, \n            0.747, \n            0.404, \n            0.067, \n            0.133, \n            0.330)\n\n\n\n\nBret Aarden (2003) argued that folk music would be a better fit than those generated from classical music. He used the Essen Folksong collection (consisting of thousands of folksongs throughout Europe, although with an uneven balance toward German folksong), to come up with the weightings below.\n\nmajor <- c(17.7661, \n            0.145624, \n            14.9265, \n            0.160186, \n            19.8049, \n            11.3587, \n            0.291248, \n            22.062, \n            0.145624, \n            8.15494, \n            0.232998, \n            4.95122)\n            \nminor <- c(18.2648, \n            0.737619, \n            14.0499, \n            16.8599, \n            0.702494, \n            14.4362, \n            0.702494, \n            18.6161, \n            4.56621, \n            1.93186, \n            7.37619, \n            1.75623)\n\n\n\n\nCraig Sapp argued that we probably didn’t even need to get frequencies from corpora or experiments. If we just assume that the tonic and the dominant (scale degrees 1 and 5) are the most important, and the other pitches in the key are less important, but more important than those not in the key, then we have a pretty simple weighting system (that works quite well!).\n\nmajor <- c(2, 0, 1, 0, 1, 1, 0, 2, 0, 1, 0, 1)\n\nminor <- c(2, 0, 1, 1, 0, 1, 0, 2, 1, 0, 0.5, 0.5)\n\n\n\n\nJosh Albrecht and I tried our hands at this problem, and picked a set of classical works from the Humdrum corpus, looking at only the first and last eight measures of each. The numbers are below.\n\nmajor <- c(0.238, \n            0.006, \n            0.111, \n            0.006, \n            0.137, \n            0.094, \n            0.016, \n            0.214, \n            0.009, \n            0.080, \n            0.008, \n            0.081) \n\nminor <- c(0.220, \n            0.006, \n            0.104, \n            0.123, \n            0.019, \n            0.103, \n            0.012, \n            0.214, \n            0.062, \n            0.022, \n            0.061, \n            0.052)\n\nWe also tried a Euclidean distance approach, rather than a correlational approach.\nWe tried to explain it as follows:\n\nIn a two-dimensional space, if there were 70% of pitch X and 30% pitch Y, the Cartesian location of the point representing this pitch-class distribution would be at X 1⁄4 0.7 and Y 1⁄4 0.3. In this case, we are examining the distribution of 12 pitch classes, resulting in a 12-dimensional Cartesian space. The pitch-class distribution of each piece is represented by a point in that 12-dimensional space. The distance is then measured between this point and the 24 points representing the 12 major and 12 minor key pitch-class distributions, and the key separated by the shortest distance is taken to be the key of the work.\n\nBelow is a table comparing how well this did to the others.\n\n\n\nComparing the Albrecht and Shanahan to others\n\n\n\n\n\nThey each perform a bit differently on different types of tasks.\n\n\n\nComparing key-finding algorithms in major, minor, and overall (from Albrecht and Shanahan, 2013)\n\n\n\n\ncorrplot 0.92 loaded\n\n\n\ncircshift <- function(v, n) {\n  if (n == 0) v else c(tail(v, n), head(v, -n))\n}\n# \n# # ### uses the Krumhansl Schmuckler Profiles\nmajor_key <- c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)\nminor_key <- c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)\n\n##sapp's simple weightings\n# major_key <- c(2, 0, 1, 0, 1, 1, 0, 2, 0, 1, 0, 1)\n# \n# minor_key <- c(2, 0, 1, 1, 0, 1, 0, 2, 1, 0, 0.5, 0.5)\n\nkey_templates <-\n  tribble(\n    ~name, ~template,\n    \"Gb:maj\", circshift(major_key, 6),\n    \"Bb:min\", circshift(minor_key, 10),\n    \"Db:maj\", circshift(major_key, 1),\n    \"F:min\", circshift(minor_key, 5),\n    \"Ab:maj\", circshift(major_key, 8),\n    \"C:min\", circshift(minor_key, 0),\n    \"Eb:maj\", circshift(major_key, 3),\n    \"G:min\", circshift(minor_key, 7),\n    \"Bb:maj\", circshift(major_key, 10),\n    \"D:min\", circshift(minor_key, 2),\n    \"F:maj\", circshift(major_key, 5),\n    \"A:min\", circshift(minor_key, 9),\n    \"C:maj\", circshift(major_key, 0),\n    \"E:min\", circshift(minor_key, 4),\n    \"G:maj\", circshift(major_key, 7),\n    \"B:min\", circshift(minor_key, 11),\n    \"D:maj\", circshift(major_key, 2),\n    \"F#:min\", circshift(minor_key, 6),\n    \"A:maj\", circshift(major_key, 9),\n    \"C#:min\", circshift(minor_key, 1),\n    \"E:maj\", circshift(major_key, 4),\n    \"G#:min\", circshift(minor_key, 8),\n    \"B:maj\", circshift(major_key, 11),\n    \"D#:min\", circshift(minor_key, 3)\n  )\n\nLet’s look at Lucy Dacus’s “Night Shift”.\nThis grabs the track and does all the magic:\n\nnight_shift <-\n  get_tidy_audio_analysis(\"1yYlpGuBiRRf33e1gY61bN\") %>%\n  compmus_align(sections, segments) %>%\n  select(sections) %>%\n  unnest(sections) %>%\n  mutate(\n    pitches =\n      map(segments,\n          compmus_summarise, pitches,\n          method = \"mean\", norm = \"manhattan\"\n      )\n  )\n\nAnd this is just a plotting function:\n\nnight_shift %>% \n  compmus_match_pitch_template(\n    key_templates,         # Change to chord_templates if descired\n    method = \"euclidean\",  # Try different distance metrics\n    norm = \"manhattan\"     # Try different norms\n  ) %>%\n  ggplot(\n    aes(x = start + duration / 2, width = duration, y = name, fill = d)\n  ) +\n  geom_tile() +\n  scale_fill_viridis_c(guide = \"none\") +\n  theme_minimal() +\n  labs(x = \"Time (s)\", y = \"\")\n\n\n\n\n\nnight_shift <-\n  get_tidy_audio_analysis(\"1yYlpGuBiRRf33e1gY61bN\") %>%\n  compmus_align(sections, segments) %>%\n  select(sections) %>%\n  unnest(sections) %>%\n  mutate(\n    pitches =\n      map(segments,\n          compmus_summarise, pitches,\n          method = \"mean\", norm = \"manhattan\"\n      )\n  )\n\nLet’s do some exercises:\n\nVisualize a song with all of these weightings.\n\nHow do the algorithms differ?\n\nCan you write a function that would call each weighting as an argument? What would that look like?"
  },
  {
    "objectID": "class_notes/week_4.html#whats-the-key-profile-for-indie-pop",
    "href": "class_notes/week_4.html#whats-the-key-profile-for-indie-pop",
    "title": "Week 4: Conceptual Debates: Key-Finding",
    "section": "What’s the Key-Profile for “Indie-Pop”",
    "text": "What’s the Key-Profile for “Indie-Pop”\nThe basic code for getting a key-profile from a playlist is below. The process is as follows:\n\nGet the audio features from a playlist, and add the audio analysis onto the datafame.\nWe then create a “segments” column by using a map function from the tidyverse. Map functions basically apply a function over each element in a list. Here, we are saying “apply the compmus_c_transpose function to the key and segments lists from the add_audio_analysis function.”\n\nWhat does the compmus_c_transpose function do? It takes all of the chroma vectors and transposes them to the key of C, so that we can construct a single set of weightings from pieces in different keys.\n\nWe then only grab this transposed segments column and turn it into a more readable list with the unnest function.\n\nWe then grab the start, duration, and pitches info.\n\nWe then create a “pitches” column, and normalize these raw pitch counts. There are a few ways to do this, and there are different options for this.\nWe then used the compmus_gather_chroma function to take all of those chroma vectors and turn them into a list.\nWe then use the group_by and summarise functions from tidyverse, and get the mean count of each pitch class in the distribution.\n\n\n### grabs the key-profile of the indie-pop playlist.\nindie_pop_key_profile <- get_playlist_audio_features(\"\", \"37i9dQZF1DWWEcRhUVtL8n\") |>\n  \n  add_audio_analysis() |>\n  ## transpose all the chroma vectors to C.\n  mutate(segments = map2(segments, key, compmus_c_transpose)) |>\n  ## grab the segments data and unnest it, then only grabbing the start, duration, and pitches info.\n  select(segments) |>\n  unnest(segments) |> \n  select(start, duration, pitches) |> \n  mutate(pitches = map(pitches, compmus_normalise, \"euclidean\")) |>\n  compmus_gather_chroma() |>\n  group_by(pitch_class) |>\n  summarise(mean_value = mean(value)) \n\nindie_pop_key_profile\n\n# A tibble: 12 × 2\n   pitch_class mean_value\n   <fct>            <dbl>\n 1 C                0.310\n 2 C#|Db            0.169\n 3 D                0.236\n 4 D#|Eb            0.190\n 5 E                0.227\n 6 F                0.225\n 7 F#|Gb            0.175\n 8 G                0.279\n 9 G#|Ab            0.179\n10 A                0.193\n11 A#|Bb            0.173\n12 B                0.215\n\n\nIdeally, we’d be able to turn this into a more reusable function. Below we’ve just turned made the playlist URI an argument:\n\nget_key_profile_broad <- function(uri){\n   get_playlist_audio_features(\"\", uri) |>\n   add_audio_analysis() |>\n   ## transpose all the chroma vectors to C. (have I mentioned how great Burgoyne's library is??)\n   mutate(segments = map2(segments, key, compmus_c_transpose)) |>\n   ## grab the segments data and unnest it, then only grabbing the start, duration, and pitches info.\n   select(segments) |>\n   unnest(segments) |> \n   select(start, duration, pitches) |> \n   mutate(pitches = map(pitches, compmus_normalise, \"euclidean\")) |>\n   compmus_gather_chroma() |>\n   group_by(pitch_class) |>\n   summarise(mean_value = mean(value)) \n}\n\nAnd now we can just run the function like so:\n\nindie_pop <- get_key_profile_broad(\"37i9dQZF1DWWEcRhUVtL8n\")\nindie_pop\n\n# A tibble: 12 × 2\n   pitch_class mean_value\n   <fct>            <dbl>\n 1 C                0.310\n 2 C#|Db            0.169\n 3 D                0.236\n 4 D#|Eb            0.190\n 5 E                0.227\n 6 F                0.225\n 7 F#|Gb            0.175\n 8 G                0.279\n 9 G#|Ab            0.179\n10 A                0.193\n11 A#|Bb            0.173\n12 B                0.215\n\n\nand we can plot it in a pretty straightforward way:\n\nbarplot(indie_pop$mean_value)\n\n\n\n\nSo we can look at other genres pretty easily. Here is me looking at Spotify’s “EDM 2023” playlist:"
  },
  {
    "objectID": "class_notes/week_4.html#whats-the-key-profile-for-edm",
    "href": "class_notes/week_4.html#whats-the-key-profile-for-edm",
    "title": "Week 4: Conceptual Debates: Key-Finding",
    "section": "What’s the Key Profile for EDM?",
    "text": "What’s the Key Profile for EDM?\n\nedm <- get_key_profile_broad(\"37i9dQZF1DX1kCIzMYtzum\")\nedm\n\n# A tibble: 12 × 2\n   pitch_class mean_value\n   <fct>            <dbl>\n 1 C                0.313\n 2 C#|Db            0.180\n 3 D                0.214\n 4 D#|Eb            0.217\n 5 E                0.214\n 6 F                0.233\n 7 F#|Gb            0.194\n 8 G                0.265\n 9 G#|Ab            0.203\n10 A                0.196\n11 A#|Bb            0.203\n12 B                0.230\n\n\nand once again we can plot it:\n\nbarplot(edm$mean_value)\n\n\n\n\n\nSome points of interest\n\nFor both of these distributions, we see a strong showing for scale degrees 1 and 5 (they aren’t really labeled in these quickie plots, but it would be the first and seventh column, respectively).\nWith the “Indie Pop” plot, we see a strong showing of scale degrees 1 and 5, and are followed by the diatonic pitches, but with the “EDM” list, scale degrees 2, flat 3, and 3 occur with pretty much the same frequency. It might be worth splitting the major and minor pieces up a bit?"
  },
  {
    "objectID": "class_notes/week_4.html#getting-separate-major-and-minor-key-profiles",
    "href": "class_notes/week_4.html#getting-separate-major-and-minor-key-profiles",
    "title": "Week 4: Conceptual Debates: Key-Finding",
    "section": "Getting separate major and minor key-profiles",
    "text": "Getting separate major and minor key-profiles\nWe could break this into a few parts for our own comfort. Let’s start by just creating a function that grabs the data. As that’s the one that’s quite time intensive, and calls to the API, let’s try to run it only once.\n\ngrab_playlist_info <- function(uri){\n   get_playlist_audio_features(\"\", uri) |>\n   add_audio_analysis() \n}\n\nOnce we have that in place, we can create a variable, and then subset it from there. Here, I’m saving the full list, and then creating a major and a minor variable.\n\nplaylist <- grab_playlist_info(\"37i9dQZF1DX1kCIzMYtzum\")  \nminor <- playlist |> filter(mode == 0)\nmajor <- playlist |> filter(mode == 1)\n\n\nget_pitch_list <- function(input){\n   input |>     \n   ## transpose all the chroma vectors to C. (have I mentioned how great Burgoyne's library is??)\n   mutate(segments = map2(segments, key, compmus_c_transpose)) |>\n   ## grab the segments data and unnest it, then only grabbing the start, duration, and pitches info.\n   select(segments) |>\n   unnest(segments) |> \n   select(start, duration, pitches) |> \n   mutate(pitches = map(pitches, compmus_normalise, \"euclidean\")) |>\n   compmus_gather_chroma() |>\n   group_by(pitch_class) |>\n   summarise(mean_value = mean(value))\n}\n\nAnd now we can get separate pitch lists for major and minor:\n\nminor_key <- get_pitch_list(minor)\nmajor_key <- get_pitch_list(major)\n\nand then of course we can use these to inform our own key mapping.\nWe can start by putting this all into a super quick and inefficient function like this (hoping to improve it as we go along):\n\nkey_plotter <- function(uri, major, minor){\n   major_key <- major\n   minor_key <- minor\n   key_templates <-\n   tribble(\n      ~name, ~template,\n      \"Gb:maj\", circshift(major_key, 6),\n      \"Bb:min\", circshift(minor_key, 10),\n      \"Db:maj\", circshift(major_key, 1),\n      \"F:min\", circshift(minor_key, 5),\n      \"Ab:maj\", circshift(major_key, 8),\n      \"C:min\", circshift(minor_key, 0),\n      \"Eb:maj\", circshift(major_key, 3),\n      \"G:min\", circshift(minor_key, 7),\n      \"Bb:maj\", circshift(major_key, 10),\n      \"D:min\", circshift(minor_key, 2),\n      \"F:maj\", circshift(major_key, 5),\n      \"A:min\", circshift(minor_key, 9),\n      \"C:maj\", circshift(major_key, 0),\n      \"E:min\", circshift(minor_key, 4),\n      \"G:maj\", circshift(major_key, 7),\n      \"B:min\", circshift(minor_key, 11),\n      \"D:maj\", circshift(major_key, 2),\n      \"F#:min\", circshift(minor_key, 6),\n      \"A:maj\", circshift(major_key, 9),\n      \"C#:min\", circshift(minor_key, 1),\n      \"E:maj\", circshift(major_key, 4),\n      \"G#:min\", circshift(minor_key, 8),\n      \"B:maj\", circshift(major_key, 11),\n      \"D#:min\", circshift(minor_key, 3)\n  )\n\ntune <-\n  get_tidy_audio_analysis(uri) %>%\n  compmus_align(sections, segments) %>%\n  select(sections) %>%\n  unnest(sections) %>%\n  mutate(\n    pitches =\n      map(segments,\n          compmus_summarise, pitches,\n          method = \"mean\", norm = \"manhattan\"\n      )\n  ) \n\ntune |> compmus_match_pitch_template(\n    key_templates,         # Change to chord_templates if descired\n    method = \"euclidean\",  # Try different distance metrics\n    norm = \"manhattan\"     # Try different norms\n  ) %>%\n  ggplot(\n    aes(x = start + duration / 2, width = duration, y = name, fill = d)\n  ) +\n  geom_tile() +\n  scale_fill_viridis_c(guide = \"none\") +\n  theme_minimal() +\n  labs(x = \"Time (s)\", y = \"\")\n\n}"
  },
  {
    "objectID": "class_notes/week_4.html#one-piece-and-many-key-profiles",
    "href": "class_notes/week_4.html#one-piece-and-many-key-profiles",
    "title": "Week 4: Conceptual Debates: Key-Finding",
    "section": "One Piece and Many Key Profiles",
    "text": "One Piece and Many Key Profiles\nLooking at Lucy Dacus’s “Night Shift” with EDM Key Profiles:\n\nedm_major_key <- c(0.2949827,0.1842662, 0.2249348, 0.1796559, 0.2532545, 0.2391564, 0.2028676, 0.2607747, 0.1765553, 0.2105823, 0.1806760, 0.2562869)\n# \nedm_minor_key <- c(0.3247214, 0.1767437, 0.2066454, 0.2482824, 0.1811887, 0.2263670, 0.1830838, 0.2662832, 0.2340293, 0.1888321, 0.2203257, 0.2047107)\nkey_plotter(\"1yYlpGuBiRRf33e1gY61bN\", edm_major_key, edm_minor_key)\n\n\n\n\nAnd here is the piece with the more traditional Krumhansl-Schmuckler key profiles:\n\nkey_plotter(\"1yYlpGuBiRRf33e1gY61bN\", ks_major_key, ks_minor_key)\n\n\n\n\nWe can load our “indie pop” but now in major and minor:\n\nplaylist <- grab_playlist_info(\"37i9dQZF1DWWEcRhUVtL8n\")  \nindie_minor <- playlist |> filter(mode == 0)\nindie_major <- playlist |> filter(mode == 1)\nindie_minor <- get_pitch_list(indie_minor)\nindie_major <- get_pitch_list(indie_major)\n\n\n\n\nAnd then we put these weightings into the plotter:\n\nkey_plotter(\"1yYlpGuBiRRf33e1gY61bN\", ks_major_key, ks_minor_key)"
  },
  {
    "objectID": "class_notes/week_4.html#exercise",
    "href": "class_notes/week_4.html#exercise",
    "title": "Week 4: Conceptual Debates: Key-Finding",
    "section": "Exercise:",
    "text": "Exercise:\n\nPick one piece and construct a genre-specific key-profile that might be used to explain its tonal make-up.\n\nExplain this musically."
  },
  {
    "objectID": "class_notes/week_5.html#looking-at-tempo-over-time",
    "href": "class_notes/week_5.html#looking-at-tempo-over-time",
    "title": "Week 5: Regression and Clustering",
    "section": "Looking at tempo over time",
    "text": "Looking at tempo over time\nWe can start by eyeballing the data. Here is how we’d do it with base R (no ggplot/tidyverse):\n\nplot(tempo ~ album_release_year, data=jayz)\nabline(lm(tempo ~ album_release_year, data=jayz), col=\"red\")\n\n\n\n\nIf we’d like to use ggplot it can give us some confidence bars (the default here is a 95% confidence interval):\n\nggplot(jayz, aes(album_release_year, tempo) ) + geom_point() +\n  stat_smooth(method = lm) +\n  theme_bw()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nSo it looks promising. We can run a linear regression with a simple lm command. Here we can get a summary of the model pretty easily, as well.\n\nsummary(lm(tempo ~ album_release_year, data=jayz))\n\n\nCall:\nlm(formula = tempo ~ album_release_year, data = jayz)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-44.84 -21.17 -11.79  12.22  92.18 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(>|t|)    \n(Intercept)        -2158.9919   396.7564  -5.442 6.99e-08 ***\nalbum_release_year     1.1306     0.1979   5.714 1.55e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 30.94 on 815 degrees of freedom\nMultiple R-squared:  0.03852,   Adjusted R-squared:  0.03734 \nF-statistic: 32.65 on 1 and 815 DF,  p-value: 1.547e-08\n\n\nSo, as we can see from the results here, it’s significant (p < .001), but it really doesn’t account for much of the variance (an adjusted R-squared of .037).\n\nPost-Hoc Analyses\nPerhaps we can look at how other variables might be predictive of the year of the recording.\nLet’s look at how tempo, danceability, valence, speechiness, and energy might improve the model.\n\nsummary(lm(album_release_year ~ tempo + danceability + valence + speechiness + energy, data=jayz))\n\n\nCall:\nlm(formula = album_release_year ~ tempo + danceability + valence + \n    speechiness + energy, data = jayz)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.2530  -3.4825  -0.2837   3.5690  18.2393 \n\nCoefficients:\n               Estimate Std. Error  t value Pr(>|t|)    \n(Intercept)   2.016e+03  1.826e+00 1104.163  < 2e-16 ***\ntempo         2.861e-02  5.603e-03    5.105 4.12e-07 ***\ndanceability -5.986e+00  1.519e+00   -3.942 8.77e-05 ***\nvalence      -6.849e+00  9.630e-01   -7.112 2.51e-12 ***\nspeechiness  -7.808e+00  1.160e+00   -6.730 3.20e-11 ***\nenergy       -4.484e+00  1.348e+00   -3.326 0.000921 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.885 on 811 degrees of freedom\nMultiple R-squared:  0.2084,    Adjusted R-squared:  0.2035 \nF-statistic: 42.69 on 5 and 811 DF,  p-value: < 2.2e-16\n\n\nSo we have a more predictive model, with an adjusted R-squared of about .20.\nThere are some remaining questions, however. Firstly, is there covariance at play?\nWe can use the variance inflation factor (VIF) to look at this. Typically, each variable should be less than 5.\n\nlibrary(car)\n\nLoading required package: carData\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\njayz_model <- lm(album_release_year ~ tempo + danceability + valence + speechiness + energy, data=jayz)\nvif(jayz_model)\n\n       tempo danceability      valence  speechiness       energy \n    1.067352     1.350387     1.228451     1.093354     1.253967 \n\n\nA correlation plot can help us to visualize this a bit more.\n\nlibrary(corrplot)\n\ncorrplot 0.92 loaded\n\njz <- jayz %>% \n    select(c(\"acousticness\", \"liveness\", \"danceability\", \"loudness\", \"speechiness\", \"valence\"))\n  x <- as.matrix(cor(jz))\n  round(x, 2)\n\n             acousticness liveness danceability loudness speechiness valence\nacousticness         1.00     0.14         0.07    -0.12        0.33    0.07\nliveness             0.14     1.00        -0.20     0.01        0.03   -0.11\ndanceability         0.07    -0.20         1.00    -0.17       -0.01    0.28\nloudness            -0.12     0.01        -0.17     1.00       -0.20    0.03\nspeechiness          0.33     0.03        -0.01    -0.20        1.00    0.18\nvalence              0.07    -0.11         0.28     0.03        0.18    1.00\n\n  corrplot(x, method=\"pie\")"
  },
  {
    "objectID": "class_notes/week_5.html#sidenote-isare-the-data-normal",
    "href": "class_notes/week_5.html#sidenote-isare-the-data-normal",
    "title": "Week 5: Regression and Clustering",
    "section": "Sidenote: Is/Are the data normal?",
    "text": "Sidenote: Is/Are the data normal?\nWe can test to see if the tempo data is normally distributed:\n\nqqnorm(jayz$tempo)\n\n\n\nhist(jayz$tempo)\n\n\n\nshapiro.test(jayz$tempo)\n\n\n    Shapiro-Wilk normality test\n\ndata:  jayz$tempo\nW = 0.78869, p-value < 2.2e-16\n\nks.test(jayz$tempo, \"pnorm\")\n\nWarning in ks.test.default(jayz$tempo, \"pnorm\"): ties should not be present for\nthe Kolmogorov-Smirnov test\n\n\n\n    Asymptotic one-sample Kolmogorov-Smirnov test\n\ndata:  jayz$tempo\nD = 1, p-value < 2.2e-16\nalternative hypothesis: two-sided\n\n\nAt the moment, it doesn’t seem to be…"
  },
  {
    "objectID": "class_notes/week_5.html#stepwise-entry-regression",
    "href": "class_notes/week_5.html#stepwise-entry-regression",
    "title": "Week 5: Regression and Clustering",
    "section": "Stepwise Entry Regression",
    "text": "Stepwise Entry Regression\n\nsummary(step(lm(album_release_year ~ danceability + \n          tempo + acousticness + \n          speechiness + valence, data=jayz), \n     direction=\"backward\"))\n\nStart:  AIC=2573.56\nalbum_release_year ~ danceability + tempo + acousticness + speechiness + \n    valence\n\n               Df Sum of Sq   RSS    AIC\n<none>                      18788 2573.6\n- danceability  1    243.73 19032 2582.1\n- tempo         1    714.38 19502 2602.1\n- acousticness  1    826.79 19614 2606.7\n- speechiness   1   1422.54 20210 2631.2\n- valence       1   1687.89 20476 2641.8\n\n\n\nCall:\nlm(formula = album_release_year ~ danceability + tempo + acousticness + \n    speechiness + valence, data = jayz)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-12.9398  -3.6858  -0.2579   3.4408  16.3345 \n\nCoefficients:\n               Estimate Std. Error  t value Pr(>|t|)    \n(Intercept)   2.011e+03  1.257e+00 1599.612  < 2e-16 ***\ndanceability -4.496e+00  1.386e+00   -3.244  0.00123 ** \ntempo         3.075e-02  5.538e-03    5.553 3.80e-08 ***\nacousticness  6.752e+00  1.130e+00    5.974 3.46e-09 ***\nspeechiness  -9.233e+00  1.178e+00   -7.836 1.46e-14 ***\nvalence      -7.750e+00  9.080e-01   -8.536  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.813 on 811 degrees of freedom\nMultiple R-squared:  0.2314,    Adjusted R-squared:  0.2267 \nF-statistic: 48.83 on 5 and 811 DF,  p-value: < 2.2e-16\n\nsummary(step(lm(album_release_year ~ danceability + \n          tempo + acousticness + \n          speechiness + valence, data=jayz), \n     direction=\"forward\"))\n\nStart:  AIC=2573.56\nalbum_release_year ~ danceability + tempo + acousticness + speechiness + \n    valence\n\n\n\nCall:\nlm(formula = album_release_year ~ danceability + tempo + acousticness + \n    speechiness + valence, data = jayz)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-12.9398  -3.6858  -0.2579   3.4408  16.3345 \n\nCoefficients:\n               Estimate Std. Error  t value Pr(>|t|)    \n(Intercept)   2.011e+03  1.257e+00 1599.612  < 2e-16 ***\ndanceability -4.496e+00  1.386e+00   -3.244  0.00123 ** \ntempo         3.075e-02  5.538e-03    5.553 3.80e-08 ***\nacousticness  6.752e+00  1.130e+00    5.974 3.46e-09 ***\nspeechiness  -9.233e+00  1.178e+00   -7.836 1.46e-14 ***\nvalence      -7.750e+00  9.080e-01   -8.536  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.813 on 811 degrees of freedom\nMultiple R-squared:  0.2314,    Adjusted R-squared:  0.2267 \nF-statistic: 48.83 on 5 and 811 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "class_notes/week_5.html#comparing-fits",
    "href": "class_notes/week_5.html#comparing-fits",
    "title": "Week 5: Regression and Clustering",
    "section": "Comparing Fits:",
    "text": "Comparing Fits:\nWe could construct a few models But how can we tell which of these is more predictable? For this, we can look at Akaike’s ‘An Information Criterion’(or AIC). The lower number is better, and any difference of more than 2 is considered a statistically significant difference fit.\n\ndance_model <- lm(danceability ~ album_release_year, data=jayz)\nacoustic_model <- lm(acousticness ~ album_release_year, data=jayz)\nspeech_model <- lm(speechiness ~ album_release_year, data=jayz)\nvalence_model <- lm(valence ~ album_release_year, data=jayz)\ntempo_model <- lm(tempo ~ album_release_year, data=jayz)\ncombined_model <- lm(album_release_year ~ tempo + acousticness + speechiness + valence, data=jayz)\n\n\nAIC(dance_model, \n    acoustic_model, \n    tempo_model,\n    speech_model, \n    valence_model, \n    combined_model)\n\n               df        AIC\ndance_model     3 -1036.7120\nacoustic_model  3  -686.8298\ntempo_model     3  7930.2701\nspeech_model    3  -782.5959\nvalence_model   3  -438.7779\ncombined_model  6  4902.6326\n\n\nThe combined model doesn’t seem to do terribly well here, which seems to muddy the question up a bit."
  },
  {
    "objectID": "class_notes/week_5.html#is-a-linear-model-the-best-approach",
    "href": "class_notes/week_5.html#is-a-linear-model-the-best-approach",
    "title": "Week 5: Regression and Clustering",
    "section": "Is a linear model the best approach?",
    "text": "Is a linear model the best approach?\nWe can adjust how we are viewing this data (linear or polynomial fit) a bit with the code below. Here we can plot our data as a line:\n\nggplot(jayz, aes(album_release_year, tempo) ) + geom_point() +\n  stat_smooth(method = lm, formula = y ~ poly(x, 1, raw = TRUE)) +\n  theme_bw()\n\n\n\n\nAnd here we have it as a second order polynomial:\n\nggplot(jayz, aes(album_release_year, tempo) ) + geom_point() +\n  stat_smooth(method = lm, formula = y ~ poly(x, 2, raw = TRUE)) +\n  theme_bw()\n\n\n\n\nAnd we can compare fits here:\n\nlinear <- lm(album_release_year ~ tempo, data = jayz)\npoly_2 <- lm(album_release_year ~ tempo + I(album_release_year^2), data = jayz)\n\nAIC(linear, \n    poly_2)\n\n       df       AIC\nlinear  3  5069.029\npoly_2  4 -5640.652"
  },
  {
    "objectID": "class_notes/week_5.html#predicting-a-categorical-variable",
    "href": "class_notes/week_5.html#predicting-a-categorical-variable",
    "title": "Week 5: Regression and Clustering",
    "section": "Predicting a categorical variable",
    "text": "Predicting a categorical variable\nWhat does it look like to predict a categorical variable in R? We might explore this question by hypothesizing that musical features might be predictive of mode (labeled in Spotify as major or minor: 1 or 0).\nHere is what a binomial logistic regression would look like:\n\njayz.log <- glm(mode ~ tempo + danceability + valence +\n                     speechiness + acousticness, family = binomial, data = jayz)\n\nAnd it looks like “speechiness” is the most predictive of mode here.\n\nsummary(jayz.log)\n\n\nCall:\nglm(formula = mode ~ tempo + danceability + valence + speechiness + \n    acousticness, family = binomial, data = jayz)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.8836  -1.2367   0.9073   1.0739   1.3806  \n\nCoefficients:\n               Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  -1.468e-01  5.364e-01  -0.274   0.7843    \ntempo         3.210e-06  2.365e-03   0.001   0.9989    \ndanceability  1.062e-01  5.893e-01   0.180   0.8570    \nvalence      -5.733e-01  3.911e-01  -1.466   0.1427    \nspeechiness   2.561e+00  5.483e-01   4.671    3e-06 ***\nacousticness -9.680e-01  4.849e-01  -1.996   0.0459 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1120.6  on 816  degrees of freedom\nResidual deviance: 1095.8  on 811  degrees of freedom\nAIC: 1107.8\n\nNumber of Fisher Scoring iterations: 4\n\n\nWe can plot the log odds ratios as well:\n\nCI <- exp(confint(jayz.log))[-1,]\n\nWaiting for profiling to be done...\n\nsjPlot::plot_model(jayz.log,\n                   axis.lim = c(min(CI), max(CI)),\n                   auto.label = F,\n                   show.values = T) +\n                   theme_bw()"
  },
  {
    "objectID": "class_notes/week_5.html#clustering",
    "href": "class_notes/week_5.html#clustering",
    "title": "Week 5: Regression and Clustering",
    "section": "Clustering",
    "text": "Clustering\nCluster analysis is a form of statistical data analysis in which subsets (called “clusters”) are formed according to some notion of similarity. There are many different variants of cluster analysis, but most are hierarchical–in which low-level clusters are successively joined together to make larger clusters, and so on, until everything is clustered into one large group. The result is a cluster tree or dendrogram.\n\nHow does the R hclust function work?\nThe hclust function is part of the default package in R, and it clusters based on dissimilarities in the data. There are different algorithms it can use, but the default is Ward’s minimum variance. It requires some distance to be calculated first, so the dist function is used on the data. Again there are many options here, but the default is to simply calculate the Euclidean distance between the values.\nThe documentation states:\n\nThis function performs a hierarchical cluster analysis using a set of dissimilarities for the n objects being clustered. Initially, each object is assigned to its own cluster and then the algorithm proceeds iteratively, at each stage joining the two most similar clusters, continuing until there is just a single cluster. At each stage distances between clusters are recomputed by the Lance–Williams dissimilarity update formula according to the particular clustering method being used.\n\nThe default is Ward’s minimum variance method, which:\n\naims at finding compact, spherical clusters. The complete linkage method finds similar clusters.\n\nAnother method is the “single linkage method”.\n\nThe single linkage method (which is closely related to the minimal spanning tree) adopts a ‘friends of friends’ clustering strategy. The other methods can be regarded as aiming for clusters with characteristics somewhere between the single and complete link methods. Note however, that methods “median” and “centroid” are not leading to a monotone distance measure, or equivalently the resulting dendrograms can have so called inversions or reversals which are hard to interpret, but note the trichotomies in Legendre and Legendre (2012).\n\n\n# cluster demo modified from here: \n### https://www.r-bloggers.com/2021/04/cluster-analysis-in-r/\nlibrary(tidyverse)\nlibrary(cluster)\nbeyonce <- read.csv(\"beyonce.csv\")\ntaylor <- read.csv(\"taylor.csv\")\n\n\ndf <- beyonce %>% \n  filter(album_name == \"4\") %>%\n  select(c(\"track_name\", \"acousticness\", \"liveness\", \"danceability\", \"loudness\", \"speechiness\", \"valence\"))\n\n\ndf <- df %>% distinct(track_name, .keep_all = TRUE)\n\n## cleaning up the data.\nz <- df[,-c(1,1)]\n\n### getting means of each category.\nmeans <- apply(z,2,mean)\n### getting standard deviation of each category.\nsds <- apply(z,2,sd)\n\n### scales the data in the matrix.\nscaled_data <- scale(z,center=means,scale=sds)\ndistance <- dist(scaled_data)\n\nAnd we can plot the data like this:\n\n### helps with the size of the image.\npar(mar = c(5, 4, 4, 1))\n\n### creates the cluster\ndf.hclust <- hclust(distance)\n\n### plots the data but with row numbers.\nplot(df.hclust)\n\n\n\n\nAnd we can add the track name like so:\n\nplot(df.hclust,labels=df$track_name,main='Default from hclust')\n\n\n\n\nWe can clean up the plot the be along a single x-axis with the hang argument.\n\nnodePar <- list(lab.cex = 0.6, pch = c(NA, 19), \n                cex = 0.7, col = \"blue\")\nplot(df.hclust,hang=-1, labels=df$track_name,main='Default from hclust')"
  },
  {
    "objectID": "class_notes/week_5.html#which-track-belongs-to-which-cluster",
    "href": "class_notes/week_5.html#which-track-belongs-to-which-cluster",
    "title": "Week 5: Regression and Clustering",
    "section": "Which track belongs to which cluster?",
    "text": "Which track belongs to which cluster?\nIt might be helpful with this analysis to look at how each of the songs fits on the tree. We can use the cutree function, which “cuts a tree” from the cluster based on how many groups we ask it for.\nThe following code can tell us how many fall into each broader tree, assuming we think that the tree should be cut into three. Notice that the third branch is the most populous, with the second being the most sparsely populated.\n\nmember <- cutree(df.hclust,3)\ntable(member)\n\nmember\n 1  2  3 \n 4  3 11 \n\n\nBut how is each category being weighted? The code below shows that acousticness and danceability do a fair bit of work in separating groups 1 and 3, and valence separates 1 and 2 from one another.\n\n##but how are these clusters calculated?\naggregate(scaled_data,list(member),mean)\n\n  Group.1 acousticness   liveness danceability   loudness speechiness\n1       1    1.2293770 -0.3037203   -1.5341655 -0.4289046   0.3971934\n2       2    0.6165508 -0.5097641    0.5152708 -1.0002027   1.0862774\n3       3   -0.6151964  0.2494703    0.4173500  0.4287478  -0.4406914\n     valence\n1 -1.2506018\n2  1.0310423\n3  0.1735709\n\n\nA slightly more even split occurs if we break it into four groups rather than three.\n\nmember <- cutree(df.hclust,4)\ntable(member)\n\nmember\n1 2 3 4 \n4 3 9 2 \n\n\nAnd that how they’re split into four is a bit different from how we might split them into three, but danceability and acousticness still playing a strong role.\n\naggregate(scaled_data,list(member),mean)\n\n  Group.1 acousticness   liveness danceability   loudness speechiness\n1       1    1.2293770 -0.3037203   -1.5341655 -0.4289046   0.3971934\n2       2    0.6165508 -0.5097641    0.5152708 -1.0002027   1.0862774\n3       3   -0.6115250 -0.1547202    0.4044253  0.3613888  -0.5628163\n4       4   -0.6317177  2.0683279    0.4755110  0.7318636   0.1088705\n      valence\n1 -1.25060184\n2  1.03104230\n3  0.05196371\n4  0.72080353\n\n\n\nK-Means Clustering\nWe can also run a simple k-means clustering on the data. With this, we are clustering the data into k groups. R’s documentation explains it like so:\n\naims to partition the points into k groups such that the sum of squares from points to the assigned cluster centres is minimized. At the minimum, all cluster centres are at the mean of their Voronoi sets (the set of data points which are nearest to the cluster centre).\n\nThere are a few algorithms to pick from. R uses the Hartigan and Wong (1979) algorithm by default.\n\n###split it into three groups\nset.seed(123)\nkc <- kmeans(scaled_data,3)\n\n### add labels.\nrow.names(scaled_data) <- df$track_name\n\n###get the shortest distance.\ndatadistshortset<-dist(scaled_data,method = \"euclidean\")\n\nThe code below will cluster it based on this k-means clustering distance, and plot them into the amount of groups listed (here 3).\n\nhc1 <- hclust(datadistshortset, method = \"complete\" )\npamvshortset <- pam(datadistshortset,3, diss = FALSE)\n\nclusplot(pamvshortset, shade = FALSE,labels=2,col.clus=\"blue\",col.p=\"red\",span=FALSE,main=\"Cluster Mapping\",cex=1.2)"
  },
  {
    "objectID": "class_notes/week_5.html#example-1-looking-at-nirvana-albumss",
    "href": "class_notes/week_5.html#example-1-looking-at-nirvana-albumss",
    "title": "Week 5: Regression and Clustering",
    "section": "Example 1: Looking at Nirvana Albumss",
    "text": "Example 1: Looking at Nirvana Albumss\nI’m going to get the global features from Nirvana, and specifically I’m just going to look at the Unplugged in New York album. I do this in two, rather inefficient, steps: I get all of the Nirvana data and put those in a dataframe, and then I create a variable that has filtered out only the specific album I’m looking for.\n\nnirvana <- get_artist_audio_features('nirvana')\nunplugged <- filter(nirvana, album_name == \"MTV Unplugged In New York\")\nboth <- filter(nirvana, album_name == \"MTV Unplugged In New York\"  | album_name == \"Nevermind\")\n\nThis gets lots of data, and I’m just interested in their global measures (tempo, danceability, liveness, etc.). Here, I’ve gone with column number rather than name, but the other version might be a bit easier/cleaner. Nevertheless, this is another way of doing it:\n\n###i've just picked out the columns I want.\nselected <- c(9,10,12,14,15,16,17,18,19,30)\n### This subsets the data based on only the columns I want.\nunplugged <- unplugged[,selected]\n\n### I assign the track name column (30) with the rownames, to have a labeled cluster.\nrownames(unplugged) <- unplugged$track_name\n\n\nhc <- hclust(dist(unplugged), method = \"complete\", members = NULL)\n\nWarning in dist(unplugged): NAs introduced by coercion\n\n\n\nPlotting the cluster\nTechnically, you could just use the plot function at this point, but there are some long title names, so I added these extra plot options to make the text smaller and increase the marins.\n\npar(cex=0.5, mar=c(5, 8, 4, 1))\nplot(hc, xlab=\"\", ylab=\"\", main=\"\", sub=\"\", axes=FALSE)\npar(cex=1)\ntitle(xlab=\"tunes\", ylab=\"height\", main=\"Nirvana unplugged\")\naxis(2)\n\n\n\n\nThis plot is a bit strange now, as we have a pretty big negative number on the y-axis. Nevertheless, we see some cool things. The songs written by the Meat Puppets cluster together, for example.\n\n\nk-means clustering\nOur next type of clustering analysis is be a k-means cluster. We will start off by using a scree plot to see how many clusters we should use. There are a number of ways of analyzing where an “elbow” on this plot might be, but many people actually just eyeball it.\n\nunplugged <- filter(nirvana, album_name == \"MTV Unplugged In New York\" )\ntitles <- unplugged$track_name\nunplugged <- unplugged[,selected]\nunplugged <- scale(unplugged[,-10]) # standardize variables\nunplugged <- as.data.frame(unplugged) # standardize variables\n\n# Determine number of clusters\nwss <- (nrow(unplugged)-1)*sum(apply(unplugged,2,var))\nfor (i in 2:9) wss[i] <- sum(kmeans(unplugged, \n                                    centers=i)$withinss)\n  plot(1:9, wss, type=\"b\", xlab=\"Number of Clusters\",\n    ylab=\"Within groups sum of squares\")\n\n\n\n\nAnd now we can look at the k-means clustering based on however many clusters we think are necessary.\n\n# K-Means Cluster Analysis\nfit <- kmeans(unplugged, 3) # 3 cluster solution\n# get cluster means \naggregate(unplugged,by=list(fit$cluster),FUN=mean)\n\n  Group.1 danceability     energy   loudness speechiness acousticness\n1       1     1.026773  0.1850689  0.4603061 -0.67242123    0.5744139\n2       2    -0.466715  0.7286156  0.3748797  0.92850767   -1.1595582\n3       3    -0.653401 -0.7679614 -0.7602098 -0.07038491    0.3532327\n  instrumentalness    liveness    valence      tempo\n1       -0.1528028  0.59048330  0.5986320  0.1965621\n2       -0.3318123  0.09159538  0.4107587  0.7767115\n3        0.4182526 -0.66375960 -0.9272390 -0.8179312\n\n# append cluster assignment\nunplugged_appended <- data.frame(unplugged, fit$cluster)\n\n\nrownames(unplugged_appended) = titles\nclusplot(unplugged_appended, fit$cluster, color=TRUE, shade=TRUE, \n   labels=3, lines=0)"
  },
  {
    "objectID": "class_notes/week_5.html#conditional-inference-tree-with-party",
    "href": "class_notes/week_5.html#conditional-inference-tree-with-party",
    "title": "Week 5: Regression and Clustering",
    "section": "Conditional Inference Tree with Party",
    "text": "Conditional Inference Tree with Party\nA conditional inference tree is basically a regression tree, and it tells you exactly how it picks apart the data in a pretty clear way.\nI’ve always thought that Weezer was a bit derivative, so we might look at how we can separate them other (much better) bands, like Pavement…\n\npavement <- get_artist_audio_features('pavement')\nweezer <- get_artist_audio_features('weezer')\npavement_weezer <-rbind(pavement, weezer)\n\nHere’s a regression tree that tries to account for the variance between deciding whether a piece is from Pavement or Weezer.\n\nlibrary(rpart.plot)\n\nLoading required package: rpart\n\n# grow tree \nfit <- rpart(as.factor(artist_name) ~ danceability + valence + tempo + liveness,  data=pavement_weezer)\n\nprintcp(fit) # display the results \n\n\nClassification tree:\nrpart(formula = as.factor(artist_name) ~ danceability + valence + \n    tempo + liveness, data = pavement_weezer)\n\nVariables actually used in tree construction:\n[1] danceability liveness     valence     \n\nRoot node error: 349/979 = 0.35649\n\nn= 979 \n\n        CP nsplit rel error  xerror     xstd\n1 0.047755      0   1.00000 1.00000 0.042940\n2 0.022923      4   0.80516 0.89398 0.041776\n3 0.020057      5   0.78223 0.85960 0.041331\n4 0.010000      6   0.76218 0.85673 0.041293\n\nplotcp(fit) # visualize cross-validation results \n\n\n\nsummary(fit) # detailed summary of splits\n\nCall:\nrpart(formula = as.factor(artist_name) ~ danceability + valence + \n    tempo + liveness, data = pavement_weezer)\n  n= 979 \n\n          CP nsplit rel error    xerror       xstd\n1 0.04775549      0 1.0000000 1.0000000 0.04294041\n2 0.02292264      4 0.8051576 0.8939828 0.04177567\n3 0.02005731      5 0.7822350 0.8595989 0.04133127\n4 0.01000000      6 0.7621777 0.8567335 0.04129270\n\nVariable importance\ndanceability      valence     liveness        tempo \n          61           28           10            1 \n\nNode number 1: 979 observations,    complexity param=0.04775549\n  predicted class=Weezer    expected loss=0.3564862  P(node) =1\n    class counts:   349   630\n   probabilities: 0.356 0.644 \n  left son=2 (618 obs) right son=3 (361 obs)\n  Primary splits:\n      danceability < 0.522    to the left,  improve=43.858290, (0 missing)\n      valence      < 0.2975   to the left,  improve=13.962690, (0 missing)\n      tempo        < 142.843  to the right, improve= 7.641565, (0 missing)\n      liveness     < 0.05625  to the right, improve= 6.520634, (0 missing)\n  Surrogate splits:\n      valence  < 0.5905   to the left,  agree=0.697, adj=0.177, (0 split)\n      liveness < 0.05625  to the right, agree=0.647, adj=0.042, (0 split)\n      tempo    < 76.103   to the right, agree=0.635, adj=0.011, (0 split)\n\nNode number 2: 618 observations,    complexity param=0.04775549\n  predicted class=Weezer    expected loss=0.4708738  P(node) =0.6312564\n    class counts:   291   327\n   probabilities: 0.471 0.529 \n  left son=4 (53 obs) right son=5 (565 obs)\n  Primary splits:\n      liveness     < 0.5765   to the right, improve=7.022553, (0 missing)\n      valence      < 0.1925   to the left,  improve=6.959549, (0 missing)\n      danceability < 0.404    to the left,  improve=5.352002, (0 missing)\n      tempo        < 125.6825 to the left,  improve=4.458552, (0 missing)\n\nNode number 3: 361 observations\n  predicted class=Weezer    expected loss=0.1606648  P(node) =0.3687436\n    class counts:    58   303\n   probabilities: 0.161 0.839 \n\nNode number 4: 53 observations\n  predicted class=Pavement  expected loss=0.2830189  P(node) =0.05413687\n    class counts:    38    15\n   probabilities: 0.717 0.283 \n\nNode number 5: 565 observations,    complexity param=0.04775549\n  predicted class=Weezer    expected loss=0.4477876  P(node) =0.5771195\n    class counts:   253   312\n   probabilities: 0.448 0.552 \n  left son=10 (61 obs) right son=11 (504 obs)\n  Primary splits:\n      valence      < 0.6655   to the right, improve=10.232180, (0 missing)\n      danceability < 0.5165   to the right, improve= 4.949092, (0 missing)\n      tempo        < 77.8915  to the left,  improve= 3.823105, (0 missing)\n      liveness     < 0.09715  to the left,  improve= 3.710828, (0 missing)\n\nNode number 10: 61 observations\n  predicted class=Pavement  expected loss=0.2786885  P(node) =0.06230848\n    class counts:    44    17\n   probabilities: 0.721 0.279 \n\nNode number 11: 504 observations,    complexity param=0.04775549\n  predicted class=Weezer    expected loss=0.4146825  P(node) =0.514811\n    class counts:   209   295\n   probabilities: 0.415 0.585 \n  left son=22 (98 obs) right son=23 (406 obs)\n  Primary splits:\n      valence      < 0.192    to the left,  improve=7.635957, (0 missing)\n      danceability < 0.5165   to the right, improve=4.863906, (0 missing)\n      tempo        < 77.8915  to the left,  improve=3.964739, (0 missing)\n      liveness     < 0.3565   to the left,  improve=3.188013, (0 missing)\n  Surrogate splits:\n      tempo        < 202.119  to the right, agree=0.813, adj=0.041, (0 split)\n      danceability < 0.196    to the left,  agree=0.812, adj=0.031, (0 split)\n\nNode number 22: 98 observations,    complexity param=0.02292264\n  predicted class=Pavement  expected loss=0.4081633  P(node) =0.1001021\n    class counts:    58    40\n   probabilities: 0.592 0.408 \n  left son=44 (90 obs) right son=45 (8 obs)\n  Primary splits:\n      danceability < 0.497    to the left,  improve=6.102494, (0 missing)\n      liveness     < 0.09655  to the left,  improve=4.191436, (0 missing)\n      tempo        < 130.712  to the right, improve=3.409439, (0 missing)\n      valence      < 0.113    to the right, improve=1.896793, (0 missing)\n\nNode number 23: 406 observations,    complexity param=0.02005731\n  predicted class=Weezer    expected loss=0.3719212  P(node) =0.4147089\n    class counts:   151   255\n   probabilities: 0.372 0.628 \n  left son=46 (7 obs) right son=47 (399 obs)\n  Primary splits:\n      danceability < 0.5165   to the right, improve=5.619653, (0 missing)\n      valence      < 0.22     to the right, improve=3.489861, (0 missing)\n      liveness     < 0.363    to the left,  improve=2.817898, (0 missing)\n      tempo        < 124.612  to the left,  improve=2.773645, (0 missing)\n\nNode number 44: 90 observations\n  predicted class=Pavement  expected loss=0.3555556  P(node) =0.09193054\n    class counts:    58    32\n   probabilities: 0.644 0.356 \n\nNode number 45: 8 observations\n  predicted class=Weezer    expected loss=0  P(node) =0.008171604\n    class counts:     0     8\n   probabilities: 0.000 1.000 \n\nNode number 46: 7 observations\n  predicted class=Pavement  expected loss=0  P(node) =0.007150153\n    class counts:     7     0\n   probabilities: 1.000 0.000 \n\nNode number 47: 399 observations\n  predicted class=Weezer    expected loss=0.3609023  P(node) =0.4075587\n    class counts:   144   255\n   probabilities: 0.361 0.639 \n\n# plot tree \nrpart.plot(fit, uniform=TRUE, \n   main=\"Regression Tree for Pavement/Weezer\")"
  },
  {
    "objectID": "class_notes/week_5.html#christmas-or-not",
    "href": "class_notes/week_5.html#christmas-or-not",
    "title": "Week 5: Regression and Clustering",
    "section": "Christmas or Not?",
    "text": "Christmas or Not?\n\nchristmas <- get_playlist_audio_features(\"\", \"5OP7itTh52BMfZS1DJrdlv\")\nchristmas$christmas <- \"yes\"\n\nnot <- get_playlist_audio_features(\"\", \"6i2Qd6OpeRBAzxfscNXeWp\")\nnot$christmas <- \"no\"\nchristmas_not <-rbind(christmas, not)\n\nfit <- rpart(as.factor(christmas) ~ danceability + valence + tempo + liveness + tempo + mode, data=christmas_not)\n\n# plot tree \nrpart.plot(fit, uniform=TRUE, \n   main=\"Regression Tree for Christmas/Not\")\n\n\n\n\n\ntable(not$mode)\n\n\n 0  1 \n34 80"
  },
  {
    "objectID": "class_notes/week_6.html",
    "href": "class_notes/week_6.html",
    "title": "Week 6: Classifying",
    "section": "",
    "text": "Look at running a principal components analysis for authorship\nWork on some models for classifying data\nDiscuss how we might evaluate our models\n\n\n\nWe will be using a of libraries today:\n\n\n\n\n\n\n\n\n\nPCAs are often used for reducing dimensions when we have lots of variables but a model might be better suited from combining those variables. PCAs have also been used a fair bit to explore questions of authorship. Here we have a question of authorship using symbolic data taken from scores. We are trying to explore the music of Josquin.\nHere we load the data in:\n\ncomplete_data <- read.csv(\"attribution_data_new.csv\", na.strings=c(\"\",\"NA\"), header=T)\ncomplete_data <- complete_data[,-62]\n\nJesse Rodin’s Josquin Research Project has given levels of security for attribution, including pieces that we know are Josquin’s, those we think might be, and those which are more questionable.\n\n# Josquin attribution level 1 and palestrina\n\njosquin <- complete_data[complete_data$Composer == 'Josquin des Prez',-12]\n\njosquin_secure <- josquin[josquin$Attribution.Level <= 2 ,]\njosquin_secure$Composer <- as.character(josquin_secure$Composer)\njosquin_less_secure <- josquin[ josquin$Attribution.Level >= 3,]\n\n\n####Other composers\nbach <- complete_data[complete_data$Composer == \"Bach_Johann Sebastian\",-12]\nlarue <- complete_data[complete_data$Composer == \"la Rue_Pierre de\",-12]\npalestrina <- complete_data[complete_data$Composer == \"Palestrina_Giovanni Perluigi da\",-12]\nockeghem <- complete_data[complete_data$Composer == \"Johannes Ockeghem\",-12]\norto <- complete_data[complete_data$Composer == \"de Orto_Marbrianus\",-12]\ndufay <- complete_data[complete_data$Composer == \"Du Fay_Guillaume\",-12]\n\njosquin_bach <- rbind(josquin_secure, bach)\njosquin_palestrina <- rbind(josquin_secure, palestrina)\njosquin_larue <- rbind(josquin_secure, larue)\n\ncomparison <- rbind(josquin_secure, bach)\n\n\ncolumns_wanted <- c(5:11)  \nMatrix <- comparison[,columns_wanted]\nMatrix <- as.matrix(Matrix)\nMatrix[is.na(Matrix)] <- 0\n# log.pieces <- log(Matrix)\nlog.pieces <- log(Matrix)\n\nWarning in log(Matrix): NaNs produced\n\ncomposer <- comparison[,1]\n\nThis code runs the actual principal components analysis.\nIt also provides a scree plot, allowing us to see which components are the most heavily weighted. This can allow us to reduce the dimensions as we see fit.\n\n####principle component analysis.\n\npieces.pca <- prcomp(Matrix,\n                 center = TRUE,\n                 scale. = TRUE) \nplot(pieces.pca, type = \"l\", main=\"Principal Components Analysis\")\n\n\n\n\nIt’s worth taking some time to explore what each of these components actually means and how they’re weighted. PCA is weighting instances of parallel motion and similar motion pretty heavily, but negatively weighting pitch entropy and oblique motion. PC2 seems to be looking at nPVI and 9-8 suspensions.\n\nprint(pieces.pca)\n\nStandard deviations (1, .., p=7):\n[1] 1.8907847 0.9923828 0.8705046 0.8298104 0.7104739 0.5567648 0.4230672\n\nRotation (n x k) = (7 x 7):\n                         PC1         PC2        PC3         PC4         PC5\nnPVI_Entire       -0.2826479  0.52894566 -0.2336756  0.74429280 -0.17804201\nNine_Eight        -0.2553594  0.61806193  0.5670902 -0.41414816 -0.23123893\npitch_correlation -0.3244143  0.05847133 -0.7184471 -0.47933237 -0.36850440\npitch_entropy     -0.4038052  0.19724082 -0.1329848 -0.14687309  0.77362200\nparallel_motion    0.4444947  0.24809410 -0.1263420 -0.08782873 -0.20277222\nsimilar_motion     0.4682238  0.29107268 -0.1026235 -0.05294267  0.04450771\noblique_motion    -0.4120550 -0.38680631  0.2519115  0.11252328 -0.37073657\n                           PC6          PC7\nnPVI_Entire        0.006914729  0.001955825\nNine_Eight         0.076657435 -0.018255504\npitch_correlation  0.076213006  0.061699313\npitch_entropy     -0.387728754  0.099780551\nparallel_motion   -0.750907432 -0.334991080\nsimilar_motion     0.016773922  0.824891734\noblique_motion    -0.523249921  0.439584522\n\n\nAs we can see, about 65% of the variance is accounted for with the first two principal components:\n\nsummary(pieces.pca)\n\nImportance of components:\n                          PC1    PC2    PC3     PC4     PC5     PC6     PC7\nStandard deviation     1.8908 0.9924 0.8705 0.82981 0.71047 0.55676 0.42307\nProportion of Variance 0.5107 0.1407 0.1082 0.09837 0.07211 0.04428 0.02557\nCumulative Proportion  0.5107 0.6514 0.7597 0.85804 0.93015 0.97443 1.00000\n\n\nPlotting our two composers with the first two principal components.\n\ng <- ggbiplot(pieces.pca, obs.scale = 1, var.scale = 1, \n              groups = composer, ellipse = TRUE, \n              circle = TRUE)\ng <- g + scale_color_discrete(name = '')\ng <- g + theme(legend.direction = 'horizontal', \n               legend.position = 'top') +\n               theme_bw()\nprint(g)\n\n\n\n# we can change the number of components\n# seven_component_model <- data.frame(pieces.pca$x[,1:8])\n\nWe can also look at how much each of these features is being weighted within the first two components.\n\ntheta <- seq(0,2*pi,length.out = 100)\ncircle <- data.frame(x = cos(theta), y = sin(theta))\np <- ggplot(circle,aes(x,y)) + geom_path()\n\nloadings <- data.frame(pieces.pca$rotation, \n                       .names = row.names(pieces.pca$rotation))\np + geom_text(data=loadings, \n              mapping=aes(x = PC1, y = PC2, label = .names, colour = .names)) +\n  coord_fixed(ratio=1) +\n  labs(x = \"PC1\", y = \"PC2\") +\n  theme_bw()\n\n\n\n\n\n\n\nA classifier is a model that assigns a label to data based on the input. There are many types of classifiers, and we will be evaluating various models throughout the week.\nOur goal will be to train a model on the features generally associated with a category, and then test the accuracy of that model. For now, a good starting point might be our Christmas Song question from last week.\n\n\n\nFirst, let’s get the data and add a column that tells us whether it’s a Christmas song or not\n\n### get the data and add yes/no column.\nchristmas <- get_playlist_audio_features(\"\", \"5OP7itTh52BMfZS1DJrdlv\")\nchristmas$christmas <- \"yes\"\n\nnot <- get_playlist_audio_features(\"\", \"6i2Qd6OpeRBAzxfscNXeWp\")\nnot$christmas <- \"no\"\n\n## combine the two datasets and get the columns we want to use.\nchristmas_subset <-rbind(christmas, not)\nchristmas_subset <- christmas_subset %>% \n    select(c(\"christmas\", \"acousticness\", \"liveness\", \"danceability\", \"loudness\", \"speechiness\", \"valence\"))\n\nNow we can use the createDataPartition function from the caret library to create a testing and a training dataset. Here, I’ve chosen a 70/30 partition of training and testing, but you can adjust as you see fit.\n\nTrain <- createDataPartition(christmas_subset$christmas, p=0.7, list=FALSE)\ntraining <- christmas_subset[ Train, ]\ntesting <- christmas_subset[ -Train, ]\n\nWe can pretty easily implement something like a neural network, using our training dataset to train it:\n\nmod_fit <- caret::train(christmas ~ .,  \n                 data=training, method=\"nnet\", importance = \"christmas\")\n\nOnce we’ve trained this model, we can test it on our testing dataset, and see how well it does:\n\npred <- predict(mod_fit, testing)\nconfusionMatrix(pred, as.factor(testing$christmas), positive = \"yes\")\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction no yes\n       no  20  13\n       yes 13  17\n                                          \n               Accuracy : 0.5873          \n                 95% CI : (0.4562, 0.7099)\n    No Information Rate : 0.5238          \n    P-Value [Acc > NIR] : 0.1888          \n                                          \n                  Kappa : 0.1727          \n                                          \n Mcnemar's Test P-Value : 1.0000          \n                                          \n            Sensitivity : 0.5667          \n            Specificity : 0.6061          \n         Pos Pred Value : 0.5667          \n         Neg Pred Value : 0.6061          \n             Prevalence : 0.4762          \n         Detection Rate : 0.2698          \n   Detection Prevalence : 0.4762          \n      Balanced Accuracy : 0.5864          \n                                          \n       'Positive' Class : yes             \n                                          \n\n\nSo what does this all mean? Let’s define some terms.\n\nAccuracy:\n\nthe accuracy rate. Just how many things it got right.\n\n95% CI:\n\nthe confidence interval of the accuracy.\n\nNo information rate:\n\ngiven no more information other than the overall distribution, how likely are you to be correct if you just pick the “majority class.”\nif you have an accuracy rate of 80%, but the majority class is 80%, then your model isn’t terribly useful.\n\nP-Value:\n\nlikelihood of chance.\n\nKappa:\n\nmeasures the agreement between two raters and ratings. Here it’s looking at the difference between observed accuracy and random chance given the distribution in the dataset.\n\nMcNemar’s Test P-Value:\n\nthis is looking at the two distributions (from a 2x2 table), and determines if they are significantly different,\n\nSensitivity:\n\ngiven that a result is actually a thing, what is the probability that our model will predict that event’s results?\n\nSpecificity:\n\ngiven that a result is not actually a thing, what is the probability that our model will predict that?\n\nPos Predictive Value:\n\nthe probability that a predicted ‘positive’ class is actually positive.\n\nNeg Predictive Value:\n\nthe probability that a predicted ‘negative’ class is actually negative.\n\nPrevalence:\n\nthe prevalence of the ‘positive event’\n\nDetection Rate:\n\nthe rate of true events also predicted to be events\n\nDetection Prevalence\n\nthe prevalence of predicted events\n\nBalanced Accuracy:\n\nthe average of the proportion corrects of each class individually\n\n\n\n\nWe can look at which features the model is using…\n\nplot(varImp(mod_fit))\n\n\n\n\n\n\n\n\n\nUse PCA to explore the works of two artists. How well do they “separate”?\nRun a classifier on two groups (it can be the same two artists, or two distinct groups). How well does your model do?"
  },
  {
    "objectID": "class_notes/other_files.qmd/sampling_exercises.html",
    "href": "class_notes/other_files.qmd/sampling_exercises.html",
    "title": "Sampling Exercise",
    "section": "",
    "text": "For each case, identify the kind of sampling employed.\n\nA researcher walks into a music library with a question: Are sharp keys more common than flat keys? Wandering through the stacks, she blindly grabs volumes off the shelves and allows each volume to open spontaneously to some page. She takes note of the key signature.\nA professional music marketer is interested in carrying out a detailed survey of musical tastes in Britain. The marketer decides to use the ACORN geodemographic profile. British households will be sampled in proportion to the second-level ACORN categories: wealthy executives (8.6 percent of the population), affluent greys (7.7%), flourishing families (8.8%), prosperous professionals (2.2%), educated urbanites (4.6%), aspiring singles (3.9%), starting out (2.5%), secured families (15.5%), settled surburbia (6.0%), prudent pensioners (2.6%), asian communities (1.6%), post-industrial families (4.8%), blue collar roots (8.0%), struggling families (14.1%), burdened singles (4.5%), high rise hardship (1.6%), and inner city adversity (2.1%).\nA researcher is interested in assembling a random sample of “classical” keyboard music. She has determined that she needs roughly 20 pieces for her study. In order to maximize data independence, she wants each piece to be written by a different composer. Using Wikipedia, she finds an alphabetical list of “classical composers.” For each letter of the alphabet, she selects the first composer who she knows has written for piano: Isaac Albéniz, Carl Philipp Emanuel Bach, Alfredo Casella, Claude Debussy, Edward Elgar, Manuel de Falla, etc.\nIn piloting an experiment, a graduate student recruits her graduate student colleagues as experimental participants.\nA team of researchers is interested in emotional expression in Hindustani film music. Indian participants are asked to characterize the emotional tenor of various film scenes. Using the descriptions, the researchers then classify the scenes into 14 categories — such as romantic, humorous, physical conflict, emotional tension, etc. The researchers then select four scenes for each of the 14 categories and analyse the associated background music. Their goal is to identify musical features in Hindustani culture that signal romance, humor, etc.\nA medievalist thinks that the Dorian mode was more likely to have been heard as comparatively “happy” whereas the Phrygian mode was more likely to have been heard as comparative “sad” for medieval listeners. In order to test this notion, the scholar examines all of the Glorias (nominally “happy” text) and Kyries (nominally “sad” text) in the Liber Usualis. The prediction is that Dorian will predominate for Glorias while Phrygian will be more likely to occur for Kyries.\nA researcher is interested in changing harmonic patterns in the masses of Palestrina. The researcher makes us of the Humdrum database of the scores for the complete 103 masses assembled by musicologist John Miller.\nPaul von Hippel and David Huron (2000) carried out a study to test the idea that melodies tend to change direction following a leap, and that this pattern is ubiquitous in musical melodies around the world. In order to test this idea, they made use of two musical samples. The first sample selected music spanning five centuries. The second sample selected music spanning five continents: Africa, Asia, Europe, North and South America.\nUnsure of the contents of a box, an archivist reaches in and grabs a couple of documents, which he then examines.\nA researcher wants to know whether there is anything Italian, French or German about augmented sixth chords. Using large computer databases, the researcher uses Humdrum to isolate 900 sonorities in which the lowered sixth and raised fourth appear concurrently (including enharmonic spellings): 300 each written by Italian, French and German composers. Each of the sonorities is then classified as either Italian, French, German or Other.\n\n\n\nPaul von Hippel & David Huron (2000). Why do skips precede reversals? The effect of tessitura on melodic structure. Music Perception, Vol. 18, No. 1, pp. 59-85."
  },
  {
    "objectID": "class_notes/week_2.html#chords",
    "href": "class_notes/week_2.html#chords",
    "title": "Week 2: Pitch",
    "section": "Chords",
    "text": "Chords\nBurgoyne’s chordogram functions allow us to look at the likely chordal spaces for specific piecses. The code below does a few things:\n\nFirst we define what a major, minor, and seventh chord looks like in terms of pitch space.\nWe then use the key-profiles from the Krumhansl-Kessler article on the probe tone experiments and store them into major_key and minor_key variables.\nThe circshift function rotates these key profiles through the chord variables and provides the best fit for that moment. This is done through the key_templates variable (Notice the compmus_match_pitch_template below).\n\n\n#      C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B\nmajor_chord <-\n  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)\nminor_chord <-\n  c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)\nseventh_chord <-\n  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)\n\nmajor_key <-\n  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)\nminor_key <-\n  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)\n\ncircshift <- function(v, n) {\n  if (n == 0) v else c(tail(v, n), head(v, -n))\n}\nchord_templates <-\n  tribble(\n    ~name, ~template,\n    \"Gb:7\", circshift(seventh_chord, 6),\n    \"Gb:maj\", circshift(major_chord, 6),\n    \"Bb:min\", circshift(minor_chord, 10),\n    \"Db:maj\", circshift(major_chord, 1),\n    \"F:min\", circshift(minor_chord, 5),\n    \"Ab:7\", circshift(seventh_chord, 8),\n    \"Ab:maj\", circshift(major_chord, 8),\n    \"C:min\", circshift(minor_chord, 0),\n    \"Eb:7\", circshift(seventh_chord, 3),\n    \"Eb:maj\", circshift(major_chord, 3),\n    \"G:min\", circshift(minor_chord, 7),\n    \"Bb:7\", circshift(seventh_chord, 10),\n    \"Bb:maj\", circshift(major_chord, 10),\n    \"D:min\", circshift(minor_chord, 2),\n    \"F:7\", circshift(seventh_chord, 5),\n    \"F:maj\", circshift(major_chord, 5),\n    \"A:min\", circshift(minor_chord, 9),\n    \"C:7\", circshift(seventh_chord, 0),\n    \"C:maj\", circshift(major_chord, 0),\n    \"E:min\", circshift(minor_chord, 4),\n    \"G:7\", circshift(seventh_chord, 7),\n    \"G:maj\", circshift(major_chord, 7),\n    \"B:min\", circshift(minor_chord, 11),\n    \"D:7\", circshift(seventh_chord, 2),\n    \"D:maj\", circshift(major_chord, 2),\n    \"F#:min\", circshift(minor_chord, 6),\n    \"A:7\", circshift(seventh_chord, 9),\n    \"A:maj\", circshift(major_chord, 9),\n    \"C#:min\", circshift(minor_chord, 1),\n    \"E:7\", circshift(seventh_chord, 4),\n    \"E:maj\", circshift(major_chord, 4),\n    \"G#:min\", circshift(minor_chord, 8),\n    \"B:7\", circshift(seventh_chord, 11),\n    \"B:maj\", circshift(major_chord, 11),\n    \"D#:min\", circshift(minor_chord, 3)\n  )\n\nkey_templates <-\n  tribble(\n    ~name, ~template,\n    \"Gb:maj\", circshift(major_key, 6),\n    \"Bb:min\", circshift(minor_key, 10),\n    \"Db:maj\", circshift(major_key, 1),\n    \"F:min\", circshift(minor_key, 5),\n    \"Ab:maj\", circshift(major_key, 8),\n    \"C:min\", circshift(minor_key, 0),\n    \"Eb:maj\", circshift(major_key, 3),\n    \"G:min\", circshift(minor_key, 7),\n    \"Bb:maj\", circshift(major_key, 10),\n    \"D:min\", circshift(minor_key, 2),\n    \"F:maj\", circshift(major_key, 5),\n    \"A:min\", circshift(minor_key, 9),\n    \"C:maj\", circshift(major_key, 0),\n    \"E:min\", circshift(minor_key, 4),\n    \"G:maj\", circshift(major_key, 7),\n    \"B:min\", circshift(minor_key, 11),\n    \"D:maj\", circshift(major_key, 2),\n    \"F#:min\", circshift(minor_key, 6),\n    \"A:maj\", circshift(major_key, 9),\n    \"C#:min\", circshift(minor_key, 1),\n    \"E:maj\", circshift(major_key, 4),\n    \"G#:min\", circshift(minor_key, 8),\n    \"B:maj\", circshift(major_key, 11),\n    \"D#:min\", circshift(minor_key, 3)\n  )\n\nHere we have a piece of code that grabs a single audio file (“Those magic changes”). In class we listened to it while going through the chordogram. Can you spot the modulation? Why do we get that yellowish color at the end of the graph?\n\nthose_magic_changes <-\n  get_tidy_audio_analysis(\"1WHauHX7U6FqOWh46lK4IV\") %>%\n  compmus_align(sections, segments) %>%\n  select(sections) %>%\n  unnest(sections) %>%\n  mutate(\n    pitches =\n      map(segments,\n          compmus_summarise, pitches,\n          method = \"mean\", norm = \"manhattan\"\n      )\n  )\n\nthose_magic_changes %>% \n  compmus_match_pitch_template(\n    key_templates,         # Change to chord_templates if descired\n    method = \"euclidean\",  # Try different distance metrics\n    norm = \"manhattan\"     # Try different norms\n  ) %>%\n  ggplot(\n    aes(x = start + duration / 2, width = duration, y = name, fill = d)\n  ) +\n  geom_tile() +\n  scale_fill_viridis_c(guide = \"none\") +\n  theme_minimal() +\n  labs(x = \"Time (s)\", y = \"\")\n\n\n\n\nSome activities:\n\nGo through some songs that you know. How close is the chordogram of providing some sort of brief explanatory analysis?\nWhat does this look like on music that might be considered less adherent to notions of western tonal music?"
  },
  {
    "objectID": "class_notes/week_3.html#midterms",
    "href": "class_notes/week_3.html#midterms",
    "title": "Week 3: Time",
    "section": "Midterms",
    "text": "Midterms\n\nDue April 28th\n1-3 pages long.\nconstruct a literature review of the topic you’re interested in writing about. This could culminate in a hypothesis, and a discussion of how your work fills the current gaps in the literature.\nBe sure to include an overview of previous research related to the topic. This should include both empirical and non-empirical work. So if you’re focusing on the memory for jazz licks, for example, research on improvisational styles should be included alongside work on memory for musical ideas. It need not be all-encompassing, but it should try to cover as much ground as possible.\nAddress a gap in the current literature, or that between theory and research. End with a discussion of your study, and how it hopes to fill in these gaps.\nFeel free to meet with me if you have any questions."
  },
  {
    "objectID": "class_notes/week_3.html#populations",
    "href": "class_notes/week_3.html#populations",
    "title": "Week 3: Time",
    "section": "Populations",
    "text": "Populations\n\nA population is everything or everyone that you’re interested in.\ne.g. all the world’s people\nall the world’s people including living and deceased\nall Western-enculturated people\nall people who enjoy listening to music\nall clarinet players\n\nA “population” does not refer only to people: Other examples:\n\nall of the music written by Vivaldi\nall solo flute music (both with and without accompaniment)\nall music in the minor mode\nall of the jazz scores available in the New York Public Library\nall performances of Rachmaninov’s 2nd piano concerto"
  },
  {
    "objectID": "class_notes/week_3.html#sample",
    "href": "class_notes/week_3.html#sample",
    "title": "Week 3: Time",
    "section": "Sample",
    "text": "Sample\n\nSample: a subset of the population that you hope closely resembles the population as a whole.\nA sample is said to be representative when the property of interest is identical in both the sample and the population."
  },
  {
    "objectID": "class_notes/week_3.html#bias",
    "href": "class_notes/week_3.html#bias",
    "title": "Week 3: Time",
    "section": "Bias",
    "text": "Bias\n\nA sample is said to be biased when the property of interest differs between the sample and the population."
  },
  {
    "objectID": "class_notes/week_3.html#weird",
    "href": "class_notes/week_3.html#weird",
    "title": "Week 3: Time",
    "section": "WEIRD",
    "text": "WEIRD\n\nWestern\nEducated\nIndustrialized\nRich\nDemocratic\n\nsee Henrich’s Work on this"
  },
  {
    "objectID": "class_notes/week_3.html#defining-your-population",
    "href": "class_notes/week_3.html#defining-your-population",
    "title": "Week 3: Time",
    "section": "Defining Your Population",
    "text": "Defining Your Population\n\nYou can’t sample a population unless you have a clear idea of what constitutes the population of interest.\n\nSuppose, for example, that you are a political pollster. Your aim is to predict the likely election results for a national election in Denmark. What, precisely, is the population you are interested in?"
  },
  {
    "objectID": "class_notes/week_3.html#defining-your-population-continued",
    "href": "class_notes/week_3.html#defining-your-population-continued",
    "title": "Week 3: Time",
    "section": "Defining Your Population (continued)",
    "text": "Defining Your Population (continued)\n\nAll Danish citizens?\nAll people living in Denmark?\nAll people living in Denmark eligible to vote?\nAll people eligible to vote in Danish elections?\nAll people likely to vote in Danish elections?"
  },
  {
    "objectID": "class_notes/week_3.html#sampling-method",
    "href": "class_notes/week_3.html#sampling-method",
    "title": "Week 3: Time",
    "section": "Sampling Method",
    "text": "Sampling Method\n\nSampling method: the way you recruit or assemble your sample. When your population consists of people, sampling methods might include soliciting information by telephone (telephone sampling), street sampling, mail sampling, web sampling, classroom sampling, concert sampling, etc."
  },
  {
    "objectID": "class_notes/week_3.html#sampling-bias",
    "href": "class_notes/week_3.html#sampling-bias",
    "title": "Week 3: Time",
    "section": "Sampling Bias",
    "text": "Sampling Bias\n\nSampling bias: when the sampling method introduces differences that cause the sample not to be representative. We try to avoid or minimize sampling bias.\nWhen conducting a telephone survey, a pollster may be tempted to ask to speak to a respondent’s spouse. However, spouses are likely to share many things in common (such as political views) so the sampling method will introduce a bias."
  },
  {
    "objectID": "class_notes/week_3.html#simple-random-sampling",
    "href": "class_notes/week_3.html#simple-random-sampling",
    "title": "Week 3: Time",
    "section": "Simple Random Sampling",
    "text": "Simple Random Sampling\n\nSimple Random Sampling. Suppose we want to know about musical instrument sales in the City of Nashville. We could use the phone book to identify all of the shops within the city boundaries that sell musical instruments. Perhaps we discover that there are 131 retailers. From this list, we might randomly select 25 retailers in order to carry out our survey."
  },
  {
    "objectID": "class_notes/week_3.html#systematic-sampling",
    "href": "class_notes/week_3.html#systematic-sampling",
    "title": "Week 3: Time",
    "section": "Systematic Sampling",
    "text": "Systematic Sampling\nSuppose that we have a questionnaire we want to distribute to people who attended a concert. There might be 500 audience members, but we have only 50 surveys to distribute. One approach would be to distribute the questionnaires to the first 50 people leaving the concert hall."
  },
  {
    "objectID": "class_notes/week_3.html#matched-random-sampling",
    "href": "class_notes/week_3.html#matched-random-sampling",
    "title": "Week 3: Time",
    "section": "Matched Random Sampling",
    "text": "Matched Random Sampling\n\nA way of linking members from two or more samples. For example, a study might involve matching each professional musician with an amateur musician who plays the same instrument."
  },
  {
    "objectID": "class_notes/week_3.html#convenience-sampling",
    "href": "class_notes/week_3.html#convenience-sampling",
    "title": "Week 3: Time",
    "section": "Convenience Sampling",
    "text": "Convenience Sampling\n\nConvenience Sampling. A convenience sample simply takes advantage of whatever might be available. For example, a sample of organ music by Gabriel Fauré might simply consist of all of the scores available in a music library. Similarly, we might stand on a street corner and ask whoever passes by to answer questions on a survey."
  },
  {
    "objectID": "class_notes/week_3.html#stratified-sampling",
    "href": "class_notes/week_3.html#stratified-sampling",
    "title": "Week 3: Time",
    "section": "Stratified Sampling",
    "text": "Stratified Sampling\n\nWhen we have reason to suspect that differences in sub-populations might influence the results, it is common to sample in such a way to ensure that each of the main sub-populations is represented.\nPost and Huron (2009) were interested in common-practice era tonal classical music. So we decided to use a stratified sample consisting of music from three periods: Baroque, Classical and Romantic. Our overall sample consisted of equivalent numbers of works from each of these historical eras."
  },
  {
    "objectID": "class_notes/week_3.html#quota-sampling",
    "href": "class_notes/week_3.html#quota-sampling",
    "title": "Week 3: Time",
    "section": "Quota Sampling",
    "text": "Quota Sampling\n\nA type of stratified sampling in which sub-samples are weighted according to their prevalence in the population.\nSuppose that we find that 52% of instrumentalists are most accomplished on guitar, 33% are most accomplished on keyboards, 12% on flute, 9% on trumpet, 8% on violin, etc. In quota sampling, we would aim to sample the same proportions for each instrument."
  },
  {
    "objectID": "class_notes/week_3.html#exercise",
    "href": "class_notes/week_3.html#exercise",
    "title": "Week 3: Time",
    "section": "Exercise",
    "text": "Exercise\nGroup Exercise"
  },
  {
    "objectID": "class_notes/week_3.html#tempo-average",
    "href": "class_notes/week_3.html#tempo-average",
    "title": "Week 3: Time",
    "section": "Tempo Average",
    "text": "Tempo Average\nThe general default at the track level is an averaging of an entire piece. This can be useful at times, but it should be noted that it is a broad average for a parameter in which the variability is often quite meaningful.\nWhat if we wanted to see how the songs of Daft Punk changed in tempo changed over time?\nFirst we would make sure that Spotify had your user access token, and then get the artist audio features:\n\naccess_token <- get_spotify_access_token()\ndaft_punk <- get_artist_audio_features('daft punk')\n\nThen we would be able to simply plot the album’s mean tempo with the variance (and outliers) with ggplot, as below:\n\nggplot(daft_punk, aes(x=album_release_year, y=tempo, group = album_name)) + geom_boxplot() +\ntheme_bw()\n\n\n\n\nWe could also explore the variance of tempo within an album by looking at the standard deviation.\n\nalbum_sd <- daft_punk %>% \n    group_by(album_name, album_release_year) %>%\n    summarise(sd_tempo = sd(tempo))\n\n`summarise()` has grouped output by 'album_name'. You can override using the\n`.groups` argument.\n\n\nAnd then we can similarly plot this information:\n\nggplot(album_sd, aes(x=album_release_year, y=sd_tempo, group = album_name)) + geom_point() +\n  geom_label(\n    aes(label=album_name)) +\ntheme_bw()\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\n\n\nWarning: Removed 1 rows containing missing values (`geom_label()`)."
  },
  {
    "objectID": "class_notes/week_3.html#other-aspects-of-tempo",
    "href": "class_notes/week_3.html#other-aspects-of-tempo",
    "title": "Week 3: Time",
    "section": "Other aspects of tempo:",
    "text": "Other aspects of tempo:\nWe can also look at other elements of tempo, such as variability within sections. Here we have a question about the differences in tempo between punk in the 1980s and later punk (1990s and 2000s). I’m interested not just in the tempo, but also the variation of tempo.\nThere are a couple of points to notice in this code:\n\nNote how we are able to get data from a playlist. A playlist can be a good way for you to construct a sample.\nNote the add_audio_analysis function from the compmus library. This adds track level analysis information to the broader list of global information. It’s great.\n\nSit for a minute with this data. You’ll see the columns at the end that provided the specific audio analysis for each piece.\n\neighties_punk <-\n  get_playlist_audio_features(\n    \"kristian\",\n    \"5sxuwIQlaByb6Sx2OEwWTx\"\n  ) %>%\n  slice(1:30) %>%\n  add_audio_analysis()\n\nnineties_and_aughts_punk <-\n  get_playlist_audio_features(\n    \"CW\",\n    \"39sVxPTg7BKwrf2MfgrtcD\"\n  ) %>%\n  slice(1:30) %>%\n  add_audio_analysis()\n\nHere we bind both playlists together as a single data frame:\n\npunk <- \n  eighties_punk %>%\n  mutate(genre = \"eighties\") %>%\n  bind_rows(nineties_and_aughts_punk %>% mutate(genre = \"newer\"))\n\nThe spotify analysis gives us section markers as well, and we can use the code below to summarise the tempo, loudness, and duration for each section. Note the use of the map function, which takes the input and applies a function to that input (here the summarise_at function, and the summarise_at itself, which provides a summary of each of these columns.\nHere we are storing it in a variable called summarised_punk.\n\nsummarised_punk <- punk %>%\n  mutate(\n    sections =\n      map(\n        sections,                                    # sections or segments\n        summarise_at,\n        vars(tempo, loudness, duration),             # features of interest\n        list(section_mean = mean, section_sd = sd)   # aggregation functions\n      )\n  )\n\nNow we take this variable and plot it using ggplot.\nThe process below is as follows:\n\ntake the table above with summarized section information and unnest it (this takes the sections list of information and turns it into rows and columns).\nPipe that into ggplot, with the aesthetics function plotting the tempo on the x-axis, the standard deviation on the y-axis, the color being which genre we used (eighties or not). The color saturation is set to the loudness variable.\nWe then tell ggplot that we want this to be a scatterplot with the geom_point function, and that the size of each point should be the duration of the piece (divided by 60 as Spotify just gives it in seconds).\nWe then add a rug plot which gives the ticks on both axes to show the distribution of events.\nWe then add a black and white theme because nobody likes default graphics.\nWe then add the size of the graph and the axis labels.\n\n\n  summarised_punk %>%\n  unnest(sections) %>%\n  ggplot(\n    aes(\n      x = tempo,\n      y = tempo_section_sd,\n      colour = genre,\n      alpha = loudness\n    )\n  ) +\n  geom_point(aes(size = duration / 60)) +\n  geom_rug() +\n  theme_bw() +\n  ylim(0, 5) +\n  labs(\n    x = \"Mean Tempo (bpm)\",\n    y = \"SD Tempo\",\n    colour = \"Genre\",\n    size = \"Duration (min)\",\n    alpha = \"Volume (dBFS)\"\n  )  \n\nWarning: Removed 7 rows containing missing values (`geom_point()`).\n\n\n\n\n\nIn-class exercises:\n\nHow is tempo treated differently across the albums of the Beatles?"
  },
  {
    "objectID": "class_notes/week_3.html#plan-for-the-day",
    "href": "class_notes/week_3.html#plan-for-the-day",
    "title": "Week 3: Time",
    "section": "Plan for the day:",
    "text": "Plan for the day:\n\nTalk about your homework a bit.\nLook at some tap data, and what we might actually be able to do with it.\nTalk about how Spotify (might) calculate tempo.\n\nHow might we look at tempo across pieces?\nOne way to get this is to just tap the tempo, and then align it to the onsets. But how do we find onsets?\nThis is where a novelty function comes in. (see Müller on Fourier Tempograms). Put (extremely) succinctly, a novelty function detects changes in the energy or the spectrum of the signal. So looking for energy peaks might be a good marker for “peaks in energy”.\n\n\n\nMüller’s Onset Detection Example (p.311)\n\n\nAfter finding these onsets, it then examines a correlation between various sinusoids and picks the most likely one. There are many different ways of approaching this.\nOne issue is the presence of so-called “tempo octaves”. That is, it finds tempos at twice the beat, half the beat, etc..\nHere’s a graph of AJR’s “World’s Smallest Violin”:\n\nget_tidy_audio_analysis(\"3jHBgKdLCf46aP3HRI0WYv\") %>%\n  tempogram(window_size = 8, hop_size = 2) %>%\n   ggplot(aes(x = time, y = bpm, fill = power)) + \n    geom_raster() + \n    scale_fill_viridis_c(guide = 'none') +\n    labs(x = 'Time (s)', y = 'Tempo (BPM)') +\n    theme_classic()\n\n\n\n\nThis seems quite strange, though. It’s not really a great indicator of tempo…\nMüller points out that the Fourier-based method tends to struggle with these tempo-octaves, and a cyclic model, which look at “subharmonics” rather than harmonics, and are a bit better for mid-level tempo finding. The example below seems to work a bit better. Notice how the cyclic option has been switched to TRUE.\n\nget_tidy_audio_analysis(\"3jHBgKdLCf46aP3HRI0WYv\") %>%\n  tempogram(window_size = 8, hop_size = 2, cyclic=TRUE) %>%\n   ggplot(aes(x = time, y = bpm, fill = power)) + \n    geom_raster() + \n    scale_fill_viridis_c(guide = 'none') +\n    labs(x = 'Time (s)', y = 'Tempo (BPM)') +\n    theme_classic()"
  },
  {
    "objectID": "class_notes/week_1.html",
    "href": "class_notes/week_1.html",
    "title": "Week 1: Representing Musical Data",
    "section": "",
    "text": "In the first week, we worked through basic introductions for the class, and went through the syllabus and the course structure.\n\n\nHere, we install the necessary library. As you can see, you will need to install devtools, which will allow you to install packages that aren’t on CRAN from github.\nThen, we install the package (you can uncomment these installation lines as necessary for you).\n\n### installing everything as needed\n# library(devtools)\n# devtools::install_github(\"Computational-Cognitive-Musicology-Lab/humdrumR\", build_vignettes = TRUE)\nlibrary(humdrumR)\n\nIn the code below, you can see how we load all of the Chopin files into a preludes variable with the readHumdrum function.\nThen we subset it by spines. We are interested in various ways of calculating pitch, so we looked at pc (pitch class), as well as solfa and deg, which gave us solfege syllables and scale degrees, respectively.\nWe then plot this data in a barplot. Note the |> or “pipe” that we are using. The older tidyverse-style pipe (%>%) will also work here.\n\n### Load in Chopin preludes, grab the left hand and see all the scale degrees.\npreludes <- readHumdrum(\"~/gitcloud/corpora/humdrum_scores/Chopin/Preludes/*.krn\")\nleft_hand <- subset(preludes, Spine == 1)\n###solfa, deg, pc\ntable_data <- with(left_hand, pc(Token,simple=TRUE)) |> table() \nbarplot(table_data)\n\n\n\n\nYou can use a similar with syntax to get rhythm variables, as seen below:\n\n## rhythminterval\nrhythms <- with(preludes[2], duration(Token))\n\nWarning in is.na(x) || is.na(y): 'length(x) = 93 > 1' in coercion to\n'logical(1)'\n\nWarning in is.na(x) || is.na(y): 'length(x) = 93 > 1' in coercion to\n'logical(1)'\n\nWarning in is.na(x) || is.na(y): 'length(x) = 93 > 1' in coercion to\n'logical(1)'\n\nWarning in is.na(x) || is.na(y): 'length(x) = 93 > 1' in coercion to\n'logical(1)'\n\n\nWarning in is.na(x) || is.na(y): 'length(x) = 84 > 1' in coercion to\n'logical(1)'\n\nWarning in is.na(x) || is.na(y): 'length(x) = 84 > 1' in coercion to\n'logical(1)'\n\n\nWarning in is.na(x) || is.na(y): 'length(x) = 93 > 1' in coercion to\n'logical(1)'\n\nWarning in is.na(x) || is.na(y): 'length(x) = 93 > 1' in coercion to\n'logical(1)'\n\n\nWarning in is.na(x) || is.na(y): 'length(x) = 11 > 1' in coercion to\n'logical(1)'\n\nWarning in is.na(x) || is.na(y): 'length(x) = 11 > 1' in coercion to\n'logical(1)'\n\n\nWarning in is.na(x) || is.na(y): 'length(x) = 2 > 1' in coercion to\n'logical(1)'\n\nWarning in is.na(x) || is.na(y): 'length(x) = 2 > 1' in coercion to\n'logical(1)'\n\n\nWarning in is.na(x) || is.na(y): 'length(x) = 93 > 1' in coercion to\n'logical(1)'\n\nWarning in is.na(x) || is.na(y): 'length(x) = 93 > 1' in coercion to\n'logical(1)'\n\n\nWarning in is.na(x) || is.na(y): 'length(x) = 84 > 1' in coercion to\n'logical(1)'\n\nWarning in is.na(x) || is.na(y): 'length(x) = 84 > 1' in coercion to\n'logical(1)'\n\n\nWarning in is.na(x) || is.na(y): 'length(x) = 11 > 1' in coercion to\n'logical(1)'\n\nWarning in is.na(x) || is.na(y): 'length(x) = 11 > 1' in coercion to\n'logical(1)'\n\n\nWarning in is.na(x) || is.na(y): 'length(x) = 5 > 1' in coercion to\n'logical(1)'\n\nWarning in is.na(x) || is.na(y): 'length(x) = 5 > 1' in coercion to\n'logical(1)'\n\n#### group exercise:\n#### using a repertoire in the Humdrum scores collection, \n#### print a table of most common musical events.\n\n\n\n\nWe can start by loading our spotifyr library, and tidyverse for good measure:\n\nlibrary(spotifyr)\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.2     ✔ purrr   1.0.1\n✔ tibble  3.2.1     ✔ dplyr   1.1.1\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.4     ✔ forcats 1.0.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ purrr::%@%()         masks rlang::%@%()\n✖ dplyr::changes()     masks humdrumR::changes()\n✖ dplyr::count()       masks humdrumR::count()\n✖ tidyr::expand()      masks humdrumR::expand()\n✖ dplyr::filter()      masks stats::filter()\n✖ purrr::flatten()     masks rlang::flatten()\n✖ purrr::flatten_chr() masks rlang::flatten_chr()\n✖ purrr::flatten_dbl() masks rlang::flatten_dbl()\n✖ purrr::flatten_int() masks rlang::flatten_int()\n✖ purrr::flatten_lgl() masks rlang::flatten_lgl()\n✖ purrr::flatten_raw() masks rlang::flatten_raw()\n✖ humdrumR::int()      masks rlang::int()\n✖ purrr::invoke()      masks rlang::invoke()\n✖ dplyr::lag()         masks humdrumR::lag(), stats::lag()\n✖ dplyr::lead()        masks humdrumR::lead()\n✖ purrr::splice()      masks rlang::splice()\n✖ dplyr::symdiff()     masks bit::symdiff()\n✖ purrr::transpose()   masks humdrumR::transpose()\n\n\nYou will need your own spotify client ID and client secret. You can get them by filling out the brief online form here.\n\n### setting up spotify\nSys.setenv(SPOTIFY_CLIENT_ID = YOUR SPOTIFY CLIENT ID)\nSys.setenv(SPOTIFY_CLIENT_SECRET = YOUR SPOTIFY CLIENT SECRET)\naccess_token <- get_spotify_access_token()\n\n\n\n\nFor the most part, in this class we will be looking at global features data (the “danceability” of a song), and track-level analysis features, such as chroma vectors.\nHere we see how you might grab artist features for Ryan Adams and Taylor Swift, comparing the performances of each of their 1989 albums.\n\n###getting artist level data\nryan_adams <- get_artist_audio_features('ryan adams')\ntaylor_swift <- get_artist_audio_features('taylor swift')\n\n### cleaning up the data\nadams_swift <- rbind(ryan_adams, taylor_swift)\nadams_swift_1989 <- adams_swift %>% filter(album_name == \"1989\") \nadams_swift_1989$track_name <- tolower(adams_swift_1989$track_name)\n\n## comparing energy\nggplot(adams_swift_1989, aes(x=track_name, y=energy, group=artist_name)) +\n  geom_line(aes(linetype=artist_name, color=artist_name))+\n  geom_point(aes(color=artist_name))+\n  theme(legend.position=\"top\", axis.text.x = element_text(angle = 90, hjust = 1))"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Corpus Studies and Music",
    "section": "",
    "text": "Welcome!\nWelcome to the Corpus Studies and Music class."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is the class site for Corpus Studies in Music."
  },
  {
    "objectID": "course-schedule.html",
    "href": "course-schedule.html",
    "title": "Course Schedule",
    "section": "",
    "text": "Unit\nWeek\nDate (Date)\nTopic\nImportant Dates\n\n\n\n\nRepresenting, Searching, and Counting with Symbolic Data\n1\nT (3/28; Northwestern Monday)\nIntroductions; a history of corpus-based approaches to music.\n\n\n\n\n1\nW (3/29)\nRepresenting musical data (kern, MIDI, XML, MEI)\nAssignment #1 Due\n\n\n\n2\nM (4/3)\nPitches, melodic intervals and scale degrees\nResponse #1 Due\n\n\n\n2\nW (4/5)\nRepresenting Harmony\nAssignment #2 Due\n\n\n\n3\nM (4/10)\nRepresenting Rhythm and Meter\nResponse #2 Due\n\n\n\n3\nW (4/12)\nCharacteristic Features and TF-IDF\nAssignment #3 Due\n\n\n\n4\nM (4/17)\nn-grams and Transition Probabilities\nResponse #3 Due\n\n\n\n4\nW (4/19)\nFinding patterns\nAssignment #4 Due\n\n\nConceptual Debates\n5\nM (4/24)\nMeasurements of Similarity\nMidterm Literature Review Due\n\n\n\n5\nW (4/26)\nEntropy and Variability\nAssignment #5 Due\n\n\n\n6\nM (5/1)\nKey-Finding Algorithms\nResponse #4 Due\n\n\n\n6\nW (5/3)\nKey-Finding Algorithms (continued)\nAssignment #6 Due\n\n\n\n7\nM (5/8)\nFast Fourier Transforms\nResponse #5 Due\n\n\n\n7\nW (5/10)\nClustering and Authorship\nAssignment #7 Due\n\n\nAudio Data\n8\nM (5/15)\nPerformance Data/the Mazurka Project\nFirst Draft of Final Project Due\n\n\n\n8\nW (5/17)\nTempo flexibility, and other performance data\nAssignment #8 Due\n\n\n\n9\nM (5/22)\nThe Spotify API\nResponse #6 Due; Peer Reviews Due\n\n\n\n9\nW (5/24)\nThe Spotify API (continued)\nAssignment #9 Due\n\n\n\n10\nM (5/29)\nNo Classes (Memorial Day)\nFinal Presentations\n\n\n\n10\nW (5/31)\nFinal Presentations\nFinal Papers due on Friday, 6/2"
  },
  {
    "objectID": "course-syllabus.html",
    "href": "course-syllabus.html",
    "title": "Corpus Studies (Music 348/448) Syllabus",
    "section": "",
    "text": "Instructor: Dr. Daniel Shanahan\nContact: daniel.shanahan@northwestern.edu"
  },
  {
    "objectID": "course-syllabus.html#overview",
    "href": "course-syllabus.html#overview",
    "title": "Corpus Studies (Music 348/448) Syllabus",
    "section": "Overview",
    "text": "Overview\nCorpus studies, or distant readings of multiple musical works, are often employed as a way of better understanding issues such as the relationships between pieces, authorship, trends over time, or differences and similarities between genres. In this class, we will explore the techniques, history, and philosophy of such approaches, and will construct and analyze our own corpora. For the most part, this class will deal with notated scores, and students will be encouraged to ask their own research questions of the music that they are most interested in."
  },
  {
    "objectID": "course-syllabus.html#course-info",
    "href": "course-syllabus.html#course-info",
    "title": "Corpus Studies (Music 348/448) Syllabus",
    "section": "Course info",
    "text": "Course info\n\n\n\n\n\n\n\n\n\n\nDay\nTime\nLocation\n\n\n\n\nMeeting Times:\nMon & Weds\n12:30pm - 1:50 pm\nRCMA 1-164\n\n\nOffice Hours\nTBD (and by appointment)\nTBD (and by appointment)\nRCMA 4-181"
  },
  {
    "objectID": "course-syllabus.html#learning-objectives",
    "href": "course-syllabus.html#learning-objectives",
    "title": "Corpus Studies (Music 348/448) Syllabus",
    "section": "Learning objectives",
    "text": "Learning objectives\nBy the end of the quarter, you will…\n\nhave an understanding of how music has been examined through distant readings of scores and recordings\nbe able to explore how the concepts of concordances, schemata, key-finding, clustering, and introductory machine learning approaches can be applied to music analysis\nhave a working introductory knowledge of the R programming language and the HumDrumR package."
  },
  {
    "objectID": "course-syllabus.html#course-policies",
    "href": "course-syllabus.html#course-policies",
    "title": "Corpus Studies (Music 348/448) Syllabus",
    "section": "Course Policies",
    "text": "Course Policies\n\nAcademic Integrity\nStudents in this course are required to comply with the policies found in the booklet, “Academic Integrity at Northwestern University: A Basic Guide”. All papers submitted for credit in this course must be submitted electronically unless otherwise instructed by the professor. Your written work may be tested for plagiarized content. For details regarding academic integrity at Northwestern or to download the guide, visit this page.\n\n\nAccesibility\nNorthwestern University is committed to providing the most accessible learning environment as possible for students with disabilities. Should you anticipate or experience disability-related barriers in the academic setting, please contact AccessibleNU to move forward with the university’s established accommodation process (email: accessiblenu@northwestern.edu; p: 847-467-5530). If you already have established accommodations with AccessibleNU, please let me know as soon as possible, preferably within the first two weeks of the term, so we can work together to implement your disability accommodations. Disability information, including academic accommodations, is confidential under the Family Educational Rights and Privacy Act.\n\n\nCOVID-19 Classroom Expectations\nStudents, faculty and staff must comply with University expectations regarding appropriate classroom behavior, including those outlined below and in the COVID-19 Expectations for Students. With respect to classroom procedures, this includes:\nPolicies regarding masking, social distancing and other public health measures evolve as the situation changes. Students are responsible for understanding and complying with current University, state and city requirements. In some classes, masking and/or social distancing may be required as a result of an Americans with Disabilities Act (ADA) accommodation for the instructor or a student in the class even when not generally required on campus. In such cases, the instructor will notify the class.\nIf a student fails to comply with the COVID-19 Expectations for Students or other University expectations related to COVID-19, the instructor may ask the student to leave the class. The instructor is asked to report the incident to the Office of Community Standards for additional follow-up.\n\nIf you’re feeling sick…\nMaintaining the health of the community remains our priority. If you are experiencing any symptoms of COVID do not attend class. Follow the steps outlined on the NU sites for testing, isolation and reporting a positive case. Next, contact me as soon as possible to arrange to complete coursework.\nShould public health recommendations prevent in-person class from being held on a given day, I or the university will notify students.\n\n\n\nDiversity, Equity, and Inclusion\nThis course strives to be an inclusive learning community, respecting those of differing backgrounds and beliefs. As a community, we aim to be respectful to all students in this class, regardless of race, ethnicity, socio-economic status, religion, gender identity or sexual orientation."
  },
  {
    "objectID": "course-syllabus.html#textbooks",
    "href": "course-syllabus.html#textbooks",
    "title": "Corpus Studies (Music 348/448) Syllabus",
    "section": "Textbooks",
    "text": "Textbooks\nThere is no textbook for this course, and most of the materials will be available on Canvas or the course website (or both). Many of the readings will be taken from the forthcoming Oxford Handbook of Music and Corpus Studies, edited by Daniel Shanahan, Ashley Burgoyne, and Ian Quinn.\nHaving said that, you should sign up for a free account for Posit Cloud (formerly RStudio Cloud), where many of the class notebooks will be held.I would also recommend downloading R and RStudio onto your personal machine, if possible.\nAlthough not required, I would highly recommend having a look at:\n\nR for Data Science by Garret Grolemund and Hadley Wickham\nThe Humdrum User Guide\nThe music21 documentation\nThe Oxford Handbook of Music and Corpus Studies"
  },
  {
    "objectID": "course-syllabus.html#support-for-wellness-and-mental-health",
    "href": "course-syllabus.html#support-for-wellness-and-mental-health",
    "title": "Corpus Studies (Music 348/448) Syllabus",
    "section": "Support for Wellness and Mental Health",
    "text": "Support for Wellness and Mental Health\nNorthwestern University is committed to supporting the wellness of our students. Student Affairs has multiple resources to support student wellness and mental health. If you are feeling distressed or overwhelmed, please reach out for help. Students can access confidential resources through the Counseling and Psychological Services (CAPS), Religious and Spiritual Life (RSL) and the Center for Awareness, Response and Education (CARE). Additional information on all of the resources mentioned above can be found here:\nhttps://www.northwestern.edu/counseling/\nhttps://www.northwestern.edu/religious-life/\nhttps://www.northwestern.edu/care/\n\nHomework\nThere will be regular assignments in which you will be asked to respond to do one of the following:\n\nCritically reflect upon a reading about the history, methods, and dilemmas commonly found in corpus studies.\nWrite code that addresses a musical question (e.g. what’s the most common pitch transition in this group of pieces?)\nAnalyze a given collection of musical data.\n\nTypically, we will have reading reflections due on Mondays, and code-related questions relevant to those readings due on Wednesdays.\n\n\nMidterm Project\nThe goal of this class is for you to both understand corpus studies as a method with a long history, and for you to be able to incorporate these methods in your own research. There will be a midterm project that is primarily used a stepping stone into your final project, and it will consist of presenting a literature review in which you situate your own research question within the existing literature and propose a study that examines this question. You may use existing data, but you might find it more relevant to you if you use your own dataset. Therefore, this would be a good time to have a bulk of your data encoded, so that you are aware of the time needed to construct your corpus.\n\n\nFinal Project\nThe final project will be focused on a research question of your choosing, and will be broken up into several a peer-reviewed first draft, a presentation, and a final paper."
  },
  {
    "objectID": "course-syllabus.html#grading",
    "href": "course-syllabus.html#grading",
    "title": "Corpus Studies (Music 348/448) Syllabus",
    "section": "Grading",
    "text": "Grading\nThe final course grade will be calculated as follows:\n\n\n\nCategory\nPercentage\n\n\n\n\nReading Reflection Questions\n20%\n\n\nCode-focused assignments\n30%\n\n\nMidterm Literature Review\n15%\n\n\nFinal Project (First Draft)\n10%\n\n\nPresentation\n10%\n\n\nFinal Project (Final draft)\n15%\n\n\n\nThe final letter grade will be determined based on the following thresholds:\n\n\n\nLetter Grade\nFinal Course Grade\n\n\n\n\nA\n>= 93\n\n\nA-\n90 - 92.99\n\n\nB+\n87 - 89.99\n\n\nB\n83 - 86.99\n\n\nB-\n80 - 82.99\n\n\nC+\n77 - 79.99\n\n\nC\n73 - 76.99\n\n\nC-\n70 - 72.99\n\n\nD+\n67 - 69.99\n\n\nD\n63 - 66.99\n\n\nD-\n60 - 62.99\n\n\nF\n< 60"
  },
  {
    "objectID": "course-syllabus.html#schedule",
    "href": "course-syllabus.html#schedule",
    "title": "Corpus Studies (Music 348/448) Syllabus",
    "section": "Schedule",
    "text": "Schedule\n\n\n\n\n\n\n\n\n\nUnit\nWeek\nDate (Date)\nTopic\n\n\n\n\nRepresenting, Searching, and Counting with Symbolic Data\n1\nT (3/28; Northwestern Monday)\nIntroductions; a history of corpus-based approaches to music.\n\n\n\n1\nW (3/29)\nRepresenting musical data (kern, MIDI, XML, MEI)\n\n\n\n2\nM (4/3)\nPitches, melodic intervals and scale degrees\n\n\n\n2\nW (4/5)\nRepresenting Harmony\n\n\n\n3\nM (4/10)\nRepresenting Rhythm and Meter\n\n\n\n3\nW (4/12)\nCharacteristic Features and TF-IDF\n\n\n\n4\nM (4/17)\nn-grams and Transition Probabilities\n\n\n\n4\nW (4/19)\nFinding patterns\n\n\nConceptual Debates\n5\nM (4/24)\nMeasurements of Similarity\n\n\n\n5\nW (4/26)\nEntropy and Variability\n\n\n\n6\nM (5/1)\nKey-Finding Algorithms\n\n\n\n6\nW (5/3)\nKey-Finding Algorithms (continued)\n\n\n\n7\nM (5/8)\nFast Fourier Transforms\n\n\n\n7\nW (5/10)\nClustering and Authorship\n\n\nAudio Data\n8\nM (5/15)\nPerformance Data/the Mazurka Project\n\n\n\n8\nW (5/17)\nTempo flexibility, and other performance data\n\n\n\n9\nM (5/22)\nThe Spotify API\n\n\n\n9\nW (5/24)\nThe Spotify API (continued)\n\n\n\n10\nM (5/29)\nNo Classes (Memorial Day)\n\n\n\n10\nW (5/31)\nFinal Presentations"
  },
  {
    "objectID": "class_notes/week_6.html#john-or-paul",
    "href": "class_notes/week_6.html#john-or-paul",
    "title": "Week 6: Classifying",
    "section": "John or Paul?",
    "text": "John or Paul?\nOur research process will follow a simple trajectory:\n\nGet songs by each artist’s solo career (this can be our ‘ground truth’, as it were).\nTrain the model on these pieces, and evaluate the various models.\nApply the various models to some songs by the Beatles.\n\n\nGetting the Data\n\njohn <- get_artist_audio_features('john lennon')\npaul <- get_artist_audio_features('paul mccartney')\nboth <- rbind(john, paul)\n\nWhat is the balance of pieces like? It looks like we have far more McCartney than Lennon pieces. What does this mean for our model?\n\ntable(both$artist_name)\n\n\n   John Lennon Paul McCartney \n           422           1387 \n\n\nWe then can grab only the features that we want to explore for this model.\n\nboth_subset <- both %>% select(c(\"artist_name\", \"acousticness\", \"liveness\", \"danceability\", \"loudness\", \"speechiness\", \"valence\"))\n\nBefore running a clustering, PCA, or a classifier such as a k-nearest neighbor, it’s probably good to standardize your data. This means that the data is consistent, and prevents wide ranges from dominating the results. Here we’ve scaled all of our data with the z-score of the data according with the rest of the data for that category.\nI’ve also (temporarily) split the data from the artist, and then brought it all back together with cbind.\n\ndata <- both_subset[,-1]\nartists <- both_subset[,1]\ndata <- data %>% mutate_all(~(scale(.) %>% as.vector))\nboth_artists <- cbind(artists, data)"
  },
  {
    "objectID": "class_notes/week_6.html#cross-validation",
    "href": "class_notes/week_6.html#cross-validation",
    "title": "Week 6: Classifying",
    "section": "Cross-Validation",
    "text": "Cross-Validation\nCross-validation splits the data up into a testing and training set, and evaluates it.\n\nK-folds cross validation:\nK refers to the number of groups that data is split into.\n\nIt randomizes the data\nsplits it into the specified number of groups\nfor each group, split into a training and testing set, and then evaluate\n\n\nctrl <- trainControl(method = \"repeatedcv\", number = 2, savePredictions = TRUE)\n\n\nTrain <- createDataPartition(both_artists$artists, p=0.7, list=FALSE)\ntraining <- both_artists[ Train, ]\ntesting <- both_artists[ -Train, ]\n\nLet’s look at our results with a logistic regression:\n\nmod_fit <- train(artists ~ .,  data=both_artists, method=\"glm\", family=\"binomial\",\n                 trControl = ctrl, tuneLength = 10)\n\ntesting$artists <- as.factor(testing$artists)\npred <- predict(mod_fit, newdata=testing)\nconfusionMatrix(data=pred, testing$artists)\n\nConfusion Matrix and Statistics\n\n                Reference\nPrediction       John Lennon Paul McCartney\n  John Lennon             12              5\n  Paul McCartney         114            411\n                                          \n               Accuracy : 0.7804          \n                 95% CI : (0.7432, 0.8146)\n    No Information Rate : 0.7675          \n    P-Value [Acc > NIR] : 0.256           \n                                          \n                  Kappa : 0.1191          \n                                          \n Mcnemar's Test P-Value : <2e-16          \n                                          \n            Sensitivity : 0.09524         \n            Specificity : 0.98798         \n         Pos Pred Value : 0.70588         \n         Neg Pred Value : 0.78286         \n             Prevalence : 0.23247         \n         Detection Rate : 0.02214         \n   Detection Prevalence : 0.03137         \n      Balanced Accuracy : 0.54161         \n                                          \n       'Positive' Class : John Lennon     \n                                          \n\n\nIt looks like the accuracy is about 76%, but pay attention to the sensitivity and the specificity values.\nRecall that sensitivity is a measurement of how well the model can detect a “positive” instance, and specificity measures how well the model is finding true negatives.\nSensitivity can be defined as follows:\n\nSensitivity = (True Positive)/(True Positive + False Negative)\n\nand specificity can be defined as follows:\n\nSpecificity = (True Negative)/(True Negative + False Positive)\n\nSo this model is quite good at finding the negative class (here defined as McCartney), but not great at finding the positive class (Lennon)."
  },
  {
    "objectID": "class_notes/week_6.html#other-models",
    "href": "class_notes/week_6.html#other-models",
    "title": "Week 6: Classifying",
    "section": "Other Models",
    "text": "Other Models\nLet’s run the same code again, but now with a k-nearest neighbor. For our sanity, let’s put it into a function.\n\nmodel_evaluation <- function(method){\n    Train <- createDataPartition(both_artists$artists, p=0.7, list=FALSE)\n    training <- both_artists[ Train, ]\n    testing <- both_artists[ -Train, ]\n    mod_fit <- train(artists ~ .,  \n                     data=training, method=method)\n    pred <- predict(mod_fit, newdata=testing)\n\n    accuracy <- table(pred, testing[,\"artists\"])\n    sum(diag(accuracy))/sum(accuracy)\n    testing$artists <- as.factor(testing$artists)\n    confusionMatrix(data=pred, testing$artists)\n    \n}\nmodel_evaluation(\"kknn\")\n\nConfusion Matrix and Statistics\n\n                Reference\nPrediction       John Lennon Paul McCartney\n  John Lennon             77             44\n  Paul McCartney          49            372\n                                         \n               Accuracy : 0.8284         \n                 95% CI : (0.794, 0.8592)\n    No Information Rate : 0.7675         \n    P-Value [Acc > NIR] : 0.000326       \n                                         \n                  Kappa : 0.5124         \n                                         \n Mcnemar's Test P-Value : 0.678302       \n                                         \n            Sensitivity : 0.6111         \n            Specificity : 0.8942         \n         Pos Pred Value : 0.6364         \n         Neg Pred Value : 0.8836         \n             Prevalence : 0.2325         \n         Detection Rate : 0.1421         \n   Detection Prevalence : 0.2232         \n      Balanced Accuracy : 0.7527         \n                                         \n       'Positive' Class : John Lennon    \n                                         \n\n\nNote that it performs quite well! It’s better at finding the “John Lennon” model.\nWhy do we think this model performed better? A comparison of models can be found here.\n\nNeural Net\nA neural net doesn’t seem to do as well.\n\nmodel_evaluation(\"nnet\")"
  },
  {
    "objectID": "class_notes/week_6.html#comparing-models",
    "href": "class_notes/week_6.html#comparing-models",
    "title": "Week 6: Classifying",
    "section": "Comparing Models",
    "text": "Comparing Models\n\nLogistic Regression\nK-nearest neighbor\nneural net\nLearning Vector Quantization\ngradient boosted machine\nsupport vector machine\n\nWe can train different models explicitly (without a function) for now.\n\nset.seed(1234)\ncontrol <- trainControl(method=\"repeatedcv\", number=10, repeats=3)\n\n# train logistic regression\nmodelglm <- train(artists ~ ., data=both_artists, method=\"glm\", trControl=control)\n\n# train knn\nmodelknn <- train(artists ~ ., data=both_artists, method=\"kknn\", trControl=control)\n\n# train nnet\nmodelnnet <- train(artists ~ ., data=both_artists, method=\"nnet\", trControl=control)\n\n# train the LVQ model\nmodelLvq <- train(artists ~ ., data=both_artists, method=\"lvq\", trControl=control)\n\n# train the GBM model\nset.seed(7)\nmodelGbm <- train(artists ~ ., data=both_artists, method=\"gbm\", trControl=control)\n\n# train the SVM model\nset.seed(7)\nmodelSvm <- train(artists ~., data=both_artists, method=\"svmRadial\", trControl=control)\n\n# train the random forest\nrandomforest <- train(artists~., data=both_artists, method=\"ranger\", trControl=control)\n\nWe can actually look at the resampling of the dataset for each model, and get the results for each model:\n\n# collect resamples\nresults <- resamples(list(LVQ=modelLvq, GBM=modelGbm, SVM=modelSvm,knn=modelknn, nnet=modelnnet, glm=modelglm, rf=randomforest))\n\n# summarize the distributions\nsummary(results)\n\n\nCall:\nsummary.resamples(object = results)\n\nModels: LVQ, GBM, SVM, knn, nnet, glm, rf \nNumber of resamples: 30 \n\nAccuracy \n          Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nLVQ  0.6850829 0.7555556 0.7645488 0.7617583 0.7734807 0.7888889    0\nGBM  0.7845304 0.8088079 0.8254911 0.8229457 0.8395565 0.8508287    0\nSVM  0.7679558 0.7790055 0.7900552 0.7888529 0.7988950 0.8111111    0\nknn  0.7833333 0.8179300 0.8397790 0.8361661 0.8506215 0.8895028    0\nnnet 0.7403315 0.7725368 0.7873016 0.7936387 0.8099908 0.8500000    0\nglm  0.7458564 0.7572400 0.7624309 0.7658017 0.7734807 0.7900552    0\nrf   0.8111111 0.8613029 0.8729282 0.8697308 0.8893493 0.9060773    0\n\nKappa \n             Min.    1st Qu.     Median       Mean   3rd Qu.      Max. NA's\nLVQ  -0.009392427 0.02663193 0.07100195 0.07419685 0.1190372 0.1670796    0\nGBM   0.248634477 0.36345672 0.42324938 0.41069041 0.4661853 0.5403931    0\nSVM   0.071114370 0.15186812 0.19162277 0.19665202 0.2298367 0.3126685    0\nknn   0.389352818 0.47400876 0.52339123 0.52601569 0.5683450 0.6899623    0\nnnet  0.091489847 0.25010786 0.30093890 0.32095848 0.4082260 0.5212766    0\nglm   0.000000000 0.02688172 0.07086253 0.06823155 0.1020063 0.1595797    0\nrf    0.413793103 0.55728402 0.60804017 0.58941910 0.6453960 0.7041763    0\n\n\nIt might be better to look at the accuracy for each model. Here we have the accuracy rating as well as Cohen’s Kappa, which is like accuracy but also incorporates the imbalance of the dataset.\n\n# boxplots of results\nbwplot(results)\n\n\n\n\nHere’s another plot:\n\n# dot plots of results\ndotplot(results)\n\n\n\n\nIs it possible to use this for a research question??\nWhat if we use our neural net model but on a different dataset? How about the beatles dataset that is available on Spotify (which admittedly isn’t as much as we’d like).\n\nGrabbing Beatles Data\nWe can start by getting the data from Spotify:\n\nbeatles <- get_artist_audio_features('the beatles')\nbeatles_subset <- beatles %>% select(c(\"artist_name\", \"acousticness\", \"energy\", \"instrumentalness\", \"liveness\", \"danceability\", \"loudness\", \"speechiness\", \"valence\"))\n\n\n\nPredicting\nNow we can use the models that we’ve trained, but on new data. Here we use the random forest and the k-nearest neighbor models.\n\nbeatles_knn <- predict(modelknn, newdata=beatles_subset)\nbeatles_rf <- predict(randomforest, newdata=beatles_subset)\n\nNow, we are going to create a data frame of the track name, and both models.\n\nclassified_data <- as.data.frame(cbind(beatles_knn, beatles_rf, beatles$track_name))\n\nThis returns data as either 1 or 2, so we can clean up the columns a bit. Here, if it’s a 2, I label it as “Paul”, otherwise, it’s a “John”.\n\nclassified_data$beatles_knn <- if_else(classified_data$beatles_knn == 2, \"Paul\", \"John\")\nclassified_data$beatles_rf<- if_else(classified_data$beatles_rf == 2, \"Paul\", \"John\")\n\nWith the caret package, we can extract the probabilities of each guess. We can also add track info here as needed:\n\nprobabilities <- extractProb(list(modelnnet), unkX = beatles_subset)\ncolnames(classified_data)[3] <- \"track\"\nprobability_data <- cbind(probabilities[,1:2], classified_data)\nprobability_data %>% datatable(filter =\"top\")"
  },
  {
    "objectID": "class_notes/week_6.html#summary",
    "href": "class_notes/week_6.html#summary",
    "title": "Week 6: Classifying",
    "section": "Summary",
    "text": "Summary\nWhat I like about this is that we can take something about authorship that we know, and then use it to explore authorship of things that are a little more ambiguous. It can also teach us a fair bit about the specific models. Why do we think some performed so much better than others?"
  }
]