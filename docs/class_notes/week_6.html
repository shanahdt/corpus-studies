<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Corpus Studies in Music - Week 6: Classifying</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../class_notes/week_5.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

<script src="../site_libs/htmlwidgets-1.6.1/htmlwidgets.js"></script>
<link href="../site_libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet">
<script src="../site_libs/datatables-binding-0.27/datatables.js"></script>
<script src="../site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<link href="../site_libs/dt-core-1.12.1/css/jquery.dataTables.min.css" rel="stylesheet">
<link href="../site_libs/dt-core-1.12.1/css/jquery.dataTables.extra.css" rel="stylesheet">
<script src="../site_libs/dt-core-1.12.1/js/jquery.dataTables.min.js"></script>
<link href="../site_libs/nouislider-7.0.10/jquery.nouislider.min.css" rel="stylesheet">
<script src="../site_libs/nouislider-7.0.10/jquery.nouislider.min.js"></script>
<link href="../site_libs/selectize-0.12.0/selectize.bootstrap3.css" rel="stylesheet">
<script src="../site_libs/selectize-0.12.0/selectize.min.js"></script>
<link href="../site_libs/crosstalk-1.2.0/css/crosstalk.min.css" rel="stylesheet">
<script src="../site_libs/crosstalk-1.2.0/js/crosstalk.min.js"></script>


<meta property="og:title" content="Corpus Studies in Music - Week 6: Classifying">
<meta property="og:description" content="Today we are going to look at different models and evaluating models. Our research question will be training a model to decipher John Lennon songs from Paul McCartney songs with various classifiers.">
<meta property="og:site-name" content="Corpus Studies in Music">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title">Week 6: Classifying</h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Corpus Studies in Music</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle sidebar-tool" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">Resources</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../course-syllabus.html" class="sidebar-item-text sidebar-link">Corpus Studies (Music 348/448) Syllabus</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../course-schedule.html" class="sidebar-item-text sidebar-link">Course Schedule</a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">Class Notes</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../class_notes/week_1.html" class="sidebar-item-text sidebar-link">Week 1: Representing Musical Data</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../class_notes/week_2.html" class="sidebar-item-text sidebar-link">Week 2: Pitch</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../class_notes/week_3.html" class="sidebar-item-text sidebar-link">Week 3: Time</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../class_notes/week_4.html" class="sidebar-item-text sidebar-link">Week 4: Conceptual Debates: Key-Finding</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../class_notes/week_5.html" class="sidebar-item-text sidebar-link">Week 5: Regression and Clustering</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../class_notes/week_6.html" class="sidebar-item-text sidebar-link active">Week 6: Classifying</a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#this-weeks-plan" id="toc-this-weeks-plan" class="nav-link active" data-scroll-target="#this-weeks-plan">This Week’s Plan</a>
  <ul class="collapse">
  <li><a href="#getting-started" id="toc-getting-started" class="nav-link" data-scroll-target="#getting-started">Getting Started</a></li>
  <li><a href="#pca-and-authorship" id="toc-pca-and-authorship" class="nav-link" data-scroll-target="#pca-and-authorship">PCA and Authorship</a></li>
  <li><a href="#classifiers" id="toc-classifiers" class="nav-link" data-scroll-target="#classifiers">Classifiers</a></li>
  <li><a href="#returning-to-our-christmas-song-problem" id="toc-returning-to-our-christmas-song-problem" class="nav-link" data-scroll-target="#returning-to-our-christmas-song-problem">Returning to our Christmas Song Problem</a>
  <ul class="collapse">
  <li><a href="#what-is-the-model-using" id="toc-what-is-the-model-using" class="nav-link" data-scroll-target="#what-is-the-model-using">What is the model using?</a></li>
  </ul></li>
  <li><a href="#exercise" id="toc-exercise" class="nav-link" data-scroll-target="#exercise">Exercise</a></li>
  </ul></li>
  <li><a href="#wednesday" id="toc-wednesday" class="nav-link" data-scroll-target="#wednesday">Wednesday</a>
  <ul class="collapse">
  <li><a href="#john-or-paul" id="toc-john-or-paul" class="nav-link" data-scroll-target="#john-or-paul">John or Paul?</a>
  <ul class="collapse">
  <li><a href="#getting-the-data" id="toc-getting-the-data" class="nav-link" data-scroll-target="#getting-the-data">Getting the Data</a></li>
  </ul></li>
  <li><a href="#cross-validation" id="toc-cross-validation" class="nav-link" data-scroll-target="#cross-validation">Cross-Validation</a>
  <ul class="collapse">
  <li><a href="#k-folds-cross-validation" id="toc-k-folds-cross-validation" class="nav-link" data-scroll-target="#k-folds-cross-validation">K-folds cross validation:</a></li>
  </ul></li>
  <li><a href="#other-models" id="toc-other-models" class="nav-link" data-scroll-target="#other-models">Other Models</a>
  <ul class="collapse">
  <li><a href="#neural-net" id="toc-neural-net" class="nav-link" data-scroll-target="#neural-net">Neural Net</a></li>
  </ul></li>
  <li><a href="#comparing-models" id="toc-comparing-models" class="nav-link" data-scroll-target="#comparing-models">Comparing Models</a>
  <ul class="collapse">
  <li><a href="#grabbing-beatles-data" id="toc-grabbing-beatles-data" class="nav-link" data-scroll-target="#grabbing-beatles-data">Grabbing Beatles Data</a></li>
  <li><a href="#predicting" id="toc-predicting" class="nav-link" data-scroll-target="#predicting">Predicting</a></li>
  </ul></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary">Summary</a>
  <ul class="collapse">
  <li><a href="#exercise-1" id="toc-exercise-1" class="nav-link" data-scroll-target="#exercise-1">Exercise:</a></li>
  </ul></li>
  </ul></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/shanahdt/corpus-studies/edit/main/class_notes/week_6.qmd" class="toc-action">Edit this page</a></p><p><a href="https://github.com/shanahdt/corpus-studies/issues/new" class="toc-action">Report an issue</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block">Week 6: Classifying</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<section id="this-weeks-plan" class="level1">
<h1>This Week’s Plan</h1>
<ul>
<li>Look at running a principal components analysis for authorship</li>
<li>Work on some models for classifying data</li>
<li>Discuss how we might evaluate our models</li>
</ul>
<section id="getting-started" class="level2">
<h2 class="anchored" data-anchor-id="getting-started">Getting Started</h2>
<p>We will be using a of libraries today:</p>
<div class="cell" data-warnings="false">

</div>
<div class="cell" data-warnings="false">

</div>
</section>
<section id="pca-and-authorship" class="level2">
<h2 class="anchored" data-anchor-id="pca-and-authorship">PCA and Authorship</h2>
<p>PCAs are often used for reducing dimensions when we have lots of variables but a model might be better suited from combining those variables. PCAs have also been used a fair bit to explore questions of authorship. Here we have a question of authorship using symbolic data taken from scores. We are trying to explore the music of Josquin.</p>
<p>Here we load the data in:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>complete_data <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">"attribution_data_new.csv"</span>, <span class="at">na.strings=</span><span class="fu">c</span>(<span class="st">""</span>,<span class="st">"NA"</span>), <span class="at">header=</span>T)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>complete_data <span class="ot">&lt;-</span> complete_data[,<span class="sc">-</span><span class="dv">62</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Jesse Rodin’s <em>Josquin Research Project</em> has given levels of security for attribution, including pieces that we know are Josquin’s, those we think might be, and those which are more questionable.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Josquin attribution level 1 and palestrina</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>josquin <span class="ot">&lt;-</span> complete_data[complete_data<span class="sc">$</span>Composer <span class="sc">==</span> <span class="st">'Josquin des Prez'</span>,<span class="sc">-</span><span class="dv">12</span>]</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>josquin_secure <span class="ot">&lt;-</span> josquin[josquin<span class="sc">$</span>Attribution.Level <span class="sc">&lt;=</span> <span class="dv">2</span> ,]</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>josquin_secure<span class="sc">$</span>Composer <span class="ot">&lt;-</span> <span class="fu">as.character</span>(josquin_secure<span class="sc">$</span>Composer)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>josquin_less_secure <span class="ot">&lt;-</span> josquin[ josquin<span class="sc">$</span>Attribution.Level <span class="sc">&gt;=</span> <span class="dv">3</span>,]</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="do">####Other composers</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>bach <span class="ot">&lt;-</span> complete_data[complete_data<span class="sc">$</span>Composer <span class="sc">==</span> <span class="st">"Bach_Johann Sebastian"</span>,<span class="sc">-</span><span class="dv">12</span>]</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>larue <span class="ot">&lt;-</span> complete_data[complete_data<span class="sc">$</span>Composer <span class="sc">==</span> <span class="st">"la Rue_Pierre de"</span>,<span class="sc">-</span><span class="dv">12</span>]</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>palestrina <span class="ot">&lt;-</span> complete_data[complete_data<span class="sc">$</span>Composer <span class="sc">==</span> <span class="st">"Palestrina_Giovanni Perluigi da"</span>,<span class="sc">-</span><span class="dv">12</span>]</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>ockeghem <span class="ot">&lt;-</span> complete_data[complete_data<span class="sc">$</span>Composer <span class="sc">==</span> <span class="st">"Johannes Ockeghem"</span>,<span class="sc">-</span><span class="dv">12</span>]</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>orto <span class="ot">&lt;-</span> complete_data[complete_data<span class="sc">$</span>Composer <span class="sc">==</span> <span class="st">"de Orto_Marbrianus"</span>,<span class="sc">-</span><span class="dv">12</span>]</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>dufay <span class="ot">&lt;-</span> complete_data[complete_data<span class="sc">$</span>Composer <span class="sc">==</span> <span class="st">"Du Fay_Guillaume"</span>,<span class="sc">-</span><span class="dv">12</span>]</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>josquin_bach <span class="ot">&lt;-</span> <span class="fu">rbind</span>(josquin_secure, bach)</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>josquin_palestrina <span class="ot">&lt;-</span> <span class="fu">rbind</span>(josquin_secure, palestrina)</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>josquin_larue <span class="ot">&lt;-</span> <span class="fu">rbind</span>(josquin_secure, larue)</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>comparison <span class="ot">&lt;-</span> <span class="fu">rbind</span>(josquin_secure, bach)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-warnings="false">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>columns_wanted <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">5</span><span class="sc">:</span><span class="dv">11</span>)  </span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>Matrix <span class="ot">&lt;-</span> comparison[,columns_wanted]</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>Matrix <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(Matrix)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>Matrix[<span class="fu">is.na</span>(Matrix)] <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co"># log.pieces &lt;- log(Matrix)</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>log.pieces <span class="ot">&lt;-</span> <span class="fu">log</span>(Matrix)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning in log(Matrix): NaNs produced</code></pre>
</div>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>composer <span class="ot">&lt;-</span> comparison[,<span class="dv">1</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This code runs the actual principal components analysis.</p>
<p>It also provides a scree plot, allowing us to see which components are the most heavily weighted. This can allow us to reduce the dimensions as we see fit.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="do">####principle component analysis.</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>pieces.pca <span class="ot">&lt;-</span> <span class="fu">prcomp</span>(Matrix,</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>                 <span class="at">center =</span> <span class="cn">TRUE</span>,</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>                 <span class="at">scale. =</span> <span class="cn">TRUE</span>) </span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(pieces.pca, <span class="at">type =</span> <span class="st">"l"</span>, <span class="at">main=</span><span class="st">"Principal Components Analysis"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="week_6_files/figure-html/unnamed-chunk-6-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>It’s worth taking some time to explore what each of these components actually means and how they’re weighted. PCA is weighting instances of parallel motion and similar motion pretty heavily, but negatively weighting pitch entropy and oblique motion. PC2 seems to be looking at nPVI and 9-8 suspensions.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(pieces.pca)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Standard deviations (1, .., p=7):
[1] 1.8907847 0.9923828 0.8705046 0.8298104 0.7104739 0.5567648 0.4230672

Rotation (n x k) = (7 x 7):
                         PC1         PC2        PC3         PC4         PC5
nPVI_Entire       -0.2826479  0.52894566 -0.2336756  0.74429280 -0.17804201
Nine_Eight        -0.2553594  0.61806193  0.5670902 -0.41414816 -0.23123893
pitch_correlation -0.3244143  0.05847133 -0.7184471 -0.47933237 -0.36850440
pitch_entropy     -0.4038052  0.19724082 -0.1329848 -0.14687309  0.77362200
parallel_motion    0.4444947  0.24809410 -0.1263420 -0.08782873 -0.20277222
similar_motion     0.4682238  0.29107268 -0.1026235 -0.05294267  0.04450771
oblique_motion    -0.4120550 -0.38680631  0.2519115  0.11252328 -0.37073657
                           PC6          PC7
nPVI_Entire        0.006914729  0.001955825
Nine_Eight         0.076657435 -0.018255504
pitch_correlation  0.076213006  0.061699313
pitch_entropy     -0.387728754  0.099780551
parallel_motion   -0.750907432 -0.334991080
similar_motion     0.016773922  0.824891734
oblique_motion    -0.523249921  0.439584522</code></pre>
</div>
</div>
<p>As we can see, about 65% of the variance is accounted for with the first two principal components:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(pieces.pca)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Importance of components:
                          PC1    PC2    PC3     PC4     PC5     PC6     PC7
Standard deviation     1.8908 0.9924 0.8705 0.82981 0.71047 0.55676 0.42307
Proportion of Variance 0.5107 0.1407 0.1082 0.09837 0.07211 0.04428 0.02557
Cumulative Proportion  0.5107 0.6514 0.7597 0.85804 0.93015 0.97443 1.00000</code></pre>
</div>
</div>
<p>Plotting our two composers with the first two principal components.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>g <span class="ot">&lt;-</span> <span class="fu">ggbiplot</span>(pieces.pca, <span class="at">obs.scale =</span> <span class="dv">1</span>, <span class="at">var.scale =</span> <span class="dv">1</span>, </span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>              <span class="at">groups =</span> composer, <span class="at">ellipse =</span> <span class="cn">TRUE</span>, </span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>              <span class="at">circle =</span> <span class="cn">TRUE</span>)</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>g <span class="ot">&lt;-</span> g <span class="sc">+</span> <span class="fu">scale_color_discrete</span>(<span class="at">name =</span> <span class="st">''</span>)</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>g <span class="ot">&lt;-</span> g <span class="sc">+</span> <span class="fu">theme</span>(<span class="at">legend.direction =</span> <span class="st">'horizontal'</span>, </span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>               <span class="at">legend.position =</span> <span class="st">'top'</span>) <span class="sc">+</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>               <span class="fu">theme_bw</span>()</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(g)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="week_6_files/figure-html/unnamed-chunk-9-1.png" class="img-fluid" width="672"></p>
</div>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># we can change the number of components</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="co"># seven_component_model &lt;- data.frame(pieces.pca$x[,1:8])</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can also look at how much each of these features is being weighted within the first two components.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>theta <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>,<span class="dv">2</span><span class="sc">*</span>pi,<span class="at">length.out =</span> <span class="dv">100</span>)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>circle <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x =</span> <span class="fu">cos</span>(theta), <span class="at">y =</span> <span class="fu">sin</span>(theta))</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(circle,<span class="fu">aes</span>(x,y)) <span class="sc">+</span> <span class="fu">geom_path</span>()</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>loadings <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(pieces.pca<span class="sc">$</span>rotation, </span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>                       <span class="at">.names =</span> <span class="fu">row.names</span>(pieces.pca<span class="sc">$</span>rotation))</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>p <span class="sc">+</span> <span class="fu">geom_text</span>(<span class="at">data=</span>loadings, </span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>              <span class="at">mapping=</span><span class="fu">aes</span>(<span class="at">x =</span> PC1, <span class="at">y =</span> PC2, <span class="at">label =</span> .names, <span class="at">colour =</span> .names)) <span class="sc">+</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">coord_fixed</span>(<span class="at">ratio=</span><span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"PC1"</span>, <span class="at">y =</span> <span class="st">"PC2"</span>) <span class="sc">+</span></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_bw</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="week_6_files/figure-html/unnamed-chunk-10-1.png" class="img-fluid" width="672"></p>
</div>
</div>
</section>
<section id="classifiers" class="level2">
<h2 class="anchored" data-anchor-id="classifiers">Classifiers</h2>
<p>A classifier is a model that assigns a label to data based on the input. There are many types of classifiers, and we will be evaluating various models throughout the week.</p>
<p>Our goal will be to train a model on the features generally associated with a category, and then test the accuracy of that model. For now, a good starting point might be our Christmas Song question from last week.</p>
</section>
<section id="returning-to-our-christmas-song-problem" class="level2">
<h2 class="anchored" data-anchor-id="returning-to-our-christmas-song-problem">Returning to our Christmas Song Problem</h2>
<p>First, let’s get the data and add a column that tells us whether it’s a Christmas song or not</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="do">### get the data and add yes/no column.</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>christmas <span class="ot">&lt;-</span> <span class="fu">get_playlist_audio_features</span>(<span class="st">""</span>, <span class="st">"5OP7itTh52BMfZS1DJrdlv"</span>)</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>christmas<span class="sc">$</span>christmas <span class="ot">&lt;-</span> <span class="st">"yes"</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>not <span class="ot">&lt;-</span> <span class="fu">get_playlist_audio_features</span>(<span class="st">""</span>, <span class="st">"6i2Qd6OpeRBAzxfscNXeWp"</span>)</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>not<span class="sc">$</span>christmas <span class="ot">&lt;-</span> <span class="st">"no"</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a><span class="do">## combine the two datasets and get the columns we want to use.</span></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>christmas_subset <span class="ot">&lt;-</span><span class="fu">rbind</span>(christmas, not)</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>christmas_subset <span class="ot">&lt;-</span> christmas_subset <span class="sc">%&gt;%</span> </span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>    <span class="fu">select</span>(<span class="fu">c</span>(<span class="st">"christmas"</span>, <span class="st">"acousticness"</span>, <span class="st">"liveness"</span>, <span class="st">"danceability"</span>, <span class="st">"loudness"</span>, <span class="st">"speechiness"</span>, <span class="st">"valence"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now we can use the <code>createDataPartition</code> function from the <code>caret</code> library to create a testing and a training dataset. Here, I’ve chosen a 70/30 partition of training and testing, but you can adjust as you see fit.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>Train <span class="ot">&lt;-</span> <span class="fu">createDataPartition</span>(christmas_subset<span class="sc">$</span>christmas, <span class="at">p=</span><span class="fl">0.7</span>, <span class="at">list=</span><span class="cn">FALSE</span>)</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>training <span class="ot">&lt;-</span> christmas_subset[ Train, ]</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>testing <span class="ot">&lt;-</span> christmas_subset[ <span class="sc">-</span>Train, ]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can pretty easily implement something like a neural network, using our training dataset to train it:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>mod_fit <span class="ot">&lt;-</span> caret<span class="sc">::</span><span class="fu">train</span>(christmas <span class="sc">~</span> .,  </span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>                 <span class="at">data=</span>training, <span class="at">method=</span><span class="st">"nnet"</span>, <span class="at">importance =</span> <span class="st">"christmas"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Once we’ve trained this model, we can test it on our testing dataset, and see how well it does:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(mod_fit, testing)</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="fu">confusionMatrix</span>(pred, <span class="fu">as.factor</span>(testing<span class="sc">$</span>christmas), <span class="at">positive =</span> <span class="st">"yes"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Confusion Matrix and Statistics

          Reference
Prediction no yes
       no  25  14
       yes  9  16
                                         
               Accuracy : 0.6406         
                 95% CI : (0.511, 0.7568)
    No Information Rate : 0.5312         
    P-Value [Acc &gt; NIR] : 0.0509         
                                         
                  Kappa : 0.2713         
                                         
 Mcnemar's Test P-Value : 0.4042         
                                         
            Sensitivity : 0.5333         
            Specificity : 0.7353         
         Pos Pred Value : 0.6400         
         Neg Pred Value : 0.6410         
             Prevalence : 0.4688         
         Detection Rate : 0.2500         
   Detection Prevalence : 0.3906         
      Balanced Accuracy : 0.6343         
                                         
       'Positive' Class : yes            
                                         </code></pre>
</div>
</div>
<p>So what does this all mean? Let’s define some terms.</p>
<ul>
<li><strong>Accuracy:</strong>
<ul>
<li>the accuracy rate. Just how many things it got right.</li>
</ul></li>
<li><strong>95% CI:</strong>
<ul>
<li>the confidence interval of the accuracy.</li>
</ul></li>
<li><strong>No information rate:</strong>
<ul>
<li>given no more information other than the overall distribution, how likely are you to be correct if you just pick the “majority class.”</li>
<li>if you have an accuracy rate of 80%, but the majority class is 80%, then your model isn’t terribly useful.</li>
</ul></li>
<li><strong>P-Value:</strong>
<ul>
<li>likelihood of chance.</li>
</ul></li>
<li><strong>Kappa:</strong>
<ul>
<li>measures the agreement between two raters and ratings. Here it’s looking at the difference between observed accuracy and random chance given the distribution in the dataset.</li>
</ul></li>
<li><strong>McNemar’s Test P-Value:</strong>
<ul>
<li>this is looking at the two distributions (from a 2x2 table), and determines if they are significantly different,</li>
</ul></li>
<li><strong>Sensitivity:</strong>
<ul>
<li>given that a result is actually a thing, what is the probability that our model will predict that event’s results?</li>
</ul></li>
<li><strong>Specificity:</strong>
<ul>
<li>given that a result is <em>not</em> actually a thing, what is the probability that our model will predict that?</li>
</ul></li>
<li><strong>Pos Predictive Value:</strong>
<ul>
<li>the probability that a predicted ‘positive’ class is actually positive.</li>
</ul></li>
<li><strong>Neg Predictive Value:</strong>
<ul>
<li>the probability that a predicted ‘negative’ class is actually negative.</li>
</ul></li>
<li><strong>Prevalence:</strong>
<ul>
<li>the prevalence of the ‘positive event’</li>
</ul></li>
<li><strong>Detection Rate:</strong>
<ul>
<li>the rate of true events also predicted to be events</li>
</ul></li>
<li><strong>Detection Prevalence</strong>
<ul>
<li>the prevalence of predicted events</li>
</ul></li>
<li><strong>Balanced Accuracy:</strong>
<ul>
<li>the average of the proportion corrects of each class individually</li>
</ul></li>
</ul>
<section id="what-is-the-model-using" class="level3">
<h3 class="anchored" data-anchor-id="what-is-the-model-using">What is the model using?</h3>
<p>We can look at which features the model is using…</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">varImp</span>(mod_fit))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="week_6_files/figure-html/unnamed-chunk-15-1.png" class="img-fluid" width="672"></p>
</div>
</div>
</section>
</section>
<section id="exercise" class="level2">
<h2 class="anchored" data-anchor-id="exercise">Exercise</h2>
<ol type="1">
<li>Use PCA to explore the works of two artists. How well do they “separate”?</li>
<li>Run a classifier on two groups (it can be the same two artists, or two distinct groups). How well does your model do?</li>
</ol>
</section>
</section>
<section id="wednesday" class="level1">
<h1>Wednesday</h1>
<p>Today we are going to look at different models and evaluating models. Our research question will be training a model to decipher John Lennon songs from Paul McCartney songs with various classifiers.</p>
<section id="john-or-paul" class="level2">
<h2 class="anchored" data-anchor-id="john-or-paul">John or Paul?</h2>
<p>Our research process will follow a simple trajectory:</p>
<ul>
<li>Get songs by each artist’s solo career (this can be our ‘ground truth’, as it were).</li>
<li>Train the model on these pieces, and evaluate the various models.</li>
<li>Apply the various models to some songs by the Beatles.</li>
</ul>
<section id="getting-the-data" class="level3">
<h3 class="anchored" data-anchor-id="getting-the-data">Getting the Data</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>john <span class="ot">&lt;-</span> <span class="fu">get_artist_audio_features</span>(<span class="st">'john lennon'</span>)</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>paul <span class="ot">&lt;-</span> <span class="fu">get_artist_audio_features</span>(<span class="st">'paul mccartney'</span>)</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>both <span class="ot">&lt;-</span> <span class="fu">rbind</span>(john, paul)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>What is the balance of pieces like? It looks like we have far more McCartney than Lennon pieces. What does this mean for our model?</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(both<span class="sc">$</span>artist_name)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
   John Lennon Paul McCartney 
           422           1387 </code></pre>
</div>
</div>
<p>We then can grab only the features that we want to explore for this model.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>both_subset <span class="ot">&lt;-</span> both <span class="sc">%&gt;%</span> <span class="fu">select</span>(<span class="fu">c</span>(<span class="st">"artist_name"</span>, <span class="st">"acousticness"</span>, <span class="st">"liveness"</span>, <span class="st">"danceability"</span>, <span class="st">"loudness"</span>, <span class="st">"speechiness"</span>, <span class="st">"valence"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Before running a clustering, PCA, or a classifier such as a k-nearest neighbor, it’s probably good to standardize your data. This means that the data is consistent, and prevents wide ranges from dominating the results. Here we’ve scaled all of our data with the z-score of the data according with the rest of the data for that category.</p>
<p>I’ve also (temporarily) split the data from the artist, and then brought it all back together with cbind.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> both_subset[,<span class="sc">-</span><span class="dv">1</span>]</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>artists <span class="ot">&lt;-</span> both_subset[,<span class="dv">1</span>]</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> data <span class="sc">%&gt;%</span> <span class="fu">mutate_all</span>(<span class="sc">~</span>(<span class="fu">scale</span>(.) <span class="sc">%&gt;%</span> as.vector))</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>both_artists <span class="ot">&lt;-</span> <span class="fu">cbind</span>(artists, data)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="cross-validation" class="level2">
<h2 class="anchored" data-anchor-id="cross-validation">Cross-Validation</h2>
<p>Cross-validation splits the data up into a testing and training set, and evaluates it.</p>
<section id="k-folds-cross-validation" class="level3">
<h3 class="anchored" data-anchor-id="k-folds-cross-validation">K-folds cross validation:</h3>
<p>K refers to the number of groups that data is split into.</p>
<ul>
<li>It randomizes the data</li>
<li>splits it into the specified number of groups</li>
<li>for each group, split into a training and testing set, and then evaluate</li>
</ul>
<div class="cell">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>ctrl <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">"repeatedcv"</span>, <span class="at">number =</span> <span class="dv">2</span>, <span class="at">savePredictions =</span> <span class="cn">TRUE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>Train <span class="ot">&lt;-</span> <span class="fu">createDataPartition</span>(both_artists<span class="sc">$</span>artists, <span class="at">p=</span><span class="fl">0.7</span>, <span class="at">list=</span><span class="cn">FALSE</span>)</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>training <span class="ot">&lt;-</span> both_artists[ Train, ]</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>testing <span class="ot">&lt;-</span> both_artists[ <span class="sc">-</span>Train, ]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s look at our results with a logistic regression:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>mod_fit <span class="ot">&lt;-</span> <span class="fu">train</span>(artists <span class="sc">~</span> .,  <span class="at">data=</span>both_artists, <span class="at">method=</span><span class="st">"glm"</span>, <span class="at">family=</span><span class="st">"binomial"</span>,</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>                 <span class="at">trControl =</span> ctrl, <span class="at">tuneLength =</span> <span class="dv">10</span>)</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>testing<span class="sc">$</span>artists <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(testing<span class="sc">$</span>artists)</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(mod_fit, <span class="at">newdata=</span>testing)</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a><span class="fu">confusionMatrix</span>(<span class="at">data=</span>pred, testing<span class="sc">$</span>artists)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Confusion Matrix and Statistics

                Reference
Prediction       John Lennon Paul McCartney
  John Lennon             11             14
  Paul McCartney         115            402
                                          
               Accuracy : 0.762           
                 95% CI : (0.7238, 0.7973)
    No Information Rate : 0.7675          
    P-Value [Acc &gt; NIR] : 0.642           
                                          
                  Kappa : 0.0744          
                                          
 Mcnemar's Test P-Value : &lt;2e-16          
                                          
            Sensitivity : 0.08730         
            Specificity : 0.96635         
         Pos Pred Value : 0.44000         
         Neg Pred Value : 0.77756         
             Prevalence : 0.23247         
         Detection Rate : 0.02030         
   Detection Prevalence : 0.04613         
      Balanced Accuracy : 0.52682         
                                          
       'Positive' Class : John Lennon     
                                          </code></pre>
</div>
</div>
<p>It looks like the accuracy is about 76%, but pay attention to the <em>sensitivity</em> and the <em>specificity</em> values.</p>
<p>Recall that sensitivity is a measurement of how well the model can detect a “positive” instance, and specificity measures how well the model is finding true negatives.</p>
<p>Sensitivity can be defined as follows:</p>
<blockquote class="blockquote">
<p>Sensitivity = (True Positive)/(True Positive + False Negative)</p>
</blockquote>
<p>and specificity can be defined as follows:</p>
<blockquote class="blockquote">
<p>Specificity = (True Negative)/(True Negative + False Positive)</p>
</blockquote>
<p>So this model is quite good at finding the negative class (here defined as McCartney), but not great at finding the positive class (Lennon).</p>
</section>
</section>
<section id="other-models" class="level2">
<h2 class="anchored" data-anchor-id="other-models">Other Models</h2>
<p>Let’s run the same code again, but now with a k-nearest neighbor. For our sanity, let’s put it into a function.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>model_evaluation <span class="ot">&lt;-</span> <span class="cf">function</span>(method){</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>    Train <span class="ot">&lt;-</span> <span class="fu">createDataPartition</span>(both_artists<span class="sc">$</span>artists, <span class="at">p=</span><span class="fl">0.7</span>, <span class="at">list=</span><span class="cn">FALSE</span>)</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>    training <span class="ot">&lt;-</span> both_artists[ Train, ]</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>    testing <span class="ot">&lt;-</span> both_artists[ <span class="sc">-</span>Train, ]</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>    mod_fit <span class="ot">&lt;-</span> <span class="fu">train</span>(artists <span class="sc">~</span> .,  </span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>                     <span class="at">data=</span>training, <span class="at">method=</span>method)</span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>    pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(mod_fit, <span class="at">newdata=</span>testing)</span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a>    accuracy <span class="ot">&lt;-</span> <span class="fu">table</span>(pred, testing[,<span class="st">"artists"</span>])</span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a>    <span class="fu">sum</span>(<span class="fu">diag</span>(accuracy))<span class="sc">/</span><span class="fu">sum</span>(accuracy)</span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a>    testing<span class="sc">$</span>artists <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(testing<span class="sc">$</span>artists)</span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a>    <span class="fu">confusionMatrix</span>(<span class="at">data=</span>pred, testing<span class="sc">$</span>artists)</span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a><span class="fu">model_evaluation</span>(<span class="st">"kknn"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Confusion Matrix and Statistics

                Reference
Prediction       John Lennon Paul McCartney
  John Lennon             51             33
  Paul McCartney          75            383
                                          
               Accuracy : 0.8007          
                 95% CI : (0.7646, 0.8336)
    No Information Rate : 0.7675          
    P-Value [Acc &gt; NIR] : 0.03586         
                                          
                  Kappa : 0.3682          
                                          
 Mcnemar's Test P-Value : 7.972e-05       
                                          
            Sensitivity : 0.4048          
            Specificity : 0.9207          
         Pos Pred Value : 0.6071          
         Neg Pred Value : 0.8362          
             Prevalence : 0.2325          
         Detection Rate : 0.0941          
   Detection Prevalence : 0.1550          
      Balanced Accuracy : 0.6627          
                                          
       'Positive' Class : John Lennon     
                                          </code></pre>
</div>
</div>
<p>Note that it performs quite well! It’s better at finding the “John Lennon” model.</p>
<p>Why do we think this model performed better? A comparison of models can be found <a href="https://towardsdatascience.com/comparative-study-on-classic-machine-learning-algorithms-24f9ff6ab222">here</a>.</p>
<section id="neural-net" class="level3">
<h3 class="anchored" data-anchor-id="neural-net">Neural Net</h3>
<p>A neural net doesn’t seem to do as well.</p>
<div class="cell" data-messages="false">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="fu">model_evaluation</span>(<span class="st">"nnet"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="comparing-models" class="level2">
<h2 class="anchored" data-anchor-id="comparing-models">Comparing Models</h2>
<ul>
<li>Logistic Regression</li>
<li>K-nearest neighbor</li>
<li>neural net</li>
<li>Learning Vector Quantization</li>
<li>gradient boosted machine</li>
<li>support vector machine</li>
</ul>
<p>We can train different models explicitly (without a function) for now.</p>
<div class="cell" data-messages="false">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>control <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method=</span><span class="st">"repeatedcv"</span>, <span class="at">number=</span><span class="dv">10</span>, <span class="at">repeats=</span><span class="dv">3</span>)</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a><span class="co"># train logistic regression</span></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>modelglm <span class="ot">&lt;-</span> <span class="fu">train</span>(artists <span class="sc">~</span> ., <span class="at">data=</span>both_artists, <span class="at">method=</span><span class="st">"glm"</span>, <span class="at">trControl=</span>control)</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a><span class="co"># train knn</span></span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>modelknn <span class="ot">&lt;-</span> <span class="fu">train</span>(artists <span class="sc">~</span> ., <span class="at">data=</span>both_artists, <span class="at">method=</span><span class="st">"kknn"</span>, <span class="at">trControl=</span>control)</span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a><span class="co"># train nnet</span></span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a>modelnnet <span class="ot">&lt;-</span> <span class="fu">train</span>(artists <span class="sc">~</span> ., <span class="at">data=</span>both_artists, <span class="at">method=</span><span class="st">"nnet"</span>, <span class="at">trControl=</span>control)</span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a><span class="co"># train the LVQ model</span></span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a>modelLvq <span class="ot">&lt;-</span> <span class="fu">train</span>(artists <span class="sc">~</span> ., <span class="at">data=</span>both_artists, <span class="at">method=</span><span class="st">"lvq"</span>, <span class="at">trControl=</span>control)</span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-16"><a href="#cb32-16" aria-hidden="true" tabindex="-1"></a><span class="co"># train the GBM model</span></span>
<span id="cb32-17"><a href="#cb32-17" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">7</span>)</span>
<span id="cb32-18"><a href="#cb32-18" aria-hidden="true" tabindex="-1"></a>modelGbm <span class="ot">&lt;-</span> <span class="fu">train</span>(artists <span class="sc">~</span> ., <span class="at">data=</span>both_artists, <span class="at">method=</span><span class="st">"gbm"</span>, <span class="at">trControl=</span>control)</span>
<span id="cb32-19"><a href="#cb32-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-20"><a href="#cb32-20" aria-hidden="true" tabindex="-1"></a><span class="co"># train the SVM model</span></span>
<span id="cb32-21"><a href="#cb32-21" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">7</span>)</span>
<span id="cb32-22"><a href="#cb32-22" aria-hidden="true" tabindex="-1"></a>modelSvm <span class="ot">&lt;-</span> <span class="fu">train</span>(artists <span class="sc">~</span>., <span class="at">data=</span>both_artists, <span class="at">method=</span><span class="st">"svmRadial"</span>, <span class="at">trControl=</span>control)</span>
<span id="cb32-23"><a href="#cb32-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-24"><a href="#cb32-24" aria-hidden="true" tabindex="-1"></a><span class="co"># train the random forest</span></span>
<span id="cb32-25"><a href="#cb32-25" aria-hidden="true" tabindex="-1"></a>randomforest <span class="ot">&lt;-</span> <span class="fu">train</span>(artists<span class="sc">~</span>., <span class="at">data=</span>both_artists, <span class="at">method=</span><span class="st">"ranger"</span>, <span class="at">trControl=</span>control)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can actually look at the resampling of the dataset for each model, and get the results for each model:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="co"># collect resamples</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>results <span class="ot">&lt;-</span> <span class="fu">resamples</span>(<span class="fu">list</span>(<span class="at">LVQ=</span>modelLvq, <span class="at">GBM=</span>modelGbm, <span class="at">SVM=</span>modelSvm,<span class="at">knn=</span>modelknn, <span class="at">nnet=</span>modelnnet, <span class="at">glm=</span>modelglm, <span class="at">rf=</span>randomforest))</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a><span class="co"># summarize the distributions</span></span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(results)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
summary.resamples(object = results)

Models: LVQ, GBM, SVM, knn, nnet, glm, rf 
Number of resamples: 30 

Accuracy 
          Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's
LVQ  0.6850829 0.7555556 0.7645488 0.7617583 0.7734807 0.7888889    0
GBM  0.7845304 0.8088079 0.8254911 0.8229457 0.8395565 0.8508287    0
SVM  0.7679558 0.7790055 0.7900552 0.7888529 0.7988950 0.8111111    0
knn  0.7833333 0.8179300 0.8397790 0.8361661 0.8506215 0.8895028    0
nnet 0.7403315 0.7725368 0.7873016 0.7936387 0.8099908 0.8500000    0
glm  0.7458564 0.7572400 0.7624309 0.7658017 0.7734807 0.7900552    0
rf   0.8111111 0.8613029 0.8729282 0.8697308 0.8893493 0.9060773    0

Kappa 
             Min.    1st Qu.     Median       Mean   3rd Qu.      Max. NA's
LVQ  -0.009392427 0.02663193 0.07100195 0.07419685 0.1190372 0.1670796    0
GBM   0.248634477 0.36345672 0.42324938 0.41069041 0.4661853 0.5403931    0
SVM   0.071114370 0.15186812 0.19162277 0.19665202 0.2298367 0.3126685    0
knn   0.389352818 0.47400876 0.52339123 0.52601569 0.5683450 0.6899623    0
nnet  0.091489847 0.25010786 0.30093890 0.32095848 0.4082260 0.5212766    0
glm   0.000000000 0.02688172 0.07086253 0.06823155 0.1020063 0.1595797    0
rf    0.413793103 0.55728402 0.60804017 0.58941910 0.6453960 0.7041763    0</code></pre>
</div>
</div>
<p>It might be better to look at the accuracy for each model. Here we have the accuracy rating as well as Cohen’s Kappa, which is like accuracy but also incorporates the imbalance of the dataset.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="co"># boxplots of results</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a><span class="fu">bwplot</span>(results)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="week_6_files/figure-html/unnamed-chunk-27-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Here’s another plot:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="co"># dot plots of results</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a><span class="fu">dotplot</span>(results)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="week_6_files/figure-html/unnamed-chunk-28-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Is it possible to use this for a research question??</p>
<p>What if we use our neural net model but on a different dataset? How about the beatles dataset that is available on Spotify (which admittedly isn’t as much as we’d like).</p>
<section id="grabbing-beatles-data" class="level3">
<h3 class="anchored" data-anchor-id="grabbing-beatles-data">Grabbing Beatles Data</h3>
<p>We can start by getting the data from Spotify:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>beatles <span class="ot">&lt;-</span> <span class="fu">get_artist_audio_features</span>(<span class="st">'the beatles'</span>)</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>beatles_subset <span class="ot">&lt;-</span> beatles <span class="sc">%&gt;%</span> <span class="fu">select</span>(<span class="fu">c</span>(<span class="st">"artist_name"</span>, <span class="st">"acousticness"</span>, <span class="st">"energy"</span>, <span class="st">"instrumentalness"</span>, <span class="st">"liveness"</span>, <span class="st">"danceability"</span>, <span class="st">"loudness"</span>, <span class="st">"speechiness"</span>, <span class="st">"valence"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="predicting" class="level3">
<h3 class="anchored" data-anchor-id="predicting">Predicting</h3>
<p>Now we can use the models that we’ve trained, but on new data. Here we use the random forest and the k-nearest neighbor models.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>beatles_knn <span class="ot">&lt;-</span> <span class="fu">predict</span>(modelknn, <span class="at">newdata=</span>beatles_subset)</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>beatles_rf <span class="ot">&lt;-</span> <span class="fu">predict</span>(randomforest, <span class="at">newdata=</span>beatles_subset)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now, we are going to create a data frame of the track name, and both models.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>classified_data <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(<span class="fu">cbind</span>(beatles_knn, beatles_rf, beatles<span class="sc">$</span>track_name))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This returns data as either 1 or 2, so we can clean up the columns a bit. Here, if it’s a 2, I label it as “Paul”, otherwise, it’s a “John”.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>classified_data<span class="sc">$</span>beatles_knn <span class="ot">&lt;-</span> <span class="fu">if_else</span>(classified_data<span class="sc">$</span>beatles_knn <span class="sc">==</span> <span class="dv">2</span>, <span class="st">"Paul"</span>, <span class="st">"John"</span>)</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>classified_data<span class="sc">$</span>beatles_rf<span class="ot">&lt;-</span> <span class="fu">if_else</span>(classified_data<span class="sc">$</span>beatles_rf <span class="sc">==</span> <span class="dv">2</span>, <span class="st">"Paul"</span>, <span class="st">"John"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>With the <code>caret</code> package, we can extract the probabilities of each guess. We can also add track info here as needed:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>probabilities <span class="ot">&lt;-</span> <span class="fu">extractProb</span>(<span class="fu">list</span>(modelnnet), <span class="at">unkX =</span> beatles_subset)</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(classified_data)[<span class="dv">3</span>] <span class="ot">&lt;-</span> <span class="st">"track"</span></span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>probability_data <span class="ot">&lt;-</span> <span class="fu">cbind</span>(probabilities[,<span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>], classified_data)</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>probability_data <span class="sc">%&gt;%</span> <span class="fu">datatable</span>(<span class="at">filter =</span><span class="st">"top"</span>) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">

<div class="datatables html-widget html-fill-item-overflow-hidden html-fill-item" id="htmlwidget-bb60d335b193a27071cd" style="width:100%;height:auto;"></div>
<script type="application/json" data-for="htmlwidget-bb60d335b193a27071cd">{"x":{"filter":"top","vertical":false,"filterHTML":"<tr>\n  <td><\/td>\n  <td data-type=\"number\" style=\"vertical-align: top;\">\n    <div class=\"form-group has-feedback\" style=\"margin-bottom: auto;\">\n      <input type=\"search\" placeholder=\"All\" class=\"form-control\" style=\"width: 100%;\"/>\n      <span class=\"glyphicon glyphicon-remove-circle form-control-feedback\"><\/span>\n    <\/div>\n    <div style=\"display: none;position: absolute;width: 200px;opacity: 1\">\n      <div data-min=\"0.973276950295615\" data-max=\"0.973448164169905\" data-scale=\"15\"><\/div>\n      <span style=\"float: left;\"><\/span>\n      <span style=\"float: right;\"><\/span>\n    <\/div>\n  <\/td>\n  <td data-type=\"number\" style=\"vertical-align: top;\">\n    <div class=\"form-group has-feedback\" style=\"margin-bottom: auto;\">\n      <input type=\"search\" placeholder=\"All\" class=\"form-control\" style=\"width: 100%;\"/>\n      <span class=\"glyphicon glyphicon-remove-circle form-control-feedback\"><\/span>\n    <\/div>\n    <div style=\"display: none;position: absolute;width: 200px;opacity: 1\">\n      <div data-min=\"0.026551835830095\" data-max=\"0.026723049704385\" data-scale=\"15\"><\/div>\n      <span style=\"float: left;\"><\/span>\n      <span style=\"float: right;\"><\/span>\n    <\/div>\n  <\/td>\n  <td data-type=\"character\" style=\"vertical-align: top;\">\n    <div class=\"form-group has-feedback\" style=\"margin-bottom: auto;\">\n      <input type=\"search\" placeholder=\"All\" class=\"form-control\" style=\"width: 100%;\"/>\n      <span class=\"glyphicon glyphicon-remove-circle form-control-feedback\"><\/span>\n    <\/div>\n  <\/td>\n  <td data-type=\"character\" style=\"vertical-align: top;\">\n    <div class=\"form-group has-feedback\" style=\"margin-bottom: auto;\">\n      <input type=\"search\" placeholder=\"All\" class=\"form-control\" style=\"width: 100%;\" disabled=\"\"/>\n      <span class=\"glyphicon glyphicon-remove-circle form-control-feedback\"><\/span>\n    <\/div>\n  <\/td>\n  <td data-type=\"character\" style=\"vertical-align: top;\">\n    <div class=\"form-group has-feedback\" style=\"margin-bottom: auto;\">\n      <input type=\"search\" placeholder=\"All\" class=\"form-control\" style=\"width: 100%;\"/>\n      <span class=\"glyphicon glyphicon-remove-circle form-control-feedback\"><\/span>\n    <\/div>\n  <\/td>\n<\/tr>","data":[["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20","21","22","23","24","25","26","27","28","29","30","31","32","33","34","35","36","37","38","39","40","41","42","43","44","45","46","47","48","49","50","51","52","53","54","55","56","57","58","59","60","61","62","63","64","65","66","67","68","69","70","71","72","73","74","75","76","77","78","79","80","81","82","83","84","85","86","87","88","89","90","91","92","93","94","95","96","97","98","99","100","101","102","103","104","105","106","107","108","109","110","111","112","113","114","115","116","117","118","119","120","121","122","123","124","125","126","127","128","129","130","131","132","133","134","135","136","137","138","139","140","141","142","143","144","145","146","147","148","149","150","151","152","153","154","155","156","157","158","159","160","161","162","163","164","165","166","167","168","169","170","171","172","173","174","175","176","177","178","179","180","181","182","183","184","185","186","187","188","189","190","191","192","193","194","195","196","197","198","199","200","201","202","203","204","205","206","207","208","209","210","211","212","213","214","215","216","217","218","219","220","221","222","223","224","225","226","227","228","229","230","231","232","233","234","235","236","237","238","239","240","241","242","243","244","245","246","247","248","249","250","251","252","253","254","255","256","257","258","259","260","261","262","263","264","265","266","267","268","269","270","271","272","273","274","275","276","277","278","279","280","281","282","283","284","285","286","287","288","289","290","291","292","293","294","295","296","297","298","299","300","301","302","303","304","305","306","307","308","309","310","311","312","313","314","315","316","317","318","319","320","321","322","323","324","325","326","327","328","329","330","331","332","333","334","335","336","337","338","339","340","341","342","343","344","345","346","347","348","349","350","351","352","353","354","355","356","357","358","359","360","361","362","363","364","365","366","367","368","369","370","371","372","373","374","375","376","377","378","379","380","381","382","383","384","385","386","387","388","389","390","391","392","393","394","395","396","397","398","399","400","401","402","403","404","405","406","407","408","409","410","411","412","413","414","415","416","417","418","419","420","421","422","423","424","425","426","427","428","429","430","431","432","433","434","435","436","437","438","439","440","441","442","443","444","445","446","447","448","449","450","451","452","453","454","455","456","457","458","459","460","461","462","463","464","465","466","467","468","469","470","471","472","473","474","475","476","477","478","479","480","481","482","483","484","485","486","487","488","489","490","491","492","493","494","495","496","497","498","499","500","501","502","503","504","505","506","507","508","509","510","511","512","513","514","515","516","517","518","519","520","521","522","523","524","525","526","527","528","529","530","531","532","533","534","535","536","537","538","539","540","541","542","543","544","545","546","547","548","549","550","551","552","553","554","555","556","557","558","559","560","561","562","563","564","565","566","567","568","569","570","571","572","573","574","575","576","577","578","579","580","581","582","583","584","585","586","587","588","589","590","591","592","593","594","595","596","597","598","599","600","601","602","603","604","605","606","607","608","609","610","611","612","613","614","615","616","617","618","619","620","621","622","623","624","625","626","627","628","629","630","631","632","633","634","635","636","637","638","639","640","641","642","643","644","645","646","647","648","649","650","651","652","653","654","655","656","657","658","659","660","661","662","663","664","665","666","667","668","669","670","671","672","673","674","675","676","677","678","679","680","681","682","683","684","685","686","687","688","689","690","691","692","693","694","695","696","697","698","699","700","701","702","703","704","705","706","707","708","709","710","711","712","713","714","715","716","717","718","719","720","721","722","723","724","725","726","727","728","729","730","731","732","733","734","735","736","737","738","739","740","741","742","743","744","745","746","747","748","749","750","751","752","753","754","755","756","757","758","759","760","761","762","763","764","765","766","767","768","769","770","771","772","773","774","775","776","777","778","779","780","781","782","783","784","785","786","787","788","789","790","791","792","793","794","795","796","797","798","799","800","801"],[0.973448106984175,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973447926486801,0.973448164169904,0.973448164169904,0.973448164169904,0.973447999904194,0.973448057432745,0.973448164169904,0.973448021848769,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973447910427866,0.973447758462296,0.973447171507008,0.973443591675308,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973447871012255,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448089761549,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448131667055,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448080127552,0.973448164169904,0.973448164169904,0.973448090549696,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973442837388156,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973447765766599,0.973448164169904,0.973447955259214,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448101535738,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973400253235708,0.973436541016322,0.973276950295615,0.973447709734843,0.973436444495343,0.973446367714653,0.973448164169904,0.973447800049625,0.973448117157364,0.973448127753014,0.973448164169904,0.973448164169904,0.973448164169904,0.973448087867811,0.973448164169904,0.973448164169904,0.973448028075717,0.973447926259604,0.97344747194944,0.973446647504954,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973447375933799,0.973448164169904,0.973448164169904,0.973447817235104,0.973448164169904,0.973447781171847,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448134446783,0.973448164169904,0.973448164169904,0.973448164169904,0.973447371425411,0.973448164169904,0.973448164169904,0.973447785816634,0.973448164169904,0.973447805450138,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448100134435,0.973448164169904,0.973448164169904,0.973448164169904,0.97344654721771,0.973448164169904,0.973448164169904,0.973447334416693,0.973448164169904,0.973447243312822,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.97344803501936,0.973448038013193,0.973448164169904,0.973448112138082,0.973448164169904,0.973447914387155,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448033332251,0.97344809679268,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448061474801,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448125887288,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448084541414,0.973447998504386,0.973448164169904,0.973448164169904,0.973448164169904,0.973448077466629,0.973448095606852,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448058582602,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904,0.973448164169904],[0.0265518930158251,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265520735131988,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265520000958059,0.0265519425672549,0.0265518358300958,0.0265519781512311,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265520895721338,0.0265522415377039,0.0265528284929925,0.0265564083246924,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265521289877449,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265519102384507,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518683329454,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265519198724481,0.0265518358300958,0.0265518358300958,0.0265519094503041,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265571626118439,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.026552234233401,0.0265518358300958,0.0265520447407864,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518984642616,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265997467642921,0.0265634589836775,0.0267230497043847,0.0265522902651574,0.0265635555046566,0.026553632285347,0.0265518358300958,0.0265521999503749,0.0265518828426361,0.0265518722469861,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265519121321888,0.0265518358300958,0.0265518358300958,0.0265519719242835,0.026552073740396,0.0265525280505601,0.0265533524950459,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265526240662006,0.0265518358300958,0.0265518358300958,0.0265521827648963,0.0265518358300958,0.0265522188281531,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518655532168,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265526285745887,0.0265518358300958,0.0265518358300958,0.026552214183366,0.0265518358300958,0.0265521945498622,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518998655646,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265534527822897,0.0265518358300958,0.0265518358300958,0.0265526655833073,0.0265518358300958,0.0265527566871784,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265519649806396,0.026551961986807,0.0265518358300958,0.0265518878619178,0.0265518358300958,0.0265520856128448,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265519666677488,0.0265519032073198,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265519385251987,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518741127119,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265519154585856,0.0265520014956137,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265519225333714,0.0265519043931484,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265519414173978,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958,0.0265518358300958],["Paul","Paul","Paul","John","John","John","Paul","Paul","Paul","Paul","Paul","John","Paul","Paul","John","Paul","John","Paul","John","John","John","John","John","John","John","John","John","John","John","Paul","John","John","Paul","Paul","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","Paul","John","Paul","Paul","Paul","Paul","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","Paul","Paul","Paul","John","John","John","John","Paul","Paul","John","John","John","Paul","John","John","John","Paul","Paul","John","John","John","John","John","John","John","John","John","John","John","Paul","John","John","John","Paul","John","John","John","John","John","John","John","John","John","John","John","John","John","Paul","John","John","John","John","John","John","Paul","Paul","John","John","John","John","John","Paul","John","Paul","John","Paul","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","Paul","John","John","John","John","John","John","John","John","John","Paul","Paul","John","John","Paul","John","John","Paul","John","Paul","John","John","John","Paul","John","John","John","John","John","John","Paul","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","Paul","Paul","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","Paul","John","Paul","John","John","Paul","Paul","Paul","John","John","Paul","John","John","Paul","John","John","John","John","John","John","Paul","Paul","John","John","John","John","Paul","Paul","Paul","John","Paul","Paul","Paul","Paul","Paul","Paul","Paul","John","John","John","Paul","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","John","John","John","Paul","John","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","Paul","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","Paul","John","John","Paul","John","John","John","John","John","John","John","John","John","John","John","John","John","Paul","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","Paul","Paul","Paul","Paul","Paul","Paul","John","Paul","John","John","Paul","Paul","Paul","John","Paul","Paul","John","John","Paul","John","John","John","John","Paul","John","Paul","John","John","John","John","John","Paul","Paul","Paul","Paul","Paul","Paul","John","Paul","John","John","Paul","Paul","Paul","John","Paul","Paul","John","John","Paul","John","John","John","John","Paul","John","Paul","John","John","John","John","John","Paul","John","John","John","Paul","John","John","John","John","John","John","John","Paul","John","Paul","Paul","Paul","Paul","Paul","John","Paul","John","John","Paul","Paul","Paul","John","John","John","John","Paul","Paul","John","Paul","John","Paul","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","Paul","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","John","Paul","Paul","Paul","John","John","John","Paul","Paul","John","John","John","John","John","John","John","John","John","John","John","John","John","John","Paul","Paul","John","John","Paul","Paul","Paul","John","John","Paul","Paul","John","John","Paul","John","Paul","Paul","Paul","John","John","Paul","Paul","John","John","John","Paul","Paul","Paul","Paul","Paul","John","Paul","John","Paul","Paul","Paul","Paul","John","Paul","Paul","Paul","John","John","John","John","John","John","John","John","John","John","Paul","John","John","John","John","John","John","John","John","John","John","Paul","John","John","John","John","John","John"],["Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul","Paul"],["Taxman - 2022 Mix","Eleanor Rigby - 2022 Mix","I'm Only Sleeping - 2022 Mix","Love You To - 2022 Mix","Here, There And Everywhere - 2022 Mix","Yellow Submarine - 2022 Mix","She Said She Said - 2022 Mix","Good Day Sunshine - 2022 Mix","And Your Bird Can Sing - 2022 Mix","For No One - 2022 Mix","Doctor Robert - 2022 Mix","I Want To Tell You - 2022 Mix","Got To Get You Into My Life - 2022 Mix","Tomorrow Never Knows - 2022 Mix","Tomorrow Never Knows - Take 1","Tomorrow Never Knows - Mono Mix RM 11","Got To Get You Into My Life - First Version / Take 5","Got To Get You Into My Life - Second Version / Unnumbered Mix","Got To Get You Into My Life - Second Version / Take 8","Love You To - Take 1","Love You To - Unnumbered Rehearsal","Love You To - Take 7","Paperback Writer - Takes 1 &amp; 2 / Backing Track","Rain - Take 5 / Actual Speed","Rain - Take 5 / Slowed Down For Master Tape","Doctor Robert - Take 7","And Your Bird Can Sing - First Version / Take 2","And Your Bird Can Sing - First Version / Take 2 / Giggling","And Your Bird Can Sing - Second Version / Take 5","Taxman - Take 11","I’m Only Sleeping - Rehearsal Fragment","I’m Only Sleeping - Take 2","I’m Only Sleeping - Take 5","I’m Only Sleeping - Mono Mix RM1","Eleanor Rigby - Speech Before Take 2","Eleanor Rigby - Take 2","For No One - Take 10 / Backing Track","Yellow Submarine - Songwriting Work Tape / Part 1","Yellow Submarine - Songwriting Work Tape / Part 2","Yellow Submarine - Take 4 Before Sound Effects","Yellow Submarine - Highlighted Sound Effects","I Want To Tell You - Speech &amp; Take 4","Here, There And Everywhere - Take 6","She Said She Said - John's Demo","She Said She Said - Take 15 / Backing Track Rehearsal","Taxman - Mono","Eleanor Rigby - Mono","I'm Only Sleeping - Mono","Love You To - Mono","Here, There And Everywhere - Mono","Yellow Submarine - Mono","She Said She Said - Mono","Good Day Sunshine - Mono","And Your Bird Can Sing - Mono","For No One - Mono","Doctor Robert - Mono","I Want To Tell You - Mono","Got To Get You Into My Life - Mono","Tomorrow Never Knows - Mono","Paperback Writer - 2022 Stereo Mix","Rain - 2022 Stereo Mix","Paperback Writer - Mono","Rain - Mono","Get Back - Rooftop Performance / Take 1","Get Back - Rooftop Performance / Take 2","Don't Let Me Down - Rooftop Performance / Take 1","I've Got A Feeling - Rooftop Performance / Take 1","One After 909 - Rooftop Performance","Dig A Pony - Rooftop Performance","God Save The Queen - Rooftop Performance / Jam","I've Got A Feeling - Rooftop Performance / Take 2","Don't Let Me Down - Rooftop Performance / Take 2","Get Back - Rooftop Performance / Take 3","Two Of Us - 2021 Mix","Dig A Pony - 2021 Mix","Across The Universe - 2021 Mix","I Me Mine - 2021 Mix","Dig It - 2021 Mix","Let It Be - 2021 Mix","Maggie Mae - 2021 Mix","I've Got A Feeling - 2021 Mix","One After 909 - 2021 Mix","The Long And Winding Road - 2021 Mix","For You Blue - 2021 Mix","Get Back - 2021 Mix","Morning Camera / Two Of Us - Take 4","Maggie Mae / Fancy My Chances With You - Mono","Can You Dig It?","I Don't Know Why I'm Moaning - Speech / Mono","For You Blue - Take 4","Let It Be / Please Please Me / Let It Be - Take 10","I’ve Got A Feeling - Take 10","Dig A Pony - Take 14","Get Back - Take 19","Like Making An Album? - Speech","One After 909 - Take 3","Don’t Let Me Down - First Rooftop Performance","The Long And Winding Road - Take 19","Wake Up Little Susie / I Me Mine - Take 11","On The Day Shift Now / All Things Must Pass - Rehearsals / Mono","Concentrate On The Sound - Mono","Gimme Some Truth - Rehearsal / Mono","I Me Mine - Rehearsal / Mono","She Came In Through The Bathroom Window - Rehearsal","Polythene Pam - Rehearsal / Mono","Octopus’s Garden - Rehearsal / Mono","Oh! Darling - Jam","Get Back - Take 8","The Walk - Jam","Without A Song - Jam","Something - Rehearsal / Mono","Let It Be - Take 28","One After 909 - 1969 Glyn Johns Mix","Medley: I’m Ready (aka Rocker) / Save The Last Dance For Me / Don’t Let Me Down - 1969 Glyn Johns Mix","Don't Let Me Down - 1969 Glyn Johns Mix","Dig A Pony - 1969 Glyn Johns Mix","I've Got A Feeling - 1969 Glyn Johns Mix","Get Back - 1969 Glyn Johns Mix","For You Blue - 1969 Glyn Johns Mix","Teddy Boy - 1969 Glyn Johns Mix","Two Of Us - 1969 Glyn Johns Mix","Maggie Mae - 1969 Glyn Johns Mix","Dig It - 1969 Glyn Johns Mix","Let It Be - 1969 Glyn Johns Mix","The Long And Winding Road - 1969 Glyn Johns Mix","Get Back - Reprise / 1969 Glyn Johns Mix","Across The Universe - 1970 Glyn Johns Mix","I Me Mine - 1970 Glyn Johns Mix","Don’t Let Me Down - Single Version / 2021 Mix","Let It Be - Single Version / 2021 Mix","Come Together - 2019 Mix","Something - 2019 Mix","Maxwell's Silver Hammer - 2019 Mix","Oh! Darling - 2019 Mix","Octopus's Garden - 2019 Mix","I Want You (She's So Heavy) - 2019 Mix","Here Comes The Sun - 2019 Mix","Because - 2019 Mix","You Never Give Me Your Money - 2019 Mix","Sun King - 2019 Mix","Mean Mr Mustard - 2019 Mix","Polythene Pam - 2019 Mix","She Came In Through The Bathroom Window - 2019 Mix","Golden Slumbers - 2019 Mix","Carry That Weight - 2019 Mix","The End - 2019 Mix","Her Majesty - 2019 Mix","I Want You (She's So Heavy) - Trident Recording Session &amp; Reduction Mix","Goodbye - Home Demo","Something - Studio Demo","The Ballad Of John And Yoko - Take 7","Old Brown Shoe - Take 2","Oh! Darling - Take 4","Octopus's Garden - Take 9","You Never Give Me Your Money - Take 36","Her Majesty - Takes 1-3","Golden Slumbers / Carry That Weight - Takes 1-3 / Medley","Here Comes The Sun - Take 9","Maxwell's Silver Hammer - Take 12","Come Together - Take 5","The End - Take 3","Come And Get It - Studio Demo","Sun King - Take 20","Mean Mr. Mustard - Take 20","Polythene Pam - Take 27","She Came In Through The Bathroom Window - Take 27","Because - Take 1 / Instrumental","The Long One - Comprising of ‘You Never Give Me Your Money’, ’Sun King’/’Mean Mr Mustard’, ‘Her Majesty’, ‘Polythene Pam’/’She Came In Through The Bathroom Window’, ’Golden Slumbers’/ ’Carry That Weight’, ’The End’","Something - Take 39 / Instrumental / Strings Only","Golden Slumbers / Carry That Weight - Take 17 / Instrumental / Strings &amp; Brass Only","Back In The U.S.S.R. - 2018 Mix","Dear Prudence - 2018 Mix","Glass Onion - 2018 Mix","Ob-La-Di, Ob-La-Da - 2018 Mix","Wild Honey Pie - 2018 Mix","The Continuing Story Of Bungalow Bill - 2018 Mix","While My Guitar Gently Weeps - 2018 Mix","Happiness Is A Warm Gun - 2018 Mix","Martha My Dear - 2018 Mix","I'm So Tired - 2018 Mix","Blackbird - 2018 Mix","Piggies - 2018 Mix","Rocky Raccoon - 2018 Mix","Don't Pass Me By - 2018 Mix","Why Don't We Do It In The Road? - 2018 Mix","I Will - 2018 Mix","Julia - 2018 Mix","Birthday - 2018 Mix","Yer Blues - 2018 Mix","Mother Nature's Son - 2018 Mix","Everybody's Got Something To Hide Except Me And My Monkey - 2018 Mix","Sexy Sadie - 2018 Mix","Helter Skelter - 2018 Mix","Long, Long, Long - 2018 Mix","Revolution 1 - 2018 Mix","Honey Pie - 2018 Mix","Savoy Truffle - 2018 Mix","Cry Baby Cry - 2018 Mix","Revolution 9 - 2018 Mix","Good Night - 2018 Mix","Back In The U.S.S.R. - Esher Demo","Dear Prudence - Esher Demo","Glass Onion - Esher Demo","Ob-La-Di, Ob-La-Da - Esher Demo","The Continuing Story Of Bungalow Bill - Esher Demo","While My Guitar Gently Weeps - Esher Demo","Happiness Is A Warm Gun - Esher Demo","I'm So Tired - Esher Demo","Blackbird - Esher Demo","Piggies - Esher Demo","Rocky Raccoon - Esher Demo","Julia - Esher Demo","Yer Blues - Esher Demo","Mother Nature's Son - Esher Demo","Everybody's Got Something To Hide Except Me And My Monkey - Esher Demo","Sexy Sadie - Esher Demo","Revolution - Esher Demo","Honey Pie - Esher Demo","Cry Baby Cry - Esher Demo","Sour Milk Sea - Esher Demo","Junk - Esher Demo","Child Of Nature - Esher Demo","Circles - Esher Demo","Mean Mr Mustard - Esher Demo","Polythene Pam - Esher Demo","Not Guilty - Esher Demo","What’s The New Mary Jane - Esher Demo","Revolution 1 - Take 18","A Beginning (Take 4) / Don’t Pass Me By (Take 7)","Blackbird - Take 28","Everybody's Got Something To Hide Except Me And My Monkey - Unnumbered Rehearsal","Good Night - Unnumbered Rehearsal","Good Night - Take 10 With A Guitar Part From Take 5","Good Night - Take 22","Ob-La-Di, Ob-La-Da - Take 3","Revolution - Unnumbered Rehearsal","Revolution - Take 14 / Instrumental Backing Track","Cry Baby Cry - Unnumbered Rehearsal","Helter Skelter - First Version / Take 2","Sexy Sadie - Take 3","While My Guitar Gently Weeps - Acoustic Version / Take 2","Hey Jude - Take 1","St Louis Blues - Studio Jam","Not Guilty - Take 102","Mother Nature's Son - Take 15","Yer Blues - Take 5 With Guide Vocal","What’s The New Mary Jane - Take 1","Rocky Raccoon - Take 8","Back In The U.S.S.R. - Take 5 / Instrumental Backing Track","Dear Prudence - Vocal, Guitar &amp; Drums","Let It Be - Unnumbered Rehearsal","While My Guitar Gently Weeps - Third Version / Take 27","(You're So Square) Baby I Don’t Care - Studio Jam","Helter Skelter - Second Version / Take 17","Glass Onion - Take 10","I Will - Take 13","Blue Moon - Studio Jam","I Will - Take 29","Step Inside Love - Studio Jam","Los Paranoias - Studio Jam","Can You Take Me Back? - Take 1","Birthday - Take 2 / Instrumental Backing Track","Piggies - Take 12 / Instrumental Backing Track","Happiness Is A Warm Gun - Take 19","Honey Pie - Instrumental Backing Track","Savoy Truffle - Instrumental Backing Track","Martha My Dear - Without Brass And Strings","Long, Long, Long - Take 44","I'm So Tired - Take 7","I'm So Tired - Take 14","The Continuing Story Of Bungalow Bill - Take 2","Why Don’t We Do It In The Road? - Take 5","Julia - Two Rehearsals","The Inner Light - Take 6 / Instrumental Backing Track","Lady Madonna - Take 2 / Piano &amp; Drums","Lady Madonna - Backing Vocals From Take 3","Across The Universe - Take 6","Twist And Shout - Live / Remastered","She's A Woman - Live / Remastered","Dizzy Miss Lizzy - Live / Remastered","Ticket To Ride - Live / Remastered","Can't Buy Me Love - Live / Remastered","Things We Said Today - Live / Remastered","Roll Over Beethoven - Live / Remastered","Boys - Live / Remastered","A Hard Day's Night - Live / Remastered","Help! - Live / Remastered","All My Loving - Live / Remastered","She Loves You - Live / Remastered","Long Tall Sally - Live / Remastered","You Can't Do That - Live / Bonus Track","I Want To Hold Your Hand - Live / Bonus Track","Everybody’s Trying To Be My Baby - Live / Bonus Track","Baby's In Black - Live / Bonus Track","Get Back - Naked Version / Remastered 2013","Dig A Pony - Naked Version / Remastered 2013","For You Blue - Naked Version / Remastered 2013","The Long And Winding Road - Naked Version / Remastered 2013","Two Of Us - Naked Version / Remastered 2013","I've Got A Feeling - Naked Version / Remastered 2013","One After 909 - Naked Version / Remastered 2013","Don't Let Me Down - Naked Version / Remastered 2013","I Me Mine - Naked Version / Remastered 2013","Across The Universe - Naked Version / Remastered 2013","Let It Be - Naked Version / Remastered 2013","Yellow Submarine","Hey Bulldog","Eleanor Rigby","Love You To","All Together Now","Lucy In The Sky With Diamonds","Think For Yourself","Sgt Pepper's Lonely Hearts Club Band","With A Little Help From My Friends","Baby You're A Rich Man","Only A Northern Song","All You Need Is Love","When I'm Sixty Four","Nowhere Man","It's All Too Much","And Here We Are Again - Live At The BBC For \"Pop Go The Beatles\" / 23rd July, 1963","Words Of Love - Live At The BBC For \"Pop Go The Beatles\" / 20th August, 1963","How About It, Gorgeous? - Live At The BBC For \"Pop Go The Beatles\" / 30th July, 1963","Do You Want To Know A Secret - Live At The BBC For \"Pop Go The Beatles\" / 30th July, 1963","Lucille - Live At The BBC For \"Pop Go The Beatles\" / 17th September, 1963","Hey Paul…. - Live At The BBC For \"Pop Go The Beatles\" / 25th June, 1963","Anna (Go To Him) - Live At The BBC For \"Pop Go The Beatles\" / 27th August, 1963","Hello! - Live At The BBC For \"Pop Go The Beatles\" / 25th June, 1963","Please Please Me - Live At The BBC For \"Pop Go The Beatles\" / 13th August, 1963","Misery - Live At The BBC For \"Here We Go\" / 12th March, 1963","I'm Talking About You - Live At The BBC For \"Saturday Club\" / 16th March, 1963","A Real Treat - Live At The BBC For \"Pop Go The Beatles\" / 25th June, 1963","Boys - Live At The BBC For \"Pop Go The Beatles\" / 25th June, 1963","Absolutely Fab - Live At The BBC For \"Pop Go The Beatles\" / 25th June, 1963","Chains - Live At The BBC For \"Pop Go The Beatles\" / 25th June, 1963","Ask Me Why - Live At The BBC For \"Pop Go The Beatles\" / 24th September, 1963","Till There Was You - Live At The BBC For \"Pop Go The Beatles\" / 30th July, 1963","Lend Me Your Comb - Live At The BBC For \"Pop Go The Beatles\" / 16th July, 1963","Lower 5E - Live At The BBC For \"Pop Go The Beatles\" / 10th September, 1963","The Hippy Hippy Shake - Live At The BBC For \"Pop Go The Beatles\" / 10th September, 1963","Roll Over Beethoven - Live At The BBC For \"Pop Go The Beatles\" / 3rd September, 1963","There's A Place - Live At The BBC For \"Pop Go The Beatles\" / 3rd September, 1963","Bumper Bundle - Live At The BBC For \"Pop Go The Beatles\" / 25th June, 1963","P.S. I Love You - Live At The BBC For \"Pop Go The Beatles\" / 25th June, 1963","Please Mister Postman - Live At The BBC For \"Pop Go The Beatles\" / 30th July 1963","Beautiful Dreamer - Live At The BBC For \"Saturday Club\" / 26th January, 1963","Devil In Her Heart - Live At The BBC For \"Pop Go The Beatles\" / 25th September, 1963","The 49 Weeks - Live At The BBC For \"Pop Go The Beatles\" / 24th September, 1963","Sure To Fall (In Love With You) - Live At The BBC For \"Pop Go The Beatles\" / 24th September, 1963","Never Mind, Eh? - Live At The BBC For \"Pop Go The Beatles\" / 24th September, 1963","Twist And Shout - Live At The BBC For \"Pop Go The Beatles\" / 6th August, 1963","Bye, Bye - Live At The BBC For \"Pop Go The Beatles\" / 24th September, 1963","John - Pop Profile - Live At The BBC / 30th November, 1965","George - Pop Profile - Live At The BBC / 30th November, 1965","I Saw Her Standing There - Live At The BBC For \"Saturday Club\" / 5th October, 1963","Glad All Over - Live At The BBC For \"Saturday Club\" / 24th August, 1963","Lift Lid Again - Live At The BBC For \"Saturday Club\" / 24th August, 1963","I'll Get You - Live At The BBC For \"Saturday Club\" / 5th October, 1963","She Loves You - Live At The BBC For \"Saturday Club\" / 5th October, 1963","Memphis, Tennessee - Live At The BBC For \"Saturday Club\" / 5th October, 1963","Happy Birthday Dear Saturday Club - Live At The BBC For \"Saturday Club\" / 5th October, 1963","Now Hush, Hush - Live At The BBC For \"Easy Beat\" / 20th October, 1963","From Me To You - Live At The BBC For \"Easy Beat\" / 20th October, 1963","Money (That's What I Want) - Live At The BBC For \"The Beatles Say From Us To You\" / 26th December, 1963","I Want To Hold Your Hand - Live At The BBC For \"The Beatles Say From Us To You\" / 26th December, 1963","Brian Bathtubes - Live At The BBC For \"Saturday Club\" / 21st December, 1963","This Boy - Live At The BBC For \"Saturday Club\" / 21st December, 1963","If I Wasn’t In America - Live At The BBC For \"Saturday Club\" / 15th February, 1964","I Got A Woman - Live At The BBC For \"Saturday Club\" / 4th April, 1964","Long Tall Sally - Live At The BBC For \"Top Gear\" / 16th July, 1964","If I Fell - Live At The BBC For \"Top Gear\" / 16th July, 1964","A Hard Job Writing Them - Live At The BBC For \"Top Gear\" / 16th July, 1964","And I Love Her - Live At The BBC For \"Top Gear\" / 16th July, 1964","Oh, Can’t We? Yes We Can - Live At The BBC For \"From Us To You Say The Beatles\" / 30th March, 1964","You Can't Do That - Live At The BBC For \"Top Gear\" / 16th July, 1964","Honey Don't - Live At The BBC For \"Top Gear\" / 26th November, 1964","I'll Follow The Sun - Live At The BBC For \"Top Gear\" / 26th November, 1964","Green With Black Shutters - Live At The BBC / 1965","Kansas City / Hey-Hey-Hey-Hey! - Live At The BBC For \"Saturday Club\" / 26th December 1964","That’s What We’re Here For - Live At The BBC For \"Top Gear\" / 26th November, 1964","I Feel Fine (Studio Out-take) - Live At The BBC For \"Top Gear\" / 17th November, 1964","Paul - Pop Profile - Live At The BBC / 2nd May, 1966","Ringo - Pop Profile - Live At The BBC / 2nd May, 1966","Love Me Do - Mono / Remastered","From Me To You - Mono / Remastered","She Loves You - Mono / Remastered","I Want To Hold Your Hand - Remastered 2015","Can't Buy Me Love - Remastered 2015","A Hard Day's Night - Remastered 2015","I Feel Fine - Remastered 2015","Eight Days A Week - Remastered 2015","Ticket To Ride - Remastered 2015","Help! - Remastered 2015","Yesterday - Remastered 2015","Day Tripper - Remastered 2015","We Can Work It Out - Remastered 2015","Paperback Writer - Remastered 2015","Yellow Submarine - Remastered 2015","Eleanor Rigby - Remastered 2015","Penny Lane - Remastered 2015","All You Need Is Love - Remastered 2015","Hello, Goodbye - Remastered 2015","Lady Madonna - Remastered 2015","Hey Jude - Remastered 2015","Get Back - Remastered 2015","The Ballad Of John And Yoko - Remastered 2015","Something - Remastered 2015","Come Together - Remastered 2015","Let It Be - Remastered 2015","The Long And Winding Road - Remastered 2015","Beatles Greetings - Live At The BBC For \"The Public Ear\" / 3rd November, 1963","From Us To You - Live At The BBC / Opening Theme From \"From Us To You\" / 1964","Riding On A Bus - Live At The BBC For \"Top Gear\" / 26th November, 1964","I Got A Woman - Live At The BBC For \"Pop Go The Beatles\" / 13th August, 1963","Too Much Monkey Business - Live At The BBC For \"Pop Go The Beatles\" / 10th September, 1963","Keep Your Hands Off My Baby - Live At The BBC For \"Saturday Club\" / 26th January, 1963","I'll Be On My Way - Live At The BBC For \"Side By Side\" / 24th June, 1963","Young Blood - Live At The BBC For \"Pop Go The Beatles\" / 11th June, 1963","A Shot Of Rhythm And Blues - Live At The BBC For \"Pop Go The Beatles\" / 27th August, 1963","Sure To Fall (In Love With You) - Live At The BBC For \"Pop Go The Beatles\" / 18th June, 1963","Some Other Guy - Live At The BBC For \"Easy Beat\" / 23rd June, 1963","Thank You Girl - Live At The BBC For \"Easy Beat\" / 23rd June, 1963","Sha La La La La! - Live At The BBC For \"Pop Go The Beatles\" / 11th June, 1963","Baby It's You - Live At The BBC For \"Pop Go The Beatles\" / 11th June, 1963","That's All Right (Mama) - Live At The BBC For \"Pop Go The Beatles\" / 16th July, 1963","Carol - Live At The BBC For \"Pop Go The Beatles\" / 16th July, 1963","What Is It, George? - Live At The BBC For \"Pop Go The Beatles\" / 16th July, 1963","Soldier Of Love - Live At The BBC For \"Pop Go The Beatles\" / 16th July, 1963","A Little Rhyme - Live At The BBC For \"Pop Go The Beatles\" / 16th July, 1963","Clarabella - Live At The BBC For \"Pop Go The Beatles\" / 16th July, 1963","I'm Gonna Sit Right Down And Cry (Over You) - Live At The BBC For \"Pop Go The Beatles\" / 6th August, 1963","Crying, Waiting, Hoping - Live At The BBC For \"Pop Go The Beatles\" / 6th August, 1963","Dear Wack! - Live At The BBC For \"Saturday Club\" / 24th August, 1963","You Really Got A Hold On Me - Live At The BBC For \"Saturday Club\" / 24th August, 1963","To Know Her Is To Love Her - Live At The BBC For \"Pop Go The Beatles\" / 6th August, 1963","A Taste Of Honey - Live At The BBC For \"Pop Go The Beatles\" / 23rd July, 1963","Long Tall Sally - Live At The BBC For \"Pop Go The Beatles\" / 13th August, 1963","I Saw Her Standing There - Live At The BBC For \"Easy Beat\" / 20th October 1963","The Honeymoon Song - Live At The BBC For \"Pop Go The Beatles\" / 6th August, 1963","Johnny B Goode - Live At The BBC For \"Saturday Club\" / 15th February, 1964","Memphis, Tennessee - Live At The BBC For \"Pop Go The Beatles\" / 30th July, 1963","Lucille - Live At The BBC For \"Saturday Club\" / 5th October, 1963","Can't Buy Me Love - Live At The BBC For \"From Us To You Say The Beatles\" / 10th March, 1964","From Fluff To You - Live At The BBC For \"From Us To You Say The Beatles\" / 10th March, 1964","Till There Was You - Live At The BBC For \"From Us To You Say The Beatles\" / 10th March, 1964","Crinsk Dee Night - Live At The BBC For \"Top Gear\" / 16th July, 1964","A Hard Day's Night - Live At The BBC For \"Top Gear\" / 16th July, 1964","Ringo? Yep! - Live At The BBC For \"From Us To You Say The Beatles\" / 30th March, 1964","I Wanna Be Your Man - Live At The BBC For \"From Us To You Say The Beatles\" / 30th March, 1964","Just A Rumour - Live At The BBC For \"From Us To You Say The Beatles\" / 30th March, 1964","Roll Over Beethoven - Live At The BBC For \"From Us To You Say The Beatles\" / 30th March, 1964","All My Loving - Live At The BBC For \"From Us To You Say The Beatles\" / 30th March, 1964","Things We Said Today - Live At The BBC For \"Top Gear\" / 16th July, 1964","She's A Woman - Live At The BBC For \"Top Gear\" / 26th November, 1964","Sweet Little Sixteen - Live At The BBC For \"Pop Go The Beatles\" / 23rd July, 1963","1822! - Live At The BBC For \"Pop Go The Beatles\" / 23rd July, 1963","Lonesome Tears In My Eyes - Live At The BBC For \"Pop Go The Beatles\" / 23rd July, 1963","Nothin' Shakin' - Live At The BBC For \"Pop Go The Beatles\" / 23rd July, 1963","The Hippy Hippy Shake - Live At The BBC For \"Pop Go The Beatles\" / 30th July, 1963","Glad All Over - Live At The BBC For \"Pop Go The Beatles\" / 20th August, 1963","I Just Don't Understand - Live At The BBC For \"Pop Go The Beatles\" / 20th August, 1963","So How Come (No One Loves Me) - Live At The BBC For \"Pop Go The Beatles\" / 23rd July, 1963","I Feel Fine - Live At The BBC For \"Top Gear\" / 26th November, 1964","I'm A Loser - Live At The BBC For \"Top Gear\" / 26th November, 1964","Everybody's Trying To Be My Baby - Live At The BBC For \"Top Gear\" / 26th November, 1964","Rock and Roll Music - Live At The BBC For \"Saturday Club\" / 26th December, 1964","Ticket To Ride - Live At The BBC For \"The Beatles Invite You To Take A Ticket To Ride\" / 7th June, 1965","Dizzy Miss Lizzy - Live At The BBC For \"The Beatles Invite You To Take A Ticket To Ride\" / 7th June, 1965","Kansas City / Hey-Hey-Hey-Hey! - Live At The BBC For \"Pop Go The Beatles\" / 6th August, 1963","Set Fire To That Lot! - Live At The BBC For \"Pop Go The Beatles\" / 30th July, 1963","Matchbox - Live At The BBC For \"Pop Go The Beatles\" / 30th July, 1963","I Forgot To Remember To Forget - Live At The BBC For \"From Us To You Say The Beatles\" / 18th May, 1964","Love These Goon Shows! - Live At The BBC For \"Pop Go The Beatles\" / 11th June, 1963","I Got To Find My Baby - Live At The BBC For \"Pop Go The Beatles\" / 11th June, 1963","Ooh! My Soul - Live At The BBC For \"Pop Go The Beatles\" / 27th August, 1963","Ooh! My Arms - Live At The BBC For \"Pop Go The Beatles\" / 27th August, 1963","Don't Ever Change - Live At The BBC For \"Pop Go The Beatles\" / 27th August, 1963","Slow Down - Live At The BBC For \"Pop Go The Beatles\" / 20th August, 1963","Honey Don't - Live At The BBC For \"Pop Go The Beatles\" / 3rd September, 1963","Love Me Do - Live At The BBC For \"Pop Go The Beatles\" / 23rd July, 1963","From Us To You - Live At The BBC / Closing Theme From \"From Us To You\" / 1964","Two Of Us - Remastered 2009","Dig A Pony - Remastered 2009","Across The Universe - Remastered 2009","I Me Mine - Remastered 2009","Dig It - Remastered 2009","Let It Be - Remastered 2009","Maggie Mae - Remastered 2009","I've Got A Feeling - Remastered 2009","One After 909 - Remastered 2009","The Long And Winding Road - Remastered 2009","For You Blue - Remastered 2009","Get Back - Remastered 2009","Come Together - Remastered 2009","Something - Remastered 2009","Maxwell's Silver Hammer - Remastered 2009","Oh! Darling - Remastered 2009","Octopus's Garden - Remastered 2009","I Want You (She's So Heavy) - Remastered 2009","Here Comes The Sun - Remastered 2009","Because - Remastered 2009","You Never Give Me Your Money - Remastered 2009","Sun King - Remastered 2009","Mean Mr Mustard - Remastered 2009","Polythene Pam - Remastered 2009","She Came In Through The Bathroom Window - Remastered 2009","Golden Slumbers - Remastered 2009","Carry That Weight - Remastered 2009","The End - Remastered 2009","Her Majesty - Remastered 2009","Yellow Submarine - Remastered 2009","Only A Northern Song - Remastered 2009","All Together Now - Remastered 2009","Hey Bulldog - Remastered 2009","It's All Too Much - Remastered 2009","All You Need Is Love - Remastered 2009","Pepperland - Remastered 2009","Sea Of Time - Remastered 2009","Sea Of Holes - Remastered 2009","Sea Of Monsters - Remastered 2009","March Of The Meanies - Remastered 2009","Pepperland Laid Waste - Remastered 2009","Yellow Submarine In Pepperland - Remastered 2009","Back In The U.S.S.R. - Remastered 2009","Dear Prudence - Remastered 2009","Glass Onion - Remastered 2009","Ob-La-Di, Ob-La-Da - Remastered 2009","Wild Honey Pie - Remastered 2009","The Continuing Story Of Bungalow Bill - Remastered 2009","While My Guitar Gently Weeps - Remastered 2009","Happiness Is A Warm Gun - Remastered 2009","Martha My Dear - Remastered 2009","I'm So Tired - Remastered 2009","Blackbird - Remastered 2009","Piggies - Remastered 2009","Rocky Raccoon - Remastered 2009","Don't Pass Me By - Remastered 2009","Why Don't We Do It In The Road? - Remastered 2009","I Will - Remastered 2009","Julia - Remastered 2009","Birthday - Remastered 2009","Yer Blues - Remastered 2009","Mother Nature's Son - Remastered 2009","Everybody's Got Something To Hide Except Me And My Monkey - Remastered 2009","Sexy Sadie - Remastered 2009","Helter Skelter - Remastered 2009","Long, Long, Long - Remastered 2009","Revolution 1 - Remastered 2009","Honey Pie - Remastered 2009","Savoy Truffle - Remastered 2009","Cry Baby Cry - Remastered 2009","Revolution 9 - Remastered 2009","Good Night - Remastered 2009","Magical Mystery Tour - Remastered 2009","The Fool On The Hill - Remastered 2009","Flying - Remastered 2009","Blue Jay Way - Remastered 2009","Your Mother Should Know - Remastered 2009","I Am The Walrus - Remastered 2009","Hello, Goodbye - Remastered 2009","Strawberry Fields Forever - Remastered 2009","Penny Lane - Remastered 2009","Baby, You're A Rich Man - Remastered 2009","All You Need Is Love - Remastered 2009","Sgt. Pepper's Lonely Hearts Club Band - Remix","With A Little Help From My Friends - Remix","Lucy In The Sky With Diamonds - Remix","Getting Better - Remix","Fixing A Hole - Remix","She's Leaving Home - Remix","Being For The Benefit Of Mr. Kite! - Remix","Within You Without You - Remix","When I'm Sixty-Four - Remix","Lovely Rita - Remix","Good Morning Good Morning - Remix","Sgt. Pepper's Lonely Hearts Club Band (Reprise) - Remix","A Day In The Life - Remix","Sgt. Pepper's Lonely Hearts Club Band - Take 9 And Speech","With A Little Help From My Friends - Take 1 / False Start And Take 2 / Instrumental","Lucy In The Sky With Diamonds - Take 1","Getting Better - Take 1 / Instrumental And Speech At The End","Fixing A Hole - Speech And Take 3","She's Leaving Home - Take 1 / Instrumental","Being For The Benefit Of Mr. Kite! - Take 4","Within You Without You - Take 1 / Indian Instruments","When I'm Sixty-Four - Take 2","Lovely Rita - Speech And Take 9","Good Morning Good Morning - Take 8","Sgt. Pepper's Lonely Hearts Club Band (Reprise) - Speech And Take 8","A Day In The Life - Take 1 With Hums","Strawberry Fields Forever - Take 7","Strawberry Fields Forever - Take 26","Strawberry Fields Forever - Stereo Mix 2015","Penny Lane - Take 6 / Instrumental","Penny Lane - Stereo Mix 2017","Sgt. Pepper's Lonely Hearts Club Band - Remix","With A Little Help From My Friends - Remix","Lucy In The Sky With Diamonds - Remix","Getting Better - Remix","Fixing A Hole - Remix","She's Leaving Home - Remix","Being For The Benefit Of Mr. Kite! - Remix","Within You Without You - Remix","When I'm Sixty-Four - Remix","Lovely Rita - Remix","Good Morning Good Morning - Remix","Sgt. Pepper's Lonely Hearts Club Band (Reprise) - Remix","A Day In The Life - Remix","Sgt. Pepper's Lonely Hearts Club Band - Take 9 And Speech","With A Little Help From My Friends - Take 1 / False Start And Take 2 / Instrumental","Lucy In The Sky With Diamonds - Take 1","Getting Better - Take 1 / Instrumental And Speech At The End","Fixing A Hole - Speech And Take 3","She's Leaving Home - Take 1 / Instrumental","Being For The Benefit Of Mr. Kite! - Take 4","Within You Without You - Take 1 / Indian Instruments","When I'm Sixty-Four - Take 2","Lovely Rita - Speech And Take 9","Good Morning Good Morning - Take 8","Sgt. Pepper's Lonely Hearts Club Band (Reprise) - Speech And Take 8","A Day In The Life - Take 1 With Hums","Strawberry Fields Forever - Take 7","Strawberry Fields Forever - Take 26","Strawberry Fields Forever - Stereo Mix 2015","Penny Lane - Take 6 / Instrumental","Penny Lane - Stereo Mix 2017","Sgt. Pepper's Lonely Hearts Club Band - Remastered 2009","With A Little Help From My Friends - Remastered 2009","Lucy In The Sky With Diamonds - Remastered 2009","Getting Better - Remastered 2009","Fixing A Hole - Remastered 2009","She's Leaving Home - Remastered 2009","Being For The Benefit Of Mr. Kite! - Remastered 2009","Within You Without You - Remastered 2009","When I'm Sixty Four - Remastered 2009","Lovely Rita - Remastered 2009","Good Morning Good Morning - Remastered 2009","Sgt. Pepper's Lonely Hearts Club Band - Reprise / Remastered 2009","A Day In The Life - Remastered 2009","Sgt. Pepper's Lonely Hearts Club Band - Remix","With A Little Help From My Friends - Remix","Lucy In The Sky With Diamonds - Remix","Getting Better - Remix","Fixing A Hole - Remix","She's Leaving Home - Remix","Being For The Benefit Of Mr. Kite! - Remix","Within You Without You - Remix","When I'm Sixty-Four - Remix","Lovely Rita - Remix","Good Morning Good Morning - Remix","Sgt. Pepper's Lonely Hearts Club Band (Reprise) - Remix","A Day In The Life - Remix","Strawberry Fields Forever - Take 1","Strawberry Fields Forever - Take 4","Strawberry Fields Forever - Take 7","Strawberry Fields Forever - Take 26","Strawberry Fields Forever - Stereo Mix 2015","When I'm Sixty-Four - Take 2","Penny Lane - Take 6 / Instrumental","Penny Lane - Vocal Overdubs And Speech","Penny Lane - Stereo Mix 2017","A Day In The Life - Take 1","A Day In The Life - Take 2","A Day In The Life - Orchestra Overdub","A Day In The Life - Hummed Last Chord / Takes 8, 9, 10 And 11","A Day In The Life - The Last Chord","Sgt. Pepper's Lonely Hearts Club Band - Take 1 / Instrumental","Sgt. Pepper's Lonely Hearts Club Band - Take 9 And Speech","Good Morning Good Morning - Take 1 / Instrumental Breakdown","Good Morning Good Morning - Take 8","Fixing A Hole - Take 1","Fixing A Hole - Speech And Take 3","Being For The Benefit Of Mr. Kite! - Speech From Before Take 1 / Take 4 And Speech At End","Being For The Benefit Of Mr. Kite! - Take 7","Lovely Rita - Speech And Take 9","Lucy In The Sky With Diamonds - Take 1 And Speech At The End","Lucy In The Sky With Diamonds - Speech, False Start And Take 5","Getting Better - Take 1 / Instrumental And Speech At The End","Getting Better - Take 12","Within You Without You - Take 1 / Indian Instruments","Within You Without You - George Coaching The Musicians","She's Leaving Home - Take 1 / Instrumental","She's Leaving Home - Take 6 / Instrumental","With A Little Help From My Friends - Take 1 / False Start And Take 2 / Instrumental","Sgt. Pepper's Lonely Hearts Club Band (Reprise) - Speech And Take 8","Sgt Pepper's Lonely Hearts Club Band","With A Little Help From My Friends","Lucy In The Sky With Diamonds","Getting Better","Fixing A Hole","She's Leaving Home","Being For The Benefit Of Mr Kite!","Within You Without You","When I'm Sixty Four","Lovely Rita","Good Morning Good Morning","Sgt Pepper's Lonely Hearts Club Band (Reprise)","A Day In The Life","Strawberry Fields Forever","Penny Lane","A Day In The Life - First Mono Mix","Lucy In The Sky With Diamonds - Original Mono Mix - No. 11","She's Leaving Home - First Mono Mix","Penny Lane - Capitol Records Mono US Promo Mix","Taxman - Remastered 2009","Eleanor Rigby - Remastered 2009","I'm Only Sleeping - Remastered 2009","Love You To - Remastered 2009","Here, There And Everywhere - Remastered 2009","Yellow Submarine - Remastered 2009","She Said She Said - Remastered 2009","Good Day Sunshine - Remastered 2009","And Your Bird Can Sing - Remastered 2009","For No One - Remastered 2009","Doctor Robert - Remastered 2009","I Want To Tell You - Remastered 2009","Got To Get You Into My Life - Remastered 2009","Tomorrow Never Knows - Remastered 2009","Drive My Car - Remastered 2009","Norwegian Wood (This Bird Has Flown) - Remastered 2009","You Won't See Me - Remastered 2009","Nowhere Man - Remastered 2009","Think For Yourself - Remastered 2009","The Word - Remastered 2009","Michelle - Remastered 2009","What Goes On - Remastered 2009","Girl - Remastered 2009","I'm Looking Through You - Remastered 2009","In My Life - Remastered 2009","Wait - Remastered 2009","If I Needed Someone - Remastered 2009","Run For Your Life - Remastered 2009","Help! - Remastered 2009","The Night Before - Remastered 2009","You've Got To Hide Your Love Away - Remastered 2009","I Need You - Remastered 2009","Another Girl - Remastered 2009","You're Going To Lose That Girl - Remastered 2009","Ticket To Ride - Remastered 2009","Act Naturally - Remastered 2009","It's Only Love - Remastered 2009","You Like Me Too Much - Remastered 2009","Tell Me What You See - Remastered 2009","I've Just Seen A Face - Remastered 2009","Yesterday - Remastered 2009","Dizzy Miss Lizzy - Remastered 2009","No Reply - Remastered 2009","I'm A Loser - Remastered 2009","Baby's In Black - Remastered 2009","Rock And Roll Music - Remastered 2009","I'll Follow The Sun - Remastered 2009","Mr Moonlight - Remastered 2009","Kansas City / Hey-Hey-Hey-Hey - Medley / Remastered 2009","Eight Days A Week - Remastered 2009","Words Of Love - Remastered 2009","Honey Don't - Remastered 2009","Every Little Thing - Remastered 2009","I Don't Want To Spoil The Party - Remastered 2009","What You're Doing - Remastered 2009","Everybody's Trying To Be My Baby - Remastered 2009","A Hard Day's Night - Remastered 2009","I Should Have Known Better - Remastered 2009","If I Fell - Remastered 2009","I'm Happy Just To Dance With You - Remastered 2009","And I Love Her - Remastered 2009","Tell Me Why - Remastered 2009","Can't Buy Me Love - Remastered 2009","Any Time At All - Remastered 2009","I'll Cry Instead - Remastered 2009","Things We Said Today - Remastered 2009","When I Get Home - Remastered 2009","You Can't Do That - Remastered 2009","I'll Be Back - Remastered 2009","It Won't Be Long - Remastered 2009","All I've Got To Do - Remastered 2009","All My Loving - Remastered 2009","Don't Bother Me - Remastered 2009","Little Child - Remastered 2009","Till There Was You - Remastered 2009","Please Mister Postman - Remastered 2009","Roll Over Beethoven - Remastered 2009","Hold Me Tight - Remastered 2009","You Really Got A Hold On Me - Remastered 2009","I Wanna Be Your Man - Remastered 2009","Devil In Her Heart - Remastered 2009","Not A Second Time - Remastered 2009","Money (That's What I Want) - Remastered 2009","I Saw Her Standing There - Remastered 2009","Misery - Remastered 2009","Anna (Go To Him) - Remastered 2009","Chains - Remastered 2009","Boys - Remastered 2009","Ask Me Why - Remastered 2009","Please Please Me - Remastered 2009","Love Me Do - Remastered 2009","P.S. I Love You - Remastered 2009","Baby It's You - Remastered 2009","Do You Want To Know A Secret - Remastered 2009","A Taste Of Honey - Remastered 2009","There's A Place - Remastered 2009","Twist And Shout - Remastered 2009"]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>Paul.McCartney<\/th>\n      <th>John.Lennon<\/th>\n      <th>beatles_knn<\/th>\n      <th>beatles_rf<\/th>\n      <th>track<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"columnDefs":[{"className":"dt-right","targets":[1,2]},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false,"orderCellsTop":true},"selection":{"mode":"multiple","selected":null,"target":"row","selectable":null}},"evals":[],"jsHooks":[]}</script>
</div>
</div>
<p>(Note that this table doesn’t seem to be rendering correctly when pushed online).</p>
</section>
</section>
<section id="summary" class="level2">
<h2 class="anchored" data-anchor-id="summary">Summary</h2>
<p>What I like about this is that we can take something about authorship that we know, and then use it to explore authorship of things that are a little more ambiguous. It can also teach us a fair bit about the specific models. Why do we think some performed so much better than others?</p>
<section id="exercise-1" class="level3">
<h3 class="anchored" data-anchor-id="exercise-1">Exercise:</h3>
<p>Let’s try to build an east/west coast rap classifier:</p>
<p>Steps!:</p>
<ul>
<li>Grab data</li>
<li>partition and train model</li>
<li>compare models</li>
<li>use it to predict a new dataset.</li>
</ul>
<div class="cell">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>east_coast <span class="ot">&lt;-</span> <span class="fu">get_playlist_audio_features</span>(<span class="st">""</span>, <span class="st">"3pu8tsqTW52aUtYFZN3g4A"</span>)</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>east_coast<span class="sc">$</span>coast <span class="ot">&lt;-</span> <span class="st">"east"</span></span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>west_coast <span class="ot">&lt;-</span> <span class="fu">get_playlist_audio_features</span>(<span class="st">""</span>, <span class="st">"6lAOSVxxvGuEhPtZguaeav"</span>)</span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a>west_coast<span class="sc">$</span>coast <span class="ot">&lt;-</span> <span class="st">"west"</span></span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a>both <span class="ot">&lt;-</span> <span class="fu">rbind</span>(east_coast, west_coast)</span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a><span class="do">####standardize and clean a bit</span></span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a>both <span class="ot">&lt;-</span> both <span class="sc">%&gt;%</span> <span class="fu">select</span>(<span class="fu">c</span>(<span class="st">"coast"</span>, <span class="st">"acousticness"</span>, <span class="st">"energy"</span>, <span class="st">"instrumentalness"</span>, <span class="st">"liveness"</span>, <span class="st">"danceability"</span>, <span class="st">"loudness"</span>, <span class="st">"speechiness"</span>, <span class="st">"valence"</span>))</span>
<span id="cb42-10"><a href="#cb42-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-11"><a href="#cb42-11" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> both[,<span class="sc">-</span><span class="dv">1</span>]</span>
<span id="cb42-12"><a href="#cb42-12" aria-hidden="true" tabindex="-1"></a>coast <span class="ot">&lt;-</span> both[,<span class="dv">1</span>]</span>
<span id="cb42-13"><a href="#cb42-13" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> data <span class="sc">%&gt;%</span> <span class="fu">mutate_all</span>(<span class="sc">~</span>(<span class="fu">scale</span>(.) <span class="sc">%&gt;%</span> as.vector))</span>
<span id="cb42-14"><a href="#cb42-14" aria-hidden="true" tabindex="-1"></a>both <span class="ot">&lt;-</span> <span class="fu">cbind</span>(coast, data)</span>
<span id="cb42-15"><a href="#cb42-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-16"><a href="#cb42-16" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb42-17"><a href="#cb42-17" aria-hidden="true" tabindex="-1"></a>control <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method=</span><span class="st">"repeatedcv"</span>, <span class="at">number=</span><span class="dv">10</span>, <span class="at">repeats=</span><span class="dv">3</span>)</span>
<span id="cb42-18"><a href="#cb42-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-19"><a href="#cb42-19" aria-hidden="true" tabindex="-1"></a><span class="co"># train logistic regression</span></span>
<span id="cb42-20"><a href="#cb42-20" aria-hidden="true" tabindex="-1"></a>modelglm <span class="ot">&lt;-</span> <span class="fu">train</span>(coast <span class="sc">~</span> ., <span class="at">data=</span>both, <span class="at">method=</span><span class="st">"glm"</span>, <span class="at">trControl=</span>control)</span>
<span id="cb42-21"><a href="#cb42-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-22"><a href="#cb42-22" aria-hidden="true" tabindex="-1"></a><span class="co"># train knn</span></span>
<span id="cb42-23"><a href="#cb42-23" aria-hidden="true" tabindex="-1"></a>modelknn <span class="ot">&lt;-</span> <span class="fu">train</span>(coast <span class="sc">~</span> ., <span class="at">data=</span>both, <span class="at">method=</span><span class="st">"kknn"</span>, <span class="at">trControl=</span>control)</span>
<span id="cb42-24"><a href="#cb42-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-25"><a href="#cb42-25" aria-hidden="true" tabindex="-1"></a><span class="co"># train nnet</span></span>
<span id="cb42-26"><a href="#cb42-26" aria-hidden="true" tabindex="-1"></a>modelnnet <span class="ot">&lt;-</span> <span class="fu">train</span>(coast <span class="sc">~</span> ., <span class="at">data=</span>both, <span class="at">method=</span><span class="st">"nnet"</span>, <span class="at">trControl=</span>control)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># weights:  11
initial  value 179.713406 
iter  10 value 139.274270
iter  20 value 138.418159
iter  30 value 138.311368
final  value 138.294899 
converged
# weights:  31
initial  value 159.063233 
iter  10 value 136.332036
iter  20 value 126.730188
iter  30 value 119.138385
iter  40 value 114.350246
iter  50 value 110.525689
iter  60 value 109.317970
iter  70 value 109.034288
iter  80 value 109.025799
iter  90 value 109.024934
final  value 109.024850 
converged
# weights:  51
initial  value 180.830976 
iter  10 value 133.260420
iter  20 value 118.436490
iter  30 value 104.516811
iter  40 value 97.422974
iter  50 value 95.549203
iter  60 value 95.406004
iter  70 value 95.404046
final  value 95.404045 
converged
# weights:  11
initial  value 160.997220 
iter  10 value 140.103830
iter  20 value 139.649981
final  value 139.649000 
converged
# weights:  31
initial  value 183.358389 
iter  10 value 138.135160
iter  20 value 133.504773
iter  30 value 132.232569
iter  40 value 131.823810
iter  50 value 131.813630
iter  50 value 131.813629
iter  50 value 131.813629
final  value 131.813629 
converged
# weights:  51
initial  value 188.375733 
iter  10 value 137.715389
iter  20 value 133.857201
iter  30 value 127.299427
iter  40 value 122.821534
iter  50 value 122.276359
iter  60 value 122.234125
iter  70 value 122.231352
final  value 122.231336 
converged
# weights:  11
initial  value 184.044708 
iter  10 value 138.367892
iter  20 value 137.987598
iter  30 value 136.782436
iter  40 value 134.813322
iter  50 value 134.429955
iter  60 value 134.318182
iter  70 value 134.314470
iter  80 value 134.311792
iter  90 value 134.306794
iter 100 value 134.293605
final  value 134.293605 
stopped after 100 iterations
# weights:  31
initial  value 161.912438 
iter  10 value 138.527760
iter  20 value 132.896082
iter  30 value 120.564804
iter  40 value 116.472165
iter  50 value 114.106373
iter  60 value 113.931795
iter  70 value 113.703512
iter  80 value 113.583707
iter  90 value 113.522543
iter 100 value 113.334040
final  value 113.334040 
stopped after 100 iterations
# weights:  51
initial  value 163.530519 
iter  10 value 135.397425
iter  20 value 117.760201
iter  30 value 98.757667
iter  40 value 90.530530
iter  50 value 85.899673
iter  60 value 85.639340
iter  70 value 85.567108
iter  80 value 85.461082
iter  90 value 85.337051
iter 100 value 85.329269
final  value 85.329269 
stopped after 100 iterations
# weights:  11
initial  value 179.506932 
iter  10 value 137.970252
iter  20 value 137.646657
iter  30 value 137.290045
iter  40 value 135.628243
iter  50 value 134.060107
iter  60 value 134.054853
final  value 134.054593 
converged
# weights:  31
initial  value 162.801024 
iter  10 value 136.586035
iter  20 value 122.061440
iter  30 value 112.709036
iter  40 value 105.942660
iter  50 value 104.607989
iter  60 value 104.550472
final  value 104.550340 
converged
# weights:  51
initial  value 162.840044 
iter  10 value 133.804152
iter  20 value 122.210161
iter  30 value 107.034765
iter  40 value 97.298628
iter  50 value 95.311873
iter  60 value 95.178014
iter  70 value 95.175154
final  value 95.175133 
converged
# weights:  11
initial  value 172.786887 
iter  10 value 142.144810
iter  20 value 139.504127
final  value 139.499624 
converged
# weights:  31
initial  value 162.282873 
iter  10 value 138.859534
iter  20 value 136.843247
iter  30 value 135.628344
iter  40 value 135.496862
final  value 135.495788 
converged
# weights:  51
initial  value 159.568153 
iter  10 value 138.887708
iter  20 value 133.007936
iter  30 value 125.811940
iter  40 value 123.319577
iter  50 value 122.654103
iter  60 value 122.410055
iter  70 value 122.392654
final  value 122.391815 
converged
# weights:  11
initial  value 227.394362 
iter  10 value 145.684059
iter  20 value 141.780969
iter  30 value 138.889227
iter  40 value 138.764292
iter  50 value 138.719265
iter  60 value 136.671320
iter  70 value 136.099361
iter  80 value 136.067891
iter  90 value 136.060026
iter 100 value 136.055339
final  value 136.055339 
stopped after 100 iterations
# weights:  31
initial  value 163.796501 
iter  10 value 136.031707
iter  20 value 122.770172
iter  30 value 117.282544
iter  40 value 116.151263
iter  50 value 116.017031
iter  60 value 115.982232
iter  70 value 115.924025
iter  80 value 115.869099
iter  90 value 115.850331
iter 100 value 115.797430
final  value 115.797430 
stopped after 100 iterations
# weights:  51
initial  value 168.938205 
iter  10 value 134.137211
iter  20 value 115.865328
iter  30 value 102.191187
iter  40 value 98.907005
iter  50 value 95.491122
iter  60 value 95.082852
iter  70 value 94.871496
iter  80 value 94.699871
iter  90 value 94.283373
iter 100 value 94.098959
final  value 94.098959 
stopped after 100 iterations
# weights:  11
initial  value 209.149800 
iter  10 value 143.053952
iter  20 value 139.006978
iter  30 value 138.965950
iter  40 value 135.981395
iter  50 value 135.626680
iter  60 value 135.624187
iter  70 value 135.622937
iter  70 value 135.622937
final  value 135.622937 
converged
# weights:  31
initial  value 171.706598 
iter  10 value 137.283422
iter  20 value 131.926435
iter  30 value 122.643111
iter  40 value 118.227293
iter  50 value 116.775266
iter  60 value 111.365769
iter  70 value 110.602586
iter  80 value 110.502305
iter  90 value 110.427176
iter 100 value 110.112560
final  value 110.112560 
stopped after 100 iterations
# weights:  51
initial  value 197.280762 
iter  10 value 135.837539
iter  20 value 121.848493
iter  30 value 107.636327
iter  40 value 101.682352
iter  50 value 99.657649
iter  60 value 95.500436
iter  70 value 94.331351
iter  80 value 94.302994
iter  90 value 94.302073
final  value 94.302069 
converged
# weights:  11
initial  value 160.483879 
iter  10 value 142.714774
iter  20 value 141.077035
iter  30 value 140.882819
final  value 140.881232 
converged
# weights:  31
initial  value 166.601724 
iter  10 value 139.006468
iter  20 value 135.760186
iter  30 value 134.678546
iter  40 value 134.194038
iter  50 value 134.164016
final  value 134.163997 
converged
# weights:  51
initial  value 166.890655 
iter  10 value 140.817858
iter  20 value 136.191013
iter  30 value 132.232099
iter  40 value 131.138807
iter  50 value 129.909766
iter  60 value 129.352231
iter  70 value 129.169821
iter  80 value 129.159334
final  value 129.159301 
converged
# weights:  11
initial  value 178.516491 
iter  10 value 140.111926
iter  20 value 139.024288
iter  30 value 138.976948
iter  40 value 136.754672
iter  50 value 136.533183
iter  60 value 136.370358
iter  70 value 136.331510
iter  80 value 136.328674
iter  90 value 136.327950
iter 100 value 136.326126
final  value 136.326126 
stopped after 100 iterations
# weights:  31
initial  value 160.121464 
iter  10 value 136.538567
iter  20 value 126.361109
iter  30 value 118.613099
iter  40 value 116.316348
iter  50 value 115.910778
iter  60 value 115.852292
iter  70 value 115.754953
iter  80 value 115.731127
iter  90 value 115.697254
iter 100 value 115.678638
final  value 115.678638 
stopped after 100 iterations
# weights:  51
initial  value 165.226667 
iter  10 value 136.842955
iter  20 value 124.877308
iter  30 value 112.642368
iter  40 value 100.743099
iter  50 value 99.533481
iter  60 value 99.441176
iter  70 value 99.385135
iter  80 value 99.326497
iter  90 value 99.252275
iter 100 value 99.177754
final  value 99.177754 
stopped after 100 iterations
# weights:  11
initial  value 213.104004 
iter  10 value 140.199463
iter  20 value 139.576207
iter  30 value 137.322258
iter  40 value 135.241842
iter  50 value 134.322859
iter  60 value 134.269538
iter  70 value 134.261580
iter  80 value 134.258660
iter  90 value 134.064392
iter 100 value 133.670879
final  value 133.670879 
stopped after 100 iterations
# weights:  31
initial  value 161.865379 
iter  10 value 139.751517
iter  20 value 131.801229
iter  30 value 122.854584
iter  40 value 116.644115
iter  50 value 116.320947
iter  60 value 116.316760
final  value 116.316755 
converged
# weights:  51
initial  value 162.573078 
iter  10 value 131.870883
iter  20 value 112.170607
iter  30 value 100.847820
iter  40 value 95.371014
iter  50 value 89.976699
iter  60 value 89.195852
iter  70 value 89.051934
iter  80 value 88.946802
iter  90 value 88.736371
iter 100 value 88.425572
final  value 88.425572 
stopped after 100 iterations
# weights:  11
initial  value 191.374109 
iter  10 value 142.619027
iter  20 value 141.754410
final  value 141.752217 
converged
# weights:  31
initial  value 160.527288 
iter  10 value 142.278147
iter  20 value 139.405239
iter  30 value 136.654675
iter  40 value 135.677475
iter  50 value 135.247570
iter  60 value 135.193250
iter  70 value 134.989266
iter  80 value 134.589903
iter  90 value 134.577755
iter  90 value 134.577754
iter  90 value 134.577754
final  value 134.577754 
converged
# weights:  51
initial  value 168.921008 
iter  10 value 139.050284
iter  20 value 135.370327
iter  30 value 129.473189
iter  40 value 127.682591
iter  50 value 125.500818
iter  60 value 124.951181
iter  70 value 124.748899
iter  80 value 124.734874
final  value 124.734714 
converged
# weights:  11
initial  value 173.675872 
iter  10 value 142.732003
iter  20 value 139.426080
iter  30 value 135.432780
iter  40 value 135.169714
iter  50 value 135.142446
iter  60 value 135.103959
iter  70 value 135.088078
iter  80 value 135.079005
final  value 135.078999 
converged
# weights:  31
initial  value 210.396138 
iter  10 value 135.342801
iter  20 value 124.368749
iter  30 value 119.571781
iter  40 value 119.203721
iter  50 value 119.078813
iter  60 value 119.012281
iter  70 value 118.997014
iter  80 value 118.971654
iter  90 value 118.936224
iter 100 value 116.825862
final  value 116.825862 
stopped after 100 iterations
# weights:  51
initial  value 184.677163 
iter  10 value 134.100112
iter  20 value 118.392853
iter  30 value 104.115461
iter  40 value 98.682711
iter  50 value 96.908988
iter  60 value 96.071214
iter  70 value 95.574754
iter  80 value 95.242041
iter  90 value 94.816766
iter 100 value 94.757510
final  value 94.757510 
stopped after 100 iterations
# weights:  11
initial  value 172.278040 
iter  10 value 138.203789
iter  20 value 133.404888
iter  30 value 132.766527
iter  40 value 132.172883
iter  50 value 131.900267
iter  60 value 131.819210
iter  70 value 131.574998
iter  80 value 131.573080
iter  90 value 131.506629
iter 100 value 131.479077
final  value 131.479077 
stopped after 100 iterations
# weights:  31
initial  value 175.782901 
iter  10 value 130.165434
iter  20 value 118.981631
iter  30 value 109.802911
iter  40 value 106.231973
iter  50 value 103.878418
iter  60 value 103.782520
final  value 103.782330 
converged
# weights:  51
initial  value 158.969842 
iter  10 value 125.557857
iter  20 value 106.694682
iter  30 value 93.753454
iter  40 value 85.325967
iter  50 value 79.053747
iter  60 value 78.007393
iter  70 value 77.873553
iter  80 value 77.759841
iter  90 value 77.724905
iter 100 value 77.707215
final  value 77.707215 
stopped after 100 iterations
# weights:  11
initial  value 163.718986 
iter  10 value 135.759288
iter  20 value 135.096530
final  value 135.091882 
converged
# weights:  31
initial  value 162.804409 
iter  10 value 132.873090
iter  20 value 128.688435
iter  30 value 128.377188
iter  40 value 128.368205
final  value 128.368176 
converged
# weights:  51
initial  value 159.843967 
iter  10 value 131.070641
iter  20 value 123.919180
iter  30 value 117.729124
iter  40 value 116.201406
iter  50 value 115.729271
iter  60 value 115.689759
iter  70 value 115.687639
final  value 115.687630 
converged
# weights:  11
initial  value 227.548862 
iter  10 value 134.924409
iter  20 value 133.318694
iter  30 value 132.486381
iter  40 value 132.220948
iter  50 value 131.710555
iter  60 value 131.626297
iter  70 value 131.584002
iter  80 value 131.557200
iter  90 value 131.556912
iter  90 value 131.556911
iter  90 value 131.556911
final  value 131.556911 
converged
# weights:  31
initial  value 191.239772 
iter  10 value 129.081148
iter  20 value 120.608759
iter  30 value 115.350887
iter  40 value 112.488760
iter  50 value 111.840081
iter  60 value 111.295902
iter  70 value 111.177832
iter  80 value 111.059110
iter  90 value 110.825157
iter 100 value 110.519145
final  value 110.519145 
stopped after 100 iterations
# weights:  51
initial  value 165.458685 
iter  10 value 130.787917
iter  20 value 112.820324
iter  30 value 97.869772
iter  40 value 89.474738
iter  50 value 81.219272
iter  60 value 79.136512
iter  70 value 78.932527
iter  80 value 78.883020
iter  90 value 78.845330
iter 100 value 78.746223
final  value 78.746223 
stopped after 100 iterations
# weights:  11
initial  value 162.055938 
iter  10 value 140.589535
iter  20 value 140.072731
iter  30 value 139.934847
iter  40 value 139.897693
iter  50 value 139.888327
final  value 139.888302 
converged
# weights:  31
initial  value 177.576410 
iter  10 value 136.595687
iter  20 value 130.478324
iter  30 value 120.010050
iter  40 value 116.672710
iter  50 value 116.438731
final  value 116.438040 
converged
# weights:  51
initial  value 174.292085 
iter  10 value 137.714523
iter  20 value 125.522290
iter  30 value 107.084484
iter  40 value 95.305070
iter  50 value 91.426701
iter  60 value 91.105481
iter  70 value 91.098087
final  value 91.098030 
converged
# weights:  11
initial  value 211.564194 
iter  10 value 141.501721
iter  20 value 141.399229
final  value 141.398724 
converged
# weights:  31
initial  value 160.131240 
iter  10 value 140.891852
iter  20 value 137.304058
iter  30 value 134.735777
iter  40 value 133.688406
iter  50 value 133.588249
final  value 133.588185 
converged
# weights:  51
initial  value 171.501655 
iter  10 value 139.596755
iter  20 value 131.067752
iter  30 value 126.421477
iter  40 value 126.057469
iter  50 value 125.089448
iter  60 value 124.977675
iter  70 value 124.956850
final  value 124.956412 
converged
# weights:  11
initial  value 170.905104 
iter  10 value 140.241009
iter  20 value 140.059992
iter  30 value 139.973805
iter  40 value 139.896534
iter  50 value 139.891493
iter  60 value 139.890571
final  value 139.890465 
converged
# weights:  31
initial  value 169.542161 
iter  10 value 136.542952
iter  20 value 126.030673
iter  30 value 118.378674
iter  40 value 115.065697
iter  50 value 114.963238
iter  60 value 114.930833
iter  70 value 114.914701
iter  80 value 114.884295
iter  90 value 114.766143
iter 100 value 114.725576
final  value 114.725576 
stopped after 100 iterations
# weights:  51
initial  value 184.485265 
iter  10 value 138.103760
iter  20 value 119.882299
iter  30 value 103.685543
iter  40 value 95.462899
iter  50 value 91.187148
iter  60 value 90.780603
iter  70 value 90.681210
iter  80 value 90.518048
iter  90 value 90.398916
iter 100 value 90.297934
final  value 90.297934 
stopped after 100 iterations
# weights:  11
initial  value 171.592321 
iter  10 value 140.781938
iter  20 value 140.422259
iter  30 value 140.289618
iter  40 value 140.131506
iter  50 value 140.103460
iter  60 value 138.035852
final  value 137.724228 
converged
# weights:  31
initial  value 164.460259 
iter  10 value 140.543957
iter  20 value 136.226058
iter  30 value 128.977413
iter  40 value 122.608601
iter  50 value 116.566615
iter  60 value 114.686383
iter  70 value 114.184220
iter  80 value 113.881354
iter  90 value 113.695505
iter 100 value 113.668043
final  value 113.668043 
stopped after 100 iterations
# weights:  51
initial  value 161.403042 
iter  10 value 136.865772
iter  20 value 126.108083
iter  30 value 116.241422
iter  40 value 111.907289
iter  50 value 111.682080
iter  60 value 111.657440
final  value 111.657247 
converged
# weights:  11
initial  value 182.278156 
iter  10 value 141.824502
iter  20 value 141.658782
final  value 141.658742 
converged
# weights:  31
initial  value 190.329850 
iter  10 value 139.801605
iter  20 value 135.845564
iter  30 value 135.443615
iter  40 value 135.084678
iter  50 value 134.996332
final  value 134.996052 
converged
# weights:  51
initial  value 212.318761 
iter  10 value 140.380880
iter  20 value 134.225340
iter  30 value 128.598883
iter  40 value 127.470551
iter  50 value 126.069289
iter  60 value 125.915148
iter  70 value 125.912537
final  value 125.912530 
converged
# weights:  11
initial  value 190.986317 
iter  10 value 140.797753
iter  20 value 139.271855
iter  30 value 138.339032
iter  40 value 138.038192
iter  50 value 137.834263
iter  60 value 137.437737
iter  70 value 137.432604
iter  80 value 137.427188
iter  90 value 137.425442
iter 100 value 137.424602
final  value 137.424602 
stopped after 100 iterations
# weights:  31
initial  value 163.115806 
iter  10 value 136.900695
iter  20 value 129.052346
iter  30 value 119.348552
iter  40 value 117.912775
iter  50 value 115.451590
iter  60 value 115.105045
iter  70 value 114.274047
iter  80 value 113.884471
iter  90 value 113.860675
iter 100 value 113.841186
final  value 113.841186 
stopped after 100 iterations
# weights:  51
initial  value 165.005277 
iter  10 value 133.585182
iter  20 value 120.341988
iter  30 value 110.776771
iter  40 value 105.623942
iter  50 value 105.049834
iter  60 value 104.943054
iter  70 value 104.853483
iter  80 value 104.696672
iter  90 value 104.530329
iter 100 value 104.426164
final  value 104.426164 
stopped after 100 iterations
# weights:  11
initial  value 200.313719 
iter  10 value 139.350658
iter  20 value 136.098793
iter  30 value 134.191895
iter  40 value 133.963233
iter  50 value 133.953509
iter  60 value 133.941539
iter  70 value 133.040667
iter  80 value 133.019420
iter  90 value 132.967278
iter 100 value 132.957064
final  value 132.957064 
stopped after 100 iterations
# weights:  31
initial  value 164.684300 
iter  10 value 137.563149
iter  20 value 127.532291
iter  30 value 124.122823
iter  40 value 123.789976
iter  50 value 123.787727
iter  50 value 123.787725
iter  50 value 123.787725
final  value 123.787725 
converged
# weights:  51
initial  value 159.570397 
iter  10 value 138.379599
iter  20 value 126.041458
iter  30 value 110.774525
iter  40 value 103.415620
iter  50 value 102.669115
iter  60 value 102.667208
final  value 102.667201 
converged
# weights:  11
initial  value 158.727093 
iter  10 value 143.737524
iter  20 value 141.397593
iter  30 value 140.828659
final  value 140.826885 
converged
# weights:  31
initial  value 159.780949 
iter  10 value 140.310630
iter  20 value 138.052977
iter  30 value 135.245764
iter  40 value 134.695327
iter  50 value 134.660598
final  value 134.660031 
converged
# weights:  51
initial  value 232.326623 
iter  10 value 138.591008
iter  20 value 133.583988
iter  30 value 129.609159
iter  40 value 128.010409
iter  50 value 127.132771
iter  60 value 126.891068
final  value 126.889661 
converged
# weights:  11
initial  value 173.357705 
iter  10 value 140.184672
iter  20 value 138.776170
iter  30 value 136.417203
iter  40 value 135.751818
iter  50 value 135.727716
iter  60 value 135.727622
final  value 135.727374 
converged
# weights:  31
initial  value 163.383543 
iter  10 value 139.610489
iter  20 value 128.071009
iter  30 value 119.613759
iter  40 value 114.206623
iter  50 value 113.179046
iter  60 value 113.043883
iter  70 value 113.024941
iter  80 value 113.020857
iter  90 value 113.009616
iter 100 value 113.000503
final  value 113.000503 
stopped after 100 iterations
# weights:  51
initial  value 238.940138 
iter  10 value 134.952253
iter  20 value 114.937840
iter  30 value 102.695858
iter  40 value 96.296091
iter  50 value 95.023270
iter  60 value 94.796545
iter  70 value 94.690275
iter  80 value 94.625081
iter  90 value 94.533119
iter 100 value 94.456990
final  value 94.456990 
stopped after 100 iterations
# weights:  11
initial  value 165.027060 
iter  10 value 139.875130
iter  20 value 139.077223
iter  30 value 138.971226
iter  40 value 138.970471
iter  40 value 138.970470
iter  40 value 138.970470
final  value 138.970470 
converged
# weights:  31
initial  value 179.225982 
iter  10 value 135.469367
iter  20 value 120.012899
iter  30 value 115.696459
iter  40 value 110.558677
iter  50 value 106.791605
iter  60 value 106.244906
final  value 106.243952 
converged
# weights:  51
initial  value 167.807338 
iter  10 value 135.034359
iter  20 value 114.510593
iter  30 value 101.278542
iter  40 value 91.645607
iter  50 value 85.392046
iter  60 value 84.012795
iter  70 value 83.949672
iter  80 value 83.949243
iter  80 value 83.949242
iter  80 value 83.949242
final  value 83.949242 
converged
# weights:  11
initial  value 174.279384 
iter  10 value 145.451949
iter  20 value 140.834317
iter  30 value 140.532603
final  value 140.531101 
converged
# weights:  31
initial  value 161.342593 
iter  10 value 140.593117
iter  20 value 138.779082
iter  30 value 136.503457
iter  40 value 135.335674
iter  50 value 134.466010
iter  60 value 134.048104
iter  70 value 133.993586
final  value 133.993574 
converged
# weights:  51
initial  value 168.037064 
iter  10 value 138.323100
iter  20 value 134.352953
iter  30 value 132.511169
iter  40 value 131.902917
iter  50 value 131.199685
iter  60 value 129.261727
iter  70 value 127.711129
iter  80 value 127.611576
iter  90 value 127.609367
final  value 127.609363 
converged
# weights:  11
initial  value 194.152603 
iter  10 value 139.425041
iter  20 value 139.137106
iter  30 value 139.054512
iter  40 value 139.039935
iter  50 value 138.774238
iter  60 value 137.404061
iter  70 value 137.247646
iter  80 value 137.239981
final  value 137.238340 
converged
# weights:  31
initial  value 158.266381 
iter  10 value 132.791252
iter  20 value 124.811292
iter  30 value 118.036323
iter  40 value 112.974656
iter  50 value 112.447767
iter  60 value 112.261851
iter  70 value 112.193535
iter  80 value 111.616625
iter  90 value 111.464448
iter 100 value 111.349567
final  value 111.349567 
stopped after 100 iterations
# weights:  51
initial  value 165.468073 
iter  10 value 133.687798
iter  20 value 122.590838
iter  30 value 111.973761
iter  40 value 105.859130
iter  50 value 97.431079
iter  60 value 94.452660
iter  70 value 94.153811
iter  80 value 94.079744
iter  90 value 93.998486
iter 100 value 93.850135
final  value 93.850135 
stopped after 100 iterations
# weights:  11
initial  value 165.014561 
iter  10 value 137.899484
iter  20 value 136.232056
iter  30 value 134.549308
iter  40 value 134.513446
iter  50 value 134.508829
iter  60 value 134.500676
iter  70 value 134.401937
iter  80 value 134.340127
iter  90 value 134.330952
iter 100 value 134.324818
final  value 134.324818 
stopped after 100 iterations
# weights:  31
initial  value 160.203566 
iter  10 value 132.832429
iter  20 value 122.158532
iter  30 value 115.134376
iter  40 value 109.170960
iter  50 value 108.128700
iter  60 value 108.115362
final  value 108.115353 
converged
# weights:  51
initial  value 250.231304 
iter  10 value 135.167939
iter  20 value 120.744735
iter  30 value 111.907151
iter  40 value 97.843055
iter  50 value 89.226454
iter  60 value 83.736963
iter  70 value 81.527434
iter  80 value 81.198832
iter  90 value 81.197683
iter  90 value 81.197682
iter  90 value 81.197682
final  value 81.197682 
converged
# weights:  11
initial  value 190.116711 
iter  10 value 142.070813
iter  20 value 140.747631
final  value 140.745228 
converged
# weights:  31
initial  value 161.641477 
iter  10 value 139.415694
iter  20 value 134.327968
iter  30 value 132.965416
iter  40 value 132.859049
iter  50 value 132.858283
iter  50 value 132.858282
iter  50 value 132.858282
final  value 132.858282 
converged
# weights:  51
initial  value 157.243854 
iter  10 value 138.879049
iter  20 value 132.686310
iter  30 value 130.953890
iter  40 value 128.504956
iter  50 value 128.118506
iter  60 value 128.097075
iter  70 value 128.095948
iter  80 value 128.094710
iter  90 value 128.087408
iter 100 value 128.051531
final  value 128.051531 
stopped after 100 iterations
# weights:  11
initial  value 179.029806 
iter  10 value 139.357902
iter  20 value 138.341402
iter  30 value 138.004403
iter  40 value 136.193411
iter  50 value 135.997240
iter  60 value 135.663070
iter  70 value 135.662275
iter  80 value 135.662003
iter  90 value 135.661573
iter 100 value 135.661147
final  value 135.661147 
stopped after 100 iterations
# weights:  31
initial  value 188.396607 
iter  10 value 137.597390
iter  20 value 126.095361
iter  30 value 122.394763
iter  40 value 121.749206
iter  50 value 121.656930
iter  60 value 121.447927
iter  70 value 121.391625
iter  80 value 121.329699
iter  90 value 121.294809
iter 100 value 121.262329
final  value 121.262329 
stopped after 100 iterations
# weights:  51
initial  value 211.304252 
iter  10 value 133.154680
iter  20 value 119.582469
iter  30 value 109.802660
iter  40 value 106.671920
iter  50 value 106.129939
iter  60 value 106.034590
iter  70 value 106.018875
iter  80 value 106.001439
iter  90 value 105.984382
iter 100 value 105.969542
final  value 105.969542 
stopped after 100 iterations
# weights:  11
initial  value 159.878783 
iter  10 value 141.053876
iter  20 value 140.741966
iter  30 value 138.104921
iter  40 value 137.425453
iter  50 value 137.421048
iter  60 value 137.419952
final  value 137.419355 
converged
# weights:  31
initial  value 162.328277 
iter  10 value 139.682112
iter  20 value 130.505981
iter  30 value 120.717799
iter  40 value 112.203372
iter  50 value 109.695482
iter  60 value 109.666428
final  value 109.666397 
converged
# weights:  51
initial  value 240.785893 
iter  10 value 132.634613
iter  20 value 118.844433
iter  30 value 107.335508
iter  40 value 99.729768
iter  50 value 95.095891
iter  60 value 94.632741
iter  70 value 94.629487
final  value 94.629481 
converged
# weights:  11
initial  value 168.785632 
iter  10 value 143.488428
iter  20 value 140.813386
iter  30 value 140.626454
final  value 140.621415 
converged
# weights:  31
initial  value 162.191817 
iter  10 value 142.182906
iter  20 value 139.047756
iter  30 value 137.108331
iter  40 value 135.690700
iter  50 value 134.829325
iter  60 value 134.526339
iter  70 value 134.394948
iter  80 value 134.374081
iter  90 value 134.371950
final  value 134.371795 
converged
# weights:  51
initial  value 171.847601 
iter  10 value 140.190920
iter  20 value 131.645432
iter  30 value 128.515127
iter  40 value 127.791904
iter  50 value 127.697450
iter  60 value 127.372297
iter  70 value 127.281962
iter  80 value 127.277110
iter  80 value 127.277109
iter  80 value 127.277109
final  value 127.277109 
converged
# weights:  11
initial  value 229.184361 
iter  10 value 140.558727
iter  20 value 139.200319
iter  30 value 139.040693
iter  40 value 138.992673
final  value 138.991916 
converged
# weights:  31
initial  value 159.022505 
iter  10 value 139.290037
iter  20 value 126.427195
iter  30 value 114.027045
iter  40 value 112.523801
iter  50 value 112.352736
iter  60 value 112.170449
iter  70 value 112.131646
iter  80 value 112.032048
iter  90 value 111.629188
iter 100 value 111.539967
final  value 111.539967 
stopped after 100 iterations
# weights:  51
initial  value 206.077132 
iter  10 value 134.210703
iter  20 value 114.742557
iter  30 value 100.597042
iter  40 value 95.026590
iter  50 value 93.328071
iter  60 value 93.198664
iter  70 value 93.153741
iter  80 value 92.975239
iter  90 value 92.741525
iter 100 value 92.609810
final  value 92.609810 
stopped after 100 iterations
# weights:  11
initial  value 160.482924 
iter  10 value 141.143759
iter  20 value 140.641476
iter  30 value 140.606369
final  value 140.606302 
converged
# weights:  31
initial  value 253.981764 
iter  10 value 141.359966
iter  20 value 131.040887
iter  30 value 127.889537
iter  40 value 123.099625
iter  50 value 121.697706
iter  60 value 121.657556
iter  70 value 121.235104
iter  80 value 121.214890
final  value 121.214883 
converged
# weights:  51
initial  value 233.073560 
iter  10 value 138.019072
iter  20 value 126.263984
iter  30 value 113.162843
iter  40 value 107.090041
iter  50 value 106.607210
iter  60 value 106.583600
iter  70 value 106.580318
final  value 106.580296 
converged
# weights:  11
initial  value 197.495517 
iter  10 value 142.487626
iter  20 value 141.805689
final  value 141.796821 
converged
# weights:  31
initial  value 160.559031 
iter  10 value 139.431362
iter  20 value 136.243653
iter  30 value 135.391244
iter  40 value 133.561877
iter  50 value 133.376007
final  value 133.373608 
converged
# weights:  51
initial  value 160.877038 
iter  10 value 141.242832
iter  20 value 138.323231
iter  30 value 132.060545
iter  40 value 128.332430
iter  50 value 126.467795
iter  60 value 124.437730
iter  70 value 123.569221
iter  80 value 122.948680
iter  90 value 122.940604
final  value 122.940562 
converged
# weights:  11
initial  value 159.074580 
iter  10 value 143.054856
iter  20 value 139.328905
iter  30 value 138.029209
iter  40 value 137.878093
iter  50 value 137.797731
iter  60 value 137.693232
iter  70 value 137.678310
iter  80 value 137.664265
iter  90 value 137.646541
iter 100 value 137.627387
final  value 137.627387 
stopped after 100 iterations
# weights:  31
initial  value 168.166449 
iter  10 value 138.195648
iter  20 value 123.351662
iter  30 value 120.927095
iter  40 value 120.103912
iter  50 value 119.976717
iter  60 value 119.898606
iter  70 value 119.886486
iter  80 value 119.871386
iter  90 value 119.838751
iter 100 value 119.836236
final  value 119.836236 
stopped after 100 iterations
# weights:  51
initial  value 159.127970 
iter  10 value 136.977079
iter  20 value 118.214752
iter  30 value 103.067679
iter  40 value 93.802645
iter  50 value 90.361081
iter  60 value 89.733079
iter  70 value 89.708205
iter  80 value 89.672917
iter  90 value 89.606588
iter 100 value 89.592164
final  value 89.592164 
stopped after 100 iterations
# weights:  11
initial  value 188.478411 
iter  10 value 137.804643
iter  20 value 136.945184
iter  30 value 133.782148
iter  40 value 133.195165
iter  50 value 133.190954
iter  60 value 133.188769
iter  70 value 133.187599
final  value 133.187429 
converged
# weights:  31
initial  value 159.990338 
iter  10 value 132.314341
iter  20 value 121.645500
iter  30 value 114.189288
iter  40 value 108.426521
iter  50 value 108.276882
iter  60 value 108.271997
final  value 108.271734 
converged
# weights:  51
initial  value 160.804983 
iter  10 value 134.560690
iter  20 value 116.894522
iter  30 value 107.316336
iter  40 value 101.068077
iter  50 value 94.915218
iter  60 value 94.125586
iter  70 value 94.108013
iter  80 value 94.107608
iter  80 value 94.107607
iter  80 value 94.107607
final  value 94.107607 
converged
# weights:  11
initial  value 165.352922 
iter  10 value 140.019650
iter  20 value 139.943021
iter  20 value 139.943021
iter  20 value 139.943021
final  value 139.943021 
converged
# weights:  31
initial  value 163.534768 
iter  10 value 140.089518
iter  20 value 135.911395
iter  30 value 133.397132
iter  40 value 132.665077
iter  50 value 132.643633
final  value 132.643632 
converged
# weights:  51
initial  value 162.632690 
iter  10 value 139.891031
iter  20 value 134.466897
iter  30 value 125.833300
iter  40 value 124.044554
iter  50 value 123.950305
iter  60 value 123.946686
final  value 123.946651 
converged
# weights:  11
initial  value 161.521217 
iter  10 value 138.352578
iter  20 value 134.926945
iter  30 value 134.335225
iter  40 value 134.181046
iter  50 value 133.400835
iter  60 value 133.396980
iter  70 value 133.396382
iter  80 value 133.395956
iter  90 value 133.395424
iter 100 value 133.394751
final  value 133.394751 
stopped after 100 iterations
# weights:  31
initial  value 169.286557 
iter  10 value 134.915879
iter  20 value 131.520632
iter  30 value 127.634272
iter  40 value 121.184546
iter  50 value 120.108939
iter  60 value 120.064399
iter  70 value 119.941217
iter  80 value 119.724805
iter  90 value 119.694112
iter 100 value 119.679920
final  value 119.679920 
stopped after 100 iterations
# weights:  51
initial  value 164.834544 
iter  10 value 136.340300
iter  20 value 120.441165
iter  30 value 110.457060
iter  40 value 106.524903
iter  50 value 104.342035
iter  60 value 104.233242
iter  70 value 104.099063
iter  80 value 103.986700
iter  90 value 103.971069
iter 100 value 103.947646
final  value 103.947646 
stopped after 100 iterations
# weights:  11
initial  value 188.973656 
iter  10 value 142.657304
iter  20 value 141.820403
iter  30 value 138.071023
iter  40 value 137.962318
iter  50 value 137.541287
iter  60 value 137.466606
iter  70 value 137.425139
iter  80 value 137.378779
iter  90 value 137.371160
iter 100 value 137.351561
final  value 137.351561 
stopped after 100 iterations
# weights:  31
initial  value 170.464505 
iter  10 value 140.185394
iter  20 value 133.232531
iter  30 value 129.806869
iter  40 value 128.476237
iter  50 value 128.457513
final  value 128.457453 
converged
# weights:  51
initial  value 152.418441 
iter  10 value 138.481721
iter  20 value 117.397102
iter  30 value 102.145633
iter  40 value 96.091086
iter  50 value 87.581813
iter  60 value 85.359877
iter  70 value 84.857822
iter  80 value 84.789513
iter  90 value 84.787848
final  value 84.787843 
converged
# weights:  11
initial  value 179.923406 
iter  10 value 144.237452
iter  20 value 143.500036
final  value 143.498922 
converged
# weights:  31
initial  value 183.085748 
iter  10 value 140.308456
iter  20 value 134.859115
iter  30 value 133.874012
iter  40 value 132.885265
iter  50 value 132.685579
final  value 132.685319 
converged
# weights:  51
initial  value 198.202592 
iter  10 value 139.326123
iter  20 value 133.994387
iter  30 value 130.370564
iter  40 value 127.960817
iter  50 value 127.746961
iter  60 value 127.728360
iter  70 value 127.725337
final  value 127.725335 
converged
# weights:  11
initial  value 171.552962 
iter  10 value 141.392085
iter  20 value 140.374135
iter  30 value 138.353199
iter  40 value 138.339886
iter  50 value 137.758753
iter  60 value 137.656876
iter  70 value 137.515082
iter  80 value 137.510236
final  value 137.510208 
converged
# weights:  31
initial  value 195.362939 
iter  10 value 138.607181
iter  20 value 132.888136
iter  30 value 125.211658
iter  40 value 118.113902
iter  50 value 116.754478
iter  60 value 115.693866
iter  70 value 114.858249
iter  80 value 114.800139
iter  90 value 114.719976
iter 100 value 114.702740
final  value 114.702740 
stopped after 100 iterations
# weights:  51
initial  value 177.411284 
iter  10 value 138.948381
iter  20 value 119.965976
iter  30 value 106.346815
iter  40 value 100.431078
iter  50 value 99.481461
iter  60 value 99.418376
iter  70 value 99.188020
iter  80 value 98.894783
iter  90 value 98.878053
iter 100 value 98.776687
final  value 98.776687 
stopped after 100 iterations
# weights:  11
initial  value 199.450269 
iter  10 value 140.042863
iter  20 value 137.776892
iter  30 value 137.311594
iter  40 value 134.205065
iter  50 value 134.185100
iter  60 value 134.183479
final  value 134.183476 
converged
# weights:  31
initial  value 159.087716 
iter  10 value 136.968804
iter  20 value 129.729952
iter  30 value 115.557341
iter  40 value 106.585443
iter  50 value 105.254936
iter  60 value 104.892019
iter  70 value 104.819847
iter  80 value 104.785683
iter  90 value 104.781488
iter 100 value 104.780549
final  value 104.780549 
stopped after 100 iterations
# weights:  51
initial  value 164.990982 
iter  10 value 133.353509
iter  20 value 120.604817
iter  30 value 110.203450
iter  40 value 102.744935
iter  50 value 101.383507
iter  60 value 100.979434
iter  70 value 100.749418
iter  80 value 100.604613
iter  90 value 100.532027
iter 100 value 100.284556
final  value 100.284556 
stopped after 100 iterations
# weights:  11
initial  value 165.432812 
iter  10 value 145.097720
iter  20 value 139.793307
iter  30 value 139.767833
final  value 139.767802 
converged
# weights:  31
initial  value 157.985748 
iter  10 value 138.639982
iter  20 value 134.248349
iter  30 value 132.465309
iter  40 value 130.575138
iter  50 value 130.038744
iter  60 value 130.005944
iter  70 value 129.997270
iter  80 value 129.997025
final  value 129.997012 
converged
# weights:  51
initial  value 177.003743 
iter  10 value 137.341333
iter  20 value 130.515669
iter  30 value 126.393040
iter  40 value 124.299342
iter  50 value 123.695119
iter  60 value 123.473894
iter  70 value 123.462742
final  value 123.462711 
converged
# weights:  11
initial  value 161.228963 
iter  10 value 138.154203
iter  20 value 137.741239
iter  30 value 137.512607
iter  40 value 135.323190
iter  50 value 135.289905
iter  60 value 135.233821
iter  70 value 135.233271
iter  80 value 135.233202
final  value 135.233190 
converged
# weights:  31
initial  value 156.985964 
iter  10 value 134.450941
iter  20 value 121.976187
iter  30 value 117.060237
iter  40 value 114.954745
iter  50 value 114.526206
iter  60 value 114.430027
iter  70 value 114.341575
iter  80 value 114.306580
iter  90 value 114.215755
iter 100 value 114.153284
final  value 114.153284 
stopped after 100 iterations
# weights:  51
initial  value 163.792023 
iter  10 value 131.591132
iter  20 value 111.429147
iter  30 value 104.805306
iter  40 value 100.571772
iter  50 value 95.677671
iter  60 value 94.455232
iter  70 value 94.113590
iter  80 value 93.832495
iter  90 value 93.323754
iter 100 value 93.133845
final  value 93.133845 
stopped after 100 iterations
# weights:  11
initial  value 172.043858 
iter  10 value 138.538278
iter  20 value 136.023042
iter  30 value 134.509999
iter  40 value 134.493950
iter  50 value 134.476281
iter  60 value 134.461501
iter  70 value 134.241261
iter  80 value 133.806204
iter  90 value 133.736226
iter 100 value 133.725576
final  value 133.725576 
stopped after 100 iterations
# weights:  31
initial  value 204.421492 
iter  10 value 135.098335
iter  20 value 123.682456
iter  30 value 116.573724
iter  40 value 116.359482
final  value 116.359085 
converged
# weights:  51
initial  value 176.939858 
iter  10 value 133.275214
iter  20 value 124.037579
iter  30 value 104.534173
iter  40 value 95.374454
iter  50 value 91.888598
iter  60 value 89.447472
iter  70 value 89.266625
iter  80 value 89.247950
final  value 89.247650 
converged
# weights:  11
initial  value 160.524320 
iter  10 value 141.928302
iter  20 value 140.806229
iter  30 value 140.717047
final  value 140.717014 
converged
# weights:  31
initial  value 162.141736 
iter  10 value 139.464477
iter  20 value 136.865960
iter  30 value 135.965222
iter  40 value 135.788236
iter  50 value 135.677602
iter  60 value 135.547734
final  value 135.547416 
converged
# weights:  51
initial  value 188.957579 
iter  10 value 138.731326
iter  20 value 133.359283
iter  30 value 129.574382
iter  40 value 127.640660
iter  50 value 127.504270
iter  60 value 127.391312
iter  70 value 127.383785
iter  80 value 127.292676
iter  90 value 126.782620
iter 100 value 126.726321
final  value 126.726321 
stopped after 100 iterations
# weights:  11
initial  value 161.548062 
iter  10 value 138.675210
iter  20 value 136.289494
iter  30 value 133.858997
iter  40 value 133.847805
iter  50 value 133.846844
iter  60 value 133.846266
iter  70 value 133.843881
iter  80 value 133.842867
iter  90 value 133.842751
final  value 133.842645 
converged
# weights:  31
initial  value 204.213290 
iter  10 value 137.714825
iter  20 value 121.097166
iter  30 value 111.242819
iter  40 value 109.002456
iter  50 value 108.966775
iter  60 value 108.942534
iter  70 value 108.901155
iter  80 value 108.699595
iter  90 value 108.189617
iter 100 value 108.150521
final  value 108.150521 
stopped after 100 iterations
# weights:  51
initial  value 197.029950 
iter  10 value 136.948449
iter  20 value 117.561507
iter  30 value 104.601184
iter  40 value 99.952924
iter  50 value 99.610563
iter  60 value 99.549544
iter  70 value 99.488656
iter  80 value 99.461512
iter  90 value 99.431786
iter 100 value 99.399898
final  value 99.399898 
stopped after 100 iterations
# weights:  11
initial  value 179.253490 
iter  10 value 139.850354
iter  20 value 138.033637
iter  30 value 137.373329
iter  40 value 131.785956
iter  50 value 129.176868
iter  60 value 128.902993
iter  70 value 128.597681
iter  80 value 128.523481
iter  90 value 128.518768
iter 100 value 128.515672
final  value 128.515672 
stopped after 100 iterations
# weights:  31
initial  value 160.477444 
iter  10 value 135.784723
iter  20 value 120.603473
iter  30 value 115.084747
iter  40 value 112.484926
iter  50 value 112.407475
iter  60 value 112.325669
iter  70 value 112.322721
iter  80 value 112.322050
final  value 112.321973 
converged
# weights:  51
initial  value 163.818419 
iter  10 value 130.276743
iter  20 value 110.833405
iter  30 value 99.257209
iter  40 value 90.525321
iter  50 value 83.110115
iter  60 value 78.135598
iter  70 value 77.930262
final  value 77.929812 
converged
# weights:  11
initial  value 166.537722 
iter  10 value 139.862768
iter  20 value 139.513229
final  value 139.513212 
converged
# weights:  31
initial  value 169.288777 
iter  10 value 138.762995
iter  20 value 136.603748
iter  30 value 134.713523
iter  40 value 133.531498
iter  50 value 132.610076
iter  60 value 130.737598
iter  70 value 129.874160
iter  80 value 129.807507
final  value 129.805891 
converged
# weights:  51
initial  value 173.493044 
iter  10 value 140.358971
iter  20 value 129.542664
iter  30 value 125.051827
iter  40 value 122.719351
iter  50 value 121.321995
iter  60 value 120.970558
iter  70 value 120.883849
iter  80 value 120.878019
final  value 120.877947 
converged
# weights:  11
initial  value 175.515378 
iter  10 value 137.186811
iter  20 value 134.661196
iter  30 value 134.205658
iter  40 value 134.038036
iter  50 value 133.903787
iter  60 value 133.850532
iter  70 value 133.759368
iter  80 value 133.743357
iter  90 value 133.718864
iter 100 value 133.713921
final  value 133.713921 
stopped after 100 iterations
# weights:  31
initial  value 181.861933 
iter  10 value 134.011042
iter  20 value 124.301660
iter  30 value 117.861414
iter  40 value 116.765006
iter  50 value 116.285166
iter  60 value 116.204280
iter  70 value 116.177031
iter  80 value 116.146976
iter  90 value 116.094104
iter 100 value 115.167654
final  value 115.167654 
stopped after 100 iterations
# weights:  51
initial  value 167.131195 
iter  10 value 130.522665
iter  20 value 103.907639
iter  30 value 92.480401
iter  40 value 86.193654
iter  50 value 83.610350
iter  60 value 83.388546
iter  70 value 82.926186
iter  80 value 82.618307
iter  90 value 82.415946
iter 100 value 82.365126
final  value 82.365126 
stopped after 100 iterations
# weights:  11
initial  value 192.960373 
iter  10 value 138.930574
iter  20 value 137.145455
iter  30 value 135.996406
iter  40 value 134.921534
iter  50 value 134.744369
iter  60 value 134.726931
iter  70 value 134.668643
iter  80 value 134.631432
iter  90 value 134.618217
iter 100 value 134.610438
final  value 134.610438 
stopped after 100 iterations
# weights:  31
initial  value 168.388198 
iter  10 value 136.619693
iter  20 value 126.731615
iter  30 value 118.109688
iter  40 value 110.295249
iter  50 value 109.958570
iter  60 value 109.955609
final  value 109.955600 
converged
# weights:  51
initial  value 163.188638 
iter  10 value 135.460481
iter  20 value 110.623367
iter  30 value 97.617879
iter  40 value 90.094343
iter  50 value 87.808715
iter  60 value 87.447817
iter  70 value 87.431836
final  value 87.431811 
converged
# weights:  11
initial  value 162.425778 
iter  10 value 140.152438
iter  20 value 140.081018
iter  20 value 140.081018
iter  20 value 140.081018
final  value 140.081018 
converged
# weights:  31
initial  value 165.768558 
iter  10 value 139.564719
iter  20 value 135.203046
iter  30 value 132.031708
iter  40 value 131.354449
iter  50 value 131.300072
final  value 131.299990 
converged
# weights:  51
initial  value 159.493701 
iter  10 value 139.191401
iter  20 value 134.302744
iter  30 value 130.757610
iter  40 value 127.696812
iter  50 value 127.098708
iter  60 value 127.010605
iter  70 value 127.005975
iter  80 value 127.005351
iter  80 value 127.005350
iter  80 value 127.005350
final  value 127.005350 
converged
# weights:  11
initial  value 159.709223 
iter  10 value 139.377435
iter  20 value 138.792912
iter  30 value 138.790753
iter  40 value 138.770492
iter  50 value 138.761045
final  value 138.761040 
converged
# weights:  31
initial  value 244.290896 
iter  10 value 138.377358
iter  20 value 131.610197
iter  30 value 120.378975
iter  40 value 113.457179
iter  50 value 112.583421
iter  60 value 112.336146
iter  70 value 112.103140
iter  80 value 111.874296
iter  90 value 111.339151
iter 100 value 111.047659
final  value 111.047659 
stopped after 100 iterations
# weights:  51
initial  value 157.842271 
iter  10 value 131.424923
iter  20 value 107.582657
iter  30 value 95.062526
iter  40 value 88.139524
iter  50 value 87.097306
iter  60 value 86.767221
iter  70 value 86.607616
iter  80 value 85.459441
iter  90 value 84.207332
iter 100 value 83.900425
final  value 83.900425 
stopped after 100 iterations
# weights:  11
initial  value 158.886957 
iter  10 value 140.747054
iter  20 value 138.784156
iter  30 value 138.003545
iter  40 value 134.947623
final  value 134.930883 
converged
# weights:  31
initial  value 171.023965 
iter  10 value 132.207412
iter  20 value 124.607151
iter  30 value 117.828446
iter  40 value 117.398787
iter  50 value 117.390610
final  value 117.390595 
converged
# weights:  51
initial  value 165.954075 
iter  10 value 133.097489
iter  20 value 114.049486
iter  30 value 105.858696
iter  40 value 101.675715
iter  50 value 96.506613
iter  60 value 94.524772
iter  70 value 94.165235
iter  80 value 94.053774
iter  90 value 94.050563
iter 100 value 94.049204
final  value 94.049204 
stopped after 100 iterations
# weights:  11
initial  value 159.651221 
iter  10 value 141.825865
iter  20 value 140.593177
final  value 140.592534 
converged
# weights:  31
initial  value 168.999914 
iter  10 value 141.129249
iter  20 value 134.493849
iter  30 value 132.538362
iter  40 value 131.000960
iter  50 value 130.386917
iter  60 value 130.348890
final  value 130.348888 
converged
# weights:  51
initial  value 170.710788 
iter  10 value 137.776025
iter  20 value 132.374971
iter  30 value 127.050974
iter  40 value 125.879472
iter  50 value 125.804386
iter  60 value 125.782448
final  value 125.782313 
converged
# weights:  11
initial  value 176.052363 
iter  10 value 138.995477
iter  20 value 138.420920
iter  30 value 138.293440
iter  40 value 135.486838
iter  50 value 135.297066
iter  60 value 135.286745
final  value 135.286731 
converged
# weights:  31
initial  value 164.096424 
iter  10 value 136.062938
iter  20 value 123.147557
iter  30 value 118.836426
iter  40 value 116.672736
iter  50 value 116.500910
iter  60 value 116.493838
iter  70 value 116.488380
iter  80 value 116.486064
iter  90 value 116.480212
iter 100 value 116.478667
final  value 116.478667 
stopped after 100 iterations
# weights:  51
initial  value 179.392223 
iter  10 value 133.317270
iter  20 value 113.929925
iter  30 value 98.934877
iter  40 value 93.892153
iter  50 value 93.259076
iter  60 value 93.028304
iter  70 value 92.454578
iter  80 value 92.372844
iter  90 value 92.350128
iter 100 value 92.289403
final  value 92.289403 
stopped after 100 iterations
# weights:  11
initial  value 185.258810 
iter  10 value 133.283563
iter  20 value 131.211703
iter  30 value 129.689138
iter  40 value 129.515441
iter  50 value 129.373386
iter  60 value 129.318147
iter  70 value 129.014230
iter  80 value 128.983550
iter  90 value 128.914114
iter 100 value 128.864089
final  value 128.864089 
stopped after 100 iterations
# weights:  31
initial  value 157.546860 
iter  10 value 131.852098
iter  20 value 124.807883
iter  30 value 111.534495
iter  40 value 101.803934
iter  50 value 99.514141
iter  60 value 98.979709
iter  70 value 98.905709
iter  80 value 98.874512
iter  90 value 98.863353
iter 100 value 98.855647
final  value 98.855647 
stopped after 100 iterations
# weights:  51
initial  value 169.662196 
iter  10 value 129.598821
iter  20 value 112.814737
iter  30 value 101.511669
iter  40 value 94.600833
iter  50 value 85.601699
iter  60 value 83.665374
iter  70 value 83.612945
iter  80 value 83.608179
final  value 83.607928 
converged
# weights:  11
initial  value 165.454179 
iter  10 value 134.568201
iter  20 value 134.107537
final  value 134.107055 
converged
# weights:  31
initial  value 215.968242 
iter  10 value 134.289951
iter  20 value 131.063527
iter  30 value 129.875716
iter  40 value 129.524260
iter  50 value 129.027629
iter  60 value 127.955828
iter  70 value 127.842109
iter  80 value 127.711335
iter  90 value 127.705794
final  value 127.705790 
converged
# weights:  51
initial  value 165.465306 
iter  10 value 131.703062
iter  20 value 128.749655
iter  30 value 125.115273
iter  40 value 122.294830
iter  50 value 120.879908
iter  60 value 120.767062
iter  70 value 120.763734
final  value 120.763644 
converged
# weights:  11
initial  value 204.498422 
iter  10 value 134.166043
iter  20 value 132.696060
iter  30 value 132.640134
iter  40 value 132.219410
iter  50 value 131.988483
iter  60 value 131.875898
iter  70 value 131.720163
iter  80 value 131.715775
iter  90 value 131.684932
iter 100 value 131.674434
final  value 131.674434 
stopped after 100 iterations
# weights:  31
initial  value 153.697756 
iter  10 value 130.362273
iter  20 value 117.032633
iter  30 value 107.888303
iter  40 value 103.177739
iter  50 value 102.816678
iter  60 value 102.798527
iter  70 value 102.780593
iter  80 value 102.701685
iter  90 value 102.401169
iter 100 value 101.887210
final  value 101.887210 
stopped after 100 iterations
# weights:  51
initial  value 157.450712 
iter  10 value 130.539306
iter  20 value 114.809118
iter  30 value 108.153641
iter  40 value 101.419837
iter  50 value 100.579741
iter  60 value 100.269641
iter  70 value 100.070267
iter  80 value 99.975498
iter  90 value 99.811741
iter 100 value 99.239919
final  value 99.239919 
stopped after 100 iterations
# weights:  11
initial  value 161.066526 
iter  10 value 136.470473
iter  20 value 133.757187
iter  30 value 133.194788
iter  40 value 133.031777
iter  50 value 133.027072
iter  60 value 133.023832
iter  70 value 133.020348
iter  80 value 133.019230
iter  90 value 133.014640
iter 100 value 133.011087
final  value 133.011087 
stopped after 100 iterations
# weights:  31
initial  value 165.307845 
iter  10 value 134.843564
iter  20 value 130.583705
iter  30 value 123.290255
iter  40 value 119.500010
iter  50 value 118.444734
iter  60 value 118.419911
final  value 118.419807 
converged
# weights:  51
initial  value 193.143138 
iter  10 value 128.179869
iter  20 value 101.119992
iter  30 value 93.716822
iter  40 value 84.859905
iter  50 value 78.399057
iter  60 value 73.294573
iter  70 value 72.978280
iter  80 value 72.970780
final  value 72.970752 
converged
# weights:  11
initial  value 166.155609 
iter  10 value 138.922057
iter  20 value 138.667544
final  value 138.667364 
converged
# weights:  31
initial  value 167.884501 
iter  10 value 137.561965
iter  20 value 132.336256
iter  30 value 130.004184
iter  40 value 129.927683
iter  50 value 129.924877
final  value 129.924873 
converged
# weights:  51
initial  value 159.393294 
iter  10 value 133.906208
iter  20 value 128.433492
iter  30 value 125.605620
iter  40 value 123.865515
iter  50 value 122.295517
iter  60 value 122.040838
iter  70 value 121.997458
iter  80 value 121.637260
iter  90 value 121.470053
iter 100 value 121.418455
final  value 121.418455 
stopped after 100 iterations
# weights:  11
initial  value 157.412168 
iter  10 value 137.253113
iter  20 value 136.915892
iter  30 value 136.914603
final  value 136.914529 
converged
# weights:  31
initial  value 169.690699 
iter  10 value 134.110178
iter  20 value 125.993208
iter  30 value 116.630057
iter  40 value 116.263499
iter  50 value 116.237284
iter  60 value 116.172863
iter  70 value 115.980614
iter  80 value 115.842686
iter  90 value 115.791500
iter 100 value 115.679051
final  value 115.679051 
stopped after 100 iterations
# weights:  51
initial  value 179.720238 
iter  10 value 133.826000
iter  20 value 120.024196
iter  30 value 111.261796
iter  40 value 109.150467
iter  50 value 108.741650
iter  60 value 108.682688
iter  70 value 108.651245
iter  80 value 108.369641
iter  90 value 108.002617
iter 100 value 107.808805
final  value 107.808805 
stopped after 100 iterations
# weights:  11
initial  value 158.031740 
iter  10 value 137.451264
iter  20 value 135.526927
iter  30 value 134.473945
iter  40 value 134.165042
iter  50 value 133.997902
iter  60 value 133.963669
iter  70 value 133.956969
iter  80 value 133.951293
iter  90 value 133.941256
iter 100 value 133.915174
final  value 133.915174 
stopped after 100 iterations
# weights:  31
initial  value 230.367166 
iter  10 value 134.031586
iter  20 value 122.754717
iter  30 value 115.638333
iter  40 value 113.876328
iter  50 value 113.657571
iter  60 value 113.656668
final  value 113.656665 
converged
# weights:  51
initial  value 214.352008 
iter  10 value 129.803031
iter  20 value 110.115605
iter  30 value 101.964495
iter  40 value 91.688336
iter  50 value 84.763727
iter  60 value 84.539872
iter  70 value 84.539169
final  value 84.539154 
converged
# weights:  11
initial  value 194.569584 
iter  10 value 138.709173
iter  20 value 138.225912
final  value 138.210797 
converged
# weights:  31
initial  value 158.027512 
iter  10 value 137.410169
iter  20 value 134.359795
iter  30 value 133.080559
iter  40 value 132.955911
iter  50 value 132.201494
iter  60 value 132.086507
iter  70 value 132.084570
final  value 132.084568 
converged
# weights:  51
initial  value 167.998534 
iter  10 value 137.647499
iter  20 value 132.868126
iter  30 value 127.824018
iter  40 value 126.638547
iter  50 value 125.917193
iter  60 value 125.573917
iter  70 value 125.570306
final  value 125.570256 
converged
# weights:  11
initial  value 162.148945 
iter  10 value 137.016020
iter  20 value 136.732457
final  value 136.724127 
converged
# weights:  31
initial  value 184.299774 
iter  10 value 133.164163
iter  20 value 120.577551
iter  30 value 116.020345
iter  40 value 115.238015
iter  50 value 115.125957
iter  60 value 114.792754
iter  70 value 114.355880
iter  80 value 114.290537
iter  90 value 114.284517
iter 100 value 114.261756
final  value 114.261756 
stopped after 100 iterations
# weights:  51
initial  value 193.839725 
iter  10 value 130.611202
iter  20 value 123.025621
iter  30 value 112.905778
iter  40 value 106.468083
iter  50 value 104.427426
iter  60 value 103.786928
iter  70 value 103.531138
iter  80 value 103.274126
iter  90 value 103.071072
iter 100 value 102.854595
final  value 102.854595 
stopped after 100 iterations
# weights:  11
initial  value 192.385911 
iter  10 value 138.302345
iter  20 value 137.887375
iter  30 value 137.658180
iter  40 value 137.548979
iter  50 value 137.519346
iter  60 value 137.518645
final  value 137.518588 
converged
# weights:  31
initial  value 172.186688 
iter  10 value 137.432596
iter  20 value 131.388236
iter  30 value 122.589390
iter  40 value 117.202986
iter  50 value 117.133234
iter  60 value 117.129652
final  value 117.129622 
converged
# weights:  51
initial  value 157.466280 
iter  10 value 130.833156
iter  20 value 114.331861
iter  30 value 102.225126
iter  40 value 92.498676
iter  50 value 90.657914
iter  60 value 90.579621
iter  70 value 90.576582
final  value 90.576567 
converged
# weights:  11
initial  value 195.341170 
iter  10 value 139.392200
iter  20 value 138.994820
final  value 138.994063 
converged
# weights:  31
initial  value 165.027320 
iter  10 value 137.699420
iter  20 value 135.079906
iter  30 value 132.447683
iter  40 value 131.770743
iter  50 value 131.767812
iter  50 value 131.767812
iter  50 value 131.767812
final  value 131.767812 
converged
# weights:  51
initial  value 183.500839 
iter  10 value 135.754439
iter  20 value 127.955239
iter  30 value 126.773046
iter  40 value 126.609896
iter  50 value 126.578859
iter  60 value 126.528087
iter  70 value 126.525470
final  value 126.525467 
converged
# weights:  11
initial  value 160.164210 
iter  10 value 139.055363
iter  20 value 138.065782
iter  30 value 137.736037
iter  40 value 137.558172
iter  50 value 137.524293
iter  60 value 137.521901
final  value 137.521432 
converged
# weights:  31
initial  value 187.087121 
iter  10 value 136.747986
iter  20 value 130.310272
iter  30 value 127.255031
iter  40 value 120.241031
iter  50 value 119.043547
iter  60 value 118.691796
iter  70 value 118.634546
iter  80 value 118.590792
iter  90 value 118.428798
iter 100 value 118.399020
final  value 118.399020 
stopped after 100 iterations
# weights:  51
initial  value 169.872463 
iter  10 value 127.393512
iter  20 value 114.098134
iter  30 value 102.710133
iter  40 value 98.798822
iter  50 value 98.219617
iter  60 value 97.962040
iter  70 value 97.728683
iter  80 value 97.450258
iter  90 value 97.270700
iter 100 value 97.184010
final  value 97.184010 
stopped after 100 iterations
# weights:  11
initial  value 166.985413 
iter  10 value 139.107309
iter  20 value 137.769111
iter  30 value 135.203706
iter  40 value 135.103015
iter  50 value 133.051640
iter  60 value 132.876397
iter  70 value 132.867414
iter  80 value 132.866472
iter  90 value 132.865928
final  value 132.865799 
converged
# weights:  31
initial  value 168.552537 
iter  10 value 135.130659
iter  20 value 125.687145
iter  30 value 116.728520
iter  40 value 111.746496
iter  50 value 111.632175
iter  60 value 111.629306
final  value 111.629302 
converged
# weights:  51
initial  value 200.972068 
iter  10 value 132.016602
iter  20 value 112.889723
iter  30 value 104.480879
iter  40 value 94.650616
iter  50 value 88.853679
iter  60 value 88.033839
iter  70 value 87.965818
iter  80 value 87.964708
final  value 87.964706 
converged
# weights:  11
initial  value 193.741891 
iter  10 value 141.297141
iter  20 value 140.752942
iter  30 value 140.739887
iter  30 value 140.739886
iter  30 value 140.739886
final  value 140.739886 
converged
# weights:  31
initial  value 177.824415 
iter  10 value 140.962166
iter  20 value 138.288412
iter  30 value 134.161894
iter  40 value 132.909172
iter  50 value 131.173979
iter  60 value 130.633489
iter  70 value 130.598256
iter  80 value 130.594995
iter  90 value 130.586618
final  value 130.586604 
converged
# weights:  51
initial  value 199.933953 
iter  10 value 138.280564
iter  20 value 132.653649
iter  30 value 130.288621
iter  40 value 125.639637
iter  50 value 125.178239
iter  60 value 125.116492
iter  70 value 125.100756
iter  80 value 125.084387
final  value 125.084384 
converged
# weights:  11
initial  value 172.644308 
iter  10 value 139.327190
iter  20 value 137.820228
iter  30 value 137.185074
iter  40 value 136.912586
iter  50 value 136.369262
final  value 136.369212 
converged
# weights:  31
initial  value 171.800914 
iter  10 value 137.900419
iter  20 value 129.513765
iter  30 value 124.268187
iter  40 value 123.421270
iter  50 value 123.317372
iter  60 value 123.260872
iter  70 value 123.226965
iter  80 value 123.196956
iter  90 value 123.058609
iter 100 value 122.953587
final  value 122.953587 
stopped after 100 iterations
# weights:  51
initial  value 164.124541 
iter  10 value 136.637861
iter  20 value 121.595952
iter  30 value 108.288435
iter  40 value 99.219966
iter  50 value 94.092358
iter  60 value 91.215321
iter  70 value 89.058843
iter  80 value 86.662898
iter  90 value 86.147353
iter 100 value 86.071616
final  value 86.071616 
stopped after 100 iterations
# weights:  11
initial  value 163.390851 
iter  10 value 139.571850
iter  20 value 139.040922
iter  30 value 139.027112
final  value 139.026431 
converged
# weights:  31
initial  value 173.075321 
iter  10 value 136.369643
iter  20 value 130.718505
iter  30 value 124.650492
iter  40 value 123.675048
iter  50 value 123.632925
final  value 123.632825 
converged
# weights:  51
initial  value 182.535968 
iter  10 value 134.029641
iter  20 value 119.993783
iter  30 value 106.797251
iter  40 value 96.983994
iter  50 value 92.858233
iter  60 value 92.594900
iter  70 value 92.557038
iter  80 value 92.556553
final  value 92.556551 
converged
# weights:  11
initial  value 186.774968 
iter  10 value 141.724860
iter  20 value 140.435123
iter  30 value 140.395580
final  value 140.395455 
converged
# weights:  31
initial  value 179.934703 
iter  10 value 140.080841
iter  20 value 137.440497
iter  30 value 136.136702
iter  40 value 135.420412
iter  50 value 135.238030
final  value 135.237563 
converged
# weights:  51
initial  value 174.445735 
iter  10 value 140.174187
iter  20 value 135.740836
iter  30 value 131.382119
iter  40 value 129.812133
iter  50 value 129.307728
iter  60 value 129.267017
iter  70 value 129.264552
final  value 129.264514 
converged
# weights:  11
initial  value 159.616296 
iter  10 value 139.623133
iter  20 value 139.036754
final  value 139.028404 
converged
# weights:  31
initial  value 178.918303 
iter  10 value 137.844351
iter  20 value 127.119562
iter  30 value 122.513720
iter  40 value 118.950388
iter  50 value 118.509583
iter  60 value 118.442395
iter  70 value 118.405116
iter  80 value 118.383254
iter  90 value 118.358159
iter 100 value 118.276157
final  value 118.276157 
stopped after 100 iterations
# weights:  51
initial  value 177.222003 
iter  10 value 134.443130
iter  20 value 121.256057
iter  30 value 112.548047
iter  40 value 102.909855
iter  50 value 95.009068
iter  60 value 93.564746
iter  70 value 91.411537
iter  80 value 90.491554
iter  90 value 90.072906
iter 100 value 89.894156
final  value 89.894156 
stopped after 100 iterations
# weights:  11
initial  value 192.290896 
iter  10 value 139.667973
iter  20 value 137.895977
iter  30 value 137.862761
final  value 137.862678 
converged
# weights:  31
initial  value 160.723298 
iter  10 value 127.657535
iter  20 value 116.337159
iter  30 value 112.799218
iter  40 value 109.526983
iter  50 value 108.231725
iter  60 value 108.160944
iter  70 value 108.131158
iter  80 value 108.113056
iter  90 value 108.110564
iter 100 value 108.091503
final  value 108.091503 
stopped after 100 iterations
# weights:  51
initial  value 163.325417 
iter  10 value 133.211130
iter  20 value 122.610045
iter  30 value 108.338887
iter  40 value 100.192216
iter  50 value 95.207553
iter  60 value 93.291101
iter  70 value 93.065137
iter  80 value 93.061179
final  value 93.061174 
converged
# weights:  11
initial  value 177.284999 
iter  10 value 139.500891
iter  20 value 138.909103
final  value 138.903623 
converged
# weights:  31
initial  value 168.158514 
iter  10 value 137.260594
iter  20 value 135.159137
iter  30 value 133.582067
iter  40 value 133.435124
iter  50 value 133.433156
final  value 133.433154 
converged
# weights:  51
initial  value 199.354172 
iter  10 value 137.387085
iter  20 value 133.746317
iter  30 value 127.688698
iter  40 value 124.774655
iter  50 value 120.972303
iter  60 value 120.144036
iter  70 value 119.987111
iter  80 value 119.977193
final  value 119.977165 
converged
# weights:  11
initial  value 161.323282 
iter  10 value 138.773432
iter  20 value 137.672594
iter  30 value 137.096381
iter  40 value 136.775918
iter  50 value 136.125603
iter  60 value 134.346182
iter  70 value 134.311336
iter  80 value 134.310593
final  value 134.310517 
converged
# weights:  31
initial  value 168.919100 
iter  10 value 135.300854
iter  20 value 128.051848
iter  30 value 118.564249
iter  40 value 117.422638
iter  50 value 117.405780
iter  60 value 117.369532
iter  70 value 117.323691
iter  80 value 117.301846
iter  90 value 117.289995
iter 100 value 117.253483
final  value 117.253483 
stopped after 100 iterations
# weights:  51
initial  value 164.577881 
iter  10 value 134.774738
iter  20 value 111.460832
iter  30 value 95.564925
iter  40 value 93.257003
iter  50 value 92.489338
iter  60 value 92.437247
iter  70 value 92.309387
iter  80 value 92.240287
iter  90 value 92.122207
iter 100 value 92.006815
final  value 92.006815 
stopped after 100 iterations
# weights:  11
initial  value 169.824202 
iter  10 value 141.746071
iter  20 value 138.467693
iter  30 value 138.304236
final  value 138.302472 
converged
# weights:  31
initial  value 163.574016 
iter  10 value 139.077907
iter  20 value 129.613850
iter  30 value 123.319693
iter  40 value 120.086325
iter  50 value 119.255451
iter  60 value 119.039298
iter  70 value 118.913198
iter  80 value 118.864788
iter  90 value 118.850236
iter 100 value 118.768583
final  value 118.768583 
stopped after 100 iterations
# weights:  51
initial  value 223.961211 
iter  10 value 134.757272
iter  20 value 116.738157
iter  30 value 106.733267
iter  40 value 97.457509
iter  50 value 96.147611
iter  60 value 96.077496
iter  70 value 96.061017
final  value 96.060890 
converged
# weights:  11
initial  value 185.723274 
iter  10 value 141.253510
iter  20 value 139.942334
final  value 139.933120 
converged
# weights:  31
initial  value 171.915669 
iter  10 value 139.405029
iter  20 value 134.054367
iter  30 value 133.204872
iter  40 value 133.130672
final  value 133.128761 
converged
# weights:  51
initial  value 181.089174 
iter  10 value 138.321605
iter  20 value 130.978590
iter  30 value 129.168416
iter  40 value 127.916672
iter  50 value 127.185515
iter  60 value 126.215050
iter  70 value 125.825239
iter  80 value 125.783343
iter  90 value 125.768557
final  value 125.767998 
converged
# weights:  11
initial  value 159.702364 
iter  10 value 138.806617
iter  20 value 138.340943
iter  30 value 138.304558
final  value 138.304172 
converged
# weights:  31
initial  value 158.164570 
iter  10 value 134.863525
iter  20 value 121.192495
iter  30 value 110.147814
iter  40 value 109.150683
iter  50 value 109.067781
iter  60 value 109.039674
iter  70 value 109.037979
iter  80 value 109.037407
iter  90 value 109.037016
iter 100 value 109.036296
final  value 109.036296 
stopped after 100 iterations
# weights:  51
initial  value 183.563732 
iter  10 value 133.250253
iter  20 value 115.596484
iter  30 value 104.942684
iter  40 value 98.561275
iter  50 value 94.516263
iter  60 value 93.889302
iter  70 value 93.746517
iter  80 value 93.694930
iter  90 value 93.579447
iter 100 value 93.467051
final  value 93.467051 
stopped after 100 iterations
# weights:  11
initial  value 210.990993 
iter  10 value 141.109460
iter  20 value 139.621994
iter  30 value 139.426722
iter  40 value 136.771645
iter  50 value 136.252088
iter  60 value 135.785086
iter  70 value 135.367111
iter  80 value 134.407288
iter  90 value 133.901785
iter 100 value 133.895034
final  value 133.895034 
stopped after 100 iterations
# weights:  31
initial  value 157.651897 
iter  10 value 136.458347
iter  20 value 123.777234
iter  30 value 114.820138
iter  40 value 113.118869
iter  50 value 112.996671
iter  60 value 112.995785
iter  70 value 112.754022
final  value 112.753800 
converged
# weights:  51
initial  value 230.200381 
iter  10 value 135.891189
iter  20 value 117.156106
iter  30 value 106.768876
iter  40 value 99.532140
iter  50 value 93.432001
iter  60 value 90.172620
iter  70 value 89.633791
iter  80 value 89.627348
final  value 89.627330 
converged
# weights:  11
initial  value 184.526505 
iter  10 value 141.204785
iter  20 value 140.164955
final  value 140.157158 
converged
# weights:  31
initial  value 166.799756 
iter  10 value 139.158473
iter  20 value 135.640213
iter  30 value 134.766965
iter  40 value 133.582029
iter  50 value 133.236587
iter  60 value 133.091750
iter  70 value 133.070559
iter  80 value 133.068584
final  value 133.068550 
converged
# weights:  51
initial  value 169.428181 
iter  10 value 139.738504
iter  20 value 132.438426
iter  30 value 127.451973
iter  40 value 124.865205
iter  50 value 122.080170
iter  60 value 121.296683
iter  70 value 121.031949
iter  80 value 120.999335
final  value 120.999295 
converged
# weights:  11
initial  value 164.571641 
iter  10 value 139.360855
iter  20 value 138.670350
iter  30 value 138.667470
final  value 138.667033 
converged
# weights:  31
initial  value 165.861375 
iter  10 value 138.844446
iter  20 value 131.780867
iter  30 value 125.848215
iter  40 value 122.540201
iter  50 value 121.980379
iter  60 value 121.786424
iter  70 value 121.602115
iter  80 value 121.560017
iter  90 value 121.552885
iter 100 value 121.520548
final  value 121.520548 
stopped after 100 iterations
# weights:  51
initial  value 238.640662 
iter  10 value 130.980648
iter  20 value 113.472931
iter  30 value 100.853090
iter  40 value 91.094117
iter  50 value 87.320387
iter  60 value 87.024841
iter  70 value 86.930212
iter  80 value 86.816340
iter  90 value 86.664265
iter 100 value 86.572372
final  value 86.572372 
stopped after 100 iterations
# weights:  11
initial  value 161.057672 
iter  10 value 138.830377
iter  20 value 138.170028
iter  30 value 136.155530
iter  40 value 135.243775
iter  50 value 135.074975
iter  60 value 134.200119
iter  60 value 134.200119
final  value 134.200119 
converged
# weights:  31
initial  value 162.411411 
iter  10 value 136.802346
iter  20 value 123.948532
iter  30 value 116.964535
iter  40 value 109.705617
iter  50 value 108.482442
iter  60 value 108.439423
iter  70 value 108.357160
iter  80 value 108.354690
final  value 108.354685 
converged
# weights:  51
initial  value 176.541849 
iter  10 value 136.056560
iter  20 value 120.137106
iter  30 value 108.114876
iter  40 value 98.576863
iter  50 value 97.971905
iter  60 value 97.563713
iter  70 value 97.523066
iter  80 value 97.520911
final  value 97.520909 
converged
# weights:  11
initial  value 162.029816 
iter  10 value 140.419571
final  value 140.237497 
converged
# weights:  31
initial  value 191.671741 
iter  10 value 139.625633
iter  20 value 137.304904
iter  30 value 135.211444
iter  40 value 134.547239
iter  50 value 134.532262
iter  60 value 134.532043
final  value 134.532041 
converged
# weights:  51
initial  value 172.786659 
iter  10 value 138.400859
iter  20 value 131.102035
iter  30 value 127.784060
iter  40 value 126.770777
iter  50 value 126.516858
iter  60 value 126.470576
final  value 126.467714 
converged
# weights:  11
initial  value 191.969613 
iter  10 value 138.782044
iter  20 value 138.224327
iter  30 value 137.038323
iter  40 value 135.308439
iter  50 value 135.214981
iter  60 value 135.208777
iter  70 value 135.208637
iter  70 value 135.208636
final  value 135.208636 
converged
# weights:  31
initial  value 243.948943 
iter  10 value 135.769423
iter  20 value 124.989991
iter  30 value 120.416680
iter  40 value 117.017163
iter  50 value 115.188238
iter  60 value 114.540144
iter  70 value 114.420292
iter  80 value 114.389780
iter  90 value 114.329952
iter 100 value 114.018324
final  value 114.018324 
stopped after 100 iterations
# weights:  51
initial  value 171.944841 
iter  10 value 133.377313
iter  20 value 107.473021
iter  30 value 94.066943
iter  40 value 87.917467
iter  50 value 86.813106
iter  60 value 86.569195
iter  70 value 86.325505
iter  80 value 86.213916
iter  90 value 86.094817
iter 100 value 86.028035
final  value 86.028035 
stopped after 100 iterations
# weights:  11
initial  value 188.315370 
iter  10 value 143.404626
iter  20 value 141.366131
iter  30 value 139.680898
iter  40 value 139.477025
iter  50 value 139.117295
iter  60 value 138.593612
iter  70 value 137.534325
iter  80 value 137.522494
iter  90 value 137.514763
iter  90 value 137.514763
final  value 137.514763 
converged
# weights:  31
initial  value 162.885925 
iter  10 value 143.349381
iter  20 value 135.564946
iter  30 value 128.448168
iter  40 value 121.786271
iter  50 value 115.581435
iter  60 value 113.190182
iter  70 value 111.366912
iter  80 value 111.257158
iter  90 value 111.217949
iter 100 value 110.923213
final  value 110.923213 
stopped after 100 iterations
# weights:  51
initial  value 176.896822 
iter  10 value 136.548981
iter  20 value 119.848119
iter  30 value 106.868679
iter  40 value 99.815682
iter  50 value 93.925531
iter  60 value 88.589394
iter  70 value 86.511931
iter  80 value 86.417887
final  value 86.417375 
converged
# weights:  11
initial  value 167.894742 
iter  10 value 144.306502
iter  20 value 143.791539
final  value 143.790575 
converged
# weights:  31
initial  value 157.545477 
iter  10 value 143.681840
iter  20 value 139.193325
iter  30 value 137.162291
iter  40 value 136.927269
iter  50 value 136.925902
iter  50 value 136.925901
iter  50 value 136.925901
final  value 136.925901 
converged
# weights:  51
initial  value 161.749304 
iter  10 value 143.764260
iter  20 value 137.474766
iter  30 value 131.416877
iter  40 value 127.402761
iter  50 value 126.234765
iter  60 value 126.156524
iter  70 value 126.146580
final  value 126.146259 
converged
# weights:  11
initial  value 158.100678 
iter  10 value 144.358215
iter  20 value 143.585232
iter  30 value 142.695660
iter  40 value 140.158364
iter  50 value 139.241596
iter  60 value 139.192504
iter  70 value 139.182992
iter  80 value 139.006052
iter  90 value 138.584068
iter 100 value 138.577200
final  value 138.577200 
stopped after 100 iterations
# weights:  31
initial  value 161.855349 
iter  10 value 138.050067
iter  20 value 124.504136
iter  30 value 120.512197
iter  40 value 119.624601
iter  50 value 119.460695
iter  60 value 119.326458
iter  70 value 119.231707
iter  80 value 119.081087
iter  90 value 118.670458
iter 100 value 118.420921
final  value 118.420921 
stopped after 100 iterations
# weights:  51
initial  value 193.055188 
iter  10 value 136.806948
iter  20 value 112.941903
iter  30 value 101.689611
iter  40 value 96.312764
iter  50 value 93.224586
iter  60 value 92.561492
iter  70 value 92.214721
iter  80 value 92.036365
iter  90 value 91.933025
iter 100 value 91.641332
final  value 91.641332 
stopped after 100 iterations
# weights:  11
initial  value 181.058950 
iter  10 value 156.847456
iter  20 value 156.021342
final  value 156.014187 
converged</code></pre>
</div>
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="co"># train the LVQ model</span></span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>modelLvq <span class="ot">&lt;-</span> <span class="fu">train</span>(coast <span class="sc">~</span> ., <span class="at">data=</span>both, <span class="at">method=</span><span class="st">"lvq"</span>, <span class="at">trControl=</span>control)</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a><span class="co"># train the GBM model</span></span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a>modelGbm <span class="ot">&lt;-</span> <span class="fu">train</span>(coast <span class="sc">~</span> ., <span class="at">data=</span>both, <span class="at">method=</span><span class="st">"gbm"</span>, <span class="at">trControl=</span>control)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2795             nan     0.1000    0.0106
     2        1.2633             nan     0.1000    0.0020
     3        1.2463             nan     0.1000    0.0062
     4        1.2329             nan     0.1000    0.0060
     5        1.2173             nan     0.1000    0.0059
     6        1.2081             nan     0.1000    0.0030
     7        1.1972             nan     0.1000    0.0006
     8        1.1905             nan     0.1000    0.0025
     9        1.1847             nan     0.1000    0.0010
    10        1.1769             nan     0.1000    0.0028
    20        1.1240             nan     0.1000   -0.0013
    40        1.0687             nan     0.1000   -0.0029
    60        1.0297             nan     0.1000   -0.0035
    80        0.9996             nan     0.1000   -0.0003
   100        0.9746             nan     0.1000   -0.0010
   120        0.9506             nan     0.1000   -0.0016
   140        0.9320             nan     0.1000   -0.0020
   150        0.9198             nan     0.1000   -0.0020

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2739             nan     0.1000    0.0131
     2        1.2511             nan     0.1000    0.0051
     3        1.2348             nan     0.1000    0.0018
     4        1.2191             nan     0.1000    0.0033
     5        1.2074             nan     0.1000   -0.0018
     6        1.1852             nan     0.1000    0.0049
     7        1.1709             nan     0.1000   -0.0016
     8        1.1552             nan     0.1000    0.0022
     9        1.1451             nan     0.1000    0.0027
    10        1.1316             nan     0.1000    0.0020
    20        1.0671             nan     0.1000   -0.0021
    40        0.9843             nan     0.1000   -0.0061
    60        0.9161             nan     0.1000   -0.0024
    80        0.8360             nan     0.1000   -0.0030
   100        0.7942             nan     0.1000   -0.0042
   120        0.7504             nan     0.1000   -0.0013
   140        0.7107             nan     0.1000   -0.0033
   150        0.6962             nan     0.1000   -0.0034

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2748             nan     0.1000    0.0091
     2        1.2447             nan     0.1000    0.0095
     3        1.2170             nan     0.1000    0.0132
     4        1.1923             nan     0.1000    0.0039
     5        1.1762             nan     0.1000    0.0009
     6        1.1618             nan     0.1000   -0.0063
     7        1.1465             nan     0.1000    0.0011
     8        1.1304             nan     0.1000   -0.0021
     9        1.1189             nan     0.1000    0.0018
    10        1.1015             nan     0.1000   -0.0021
    20        1.0066             nan     0.1000   -0.0022
    40        0.8816             nan     0.1000   -0.0086
    60        0.7828             nan     0.1000   -0.0075
    80        0.7165             nan     0.1000   -0.0035
   100        0.6561             nan     0.1000   -0.0021
   120        0.5956             nan     0.1000   -0.0019
   140        0.5563             nan     0.1000   -0.0027
   150        0.5321             nan     0.1000   -0.0016

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2840             nan     0.1000    0.0071
     2        1.2697             nan     0.1000    0.0042
     3        1.2526             nan     0.1000    0.0063
     4        1.2423             nan     0.1000    0.0060
     5        1.2281             nan     0.1000    0.0024
     6        1.2173             nan     0.1000    0.0018
     7        1.2104             nan     0.1000    0.0019
     8        1.2023             nan     0.1000    0.0027
     9        1.2003             nan     0.1000   -0.0018
    10        1.1928             nan     0.1000    0.0004
    20        1.1517             nan     0.1000    0.0018
    40        1.1071             nan     0.1000   -0.0047
    60        1.0747             nan     0.1000   -0.0012
    80        1.0414             nan     0.1000   -0.0013
   100        1.0127             nan     0.1000   -0.0036
   120        0.9884             nan     0.1000   -0.0016
   140        0.9666             nan     0.1000   -0.0019
   150        0.9552             nan     0.1000   -0.0021

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2786             nan     0.1000    0.0117
     2        1.2605             nan     0.1000    0.0043
     3        1.2479             nan     0.1000   -0.0010
     4        1.2287             nan     0.1000    0.0058
     5        1.2155             nan     0.1000    0.0051
     6        1.2054             nan     0.1000   -0.0014
     7        1.1875             nan     0.1000    0.0029
     8        1.1803             nan     0.1000   -0.0021
     9        1.1717             nan     0.1000    0.0011
    10        1.1649             nan     0.1000   -0.0028
    20        1.0932             nan     0.1000   -0.0041
    40        0.9915             nan     0.1000   -0.0063
    60        0.9221             nan     0.1000   -0.0032
    80        0.8659             nan     0.1000   -0.0030
   100        0.8238             nan     0.1000   -0.0043
   120        0.7819             nan     0.1000   -0.0025
   140        0.7433             nan     0.1000   -0.0078
   150        0.7191             nan     0.1000   -0.0032

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2757             nan     0.1000    0.0000
     2        1.2447             nan     0.1000    0.0096
     3        1.2216             nan     0.1000    0.0027
     4        1.1958             nan     0.1000    0.0012
     5        1.1748             nan     0.1000    0.0023
     6        1.1602             nan     0.1000    0.0017
     7        1.1513             nan     0.1000   -0.0018
     8        1.1405             nan     0.1000   -0.0020
     9        1.1244             nan     0.1000   -0.0040
    10        1.1165             nan     0.1000   -0.0045
    20        1.0374             nan     0.1000    0.0005
    40        0.9224             nan     0.1000   -0.0057
    60        0.8244             nan     0.1000   -0.0025
    80        0.7424             nan     0.1000   -0.0046
   100        0.6821             nan     0.1000   -0.0046
   120        0.6353             nan     0.1000   -0.0029
   140        0.5884             nan     0.1000   -0.0040
   150        0.5632             nan     0.1000   -0.0042

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2844             nan     0.1000    0.0105
     2        1.2645             nan     0.1000    0.0083
     3        1.2414             nan     0.1000    0.0095
     4        1.2259             nan     0.1000    0.0084
     5        1.2225             nan     0.1000   -0.0008
     6        1.2124             nan     0.1000    0.0009
     7        1.1972             nan     0.1000    0.0045
     8        1.1842             nan     0.1000    0.0052
     9        1.1736             nan     0.1000    0.0034
    10        1.1682             nan     0.1000    0.0003
    20        1.1202             nan     0.1000   -0.0027
    40        1.0646             nan     0.1000   -0.0011
    60        1.0208             nan     0.1000   -0.0046
    80        0.9966             nan     0.1000   -0.0036
   100        0.9679             nan     0.1000   -0.0052
   120        0.9392             nan     0.1000   -0.0032
   140        0.9215             nan     0.1000   -0.0016
   150        0.9102             nan     0.1000   -0.0034

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2802             nan     0.1000    0.0093
     2        1.2467             nan     0.1000    0.0113
     3        1.2203             nan     0.1000    0.0078
     4        1.1995             nan     0.1000    0.0028
     5        1.1833             nan     0.1000    0.0034
     6        1.1697             nan     0.1000    0.0026
     7        1.1599             nan     0.1000   -0.0061
     8        1.1537             nan     0.1000   -0.0011
     9        1.1468             nan     0.1000   -0.0021
    10        1.1361             nan     0.1000   -0.0009
    20        1.0618             nan     0.1000   -0.0017
    40        0.9737             nan     0.1000   -0.0026
    60        0.9057             nan     0.1000   -0.0017
    80        0.8439             nan     0.1000   -0.0028
   100        0.7820             nan     0.1000   -0.0046
   120        0.7411             nan     0.1000   -0.0036
   140        0.7067             nan     0.1000   -0.0023
   150        0.6913             nan     0.1000   -0.0035

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2714             nan     0.1000    0.0135
     2        1.2442             nan     0.1000    0.0065
     3        1.2227             nan     0.1000    0.0040
     4        1.1998             nan     0.1000    0.0059
     5        1.1769             nan     0.1000    0.0067
     6        1.1545             nan     0.1000    0.0041
     7        1.1393             nan     0.1000    0.0012
     8        1.1253             nan     0.1000   -0.0040
     9        1.1181             nan     0.1000   -0.0027
    10        1.1090             nan     0.1000   -0.0034
    20        1.0147             nan     0.1000   -0.0044
    40        0.8857             nan     0.1000   -0.0047
    60        0.7907             nan     0.1000   -0.0019
    80        0.7140             nan     0.1000   -0.0027
   100        0.6560             nan     0.1000   -0.0045
   120        0.5938             nan     0.1000   -0.0022
   140        0.5513             nan     0.1000   -0.0051
   150        0.5312             nan     0.1000   -0.0015

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2788             nan     0.1000    0.0109
     2        1.2475             nan     0.1000    0.0122
     3        1.2267             nan     0.1000    0.0094
     4        1.2081             nan     0.1000    0.0058
     5        1.1940             nan     0.1000    0.0049
     6        1.1841             nan     0.1000    0.0037
     7        1.1715             nan     0.1000    0.0025
     8        1.1621             nan     0.1000    0.0024
     9        1.1526             nan     0.1000    0.0001
    10        1.1435             nan     0.1000    0.0031
    20        1.1006             nan     0.1000   -0.0026
    40        1.0496             nan     0.1000   -0.0042
    60        1.0155             nan     0.1000   -0.0028
    80        0.9843             nan     0.1000   -0.0013
   100        0.9544             nan     0.1000   -0.0037
   120        0.9273             nan     0.1000   -0.0035
   140        0.9017             nan     0.1000   -0.0025
   150        0.8902             nan     0.1000   -0.0041

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2747             nan     0.1000    0.0134
     2        1.2419             nan     0.1000    0.0092
     3        1.2154             nan     0.1000    0.0094
     4        1.1938             nan     0.1000    0.0050
     5        1.1713             nan     0.1000    0.0054
     6        1.1563             nan     0.1000    0.0020
     7        1.1445             nan     0.1000    0.0038
     8        1.1319             nan     0.1000    0.0013
     9        1.1176             nan     0.1000   -0.0041
    10        1.1104             nan     0.1000   -0.0022
    20        1.0437             nan     0.1000   -0.0037
    40        0.9475             nan     0.1000   -0.0063
    60        0.8563             nan     0.1000   -0.0026
    80        0.8057             nan     0.1000   -0.0033
   100        0.7549             nan     0.1000   -0.0038
   120        0.7105             nan     0.1000   -0.0017
   140        0.6702             nan     0.1000   -0.0043
   150        0.6528             nan     0.1000   -0.0020

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2684             nan     0.1000    0.0111
     2        1.2337             nan     0.1000    0.0084
     3        1.2066             nan     0.1000    0.0045
     4        1.1825             nan     0.1000    0.0057
     5        1.1553             nan     0.1000    0.0061
     6        1.1315             nan     0.1000    0.0057
     7        1.1156             nan     0.1000    0.0061
     8        1.0998             nan     0.1000    0.0014
     9        1.0844             nan     0.1000    0.0014
    10        1.0744             nan     0.1000   -0.0019
    20        0.9674             nan     0.1000   -0.0031
    40        0.8383             nan     0.1000   -0.0038
    60        0.7533             nan     0.1000   -0.0048
    80        0.6764             nan     0.1000   -0.0058
   100        0.6270             nan     0.1000   -0.0032
   120        0.5733             nan     0.1000   -0.0044
   140        0.5216             nan     0.1000   -0.0031
   150        0.4979             nan     0.1000   -0.0037

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2810             nan     0.1000    0.0104
     2        1.2607             nan     0.1000    0.0062
     3        1.2321             nan     0.1000    0.0081
     4        1.2176             nan     0.1000    0.0061
     5        1.2106             nan     0.1000   -0.0022
     6        1.2031             nan     0.1000    0.0021
     7        1.1956             nan     0.1000    0.0006
     8        1.1869             nan     0.1000    0.0034
     9        1.1793             nan     0.1000    0.0006
    10        1.1752             nan     0.1000   -0.0031
    20        1.1249             nan     0.1000   -0.0025
    40        1.0699             nan     0.1000   -0.0029
    60        1.0319             nan     0.1000   -0.0033
    80        0.9991             nan     0.1000   -0.0030
   100        0.9779             nan     0.1000   -0.0006
   120        0.9548             nan     0.1000   -0.0029
   140        0.9269             nan     0.1000   -0.0023
   150        0.9174             nan     0.1000   -0.0041

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2755             nan     0.1000    0.0097
     2        1.2454             nan     0.1000    0.0096
     3        1.2264             nan     0.1000    0.0078
     4        1.2095             nan     0.1000    0.0046
     5        1.1966             nan     0.1000    0.0013
     6        1.1879             nan     0.1000    0.0002
     7        1.1776             nan     0.1000   -0.0004
     8        1.1629             nan     0.1000    0.0028
     9        1.1512             nan     0.1000    0.0010
    10        1.1338             nan     0.1000    0.0046
    20        1.0572             nan     0.1000   -0.0011
    40        0.9708             nan     0.1000   -0.0031
    60        0.8919             nan     0.1000   -0.0034
    80        0.8454             nan     0.1000   -0.0020
   100        0.7855             nan     0.1000   -0.0053
   120        0.7443             nan     0.1000   -0.0033
   140        0.7024             nan     0.1000   -0.0041
   150        0.6791             nan     0.1000   -0.0042

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2701             nan     0.1000    0.0100
     2        1.2395             nan     0.1000    0.0041
     3        1.2116             nan     0.1000    0.0054
     4        1.1900             nan     0.1000    0.0003
     5        1.1725             nan     0.1000    0.0009
     6        1.1565             nan     0.1000    0.0029
     7        1.1386             nan     0.1000    0.0025
     8        1.1258             nan     0.1000    0.0008
     9        1.1106             nan     0.1000   -0.0019
    10        1.0995             nan     0.1000   -0.0009
    20        1.0028             nan     0.1000   -0.0036
    40        0.8822             nan     0.1000   -0.0064
    60        0.7885             nan     0.1000   -0.0043
    80        0.7075             nan     0.1000   -0.0042
   100        0.6370             nan     0.1000   -0.0060
   120        0.5855             nan     0.1000   -0.0035
   140        0.5364             nan     0.1000   -0.0016
   150        0.5176             nan     0.1000   -0.0030

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2871             nan     0.1000    0.0040
     2        1.2591             nan     0.1000    0.0128
     3        1.2380             nan     0.1000    0.0100
     4        1.2212             nan     0.1000    0.0070
     5        1.2032             nan     0.1000    0.0055
     6        1.1841             nan     0.1000    0.0043
     7        1.1747             nan     0.1000    0.0023
     8        1.1655             nan     0.1000    0.0028
     9        1.1541             nan     0.1000    0.0031
    10        1.1459             nan     0.1000    0.0016
    20        1.1111             nan     0.1000   -0.0035
    40        1.0576             nan     0.1000   -0.0036
    60        1.0140             nan     0.1000   -0.0004
    80        0.9774             nan     0.1000   -0.0032
   100        0.9491             nan     0.1000   -0.0021
   120        0.9241             nan     0.1000   -0.0025
   140        0.9014             nan     0.1000   -0.0010
   150        0.8864             nan     0.1000   -0.0025

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2748             nan     0.1000    0.0110
     2        1.2406             nan     0.1000    0.0104
     3        1.2234             nan     0.1000    0.0037
     4        1.2018             nan     0.1000    0.0095
     5        1.1733             nan     0.1000    0.0062
     6        1.1576             nan     0.1000    0.0024
     7        1.1413             nan     0.1000    0.0046
     8        1.1312             nan     0.1000   -0.0028
     9        1.1205             nan     0.1000    0.0013
    10        1.1084             nan     0.1000   -0.0007
    20        1.0355             nan     0.1000   -0.0032
    40        0.9402             nan     0.1000   -0.0013
    60        0.8757             nan     0.1000   -0.0067
    80        0.8247             nan     0.1000   -0.0027
   100        0.7743             nan     0.1000   -0.0029
   120        0.7323             nan     0.1000   -0.0038
   140        0.6914             nan     0.1000   -0.0036
   150        0.6727             nan     0.1000   -0.0031

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2878             nan     0.1000   -0.0042
     2        1.2475             nan     0.1000    0.0134
     3        1.2203             nan     0.1000    0.0093
     4        1.1929             nan     0.1000    0.0065
     5        1.1627             nan     0.1000    0.0083
     6        1.1445             nan     0.1000    0.0035
     7        1.1249             nan     0.1000    0.0048
     8        1.1042             nan     0.1000   -0.0011
     9        1.0906             nan     0.1000    0.0002
    10        1.0785             nan     0.1000    0.0022
    20        0.9859             nan     0.1000   -0.0038
    40        0.8745             nan     0.1000   -0.0004
    60        0.7791             nan     0.1000   -0.0028
    80        0.7041             nan     0.1000   -0.0030
   100        0.6415             nan     0.1000   -0.0032
   120        0.5936             nan     0.1000   -0.0042
   140        0.5446             nan     0.1000   -0.0035
   150        0.5225             nan     0.1000   -0.0041

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2896             nan     0.1000    0.0042
     2        1.2802             nan     0.1000    0.0028
     3        1.2599             nan     0.1000    0.0056
     4        1.2371             nan     0.1000    0.0078
     5        1.2319             nan     0.1000   -0.0004
     6        1.2264             nan     0.1000   -0.0013
     7        1.2127             nan     0.1000    0.0065
     8        1.1988             nan     0.1000    0.0066
     9        1.1924             nan     0.1000   -0.0007
    10        1.1840             nan     0.1000    0.0031
    20        1.1338             nan     0.1000   -0.0016
    40        1.0639             nan     0.1000   -0.0010
    60        1.0139             nan     0.1000   -0.0040
    80        0.9753             nan     0.1000   -0.0002
   100        0.9494             nan     0.1000   -0.0038
   120        0.9211             nan     0.1000   -0.0020
   140        0.8976             nan     0.1000   -0.0038
   150        0.8844             nan     0.1000   -0.0023

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2724             nan     0.1000    0.0127
     2        1.2509             nan     0.1000    0.0077
     3        1.2285             nan     0.1000    0.0090
     4        1.2136             nan     0.1000    0.0052
     5        1.1951             nan     0.1000    0.0066
     6        1.1893             nan     0.1000   -0.0057
     7        1.1743             nan     0.1000   -0.0059
     8        1.1620             nan     0.1000    0.0013
     9        1.1508             nan     0.1000    0.0023
    10        1.1416             nan     0.1000   -0.0008
    20        1.0625             nan     0.1000   -0.0027
    40        0.9557             nan     0.1000   -0.0033
    60        0.8795             nan     0.1000   -0.0049
    80        0.8139             nan     0.1000   -0.0036
   100        0.7654             nan     0.1000   -0.0037
   120        0.7215             nan     0.1000   -0.0019
   140        0.6700             nan     0.1000   -0.0030
   150        0.6525             nan     0.1000   -0.0028

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2784             nan     0.1000    0.0087
     2        1.2490             nan     0.1000    0.0029
     3        1.2263             nan     0.1000    0.0068
     4        1.2062             nan     0.1000    0.0040
     5        1.1849             nan     0.1000   -0.0002
     6        1.1657             nan     0.1000    0.0071
     7        1.1478             nan     0.1000   -0.0026
     8        1.1329             nan     0.1000   -0.0007
     9        1.1212             nan     0.1000    0.0001
    10        1.1019             nan     0.1000    0.0017
    20        0.9914             nan     0.1000    0.0001
    40        0.8663             nan     0.1000   -0.0004
    60        0.7735             nan     0.1000   -0.0022
    80        0.6857             nan     0.1000   -0.0016
   100        0.6252             nan     0.1000   -0.0021
   120        0.5680             nan     0.1000   -0.0015
   140        0.5229             nan     0.1000   -0.0031
   150        0.4987             nan     0.1000   -0.0021

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2755             nan     0.1000    0.0074
     2        1.2589             nan     0.1000    0.0064
     3        1.2424             nan     0.1000    0.0050
     4        1.2328             nan     0.1000    0.0041
     5        1.2206             nan     0.1000    0.0021
     6        1.2127             nan     0.1000   -0.0028
     7        1.2069             nan     0.1000   -0.0051
     8        1.1958             nan     0.1000   -0.0006
     9        1.1897             nan     0.1000    0.0004
    10        1.1852             nan     0.1000   -0.0054
    20        1.1438             nan     0.1000   -0.0021
    40        1.1066             nan     0.1000   -0.0010
    60        1.0683             nan     0.1000   -0.0012
    80        1.0368             nan     0.1000   -0.0013
   100        1.0113             nan     0.1000   -0.0033
   120        0.9922             nan     0.1000   -0.0033
   140        0.9720             nan     0.1000   -0.0034
   150        0.9625             nan     0.1000   -0.0034

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2833             nan     0.1000   -0.0051
     2        1.2589             nan     0.1000    0.0088
     3        1.2352             nan     0.1000    0.0073
     4        1.2171             nan     0.1000    0.0052
     5        1.2034             nan     0.1000    0.0026
     6        1.1896             nan     0.1000    0.0024
     7        1.1776             nan     0.1000   -0.0001
     8        1.1655             nan     0.1000   -0.0012
     9        1.1563             nan     0.1000   -0.0009
    10        1.1488             nan     0.1000   -0.0010
    20        1.0800             nan     0.1000   -0.0063
    40        0.9859             nan     0.1000   -0.0022
    60        0.9311             nan     0.1000   -0.0039
    80        0.8777             nan     0.1000   -0.0033
   100        0.8255             nan     0.1000   -0.0020
   120        0.7821             nan     0.1000   -0.0027
   140        0.7398             nan     0.1000   -0.0022
   150        0.7229             nan     0.1000   -0.0032

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2686             nan     0.1000    0.0073
     2        1.2443             nan     0.1000    0.0035
     3        1.2218             nan     0.1000    0.0030
     4        1.2001             nan     0.1000    0.0042
     5        1.1880             nan     0.1000   -0.0046
     6        1.1712             nan     0.1000   -0.0009
     7        1.1551             nan     0.1000   -0.0026
     8        1.1430             nan     0.1000   -0.0037
     9        1.1276             nan     0.1000    0.0013
    10        1.1157             nan     0.1000   -0.0016
    20        1.0173             nan     0.1000   -0.0032
    40        0.8996             nan     0.1000   -0.0056
    60        0.8067             nan     0.1000   -0.0041
    80        0.7396             nan     0.1000   -0.0020
   100        0.6663             nan     0.1000   -0.0055
   120        0.6140             nan     0.1000   -0.0029
   140        0.5684             nan     0.1000   -0.0053
   150        0.5473             nan     0.1000   -0.0030

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2845             nan     0.1000    0.0127
     2        1.2700             nan     0.1000    0.0081
     3        1.2502             nan     0.1000    0.0076
     4        1.2342             nan     0.1000    0.0044
     5        1.2219             nan     0.1000    0.0058
     6        1.2112             nan     0.1000    0.0032
     7        1.2004             nan     0.1000   -0.0001
     8        1.1992             nan     0.1000   -0.0059
     9        1.1916             nan     0.1000    0.0014
    10        1.1879             nan     0.1000   -0.0020
    20        1.1399             nan     0.1000   -0.0012
    40        1.0907             nan     0.1000   -0.0018
    60        1.0471             nan     0.1000   -0.0018
    80        1.0092             nan     0.1000   -0.0039
   100        0.9832             nan     0.1000   -0.0072
   120        0.9556             nan     0.1000   -0.0023
   140        0.9284             nan     0.1000   -0.0023
   150        0.9117             nan     0.1000   -0.0009

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2763             nan     0.1000    0.0113
     2        1.2565             nan     0.1000    0.0087
     3        1.2339             nan     0.1000    0.0036
     4        1.2141             nan     0.1000    0.0008
     5        1.2014             nan     0.1000    0.0015
     6        1.1871             nan     0.1000   -0.0013
     7        1.1808             nan     0.1000   -0.0039
     8        1.1686             nan     0.1000    0.0008
     9        1.1598             nan     0.1000   -0.0016
    10        1.1505             nan     0.1000   -0.0010
    20        1.0889             nan     0.1000   -0.0043
    40        0.9911             nan     0.1000   -0.0022
    60        0.9250             nan     0.1000   -0.0034
    80        0.8652             nan     0.1000   -0.0025
   100        0.8024             nan     0.1000   -0.0036
   120        0.7558             nan     0.1000   -0.0016
   140        0.7138             nan     0.1000   -0.0027
   150        0.6936             nan     0.1000   -0.0027

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2760             nan     0.1000    0.0096
     2        1.2527             nan     0.1000    0.0123
     3        1.2429             nan     0.1000   -0.0090
     4        1.2185             nan     0.1000    0.0116
     5        1.1966             nan     0.1000    0.0010
     6        1.1860             nan     0.1000   -0.0026
     7        1.1643             nan     0.1000    0.0070
     8        1.1528             nan     0.1000   -0.0010
     9        1.1343             nan     0.1000    0.0051
    10        1.1229             nan     0.1000    0.0003
    20        1.0420             nan     0.1000   -0.0036
    40        0.8942             nan     0.1000   -0.0024
    60        0.8025             nan     0.1000   -0.0044
    80        0.7349             nan     0.1000   -0.0054
   100        0.6652             nan     0.1000   -0.0042
   120        0.6034             nan     0.1000   -0.0027
   140        0.5595             nan     0.1000   -0.0044
   150        0.5380             nan     0.1000   -0.0032

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2712             nan     0.1000    0.0140
     2        1.2535             nan     0.1000    0.0083
     3        1.2372             nan     0.1000    0.0059
     4        1.2103             nan     0.1000    0.0041
     5        1.2011             nan     0.1000   -0.0012
     6        1.1846             nan     0.1000    0.0011
     7        1.1744             nan     0.1000    0.0020
     8        1.1721             nan     0.1000   -0.0015
     9        1.1628             nan     0.1000    0.0037
    10        1.1567             nan     0.1000    0.0024
    20        1.1009             nan     0.1000   -0.0012
    40        1.0576             nan     0.1000   -0.0019
    60        1.0153             nan     0.1000   -0.0039
    80        0.9862             nan     0.1000   -0.0034
   100        0.9634             nan     0.1000   -0.0008
   120        0.9403             nan     0.1000   -0.0022
   140        0.9097             nan     0.1000   -0.0024
   150        0.8952             nan     0.1000   -0.0024

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2702             nan     0.1000    0.0164
     2        1.2425             nan     0.1000    0.0141
     3        1.2225             nan     0.1000    0.0067
     4        1.2065             nan     0.1000    0.0043
     5        1.1863             nan     0.1000    0.0064
     6        1.1664             nan     0.1000    0.0083
     7        1.1505             nan     0.1000   -0.0000
     8        1.1339             nan     0.1000    0.0059
     9        1.1224             nan     0.1000    0.0005
    10        1.1126             nan     0.1000   -0.0023
    20        1.0338             nan     0.1000   -0.0047
    40        0.9557             nan     0.1000   -0.0031
    60        0.8818             nan     0.1000   -0.0068
    80        0.8231             nan     0.1000   -0.0016
   100        0.7713             nan     0.1000   -0.0048
   120        0.7200             nan     0.1000   -0.0011
   140        0.6830             nan     0.1000   -0.0017
   150        0.6564             nan     0.1000   -0.0028

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2702             nan     0.1000    0.0125
     2        1.2390             nan     0.1000    0.0106
     3        1.2069             nan     0.1000    0.0096
     4        1.1904             nan     0.1000    0.0007
     5        1.1661             nan     0.1000    0.0080
     6        1.1475             nan     0.1000    0.0017
     7        1.1316             nan     0.1000    0.0021
     8        1.1141             nan     0.1000    0.0015
     9        1.1052             nan     0.1000   -0.0032
    10        1.1001             nan     0.1000   -0.0043
    20        0.9980             nan     0.1000   -0.0005
    40        0.8655             nan     0.1000   -0.0019
    60        0.7835             nan     0.1000   -0.0067
    80        0.7066             nan     0.1000   -0.0052
   100        0.6428             nan     0.1000   -0.0037
   120        0.5931             nan     0.1000   -0.0078
   140        0.5449             nan     0.1000   -0.0054
   150        0.5234             nan     0.1000   -0.0031

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2787             nan     0.1000    0.0113
     2        1.2661             nan     0.1000    0.0004
     3        1.2441             nan     0.1000    0.0061
     4        1.2294             nan     0.1000    0.0046
     5        1.2186             nan     0.1000    0.0025
     6        1.2045             nan     0.1000    0.0044
     7        1.1950             nan     0.1000    0.0044
     8        1.1878             nan     0.1000    0.0007
     9        1.1792             nan     0.1000    0.0032
    10        1.1721             nan     0.1000    0.0001
    20        1.1319             nan     0.1000   -0.0024
    40        1.0815             nan     0.1000   -0.0005
    60        1.0555             nan     0.1000   -0.0021
    80        1.0223             nan     0.1000   -0.0058
   100        0.9945             nan     0.1000   -0.0011
   120        0.9762             nan     0.1000   -0.0022
   140        0.9482             nan     0.1000   -0.0018
   150        0.9359             nan     0.1000   -0.0009

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2735             nan     0.1000    0.0087
     2        1.2553             nan     0.1000    0.0044
     3        1.2351             nan     0.1000    0.0066
     4        1.2130             nan     0.1000    0.0072
     5        1.1999             nan     0.1000   -0.0006
     6        1.1864             nan     0.1000   -0.0013
     7        1.1702             nan     0.1000    0.0022
     8        1.1587             nan     0.1000    0.0029
     9        1.1527             nan     0.1000   -0.0016
    10        1.1438             nan     0.1000    0.0002
    20        1.0789             nan     0.1000   -0.0003
    40        0.9903             nan     0.1000   -0.0017
    60        0.9261             nan     0.1000   -0.0017
    80        0.8705             nan     0.1000   -0.0081
   100        0.8264             nan     0.1000   -0.0039
   120        0.7754             nan     0.1000   -0.0055
   140        0.7337             nan     0.1000   -0.0032
   150        0.7104             nan     0.1000   -0.0010

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2802             nan     0.1000    0.0035
     2        1.2473             nan     0.1000    0.0103
     3        1.2196             nan     0.1000    0.0057
     4        1.2091             nan     0.1000   -0.0039
     5        1.1848             nan     0.1000    0.0044
     6        1.1673             nan     0.1000    0.0013
     7        1.1449             nan     0.1000    0.0032
     8        1.1268             nan     0.1000    0.0034
     9        1.1191             nan     0.1000   -0.0041
    10        1.1138             nan     0.1000   -0.0054
    20        1.0284             nan     0.1000   -0.0052
    40        0.9058             nan     0.1000   -0.0042
    60        0.8172             nan     0.1000   -0.0033
    80        0.7404             nan     0.1000   -0.0033
   100        0.6852             nan     0.1000   -0.0091
   120        0.6297             nan     0.1000   -0.0038
   140        0.5824             nan     0.1000   -0.0012
   150        0.5619             nan     0.1000   -0.0056

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2772             nan     0.1000    0.0132
     2        1.2516             nan     0.1000    0.0133
     3        1.2321             nan     0.1000    0.0084
     4        1.2109             nan     0.1000    0.0094
     5        1.1970             nan     0.1000    0.0038
     6        1.1881             nan     0.1000    0.0013
     7        1.1771             nan     0.1000    0.0044
     8        1.1628             nan     0.1000    0.0052
     9        1.1539             nan     0.1000    0.0018
    10        1.1448             nan     0.1000    0.0012
    20        1.1055             nan     0.1000   -0.0026
    40        1.0575             nan     0.1000   -0.0015
    60        1.0159             nan     0.1000   -0.0028
    80        0.9769             nan     0.1000   -0.0004
   100        0.9463             nan     0.1000   -0.0016
   120        0.9250             nan     0.1000   -0.0028
   140        0.9066             nan     0.1000   -0.0034
   150        0.8948             nan     0.1000   -0.0016

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2617             nan     0.1000    0.0159
     2        1.2284             nan     0.1000    0.0112
     3        1.2033             nan     0.1000    0.0045
     4        1.1818             nan     0.1000    0.0095
     5        1.1586             nan     0.1000    0.0056
     6        1.1532             nan     0.1000   -0.0043
     7        1.1387             nan     0.1000    0.0025
     8        1.1335             nan     0.1000   -0.0043
     9        1.1231             nan     0.1000   -0.0030
    10        1.1111             nan     0.1000   -0.0003
    20        1.0518             nan     0.1000   -0.0018
    40        0.9561             nan     0.1000   -0.0031
    60        0.8926             nan     0.1000   -0.0015
    80        0.8379             nan     0.1000   -0.0048
   100        0.7872             nan     0.1000    0.0008
   120        0.7464             nan     0.1000   -0.0031
   140        0.6987             nan     0.1000   -0.0032
   150        0.6834             nan     0.1000    0.0004

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2613             nan     0.1000    0.0149
     2        1.2414             nan     0.1000    0.0030
     3        1.2167             nan     0.1000    0.0027
     4        1.1935             nan     0.1000    0.0064
     5        1.1747             nan     0.1000    0.0038
     6        1.1577             nan     0.1000   -0.0024
     7        1.1332             nan     0.1000    0.0058
     8        1.1128             nan     0.1000    0.0028
     9        1.0980             nan     0.1000    0.0022
    10        1.0838             nan     0.1000   -0.0001
    20        0.9811             nan     0.1000   -0.0051
    40        0.8589             nan     0.1000   -0.0025
    60        0.7694             nan     0.1000   -0.0055
    80        0.6881             nan     0.1000   -0.0024
   100        0.6246             nan     0.1000   -0.0026
   120        0.5714             nan     0.1000   -0.0048
   140        0.5255             nan     0.1000   -0.0045
   150        0.5036             nan     0.1000   -0.0047

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2920             nan     0.1000    0.0021
     2        1.2640             nan     0.1000    0.0095
     3        1.2467             nan     0.1000    0.0025
     4        1.2346             nan     0.1000    0.0031
     5        1.2252             nan     0.1000    0.0031
     6        1.2121             nan     0.1000    0.0072
     7        1.2010             nan     0.1000    0.0039
     8        1.1948             nan     0.1000    0.0017
     9        1.1856             nan     0.1000    0.0018
    10        1.1786             nan     0.1000    0.0024
    20        1.1383             nan     0.1000   -0.0051
    40        1.0789             nan     0.1000   -0.0015
    60        1.0339             nan     0.1000   -0.0013
    80        0.9994             nan     0.1000   -0.0008
   100        0.9735             nan     0.1000   -0.0007
   120        0.9535             nan     0.1000   -0.0027
   140        0.9331             nan     0.1000   -0.0020
   150        0.9233             nan     0.1000   -0.0019

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2769             nan     0.1000    0.0110
     2        1.2539             nan     0.1000    0.0053
     3        1.2313             nan     0.1000    0.0056
     4        1.2136             nan     0.1000    0.0042
     5        1.2012             nan     0.1000   -0.0026
     6        1.1899             nan     0.1000    0.0036
     7        1.1783             nan     0.1000   -0.0008
     8        1.1658             nan     0.1000    0.0028
     9        1.1598             nan     0.1000   -0.0041
    10        1.1493             nan     0.1000   -0.0006
    20        1.0699             nan     0.1000   -0.0017
    40        0.9831             nan     0.1000   -0.0045
    60        0.9162             nan     0.1000   -0.0016
    80        0.8520             nan     0.1000   -0.0038
   100        0.7991             nan     0.1000   -0.0024
   120        0.7478             nan     0.1000   -0.0024
   140        0.7130             nan     0.1000   -0.0027
   150        0.6945             nan     0.1000   -0.0055

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2808             nan     0.1000    0.0011
     2        1.2555             nan     0.1000    0.0077
     3        1.2201             nan     0.1000    0.0138
     4        1.1917             nan     0.1000    0.0037
     5        1.1661             nan     0.1000    0.0016
     6        1.1478             nan     0.1000    0.0059
     7        1.1323             nan     0.1000   -0.0030
     8        1.1173             nan     0.1000   -0.0013
     9        1.1003             nan     0.1000   -0.0028
    10        1.0844             nan     0.1000    0.0028
    20        0.9837             nan     0.1000   -0.0048
    40        0.8600             nan     0.1000   -0.0023
    60        0.7679             nan     0.1000   -0.0031
    80        0.6962             nan     0.1000   -0.0013
   100        0.6312             nan     0.1000   -0.0034
   120        0.5838             nan     0.1000   -0.0051
   140        0.5332             nan     0.1000   -0.0028
   150        0.5119             nan     0.1000   -0.0016

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2870             nan     0.1000    0.0105
     2        1.2618             nan     0.1000    0.0118
     3        1.2427             nan     0.1000    0.0075
     4        1.2262             nan     0.1000    0.0037
     5        1.2178             nan     0.1000    0.0009
     6        1.2029             nan     0.1000    0.0023
     7        1.1896             nan     0.1000    0.0034
     8        1.1809             nan     0.1000    0.0015
     9        1.1747             nan     0.1000    0.0011
    10        1.1674             nan     0.1000    0.0029
    20        1.1317             nan     0.1000   -0.0002
    40        1.0866             nan     0.1000   -0.0031
    60        1.0443             nan     0.1000   -0.0002
    80        1.0090             nan     0.1000   -0.0015
   100        0.9742             nan     0.1000   -0.0021
   120        0.9524             nan     0.1000   -0.0027
   140        0.9304             nan     0.1000   -0.0027
   150        0.9192             nan     0.1000   -0.0040

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2815             nan     0.1000    0.0135
     2        1.2562             nan     0.1000    0.0071
     3        1.2326             nan     0.1000    0.0048
     4        1.2092             nan     0.1000    0.0038
     5        1.1999             nan     0.1000    0.0023
     6        1.1838             nan     0.1000    0.0063
     7        1.1712             nan     0.1000    0.0025
     8        1.1608             nan     0.1000   -0.0008
     9        1.1502             nan     0.1000    0.0010
    10        1.1460             nan     0.1000   -0.0031
    20        1.0658             nan     0.1000   -0.0041
    40        0.9692             nan     0.1000   -0.0028
    60        0.8978             nan     0.1000   -0.0047
    80        0.8523             nan     0.1000   -0.0057
   100        0.8082             nan     0.1000   -0.0023
   120        0.7669             nan     0.1000   -0.0038
   140        0.7298             nan     0.1000   -0.0050
   150        0.7161             nan     0.1000   -0.0020

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2604             nan     0.1000    0.0129
     2        1.2366             nan     0.1000    0.0032
     3        1.2173             nan     0.1000    0.0030
     4        1.1970             nan     0.1000    0.0040
     5        1.1786             nan     0.1000    0.0030
     6        1.1600             nan     0.1000   -0.0028
     7        1.1474             nan     0.1000   -0.0008
     8        1.1306             nan     0.1000    0.0009
     9        1.1132             nan     0.1000    0.0033
    10        1.1008             nan     0.1000    0.0000
    20        1.0216             nan     0.1000   -0.0061
    40        0.8960             nan     0.1000   -0.0053
    60        0.7947             nan     0.1000   -0.0034
    80        0.7205             nan     0.1000   -0.0055
   100        0.6503             nan     0.1000   -0.0012
   120        0.6027             nan     0.1000   -0.0027
   140        0.5547             nan     0.1000   -0.0009
   150        0.5422             nan     0.1000   -0.0028

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2854             nan     0.1000    0.0095
     2        1.2618             nan     0.1000    0.0043
     3        1.2476             nan     0.1000    0.0028
     4        1.2310             nan     0.1000    0.0081
     5        1.2249             nan     0.1000    0.0007
     6        1.2161             nan     0.1000    0.0037
     7        1.2091             nan     0.1000    0.0017
     8        1.1973             nan     0.1000    0.0036
     9        1.1884             nan     0.1000    0.0011
    10        1.1849             nan     0.1000   -0.0013
    20        1.1377             nan     0.1000    0.0007
    40        1.0845             nan     0.1000   -0.0056
    60        1.0474             nan     0.1000   -0.0046
    80        1.0162             nan     0.1000   -0.0037
   100        0.9857             nan     0.1000   -0.0022
   120        0.9649             nan     0.1000   -0.0007
   140        0.9395             nan     0.1000   -0.0023
   150        0.9313             nan     0.1000   -0.0023

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2761             nan     0.1000    0.0051
     2        1.2473             nan     0.1000    0.0080
     3        1.2325             nan     0.1000    0.0030
     4        1.2211             nan     0.1000    0.0018
     5        1.2029             nan     0.1000    0.0051
     6        1.1939             nan     0.1000    0.0009
     7        1.1780             nan     0.1000   -0.0007
     8        1.1634             nan     0.1000    0.0005
     9        1.1585             nan     0.1000   -0.0032
    10        1.1543             nan     0.1000   -0.0024
    20        1.0729             nan     0.1000    0.0012
    40        0.9788             nan     0.1000   -0.0002
    60        0.9255             nan     0.1000   -0.0069
    80        0.8605             nan     0.1000   -0.0056
   100        0.8028             nan     0.1000   -0.0014
   120        0.7631             nan     0.1000   -0.0024
   140        0.7148             nan     0.1000   -0.0019
   150        0.6992             nan     0.1000   -0.0031

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2718             nan     0.1000    0.0097
     2        1.2427             nan     0.1000    0.0102
     3        1.2203             nan     0.1000    0.0025
     4        1.1984             nan     0.1000    0.0051
     5        1.1743             nan     0.1000    0.0000
     6        1.1584             nan     0.1000   -0.0004
     7        1.1430             nan     0.1000   -0.0061
     8        1.1298             nan     0.1000   -0.0005
     9        1.1123             nan     0.1000   -0.0028
    10        1.1051             nan     0.1000   -0.0057
    20        1.0163             nan     0.1000   -0.0035
    40        0.8893             nan     0.1000   -0.0048
    60        0.8073             nan     0.1000   -0.0073
    80        0.7154             nan     0.1000   -0.0050
   100        0.6491             nan     0.1000   -0.0022
   120        0.5913             nan     0.1000   -0.0021
   140        0.5508             nan     0.1000   -0.0029
   150        0.5350             nan     0.1000   -0.0016

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2782             nan     0.1000    0.0102
     2        1.2605             nan     0.1000    0.0070
     3        1.2422             nan     0.1000    0.0065
     4        1.2272             nan     0.1000    0.0041
     5        1.2128             nan     0.1000    0.0029
     6        1.1992             nan     0.1000    0.0015
     7        1.1937             nan     0.1000   -0.0015
     8        1.1861             nan     0.1000    0.0017
     9        1.1800             nan     0.1000    0.0006
    10        1.1721             nan     0.1000    0.0012
    20        1.1343             nan     0.1000   -0.0035
    40        1.0779             nan     0.1000   -0.0021
    60        1.0454             nan     0.1000   -0.0022
    80        1.0146             nan     0.1000   -0.0032
   100        0.9835             nan     0.1000   -0.0016
   120        0.9619             nan     0.1000   -0.0032
   140        0.9338             nan     0.1000   -0.0043
   150        0.9212             nan     0.1000   -0.0065

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2779             nan     0.1000    0.0058
     2        1.2626             nan     0.1000    0.0038
     3        1.2340             nan     0.1000    0.0081
     4        1.2090             nan     0.1000    0.0077
     5        1.1878             nan     0.1000    0.0040
     6        1.1776             nan     0.1000    0.0003
     7        1.1685             nan     0.1000    0.0013
     8        1.1561             nan     0.1000    0.0002
     9        1.1504             nan     0.1000   -0.0041
    10        1.1415             nan     0.1000    0.0008
    20        1.0846             nan     0.1000   -0.0079
    40        0.9791             nan     0.1000   -0.0008
    60        0.9120             nan     0.1000   -0.0007
    80        0.8621             nan     0.1000   -0.0021
   100        0.8059             nan     0.1000   -0.0059
   120        0.7629             nan     0.1000   -0.0042
   140        0.7249             nan     0.1000   -0.0072
   150        0.7039             nan     0.1000   -0.0038

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2700             nan     0.1000    0.0119
     2        1.2474             nan     0.1000    0.0069
     3        1.2310             nan     0.1000    0.0018
     4        1.2037             nan     0.1000    0.0047
     5        1.1844             nan     0.1000    0.0039
     6        1.1688             nan     0.1000    0.0007
     7        1.1490             nan     0.1000    0.0028
     8        1.1356             nan     0.1000    0.0015
     9        1.1244             nan     0.1000   -0.0039
    10        1.1092             nan     0.1000    0.0006
    20        1.0146             nan     0.1000   -0.0052
    40        0.8902             nan     0.1000   -0.0056
    60        0.7981             nan     0.1000   -0.0057
    80        0.7159             nan     0.1000   -0.0020
   100        0.6572             nan     0.1000   -0.0028
   120        0.6033             nan     0.1000   -0.0043
   140        0.5538             nan     0.1000   -0.0014
   150        0.5380             nan     0.1000   -0.0029

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2792             nan     0.1000    0.0144
     2        1.2496             nan     0.1000    0.0081
     3        1.2398             nan     0.1000   -0.0010
     4        1.2208             nan     0.1000    0.0041
     5        1.2075             nan     0.1000    0.0036
     6        1.1966             nan     0.1000    0.0027
     7        1.1804             nan     0.1000    0.0040
     8        1.1734             nan     0.1000    0.0005
     9        1.1683             nan     0.1000    0.0001
    10        1.1622             nan     0.1000   -0.0013
    20        1.1223             nan     0.1000   -0.0019
    40        1.0641             nan     0.1000   -0.0014
    60        1.0312             nan     0.1000   -0.0014
    80        0.9941             nan     0.1000   -0.0017
   100        0.9688             nan     0.1000   -0.0022
   120        0.9459             nan     0.1000   -0.0022
   140        0.9341             nan     0.1000   -0.0038
   150        0.9236             nan     0.1000   -0.0020

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2751             nan     0.1000    0.0070
     2        1.2476             nan     0.1000    0.0102
     3        1.2193             nan     0.1000    0.0076
     4        1.1943             nan     0.1000    0.0081
     5        1.1782             nan     0.1000    0.0054
     6        1.1623             nan     0.1000    0.0015
     7        1.1478             nan     0.1000    0.0018
     8        1.1397             nan     0.1000   -0.0021
     9        1.1298             nan     0.1000   -0.0042
    10        1.1230             nan     0.1000   -0.0015
    20        1.0620             nan     0.1000   -0.0035
    40        0.9566             nan     0.1000   -0.0053
    60        0.8833             nan     0.1000    0.0001
    80        0.8308             nan     0.1000   -0.0009
   100        0.7934             nan     0.1000   -0.0014
   120        0.7444             nan     0.1000   -0.0040
   140        0.7002             nan     0.1000   -0.0049
   150        0.6826             nan     0.1000   -0.0016

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2715             nan     0.1000    0.0131
     2        1.2389             nan     0.1000    0.0126
     3        1.2111             nan     0.1000    0.0029
     4        1.1973             nan     0.1000    0.0006
     5        1.1806             nan     0.1000    0.0023
     6        1.1667             nan     0.1000    0.0011
     7        1.1501             nan     0.1000    0.0006
     8        1.1464             nan     0.1000   -0.0065
     9        1.1305             nan     0.1000    0.0025
    10        1.1130             nan     0.1000    0.0019
    20        1.0180             nan     0.1000   -0.0035
    40        0.8859             nan     0.1000   -0.0014
    60        0.7697             nan     0.1000   -0.0054
    80        0.7000             nan     0.1000   -0.0028
   100        0.6335             nan     0.1000   -0.0035
   120        0.5823             nan     0.1000   -0.0031
   140        0.5355             nan     0.1000   -0.0019
   150        0.5128             nan     0.1000   -0.0055

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2773             nan     0.1000    0.0105
     2        1.2578             nan     0.1000    0.0049
     3        1.2461             nan     0.1000    0.0060
     4        1.2348             nan     0.1000    0.0064
     5        1.2332             nan     0.1000   -0.0032
     6        1.2204             nan     0.1000    0.0047
     7        1.2104             nan     0.1000    0.0025
     8        1.1982             nan     0.1000    0.0014
     9        1.1893             nan     0.1000    0.0028
    10        1.1846             nan     0.1000    0.0010
    20        1.1347             nan     0.1000    0.0003
    40        1.0790             nan     0.1000   -0.0014
    60        1.0371             nan     0.1000   -0.0023
    80        1.0061             nan     0.1000   -0.0018
   100        0.9748             nan     0.1000   -0.0002
   120        0.9512             nan     0.1000   -0.0043
   140        0.9330             nan     0.1000   -0.0010
   150        0.9253             nan     0.1000   -0.0016

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2763             nan     0.1000    0.0091
     2        1.2527             nan     0.1000    0.0106
     3        1.2275             nan     0.1000    0.0044
     4        1.2096             nan     0.1000    0.0045
     5        1.1917             nan     0.1000    0.0030
     6        1.1768             nan     0.1000    0.0020
     7        1.1602             nan     0.1000    0.0039
     8        1.1489             nan     0.1000    0.0006
     9        1.1430             nan     0.1000   -0.0031
    10        1.1333             nan     0.1000    0.0038
    20        1.0562             nan     0.1000   -0.0025
    40        0.9591             nan     0.1000   -0.0032
    60        0.8889             nan     0.1000   -0.0016
    80        0.8247             nan     0.1000   -0.0039
   100        0.7784             nan     0.1000   -0.0036
   120        0.7253             nan     0.1000   -0.0024
   140        0.6812             nan     0.1000   -0.0030
   150        0.6646             nan     0.1000   -0.0019

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2819             nan     0.1000    0.0084
     2        1.2462             nan     0.1000    0.0142
     3        1.2230             nan     0.1000    0.0033
     4        1.2076             nan     0.1000   -0.0007
     5        1.1919             nan     0.1000   -0.0009
     6        1.1672             nan     0.1000   -0.0003
     7        1.1500             nan     0.1000    0.0016
     8        1.1332             nan     0.1000   -0.0000
     9        1.1208             nan     0.1000    0.0018
    10        1.1040             nan     0.1000   -0.0046
    20        1.0095             nan     0.1000   -0.0073
    40        0.8993             nan     0.1000   -0.0016
    60        0.8087             nan     0.1000   -0.0016
    80        0.7282             nan     0.1000   -0.0058
   100        0.6677             nan     0.1000   -0.0030
   120        0.6111             nan     0.1000   -0.0028
   140        0.5565             nan     0.1000   -0.0019
   150        0.5342             nan     0.1000   -0.0022

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2766             nan     0.1000    0.0102
     2        1.2605             nan     0.1000    0.0080
     3        1.2437             nan     0.1000    0.0076
     4        1.2275             nan     0.1000    0.0046
     5        1.2134             nan     0.1000    0.0037
     6        1.2005             nan     0.1000   -0.0020
     7        1.1937             nan     0.1000    0.0009
     8        1.1882             nan     0.1000    0.0008
     9        1.1817             nan     0.1000   -0.0015
    10        1.1750             nan     0.1000   -0.0039
    20        1.1277             nan     0.1000   -0.0030
    40        1.0671             nan     0.1000   -0.0022
    60        1.0288             nan     0.1000   -0.0014
    80        0.9893             nan     0.1000   -0.0020
   100        0.9583             nan     0.1000   -0.0011
   120        0.9370             nan     0.1000   -0.0017
   140        0.9164             nan     0.1000   -0.0031
   150        0.8993             nan     0.1000   -0.0025

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2750             nan     0.1000    0.0088
     2        1.2425             nan     0.1000    0.0069
     3        1.2180             nan     0.1000    0.0025
     4        1.2000             nan     0.1000    0.0017
     5        1.1817             nan     0.1000    0.0009
     6        1.1703             nan     0.1000    0.0007
     7        1.1633             nan     0.1000   -0.0049
     8        1.1567             nan     0.1000   -0.0009
     9        1.1454             nan     0.1000   -0.0033
    10        1.1313             nan     0.1000    0.0015
    20        1.0557             nan     0.1000   -0.0041
    40        0.9577             nan     0.1000   -0.0064
    60        0.8947             nan     0.1000   -0.0022
    80        0.8355             nan     0.1000   -0.0021
   100        0.7849             nan     0.1000   -0.0055
   120        0.7372             nan     0.1000   -0.0051
   140        0.6992             nan     0.1000   -0.0050
   150        0.6803             nan     0.1000   -0.0032

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2711             nan     0.1000    0.0125
     2        1.2425             nan     0.1000    0.0019
     3        1.2106             nan     0.1000    0.0071
     4        1.2011             nan     0.1000   -0.0157
     5        1.1797             nan     0.1000    0.0041
     6        1.1629             nan     0.1000    0.0005
     7        1.1440             nan     0.1000   -0.0000
     8        1.1282             nan     0.1000   -0.0015
     9        1.1130             nan     0.1000    0.0009
    10        1.1015             nan     0.1000    0.0012
    20        1.0028             nan     0.1000   -0.0026
    40        0.8874             nan     0.1000   -0.0087
    60        0.7833             nan     0.1000   -0.0065
    80        0.7057             nan     0.1000   -0.0059
   100        0.6373             nan     0.1000   -0.0030
   120        0.5809             nan     0.1000   -0.0034
   140        0.5311             nan     0.1000   -0.0034
   150        0.5078             nan     0.1000   -0.0038

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2718             nan     0.1000    0.0141
     2        1.2470             nan     0.1000    0.0124
     3        1.2229             nan     0.1000    0.0078
     4        1.2056             nan     0.1000    0.0080
     5        1.1851             nan     0.1000    0.0064
     6        1.1814             nan     0.1000   -0.0018
     7        1.1702             nan     0.1000    0.0021
     8        1.1624             nan     0.1000    0.0035
     9        1.1536             nan     0.1000    0.0014
    10        1.1469             nan     0.1000    0.0021
    20        1.0928             nan     0.1000   -0.0010
    40        1.0314             nan     0.1000   -0.0047
    60        0.9929             nan     0.1000   -0.0044
    80        0.9619             nan     0.1000   -0.0009
   100        0.9329             nan     0.1000   -0.0026
   120        0.9047             nan     0.1000   -0.0012
   140        0.8809             nan     0.1000   -0.0001
   150        0.8718             nan     0.1000   -0.0026

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2806             nan     0.1000    0.0117
     2        1.2356             nan     0.1000    0.0052
     3        1.2081             nan     0.1000    0.0075
     4        1.1881             nan     0.1000    0.0023
     5        1.1680             nan     0.1000    0.0048
     6        1.1526             nan     0.1000    0.0027
     7        1.1504             nan     0.1000   -0.0066
     8        1.1339             nan     0.1000    0.0050
     9        1.1199             nan     0.1000    0.0015
    10        1.1080             nan     0.1000    0.0005
    20        1.0372             nan     0.1000   -0.0004
    40        0.9376             nan     0.1000   -0.0026
    60        0.8630             nan     0.1000   -0.0037
    80        0.8031             nan     0.1000   -0.0032
   100        0.7599             nan     0.1000   -0.0005
   120        0.7124             nan     0.1000   -0.0032
   140        0.6662             nan     0.1000   -0.0005
   150        0.6451             nan     0.1000   -0.0033

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2634             nan     0.1000    0.0196
     2        1.2270             nan     0.1000    0.0133
     3        1.1922             nan     0.1000    0.0099
     4        1.1627             nan     0.1000    0.0093
     5        1.1393             nan     0.1000    0.0069
     6        1.1150             nan     0.1000    0.0039
     7        1.1011             nan     0.1000    0.0016
     8        1.0905             nan     0.1000    0.0002
     9        1.0806             nan     0.1000    0.0001
    10        1.0697             nan     0.1000   -0.0032
    20        0.9760             nan     0.1000   -0.0019
    40        0.8442             nan     0.1000   -0.0015
    60        0.7463             nan     0.1000   -0.0040
    80        0.6666             nan     0.1000   -0.0049
   100        0.6006             nan     0.1000   -0.0027
   120        0.5556             nan     0.1000   -0.0057
   140        0.5067             nan     0.1000   -0.0025
   150        0.4835             nan     0.1000   -0.0037

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2841             nan     0.1000    0.0068
     2        1.2749             nan     0.1000   -0.0016
     3        1.2624             nan     0.1000    0.0028
     4        1.2427             nan     0.1000    0.0088
     5        1.2315             nan     0.1000    0.0028
     6        1.2130             nan     0.1000    0.0036
     7        1.1985             nan     0.1000    0.0027
     8        1.1949             nan     0.1000   -0.0012
     9        1.1872             nan     0.1000    0.0013
    10        1.1805             nan     0.1000    0.0009
    20        1.1367             nan     0.1000   -0.0003
    40        1.0823             nan     0.1000   -0.0018
    60        1.0389             nan     0.1000   -0.0020
    80        1.0066             nan     0.1000   -0.0046
   100        0.9809             nan     0.1000   -0.0024
   120        0.9566             nan     0.1000   -0.0024
   140        0.9296             nan     0.1000   -0.0022
   150        0.9175             nan     0.1000   -0.0029

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2745             nan     0.1000    0.0094
     2        1.2514             nan     0.1000    0.0092
     3        1.2302             nan     0.1000    0.0062
     4        1.2166             nan     0.1000   -0.0000
     5        1.2068             nan     0.1000   -0.0015
     6        1.1904             nan     0.1000    0.0057
     7        1.1805             nan     0.1000   -0.0000
     8        1.1700             nan     0.1000   -0.0031
     9        1.1625             nan     0.1000   -0.0026
    10        1.1495             nan     0.1000    0.0019
    20        1.0639             nan     0.1000   -0.0013
    40        0.9645             nan     0.1000   -0.0021
    60        0.8953             nan     0.1000   -0.0033
    80        0.8298             nan     0.1000   -0.0030
   100        0.7894             nan     0.1000   -0.0008
   120        0.7381             nan     0.1000   -0.0014
   140        0.6983             nan     0.1000   -0.0028
   150        0.6810             nan     0.1000   -0.0030

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2633             nan     0.1000    0.0058
     2        1.2417             nan     0.1000    0.0063
     3        1.2170             nan     0.1000    0.0057
     4        1.1961             nan     0.1000    0.0014
     5        1.1706             nan     0.1000    0.0016
     6        1.1507             nan     0.1000    0.0057
     7        1.1410             nan     0.1000   -0.0025
     8        1.1287             nan     0.1000   -0.0009
     9        1.1159             nan     0.1000    0.0004
    10        1.1089             nan     0.1000   -0.0031
    20        1.0256             nan     0.1000    0.0007
    40        0.9105             nan     0.1000   -0.0034
    60        0.8166             nan     0.1000   -0.0054
    80        0.7482             nan     0.1000   -0.0008
   100        0.6824             nan     0.1000   -0.0014
   120        0.6221             nan     0.1000   -0.0034
   140        0.5661             nan     0.1000   -0.0029
   150        0.5461             nan     0.1000   -0.0034

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2679             nan     0.1000    0.0091
     2        1.2484             nan     0.1000    0.0074
     3        1.2318             nan     0.1000    0.0060
     4        1.2183             nan     0.1000    0.0041
     5        1.2067             nan     0.1000    0.0026
     6        1.1969             nan     0.1000   -0.0002
     7        1.1878             nan     0.1000    0.0018
     8        1.1783             nan     0.1000    0.0013
     9        1.1713             nan     0.1000   -0.0005
    10        1.1631             nan     0.1000    0.0022
    20        1.1186             nan     0.1000   -0.0035
    40        1.0649             nan     0.1000   -0.0013
    60        1.0311             nan     0.1000   -0.0008
    80        0.9903             nan     0.1000   -0.0029
   100        0.9696             nan     0.1000   -0.0016
   120        0.9389             nan     0.1000   -0.0012
   140        0.9243             nan     0.1000   -0.0032
   150        0.9168             nan     0.1000   -0.0029

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2689             nan     0.1000    0.0097
     2        1.2488             nan     0.1000    0.0046
     3        1.2217             nan     0.1000    0.0095
     4        1.2010             nan     0.1000    0.0042
     5        1.1909             nan     0.1000    0.0005
     6        1.1739             nan     0.1000    0.0052
     7        1.1574             nan     0.1000    0.0017
     8        1.1489             nan     0.1000   -0.0042
     9        1.1348             nan     0.1000    0.0005
    10        1.1326             nan     0.1000   -0.0058
    20        1.0478             nan     0.1000   -0.0037
    40        0.9453             nan     0.1000   -0.0010
    60        0.8810             nan     0.1000   -0.0065
    80        0.8183             nan     0.1000   -0.0015
   100        0.7779             nan     0.1000   -0.0027
   120        0.7424             nan     0.1000   -0.0023
   140        0.6971             nan     0.1000   -0.0060
   150        0.6737             nan     0.1000   -0.0004

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2776             nan     0.1000    0.0059
     2        1.2428             nan     0.1000    0.0111
     3        1.2224             nan     0.1000    0.0054
     4        1.1998             nan     0.1000    0.0077
     5        1.1728             nan     0.1000    0.0070
     6        1.1541             nan     0.1000   -0.0001
     7        1.1406             nan     0.1000    0.0026
     8        1.1263             nan     0.1000   -0.0014
     9        1.1119             nan     0.1000   -0.0001
    10        1.0992             nan     0.1000   -0.0014
    20        0.9982             nan     0.1000   -0.0041
    40        0.8639             nan     0.1000   -0.0057
    60        0.7697             nan     0.1000   -0.0027
    80        0.7019             nan     0.1000   -0.0027
   100        0.6335             nan     0.1000   -0.0027
   120        0.5925             nan     0.1000   -0.0050
   140        0.5418             nan     0.1000   -0.0029
   150        0.5169             nan     0.1000   -0.0036

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2801             nan     0.1000    0.0118
     2        1.2616             nan     0.1000    0.0107
     3        1.2407             nan     0.1000    0.0090
     4        1.2216             nan     0.1000    0.0055
     5        1.2134             nan     0.1000   -0.0015
     6        1.2004             nan     0.1000    0.0040
     7        1.1861             nan     0.1000    0.0025
     8        1.1855             nan     0.1000   -0.0028
     9        1.1744             nan     0.1000    0.0020
    10        1.1705             nan     0.1000   -0.0021
    20        1.1215             nan     0.1000    0.0006
    40        1.0720             nan     0.1000   -0.0027
    60        1.0363             nan     0.1000   -0.0006
    80        1.0093             nan     0.1000   -0.0039
   100        0.9807             nan     0.1000   -0.0023
   120        0.9562             nan     0.1000   -0.0043
   140        0.9411             nan     0.1000   -0.0019
   150        0.9302             nan     0.1000   -0.0016

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2756             nan     0.1000    0.0111
     2        1.2511             nan     0.1000    0.0081
     3        1.2343             nan     0.1000    0.0074
     4        1.2180             nan     0.1000    0.0067
     5        1.2063             nan     0.1000   -0.0001
     6        1.1882             nan     0.1000    0.0048
     7        1.1770             nan     0.1000    0.0018
     8        1.1621             nan     0.1000    0.0009
     9        1.1484             nan     0.1000    0.0001
    10        1.1400             nan     0.1000    0.0001
    20        1.0754             nan     0.1000   -0.0003
    40        0.9805             nan     0.1000   -0.0049
    60        0.9047             nan     0.1000   -0.0013
    80        0.8465             nan     0.1000   -0.0030
   100        0.8005             nan     0.1000   -0.0040
   120        0.7537             nan     0.1000   -0.0057
   140        0.7075             nan     0.1000   -0.0034
   150        0.6875             nan     0.1000   -0.0016

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2713             nan     0.1000    0.0062
     2        1.2361             nan     0.1000    0.0091
     3        1.2169             nan     0.1000    0.0017
     4        1.1982             nan     0.1000   -0.0018
     5        1.1834             nan     0.1000    0.0009
     6        1.1685             nan     0.1000   -0.0001
     7        1.1485             nan     0.1000    0.0028
     8        1.1357             nan     0.1000    0.0022
     9        1.1187             nan     0.1000    0.0028
    10        1.1096             nan     0.1000   -0.0026
    20        1.0189             nan     0.1000   -0.0020
    40        0.8825             nan     0.1000   -0.0034
    60        0.7820             nan     0.1000   -0.0029
    80        0.7005             nan     0.1000   -0.0055
   100        0.6419             nan     0.1000   -0.0035
   120        0.5919             nan     0.1000   -0.0050
   140        0.5473             nan     0.1000   -0.0052
   150        0.5283             nan     0.1000   -0.0039

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2779             nan     0.1000    0.0133
     2        1.2515             nan     0.1000    0.0099
     3        1.2323             nan     0.1000    0.0086
     4        1.2173             nan     0.1000    0.0041
     5        1.2005             nan     0.1000    0.0047
     6        1.1889             nan     0.1000    0.0040
     7        1.1745             nan     0.1000    0.0008
     8        1.1658             nan     0.1000    0.0034
     9        1.1562             nan     0.1000    0.0029
    10        1.1549             nan     0.1000   -0.0048
    20        1.1079             nan     0.1000   -0.0002
    40        1.0520             nan     0.1000   -0.0045
    60        1.0129             nan     0.1000   -0.0041
    80        0.9773             nan     0.1000   -0.0015
   100        0.9507             nan     0.1000   -0.0050
   120        0.9297             nan     0.1000   -0.0034
   140        0.9100             nan     0.1000   -0.0045
   150        0.9025             nan     0.1000   -0.0019

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2640             nan     0.1000    0.0105
     2        1.2376             nan     0.1000    0.0082
     3        1.2146             nan     0.1000    0.0106
     4        1.1936             nan     0.1000    0.0055
     5        1.1773             nan     0.1000    0.0041
     6        1.1645             nan     0.1000    0.0017
     7        1.1503             nan     0.1000    0.0022
     8        1.1349             nan     0.1000    0.0004
     9        1.1253             nan     0.1000    0.0008
    10        1.1171             nan     0.1000   -0.0024
    20        1.0346             nan     0.1000    0.0014
    40        0.9494             nan     0.1000   -0.0035
    60        0.8749             nan     0.1000   -0.0025
    80        0.8057             nan     0.1000   -0.0001
   100        0.7563             nan     0.1000   -0.0037
   120        0.7148             nan     0.1000   -0.0025
   140        0.6789             nan     0.1000   -0.0030
   150        0.6643             nan     0.1000   -0.0045

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2650             nan     0.1000    0.0123
     2        1.2279             nan     0.1000    0.0105
     3        1.1959             nan     0.1000    0.0038
     4        1.1778             nan     0.1000    0.0029
     5        1.1537             nan     0.1000    0.0026
     6        1.1353             nan     0.1000    0.0029
     7        1.1173             nan     0.1000    0.0026
     8        1.1057             nan     0.1000   -0.0048
     9        1.0916             nan     0.1000   -0.0040
    10        1.0747             nan     0.1000    0.0024
    20        0.9812             nan     0.1000   -0.0044
    40        0.8566             nan     0.1000   -0.0047
    60        0.7731             nan     0.1000   -0.0054
    80        0.6859             nan     0.1000   -0.0037
   100        0.6262             nan     0.1000   -0.0056
   120        0.5772             nan     0.1000   -0.0042
   140        0.5367             nan     0.1000   -0.0029
   150        0.5138             nan     0.1000   -0.0037

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2828             nan     0.1000    0.0085
     2        1.2702             nan     0.1000    0.0073
     3        1.2474             nan     0.1000    0.0090
     4        1.2325             nan     0.1000    0.0053
     5        1.2177             nan     0.1000    0.0023
     6        1.2078             nan     0.1000    0.0023
     7        1.2017             nan     0.1000   -0.0015
     8        1.1925             nan     0.1000    0.0028
     9        1.1843             nan     0.1000    0.0015
    10        1.1783             nan     0.1000   -0.0003
    20        1.1371             nan     0.1000   -0.0025
    40        1.0687             nan     0.1000   -0.0016
    60        1.0303             nan     0.1000   -0.0036
    80        0.9979             nan     0.1000   -0.0018
   100        0.9666             nan     0.1000   -0.0046
   120        0.9442             nan     0.1000   -0.0028
   140        0.9181             nan     0.1000   -0.0012
   150        0.9068             nan     0.1000   -0.0022

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2758             nan     0.1000    0.0130
     2        1.2477             nan     0.1000    0.0102
     3        1.2301             nan     0.1000    0.0026
     4        1.2116             nan     0.1000   -0.0012
     5        1.1952             nan     0.1000    0.0015
     6        1.1813             nan     0.1000   -0.0011
     7        1.1742             nan     0.1000   -0.0007
     8        1.1682             nan     0.1000   -0.0046
     9        1.1540             nan     0.1000   -0.0015
    10        1.1439             nan     0.1000   -0.0018
    20        1.0645             nan     0.1000   -0.0016
    40        0.9561             nan     0.1000   -0.0031
    60        0.8894             nan     0.1000   -0.0049
    80        0.8237             nan     0.1000   -0.0064
   100        0.7767             nan     0.1000   -0.0074
   120        0.7335             nan     0.1000    0.0014
   140        0.6967             nan     0.1000   -0.0024
   150        0.6768             nan     0.1000   -0.0038

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2676             nan     0.1000    0.0095
     2        1.2363             nan     0.1000    0.0039
     3        1.1993             nan     0.1000    0.0075
     4        1.1699             nan     0.1000    0.0055
     5        1.1551             nan     0.1000   -0.0020
     6        1.1395             nan     0.1000   -0.0039
     7        1.1258             nan     0.1000   -0.0011
     8        1.1158             nan     0.1000   -0.0027
     9        1.1082             nan     0.1000   -0.0092
    10        1.0938             nan     0.1000   -0.0035
    20        1.0114             nan     0.1000   -0.0030
    40        0.8894             nan     0.1000   -0.0020
    60        0.7920             nan     0.1000   -0.0069
    80        0.7089             nan     0.1000   -0.0008
   100        0.6451             nan     0.1000   -0.0052
   120        0.5815             nan     0.1000   -0.0048
   140        0.5320             nan     0.1000   -0.0034
   150        0.5124             nan     0.1000   -0.0018

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2872             nan     0.1000    0.0096
     2        1.2635             nan     0.1000    0.0108
     3        1.2482             nan     0.1000    0.0079
     4        1.2341             nan     0.1000    0.0066
     5        1.2188             nan     0.1000    0.0049
     6        1.2149             nan     0.1000   -0.0008
     7        1.2072             nan     0.1000    0.0007
     8        1.2061             nan     0.1000   -0.0056
     9        1.1930             nan     0.1000    0.0046
    10        1.1876             nan     0.1000   -0.0004
    20        1.1250             nan     0.1000   -0.0003
    40        1.0749             nan     0.1000   -0.0030
    60        1.0467             nan     0.1000   -0.0036
    80        1.0101             nan     0.1000   -0.0019
   100        0.9890             nan     0.1000   -0.0013
   120        0.9553             nan     0.1000   -0.0023
   140        0.9384             nan     0.1000   -0.0031
   150        0.9288             nan     0.1000   -0.0025

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2783             nan     0.1000    0.0113
     2        1.2533             nan     0.1000    0.0034
     3        1.2356             nan     0.1000    0.0035
     4        1.2099             nan     0.1000    0.0094
     5        1.1874             nan     0.1000    0.0058
     6        1.1777             nan     0.1000    0.0012
     7        1.1689             nan     0.1000   -0.0030
     8        1.1608             nan     0.1000   -0.0002
     9        1.1493             nan     0.1000    0.0034
    10        1.1414             nan     0.1000   -0.0003
    20        1.0644             nan     0.1000   -0.0037
    40        0.9642             nan     0.1000   -0.0025
    60        0.8922             nan     0.1000   -0.0034
    80        0.8372             nan     0.1000   -0.0025
   100        0.7913             nan     0.1000   -0.0042
   120        0.7435             nan     0.1000   -0.0026
   140        0.7063             nan     0.1000   -0.0009
   150        0.6860             nan     0.1000   -0.0026

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2741             nan     0.1000    0.0082
     2        1.2492             nan     0.1000    0.0065
     3        1.2168             nan     0.1000    0.0085
     4        1.1917             nan     0.1000    0.0036
     5        1.1675             nan     0.1000    0.0070
     6        1.1494             nan     0.1000    0.0030
     7        1.1368             nan     0.1000    0.0002
     8        1.1159             nan     0.1000    0.0034
     9        1.1024             nan     0.1000   -0.0012
    10        1.0881             nan     0.1000   -0.0071
    20        1.0097             nan     0.1000   -0.0060
    40        0.8813             nan     0.1000   -0.0022
    60        0.8008             nan     0.1000   -0.0033
    80        0.7135             nan     0.1000   -0.0057
   100        0.6495             nan     0.1000   -0.0020
   120        0.5935             nan     0.1000   -0.0063
   140        0.5496             nan     0.1000   -0.0032
   150        0.5262             nan     0.1000   -0.0015

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2845             nan     0.1000    0.0094
     2        1.2620             nan     0.1000    0.0103
     3        1.2438             nan     0.1000    0.0075
     4        1.2302             nan     0.1000    0.0055
     5        1.2266             nan     0.1000   -0.0040
     6        1.2126             nan     0.1000    0.0001
     7        1.1969             nan     0.1000    0.0033
     8        1.1861             nan     0.1000    0.0024
     9        1.1780             nan     0.1000    0.0017
    10        1.1719             nan     0.1000   -0.0032
    20        1.1283             nan     0.1000   -0.0001
    40        1.0818             nan     0.1000   -0.0002
    60        1.0386             nan     0.1000   -0.0028
    80        1.0082             nan     0.1000   -0.0015
   100        0.9843             nan     0.1000   -0.0027
   120        0.9587             nan     0.1000   -0.0020
   140        0.9353             nan     0.1000   -0.0032
   150        0.9246             nan     0.1000   -0.0020

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2776             nan     0.1000    0.0070
     2        1.2594             nan     0.1000    0.0052
     3        1.2270             nan     0.1000    0.0084
     4        1.2111             nan     0.1000   -0.0052
     5        1.1943             nan     0.1000    0.0053
     6        1.1728             nan     0.1000    0.0039
     7        1.1565             nan     0.1000    0.0004
     8        1.1470             nan     0.1000   -0.0023
     9        1.1374             nan     0.1000    0.0006
    10        1.1314             nan     0.1000   -0.0021
    20        1.0629             nan     0.1000   -0.0061
    40        0.9749             nan     0.1000   -0.0049
    60        0.9044             nan     0.1000   -0.0039
    80        0.8523             nan     0.1000   -0.0055
   100        0.8038             nan     0.1000   -0.0027
   120        0.7546             nan     0.1000   -0.0022
   140        0.7185             nan     0.1000   -0.0041
   150        0.6964             nan     0.1000   -0.0023

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2706             nan     0.1000    0.0114
     2        1.2435             nan     0.1000    0.0049
     3        1.2205             nan     0.1000    0.0060
     4        1.1987             nan     0.1000    0.0044
     5        1.1764             nan     0.1000    0.0047
     6        1.1629             nan     0.1000   -0.0035
     7        1.1469             nan     0.1000    0.0034
     8        1.1336             nan     0.1000    0.0027
     9        1.1159             nan     0.1000    0.0050
    10        1.0990             nan     0.1000    0.0026
    20        1.0299             nan     0.1000   -0.0024
    40        0.8810             nan     0.1000   -0.0035
    60        0.7952             nan     0.1000   -0.0016
    80        0.7219             nan     0.1000   -0.0042
   100        0.6539             nan     0.1000   -0.0028
   120        0.5977             nan     0.1000   -0.0038
   140        0.5529             nan     0.1000   -0.0022
   150        0.5355             nan     0.1000   -0.0043

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2833             nan     0.1000    0.0111
     2        1.2552             nan     0.1000    0.0105
     3        1.2374             nan     0.1000    0.0081
     4        1.2180             nan     0.1000    0.0090
     5        1.2084             nan     0.1000    0.0040
     6        1.1938             nan     0.1000    0.0036
     7        1.1810             nan     0.1000    0.0047
     8        1.1702             nan     0.1000    0.0014
     9        1.1605             nan     0.1000   -0.0006
    10        1.1551             nan     0.1000    0.0021
    20        1.1016             nan     0.1000    0.0014
    40        1.0464             nan     0.1000   -0.0003
    60        1.0097             nan     0.1000   -0.0040
    80        0.9752             nan     0.1000   -0.0009
   100        0.9508             nan     0.1000   -0.0041
   120        0.9290             nan     0.1000   -0.0028
   140        0.9059             nan     0.1000   -0.0035
   150        0.8947             nan     0.1000   -0.0050

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2854             nan     0.1000    0.0078
     2        1.2531             nan     0.1000    0.0115
     3        1.2240             nan     0.1000    0.0096
     4        1.2072             nan     0.1000    0.0037
     5        1.1907             nan     0.1000    0.0018
     6        1.1699             nan     0.1000    0.0004
     7        1.1567             nan     0.1000    0.0009
     8        1.1447             nan     0.1000   -0.0012
     9        1.1383             nan     0.1000   -0.0042
    10        1.1294             nan     0.1000   -0.0003
    20        1.0586             nan     0.1000    0.0012
    40        0.9674             nan     0.1000   -0.0044
    60        0.8902             nan     0.1000   -0.0010
    80        0.8261             nan     0.1000   -0.0031
   100        0.7741             nan     0.1000   -0.0048
   120        0.7309             nan     0.1000   -0.0037
   140        0.6982             nan     0.1000   -0.0035
   150        0.6792             nan     0.1000   -0.0016

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2677             nan     0.1000    0.0152
     2        1.2308             nan     0.1000    0.0027
     3        1.2096             nan     0.1000    0.0065
     4        1.1853             nan     0.1000    0.0039
     5        1.1628             nan     0.1000    0.0045
     6        1.1450             nan     0.1000   -0.0006
     7        1.1336             nan     0.1000   -0.0040
     8        1.1145             nan     0.1000    0.0028
     9        1.1032             nan     0.1000   -0.0019
    10        1.0910             nan     0.1000   -0.0016
    20        1.0064             nan     0.1000   -0.0044
    40        0.8770             nan     0.1000   -0.0061
    60        0.7827             nan     0.1000   -0.0023
    80        0.7140             nan     0.1000   -0.0029
   100        0.6417             nan     0.1000   -0.0045
   120        0.5902             nan     0.1000   -0.0041
   140        0.5470             nan     0.1000   -0.0030
   150        0.5213             nan     0.1000   -0.0034

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2855             nan     0.1000    0.0089
     2        1.2607             nan     0.1000    0.0093
     3        1.2390             nan     0.1000    0.0032
     4        1.2284             nan     0.1000    0.0046
     5        1.2148             nan     0.1000    0.0050
     6        1.2049             nan     0.1000    0.0018
     7        1.1941             nan     0.1000   -0.0006
     8        1.1888             nan     0.1000    0.0019
     9        1.1785             nan     0.1000    0.0028
    10        1.1771             nan     0.1000   -0.0038
    20        1.1351             nan     0.1000   -0.0004
    40        1.0754             nan     0.1000   -0.0011
    60        1.0361             nan     0.1000   -0.0020
    80        1.0051             nan     0.1000   -0.0022
   100        0.9797             nan     0.1000   -0.0025
   120        0.9544             nan     0.1000   -0.0020
   140        0.9305             nan     0.1000   -0.0042
   150        0.9200             nan     0.1000   -0.0013

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2896             nan     0.1000    0.0065
     2        1.2583             nan     0.1000    0.0067
     3        1.2323             nan     0.1000    0.0079
     4        1.2131             nan     0.1000    0.0077
     5        1.2004             nan     0.1000    0.0007
     6        1.1828             nan     0.1000    0.0063
     7        1.1714             nan     0.1000    0.0016
     8        1.1624             nan     0.1000   -0.0057
     9        1.1580             nan     0.1000   -0.0031
    10        1.1417             nan     0.1000    0.0005
    20        1.0733             nan     0.1000    0.0002
    40        0.9894             nan     0.1000   -0.0041
    60        0.8975             nan     0.1000    0.0019
    80        0.8316             nan     0.1000   -0.0055
   100        0.7852             nan     0.1000   -0.0048
   120        0.7474             nan     0.1000   -0.0020
   140        0.7114             nan     0.1000   -0.0018
   150        0.6918             nan     0.1000   -0.0031

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2702             nan     0.1000    0.0114
     2        1.2460             nan     0.1000    0.0045
     3        1.2240             nan     0.1000    0.0066
     4        1.2086             nan     0.1000   -0.0039
     5        1.1906             nan     0.1000   -0.0016
     6        1.1724             nan     0.1000   -0.0033
     7        1.1565             nan     0.1000   -0.0003
     8        1.1422             nan     0.1000    0.0018
     9        1.1279             nan     0.1000   -0.0007
    10        1.1172             nan     0.1000   -0.0001
    20        1.0246             nan     0.1000    0.0002
    40        0.8982             nan     0.1000   -0.0031
    60        0.8085             nan     0.1000   -0.0064
    80        0.7331             nan     0.1000   -0.0036
   100        0.6634             nan     0.1000   -0.0029
   120        0.6090             nan     0.1000   -0.0029
   140        0.5630             nan     0.1000   -0.0029
   150        0.5454             nan     0.1000   -0.0025

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2824             nan     0.1000    0.0147
     2        1.2677             nan     0.1000    0.0081
     3        1.2448             nan     0.1000    0.0108
     4        1.2174             nan     0.1000    0.0054
     5        1.2049             nan     0.1000    0.0022
     6        1.1926             nan     0.1000    0.0054
     7        1.1832             nan     0.1000    0.0012
     8        1.1749             nan     0.1000    0.0022
     9        1.1686             nan     0.1000    0.0022
    10        1.1598             nan     0.1000    0.0000
    20        1.1176             nan     0.1000   -0.0016
    40        1.0675             nan     0.1000   -0.0020
    60        1.0189             nan     0.1000   -0.0014
    80        0.9909             nan     0.1000   -0.0031
   100        0.9611             nan     0.1000   -0.0028
   120        0.9310             nan     0.1000   -0.0032
   140        0.9123             nan     0.1000   -0.0038
   150        0.9043             nan     0.1000   -0.0009

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2787             nan     0.1000    0.0083
     2        1.2582             nan     0.1000    0.0067
     3        1.2368             nan     0.1000    0.0088
     4        1.2095             nan     0.1000    0.0014
     5        1.1932             nan     0.1000    0.0031
     6        1.1826             nan     0.1000   -0.0006
     7        1.1612             nan     0.1000    0.0041
     8        1.1466             nan     0.1000    0.0014
     9        1.1372             nan     0.1000   -0.0023
    10        1.1321             nan     0.1000   -0.0039
    20        1.0549             nan     0.1000   -0.0023
    40        0.9529             nan     0.1000   -0.0046
    60        0.8851             nan     0.1000   -0.0033
    80        0.8236             nan     0.1000   -0.0046
   100        0.7806             nan     0.1000   -0.0036
   120        0.7354             nan     0.1000   -0.0045
   140        0.6955             nan     0.1000   -0.0032
   150        0.6771             nan     0.1000   -0.0024

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2793             nan     0.1000    0.0101
     2        1.2509             nan     0.1000    0.0080
     3        1.2285             nan     0.1000    0.0039
     4        1.2054             nan     0.1000    0.0089
     5        1.1850             nan     0.1000    0.0036
     6        1.1671             nan     0.1000    0.0046
     7        1.1450             nan     0.1000   -0.0037
     8        1.1288             nan     0.1000    0.0010
     9        1.1141             nan     0.1000   -0.0031
    10        1.1005             nan     0.1000   -0.0013
    20        1.0135             nan     0.1000   -0.0071
    40        0.8884             nan     0.1000   -0.0059
    60        0.7986             nan     0.1000   -0.0025
    80        0.7336             nan     0.1000   -0.0056
   100        0.6581             nan     0.1000   -0.0028
   120        0.6009             nan     0.1000   -0.0033
   140        0.5420             nan     0.1000   -0.0019
   150        0.5159             nan     0.1000   -0.0035

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.2709             nan     0.1000    0.0135
     2        1.2455             nan     0.1000    0.0066
     3        1.2227             nan     0.1000    0.0031
     4        1.2004             nan     0.1000    0.0076
     5        1.1812             nan     0.1000    0.0035
     6        1.1593             nan     0.1000    0.0039
     7        1.1432             nan     0.1000   -0.0003
     8        1.1265             nan     0.1000    0.0027
     9        1.1172             nan     0.1000   -0.0006
    10        1.1070             nan     0.1000   -0.0040
    20        1.0206             nan     0.1000   -0.0036
    40        0.8905             nan     0.1000   -0.0030
    50        0.8376             nan     0.1000   -0.0023</code></pre>
</div>
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="co"># train the SVM model</span></span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>modelSvm <span class="ot">&lt;-</span> <span class="fu">train</span>(coast <span class="sc">~</span>., <span class="at">data=</span>both, <span class="at">method=</span><span class="st">"svmRadial"</span>, <span class="at">trControl=</span>control)</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a><span class="co"># train the random forest</span></span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a>randomforest <span class="ot">&lt;-</span> <span class="fu">train</span>(coast<span class="sc">~</span>., <span class="at">data=</span>both, <span class="at">method=</span><span class="st">"ranger"</span>, <span class="at">trControl=</span>control)</span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-8"><a href="#cb46-8" aria-hidden="true" tabindex="-1"></a><span class="co"># collect resamples</span></span>
<span id="cb46-9"><a href="#cb46-9" aria-hidden="true" tabindex="-1"></a>results <span class="ot">&lt;-</span> <span class="fu">resamples</span>(<span class="fu">list</span>(<span class="at">LVQ=</span>modelLvq, <span class="at">GBM=</span>modelGbm, <span class="at">SVM=</span>modelSvm,<span class="at">knn=</span>modelknn, <span class="at">nnet=</span>modelnnet, <span class="at">glm=</span>modelglm, <span class="at">rf=</span>randomforest))</span>
<span id="cb46-10"><a href="#cb46-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-11"><a href="#cb46-11" aria-hidden="true" tabindex="-1"></a><span class="co"># summarize the distributions</span></span>
<span id="cb46-12"><a href="#cb46-12" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(results)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
summary.resamples(object = results)

Models: LVQ, GBM, SVM, knn, nnet, glm, rf 
Number of resamples: 30 

Accuracy 
          Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's
LVQ  0.5185185 0.5925926 0.6602564 0.6629121 0.7142857 0.8571429    0
GBM  0.5185185 0.6127646 0.6666667 0.6730701 0.7266484 0.8148148    0
SVM  0.5000000 0.6296296 0.6666667 0.6589676 0.7008547 0.7500000    0
knn  0.4444444 0.5962302 0.6296296 0.6361620 0.6666667 0.7777778    0
nnet 0.5185185 0.6296296 0.6923077 0.6847544 0.7407407 0.8571429    0
glm  0.5000000 0.6296296 0.6662088 0.6701601 0.7382479 0.8076923    0
rf   0.4814815 0.5925926 0.6483516 0.6372473 0.6888736 0.7692308    0

Kappa 
            Min.    1st Qu.    Median      Mean   3rd Qu.      Max. NA's
LVQ  -0.15081967 0.09749683 0.1878083 0.2181786 0.3488372 0.6585366    0
GBM  -0.15081967 0.12740385 0.2727190 0.2457643 0.3863908 0.5768025    0
SVM  -0.15081967 0.06046176 0.2016393 0.1627443 0.2477190 0.3875000    0
knn  -0.32786885 0.07348993 0.1796066 0.1666885 0.2323747 0.5000000    0
nnet -0.05405405 0.16794479 0.2952255 0.2743689 0.3892479 0.6823529    0
glm  -0.13422819 0.09395973 0.1706327 0.2139387 0.3782539 0.5390071    0
rf   -0.21153846 0.06896552 0.1767987 0.1656162 0.2808609 0.4901961    0</code></pre>
</div>
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="fu">bwplot</span>(results)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="week_6_files/figure-html/unnamed-chunk-34-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Now we can test our model on a dataset from outside of our initial training/testing stage:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a>kendrick <span class="ot">&lt;-</span> <span class="fu">get_artist_audio_features</span>(<span class="st">'kendrick lamar'</span>)</span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a>kendrick <span class="ot">&lt;-</span> kendrick <span class="sc">%&gt;%</span> <span class="fu">select</span>(<span class="fu">c</span>(<span class="st">"acousticness"</span>, <span class="st">"energy"</span>, <span class="st">"instrumentalness"</span>, <span class="st">"liveness"</span>, <span class="st">"danceability"</span>, <span class="st">"loudness"</span>, <span class="st">"speechiness"</span>, <span class="st">"valence"</span>))</span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a>kendrick <span class="ot">&lt;-</span> kendrick <span class="sc">%&gt;%</span> <span class="fu">mutate_all</span>(<span class="sc">~</span>(<span class="fu">scale</span>(.) <span class="sc">%&gt;%</span> as.vector))</span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-7"><a href="#cb49-7" aria-hidden="true" tabindex="-1"></a>kendrick_rf <span class="ot">&lt;-</span> <span class="fu">predict</span>(modelnnet, <span class="at">newdata=</span>kendrick)</span>
<span id="cb49-8"><a href="#cb49-8" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(kendrick_rf)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>kendrick_rf
east west 
 327  131 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>classified_data <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(<span class="fu">cbind</span>(beatles_knn, beatles_rf, beatles<span class="sc">$</span>track_name))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>


</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../class_notes/week_5.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Week 5: Regression and Clustering</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">© Copyright 2023, Daniel Shanahan</div>   
    <div class="nav-footer-right">This page is built with ❤️ and <a href="https://quarto.org/">Quarto</a>.</div>
  </div>
</footer>



</body></html>